{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df14ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0c36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6926416b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a9ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['ID', 'TITLE', 'Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5e6bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABSTRACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            ABSTRACT\n",
       "0    Predictive models allow subject-specific inf...\n",
       "1    Rotation invariance and translation invarian...\n",
       "2    We introduce and develop the notion of spher...\n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...\n",
       "4    Fourier-transform infra-red (FTIR) spectra o..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d82eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.ABSTRACT.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f40ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"  Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n\",\n",
       " '  Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n',\n",
       " '  We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n',\n",
       " '  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n',\n",
       " '  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n',\n",
       " '  Let $\\\\Omega \\\\subset \\\\mathbb{R}^n$ be a bounded domain satisfying a\\nHayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain\\nreferred to as \"obstacle\". We are interested in the behaviour of the first\\nDirichlet eigenvalue $ \\\\lambda_1(\\\\Omega \\\\setminus (x+D)) $. First, we prove an\\nupper bound on $ \\\\lambda_1(\\\\Omega \\\\setminus (x+D)) $ in terms of the distance\\nof the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet\\nground state $ \\\\phi_{\\\\lambda_1} > 0 $ of $ \\\\Omega $. In short, a direct\\ncorollary is that if \\\\begin{equation} \\\\mu_\\\\Omega := \\\\max_{x}\\\\lambda_1(\\\\Omega\\n\\\\setminus (x+D)) \\\\end{equation} is large enough in terms of $ \\\\lambda_1(\\\\Omega)\\n$, then all maximizer sets $ x+D $ of $ \\\\mu_\\\\Omega $ are close to each maximum\\npoint $ x_0 $ of $ \\\\phi_{\\\\lambda_1} $.\\nSecond, we discuss the distribution of $ \\\\phi_{\\\\lambda_1(\\\\Omega)} $ and the\\npossibility to inscribe wavelength balls at a given point in $ \\\\Omega $.\\nFinally, we specify our observations to convex obstacles $ D $ and show that\\nif $ \\\\mu_\\\\Omega $ is sufficiently large with respect to $ \\\\lambda_1(\\\\Omega) $,\\nthen all maximizers $ x+D $ of $ \\\\mu_\\\\Omega $ contain all maximum points $ x_0\\n$ of $ \\\\phi_{\\\\lambda_1(\\\\Omega)} $.\\n',\n",
       " \"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017\\nU1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel\\nTelescope. From these observations, we derived a partial lightcurve with\\npeak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out\\nrotation periods less than 3 hr and suggests that the period is at least 5 hr.\\nOn the assumption that the variability is due to a changing cross section, the\\naxial ratio is at least 3:1. We saw no evidence for a coma or tail in either\\nindividual images or in a stacked image having an equivalent exposure time of\\n9000 s.\\n\",\n",
       " \"  The ability of metallic nanoparticles to supply heat to a liquid environment\\nunder exposure to an external optical field has attracted growing interest for\\nbiomedical applications. Controlling the thermal transport properties at a\\nsolid-liquid interface then appears to be particularly relevant. In this work,\\nwe address the thermal transport between water and a gold surface coated by a\\npolymer layer. Using molecular dynamics simulations, we demonstrate that\\nincreasing the polymer density displaces the domain resisting to the heat flow,\\nwhile it doesn't affect the final amount of thermal energy released in the\\nliquid. This unexpected behavior results from a trade-off established by the\\nincreasing polymer density which couples more efficiently with the solid but\\ninitiates a counterbalancing resistance with the liquid.\\n\",\n",
       " '  We model large-scale ($\\\\approx$2000km) impacts on a Mars-like planet using a\\nSmoothed Particle Hydrodynamics code. The effects of material strength and of\\nusing different Equations of State on the post-impact material and temperature\\ndistributions are investigated. The properties of the ejected material in terms\\nof escaping and disc mass are analysed as well. We also study potential\\nnumerical effects in the context of density discontinuities and rigid body\\nrotation. We find that in the large-scale collision regime considered here\\n(with impact velocities of 4km/s), the effect of material strength is\\nsubstantial for the post-impact distribution of the temperature and the\\nimpactor material, while the influence of the Equation of State is more subtle\\nand present only at very high temperatures.\\n',\n",
       " '  Time varying susceptibility of host at individual level due to waning and\\nboosting immunity is known to induce rich long-term behavior of disease\\ntransmission dynamics. Meanwhile, the impact of the time varying heterogeneity\\nof host susceptibility on the shot-term behavior of epidemics is not\\nwell-studied, even though the large amount of the available epidemiological\\ndata are the short-term epidemics. Here we constructed a parsimonious\\nmathematical model describing the short-term transmission dynamics taking into\\naccount natural-boosting immunity by reinfection, and obtained the explicit\\nsolution for our model. We found that our system show \"the delayed epidemic\",\\nthe epidemic takes off after negative slope of the epidemic curve at the\\ninitial phase of epidemic, in addition to the common classification in the\\nstandard SIR model, i.e., \"no epidemic\" as $\\\\mathcal{R}_{0}\\\\leq1$ or normal\\nepidemic as $\\\\mathcal{R}_{0}>1$. Employing the explicit solution we derived the\\ncondition for each classification.\\n',\n",
       " '  We present a systematic global sensitivity analysis using the Sobol method\\nwhich can be utilized to rank the variables that affect two quantity of\\ninterests -- pore pressure depletion and stress change -- around a\\nhydraulically-fractured horizontal well based on their degree of importance.\\nThese variables include rock properties and stimulation design variables. A\\nfully-coupled poroelastic hydraulic fracture model is used to account for pore\\npressure and stress changes due to production. To ease the computational cost\\nof a simulator, we also provide reduced order models (ROMs), which can be used\\nto replace the complex numerical model with a rather simple analytical model,\\nfor calculating the pore pressure and stresses at different locations around\\nhydraulic fractures. The main findings of this research are: (i) mobility,\\nproduction pressure, and fracture half-length are the main contributors to the\\nchanges in the quantities of interest. The percentage of the contribution of\\neach parameter depends on the location with respect to pre-existing hydraulic\\nfractures and the quantity of interest. (ii) As the time progresses, the effect\\nof mobility decreases and the effect of production pressure increases. (iii)\\nThese two variables are also dominant for horizontal stresses at large\\ndistances from hydraulic fractures. (iv) At zones close to hydraulic fracture\\ntips or inside the spacing area, other parameters such as fracture spacing and\\nhalf-length are the dominant factors that affect the minimum horizontal stress.\\nThe results of this study will provide useful guidelines for the stimulation\\ndesign of legacy wells and secondary operations such as refracturing and infill\\ndrilling.\\n',\n",
       " '  \"Three is a crowd\" is an old proverb that applies as much to social\\ninteractions, as it does to frustrated configurations in statistical physics\\nmodels. Accordingly, social relations within a triangle deserve special\\nattention. With this motivation, we explore the impact of topological\\nfrustration on the evolutionary dynamics of the snowdrift game on a triangular\\nlattice. This topology provides an irreconcilable frustration, which prevents\\nanti-coordination of competing strategies that would be needed for an optimal\\noutcome of the game. By using different strategy updating protocols, we observe\\ncomplex spatial patterns in dependence on payoff values that are reminiscent to\\na honeycomb-like organization, which helps to minimize the negative consequence\\nof the topological frustration. We relate the emergence of these patterns to\\nthe microscopic dynamics of the evolutionary process, both by means of\\nmean-field approximations and Monte Carlo simulations. For comparison, we also\\nconsider the same evolutionary dynamics on the square lattice, where of course\\nthe topological frustration is absent. However, with the deletion of diagonal\\nlinks of the triangular lattice, we can gradually bridge the gap to the square\\nlattice. Interestingly, in this case the level of cooperation in the system is\\na direct indicator of the level of topological frustration, thus providing a\\nmethod to determine frustration levels in an arbitrary interaction network.\\n',\n",
       " '  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se\\ndiluted-magnetic-semiconductor quantum wells using time-resolved\\nphotoluminescence (PL). The magnetic field and temperature dependencies of this\\ndynamics allow us to separate the non-magnetic and magnetic contributions to\\nthe exciton localization. We deduce the EMP energy of 14 meV, which is in\\nagreement with time-integrated measurements based on selective excitation and\\nthe magnetic field dependence of the PL circular polarization degree. The\\npolaron formation time of 500 ps is significantly longer than the corresponding\\nvalues reported earlier. We propose that this behavior is related to strong\\nself-localization of the EMP, accompanied with a squeezing of the heavy-hole\\nenvelope wavefunction. This conclusion is also supported by the decrease of the\\nexciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and\\ntemperature.\\n',\n",
       " '  The classical Eilenberg correspondence, based on the concept of the syntactic\\nmonoid, relates varieties of regular languages with pseudovarieties of finite\\nmonoids. Various modifications of this correspondence appeared, with more\\ngeneral classes of regular languages on one hand and classes of more complex\\nalgebraic structures on the other hand. For example, classes of languages need\\nnot be closed under complementation or all preimages under homomorphisms, while\\nmonoids can be equipped with a compatible order or they can have a\\ndistinguished set of generators. Such generalized varieties and pseudovarieties\\nalso have natural counterparts formed by classes of finite (ordered) automata.\\nIn this paper the previous approaches are combined. The notion of positive\\n$\\\\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final\\nstates are specified) is introduced and their correspondence with positive\\n$\\\\mathcal C$-varieties of languages is proved.\\n',\n",
       " '  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct\\nexperimental evidence for spontaneous vortex phase (SVP) formation in\\nEuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting\\n$T^{\\\\rm 0}_{\\\\rm SC}=23.6$~K and ferromagnetic $T_{\\\\rm FM}\\\\sim17.7$~K transition\\ntemperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the\\nvicinity of $T_{\\\\rm FM}$. Also, upon cooling cycle near $T_{\\\\rm FM}$ we observe\\nthe first-order transition from the short period domain structure, which\\nappears in the Meissner state, into the long period domain structure with\\nspontaneous vortices. It is the first experimental observation of this scenario\\nin the ferromagnetic superconductors. Low-temperature phase is characterized by\\nmuch larger domains in V-AV state and peculiar branched striped structures at\\nthe surface, which are typical for uniaxial ferromagnets with perpendicular\\nmagnetic anisotropy (PMA). The domain wall parameters at various temperatures\\nare estimated.\\n',\n",
       " '  The recent discovery that the exponent of matrix multiplication is determined\\nby the rank of the symmetrized matrix multiplication tensor has invigorated\\ninterest in better understanding symmetrized matrix multiplication. I present\\nan explicit rank 18 Waring decomposition of $sM_{\\\\langle 3\\\\rangle}$ and\\ndescribe its symmetry group.\\n',\n",
       " '  The process that leads to the formation of the bright star forming sites\\nobserved along prominent spiral arms remains elusive. We present results of a\\nmulti-wavelength study of a spiral arm segment in the nearby grand-design\\nspiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas\\nspurs. The combined observations of the(ionized, atomic, molecular, dusty)\\ninterstellar medium (ISM) with star formation tracers (HII regions, young\\n<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud\\n(GMC) properties between arm and gas spurs, (2) gas spurs and extinction\\nfeathers arising from the same structure with a close spatial relation between\\ngas spurs and ongoing/recent star formation (despite higher gas surface\\ndensities in the spiral arm), (3) no trend in star formation age either along\\nthe arm or along a spur, (4) evidence for strong star formation feedback in gas\\nspurs: (5) tentative evidence for star formation triggered by stellar feedback\\nfor one spur, and (6) GMC associations (GMAs) being no special entities but the\\nresult of blending of gas arm/spur cross-sections in lower resolution\\nobservations. We conclude that there is no evidence for a coherent star\\nformation onset mechanism that can be solely associated to the presence of the\\nspiral density wave. This suggests that other (more localized) mechanisms are\\nimportant to delay star formation such that it occurs in spurs. The evidence of\\nstar formation proceeding over several million years within individual spurs\\nimplies that the mechanism that leads to star formation acts or is sustained\\nover a longer time-scale.\\n',\n",
       " '  We describe a variant construction of the unstable Adams spectral the\\nsequence for a space $Y$, associated to any free simplicial resolution of\\n$H^*(Y;R)$ for $R=\\\\mathbb{F}_p$ or $\\\\mathbb{Q}$. We use this construction to\\ndescribe the differentials and filtration in the spectral sequence in terms of\\nappropriate systems of higher cohomology operations.\\n',\n",
       " '  When investigators seek to estimate causal effects, they often assume that\\nselection into treatment is based only on observed covariates. Under this\\nidentification strategy, analysts must adjust for observed confounders. While\\nbasic regression models have long been the dominant method of statistical\\nadjustment, more robust methods based on matching or weighting have become more\\ncommon. Of late, even more flexible methods based on machine learning methods\\nhave been developed for statistical adjustment. These machine learning methods\\nare designed to be black box methods with little input from the researcher.\\nRecent research used a data competition to evaluate various methods of\\nstatistical adjustment and found that black box methods out performed all other\\nmethods of statistical adjustment. Matching methods with covariate\\nprioritization are designed for direct input from substantive investigators in\\ndirect contrast to black methods. In this article, we use a different research\\ndesign to compare matching with covariate prioritization to black box methods.\\nWe use black box methods to replicate results from five studies where matching\\nwith covariate prioritization was used to customize the statistical adjustment\\nin direct response to substantive expertise. We find little difference across\\nthe methods. We conclude with advice for investigators.\\n',\n",
       " \"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the\\nthermoviscous wave equations (TWE) derived by transforming the linearized\\nNavier-Stokes equations (LNSE) to the frequency domain yields a so-called\\nHelmholtz solver, whose output is a discrete set of complex eigenfunction and\\neigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --\\nreverses such procedure by returning the value of acoustic impedance at one or\\nmore unknown impedance boundaries (IBs) of a given domain via spatial\\nintegration of the TWE for a given real-valued frequency with assigned\\nconditions on other boundaries. The iHS procedure is applied to a second-order\\nspatial discretization of the TWEs derived on an unstructured grid with\\nstaggered grid arrangement. The momentum equation only is extended to the\\ncenter of each IB face where pressure and velocity components are co-located\\nand treated as unknowns. One closure condition considered for the iHS is the\\nassignment of the surface gradient of pressure phase over the IBs,\\ncorresponding to assigning the shape of the acoustic waveform at the IB. The\\niHS procedure is carried out independently for each frequency in order to\\nreturn the complete broadband complex impedance distribution at the IBs in any\\ndesired frequency range. The iHS approach is first validated against Rott's\\ntheory for both inviscid and viscous, rectangular and circular ducts. The\\nimpedance of a geometrically complex toy cavity is then reconstructed and\\nverified against companion full compressible unstructured Navier-Stokes\\nsimulations resolving the cavity geometry and one-dimensional impedance test\\ntube calculations based on time-domain impedance boundary conditions (TDIBC).\\nThe iHS methodology is also shown to capture thermoacoustic effects, with\\nreconstructed impedance values quantitatively in agreement with thermoacoustic\\ngrowth rates.\\n\",\n",
       " '  The impact of random fluctuations on the dynamical behavior a complex\\nbiological systems is a longstanding issue, whose understanding would shed\\nlight on the evolutionary pressure that nature imposes on the intrinsic noise\\nlevels and would allow rationally designing synthetic networks with controlled\\nnoise. Using the Itō stochastic differential equation formalism, we performed\\nboth analytic and numerical analyses of several model systems containing\\ndifferent molecular species in contact with the environment and interacting\\nwith each other through mass-action kinetics. These systems represent for\\nexample biomolecular oligomerization processes, complex-breakage reactions,\\nsignaling cascades or metabolic networks. For chemical reaction networks with\\nzero deficiency values, which admit a detailed- or complex-balanced steady\\nstate, all molecular species are uncorrelated. The number of molecules of each\\nspecies follow a Poisson distribution and their Fano factors, which measure the\\nintrinsic noise, are equal to one. Systems with deficiency one have an\\nunbalanced non-equilibrium steady state and a non-zero S-flux, defined as the\\nflux flowing between the complexes multiplied by an adequate stoichiometric\\ncoefficient. In this case, the noise on each species is reduced if the flux\\nflows from the species of lowest to highest complexity, and is amplified is the\\nflux goes in the opposite direction. These results are generalized to systems\\nof deficiency two, which possess two independent non-vanishing S-fluxes, and we\\nconjecture that a similar relation holds for higher deficiency systems.\\n',\n",
       " '  Rare regions with weak disorder (Griffiths regions) have the potential to\\nspoil localization. We describe a non-perturbative construction of local\\nintegrals of motion (LIOMs) for a weakly interacting spin chain in one\\ndimension, under a physically reasonable assumption on the statistics of\\neigenvalues. We discuss ideas about the situation in higher dimensions, where\\none can no longer ensure that interactions involving the Griffiths regions are\\nmuch smaller than the typical energy-level spacing for such regions. We argue\\nthat ergodicity is restored in dimension d > 1, although equilibration should\\nbe extremely slow, similar to the dynamics of glasses.\\n',\n",
       " '  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB\\nfunctions for the analysis and solution of fault detection and model detection\\nproblems. The implemented functions are based on the computational procedures\\ndescribed in the Chapters 5, 6 and 7 of the book: \"A. Varga, Solving Fault\\nDiagnosis Problems - Linear Synthesis Techniques, Springer, 2017\". This\\ndocument is the User\\'s Guide for the version V1.0 of FDITOOLS. First, we\\npresent the mathematical background for solving several basic exact and\\napproximate synthesis problems of fault detection filters and model detection\\nfilters. Then, we give in-depth information on the command syntax of the main\\nanalysis and synthesis functions. Several examples illustrate the use of the\\nmain functions of FDITOOLS.\\n',\n",
       " '  Detectability of discrete event systems (DESs) is a question whether the\\ncurrent and subsequent states can be determined based on observations. Shu and\\nLin designed a polynomial-time algorithm to check strong (periodic)\\ndetectability and an exponential-time (polynomial-space) algorithm to check\\nweak (periodic) detectability. Zhang showed that checking weak (periodic)\\ndetectability is PSpace-complete. This intractable complexity opens a question\\nwhether there are structurally simpler DESs for which the problem is tractable.\\nIn this paper, we show that it is not the case by considering DESs represented\\nas deterministic finite automata without non-trivial cycles, which are\\nstructurally the simplest deadlock-free DESs. We show that even for such very\\nsimple DESs, checking weak (periodic) detectability remains intractable. On the\\ncontrary, we show that strong (periodic) detectability of DESs can be\\nefficiently verified on a parallel computer.\\n',\n",
       " \"  Let $X$ be a partially ordered set with the property that each family of\\norder intervals of the form $[a,b],[a,\\\\rightarrow )$ with the finite\\nintersection property has a nonempty intersection. We show that every directed\\nsubset of $X$ has a supremum. Then we apply the above result to prove that if\\n$X$ is a topological space with a partial order $\\\\preceq $ for which the order\\nintervals are compact, $\\\\mathcal{F}$ a nonempty commutative family of monotone\\nmaps from $X$ into $X$ and there exists $c\\\\in X$ such that $c\\\\preceq Tc$ for\\nevery $T\\\\in \\\\mathcal{F}$, then the set of common fixed points of $\\\\mathcal{F}$\\nis nonempty and has a maximal element. The result, specialized to the case of\\nBanach spaces gives a general fixed point theorem that drops almost all\\nassumptions from the recent results in this area. An application to the theory\\nof integral equations of Urysohn's type is also given.\\n\",\n",
       " '  Efficient methods are proposed, for computing integrals appeaing in\\nelectronic structure calculations. The methods consist of two parts: the first\\npart is to represent the integrals as contour integrals and the second one is\\nto evaluate the contour integrals by the Clenshaw-Curtis quadrature. The\\nefficiency of the proposed methods is demonstrated through numerical\\nexperiments.\\n',\n",
       " '  We present a novel sound localization algorithm for a non-line-of-sight\\n(NLOS) sound source in indoor environments. Our approach exploits the\\ndiffraction properties of sound waves as they bend around a barrier or an\\nobstacle in the scene. We combine a ray tracing based sound propagation\\nalgorithm with a Uniform Theory of Diffraction (UTD) model, which simulate\\nbending effects by placing a virtual sound source on a wedge in the\\nenvironment. We precompute the wedges of a reconstructed mesh of an indoor\\nscene and use them to generate diffraction acoustic rays to localize the 3D\\nposition of the source. Our method identifies the convergence region of those\\ngenerated acoustic rays as the estimated source position based on a particle\\nfilter. We have evaluated our algorithm in multiple scenarios consisting of a\\nstatic and dynamic NLOS sound source. In our tested cases, our approach can\\nlocalize a source position with an average accuracy error, 0.7m, measured by\\nthe L2 distance between estimated and actual source locations in a 7m*7m*3m\\nroom. Furthermore, we observe 37% to 130% improvement in accuracy over a\\nstate-of-the-art localization method that does not model diffraction effects,\\nespecially when a sound source is not visible to the robot.\\n',\n",
       " \"  In this paper we introduce the notion of $\\\\zeta$-crossbreeding in a set of\\n$\\\\zeta$-factorization formulas and also the notion of complete hybrid formula\\nas the final result of that crossbreeding. The last formula is used as a\\ncriterion for selection of families of $\\\\zeta$-kindred elements in class of\\nreal continuous functions.\\nDedicated to recalling of Gregory Mendel's pea-crossbreeding.\\n\",\n",
       " '  We consider the problem of estimating the $L_1$ distance between two discrete\\nprobability measures $P$ and $Q$ from empirical data in a nonasymptotic and\\nlarge alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,\\nwe show that for every $Q$, the minimax rate-optimal estimator with $n$ samples\\nachieves performance comparable to that of the maximum likelihood estimator\\n(MLE) with $n\\\\ln n$ samples. When both $P$ and $Q$ are unknown, we construct\\nminimax rate-optimal estimators whose worst case performance is essentially\\nthat of the known $Q$ case with $Q$ being uniform, implying that $Q$ being\\nuniform is essentially the most difficult case. The \\\\emph{effective sample size\\nenlargement} phenomenon, identified in Jiao \\\\emph{et al.} (2015), holds both in\\nthe known $Q$ case for every $Q$ and the $Q$ unknown case. However, the\\nconstruction of optimal estimators for $\\\\|P-Q\\\\|_1$ requires new techniques and\\ninsights beyond the approximation-based method of functional estimation in Jiao\\n\\\\emph{et al.} (2015).\\n',\n",
       " '  We investigate the density large deviation function for a multidimensional\\nconservation law in the vanishing viscosity limit, when the probability\\nconcentrates on weak solutions of a hyperbolic conservation law conservation\\nlaw. When the conductivity and dif-fusivity matrices are proportional, i.e. an\\nEinstein-like relation is satisfied, the problem has been solved in [4]. When\\nthis proportionality does not hold, we compute explicitly the large deviation\\nfunction for a step-like density profile, and we show that the associated\\noptimal current has a non trivial structure. We also derive a lower bound for\\nthe large deviation function, valid for a general weak solution, and leave the\\ngeneral large deviation function upper bound as a conjecture.\\n',\n",
       " '  Large deep neural networks are powerful, but exhibit undesirable behaviors\\nsuch as memorization and sensitivity to adversarial examples. In this work, we\\npropose mixup, a simple learning principle to alleviate these issues. In\\nessence, mixup trains a neural network on convex combinations of pairs of\\nexamples and their labels. By doing so, mixup regularizes the neural network to\\nfavor simple linear behavior in-between training examples. Our experiments on\\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\\nthat mixup improves the generalization of state-of-the-art neural network\\narchitectures. We also find that mixup reduces the memorization of corrupt\\nlabels, increases the robustness to adversarial examples, and stabilizes the\\ntraining of generative adversarial networks.\\n',\n",
       " \"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric\\nmeasure theory. There exist multiple variants of the original definition. Here\\nwe prove that most of them are indeed equal. One central point is to correct\\nthe proof of Brakke's §3.5, where he develops an estimate for the evolution\\nof the measure of time-dependent test functions.\\n\",\n",
       " '  With recent advancements in drone technology, researchers are now considering\\nthe possibility of deploying small cells served by base stations mounted on\\nflying drones. A major advantage of such drone small cells is that the\\noperators can quickly provide cellular services in areas of urgent demand\\nwithout having to pre-install any infrastructure. Since the base station is\\nattached to the drone, technically it is feasible for the base station to\\ndynamic reposition itself in response to the changing locations of users for\\nreducing the communication distance, decreasing the probability of signal\\nblocking, and ultimately increasing the spectral efficiency. In this paper, we\\nfirst propose distributed algorithms for autonomous control of drone movements,\\nand then model and analyse the spectral efficiency performance of a drone small\\ncell to shed new light on the fundamental benefits of dynamic repositioning. We\\nshow that, with dynamic repositioning, the spectral efficiency of drone small\\ncells can be increased by nearly 100\\\\% for realistic drone speed, height, and\\nuser traffic model and without incurring any major increase in drone energy\\nconsumption.\\n',\n",
       " '  Electronic health records (EHR) contain a large variety of information on the\\nclinical history of patients such as vital signs, demographics, diagnostic\\ncodes and imaging data. The enormous potential for discovery in this rich\\ndataset is hampered by its complexity and heterogeneity.\\nWe present the first study to assess unsupervised homogenization pipelines\\ndesigned for EHR clustering. To identify the optimal pipeline, we tested\\naccuracy on simulated data with varying amounts of redundancy, heterogeneity,\\nand missingness. We identified two optimal pipelines: 1) Multiple Imputation by\\nChained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,\\nZ-scoring, and Deep Autoencoders.\\n',\n",
       " '  Artificial Neural Network computation relies on intensive vector-matrix\\nmultiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array\\nshowed a feasibility of implementing such operations with high energy\\nefficiency, thus there are many works on efficiently utilizing emerging NVM\\ncrossbar array as analog vector-matrix multiplier. However, its nonlinear I-V\\ncharacteristics restrain critical design parameters, such as the read voltage\\nand weight range, resulting in substantial accuracy loss. In this paper,\\ninstead of optimizing hardware parameters to a given neural network, we propose\\na methodology of reconstructing a neural network itself optimized to resistive\\nmemory crossbar arrays. To verify the validity of the proposed method, we\\nsimulated various neural network with MNIST and CIFAR-10 dataset using two\\ndifferent specific Resistive Random Access Memory (RRAM) model. Simulation\\nresults show that our proposed neural network produces significantly higher\\ninference accuracies than conventional neural network when the synapse devices\\nhave nonlinear I-V characteristics.\\n',\n",
       " '  In this work, we establish a full single-letter characterization of the\\nrate-distortion region of an instance of the Gray-Wyner model with side\\ninformation at the decoders. Specifically, in this model an encoder observes a\\npair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and\\ncommunicates with two receivers over an error-free rate-limited link of\\ncapacity $R_0$, as well as error-free rate-limited individual links of\\ncapacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both\\nreceivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$\\nalso reproduces the source component $S^n_1$ lossily, to within some prescribed\\nfidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped\\nrespectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.\\nImportant in this setup, the side information sequences are arbitrarily\\ncorrelated among them, and with the source pair $(S^n_1,S^n_2)$; and are not\\nassumed to exhibit any particular ordering. Furthermore, by specializing the\\nmain result to two Heegard-Berger models with successive refinement and\\nscalable coding, we shed light on the roles of the common and private\\ndescriptions that the encoder should produce and what they should carry\\noptimally. We develop intuitions by analyzing the developed single-letter\\noptimal rate-distortion regions of these models, and discuss some insightful\\nbinary examples.\\n',\n",
       " '  This work discusses the numerical approximation of a nonlinear\\nreaction-advection-diffusion equation, which is a dimensionless form of the\\nWeertman equation. This equation models steadily-moving dislocations in\\nmaterials science. It reduces to the celebrated Peierls-Nabarro equation when\\nits advection term is set to zero. The approach rests on considering a\\ntime-dependent formulation, which admits the equation under study as its\\nlong-time limit. Introducing a Preconditioned Collocation Scheme based on\\nFourier transforms, the iterative numerical method presented solves the\\ntime-dependent problem, delivering at convergence the desired numerical\\nsolution to the Weertman equation. Although it rests on an explicit\\ntime-evolution scheme, the method allows for large time steps, and captures the\\nsolution in a robust manner. Numerical results illustrate the efficiency of the\\napproach for several types of nonlinearities.\\n',\n",
       " '  There are many web-based visualization systems available to date, each having\\nits strengths and limitations. The goals these systems set out to accomplish\\ninfluence design decisions and determine how reusable and scalable they are.\\nWeave is a new web-based visualization platform with the broad goal of enabling\\nvisualization of any available data by anyone for any purpose. Our open source\\nframework supports highly interactive linked visualizations for users of\\nvarying skill levels. What sets Weave apart from other systems is its\\nconsideration for real-time remote collaboration with session history. We\\nprovide a detailed account of the various framework designs we considered with\\ncomparisons to existing state-of-the-art systems.\\n',\n",
       " '  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using\\narchival multi-wavelength data. The Suzaku spectra are well described by\\ntwo-component thermal plasma models: The soft component is in ionization\\nequilibrium and has a temperature $\\\\sim$0.59 keV, while the hard component has\\ntemperature $\\\\sim$3.2 keV and ionization time-scale $\\\\sim$$2.6\\\\times10^{10}$\\ncm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\\\\sim$6.5 keV\\nfrom this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the\\nX-ray emission has an ejecta origin. The centroid energy of the Fe-K line\\nsupports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than\\na core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its\\nsurrounding were analyzed using about 6 years of Fermi data. We report about\\nthe non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray\\nsource in the south-west of G306.3$-$0.9 with a significance of\\n$\\\\sim$13$\\\\sigma$. We discuss several scenarios for these results with the help\\nof data from other wavebands to understand the SNR and its neighborhood.\\n',\n",
       " '  Previous approaches to training syntax-based sentiment classification models\\nrequired phrase-level annotated corpora, which are not readily available in\\nmany languages other than English. Thus, we propose the use of tree-structured\\nLong Short-Term Memory with an attention mechanism that pays attention to each\\nsubtree of the parse tree. Experimental results indicate that our model\\nachieves the state-of-the-art performance in a Japanese sentiment\\nclassification task.\\n',\n",
       " '  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior\\ninference technique that is increasingly popular due to its fast runtimes on\\nlarge-scale datasets. However, even when MFVB provides accurate posterior means\\nfor certain parameters, it often mis-estimates variances and covariances.\\nFurthermore, prior robustness measures have remained undeveloped for MFVB. By\\nderiving a simple formula for the effect of infinitesimal model perturbations\\non MFVB posterior means, we provide both improved covariance estimates and\\nlocal robustness measures for MFVB, thus greatly expanding the practical\\nusefulness of MFVB posterior approximations. The estimates for MFVB posterior\\ncovariances rely on a result from the classical Bayesian robustness literature\\nrelating derivatives of posterior expectations to posterior covariances and\\ninclude the Laplace approximation as a special case. Our key condition is that\\nthe MFVB approximation provides good estimates of a select subset of posterior\\nmeans---an assumption that has been shown to hold in many practical settings.\\nIn our experiments, we demonstrate that our methods are simple, general, and\\nfast, providing accurate posterior uncertainty estimates and robustness\\nmeasures with runtimes that can be an order of magnitude faster than MCMC.\\n',\n",
       " '  In this paper, we empirically study models for pricing Italian sovereign\\nbonds under a reduced form framework, by assuming different dynamics for the\\nshort-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek\\nmulti-factor models, with a focus on optimization algorithms applied in the\\ncalibration exercise. The Kalman filter algorithm together with a maximum\\nlikelihood estimation method are considered to fit the Italian term-structure\\nover a 12-year horizon, including the global financial crisis and the euro area\\nsovereign debt crisis. Analytic formulas for the gradient vector and the\\nHessian matrix of the likelihood function are provided.\\n',\n",
       " '  Ballistic point contact (BPC) with zigzag edges in graphene is a main\\ncandidate of a valley filter, in which the polarization of the valley degree of\\nfreedom can be selected by using a local gate voltage. Here, we propose to\\ndetect the valley filtering effect by Andreev reflection. Because electrons in\\nthe lowest conduction band and the highest valence band of the BPC possess\\nopposite chirality, the inter-band Andreev reflection is strongly suppressed,\\nafter multiple scattering and interference. We draw this conclusion by both the\\nscattering matrix analysis and the numerical simulation. The Andreev reflection\\nas a function of the incident energy of electrons and the local gate voltage at\\nthe BPC is obtained, by which the parameter region for a perfect valley filter\\nand the direction of valley polarization can be determined. The Andreev\\nreflection exhibits an oscillatory decay with the length of the BPC, indicating\\na negative correlation to valley polarization.\\n',\n",
       " '  Sparse superposition (SS) codes were originally proposed as a\\ncapacity-achieving communication scheme over the additive white Gaussian noise\\nchannel (AWGNC) [1]. Very recently, it was discovered that these codes are\\nuniversal, in the sense that they achieve capacity over any memoryless channel\\nunder generalized approximate message-passing (GAMP) decoding [2], although\\nthis decoder has never been stated for SS codes. In this contribution we\\nintroduce the GAMP decoder for SS codes, we confirm empirically the\\nuniversality of this communication scheme through its study on various channels\\nand we provide the main analysis tools: state evolution and potential. We also\\ncompare the performance of GAMP with the Bayes-optimal MMSE decoder. We\\nempirically illustrate that despite the presence of a phase transition\\npreventing GAMP to reach the optimal performance, spatial coupling allows to\\nboost the performance that eventually tends to capacity in a proper limit. We\\nalso prove that, in contrast with the AWGNC case, SS codes for binary input\\nchannels have a vanishing error floor in the limit of large codewords.\\nMoreover, the performance of Hadamard-based encoders is assessed for practical\\nimplementations.\\n',\n",
       " '  When developing general purpose robots, the overarching software architecture\\ncan greatly affect the ease of accomplishing various tasks. Initial efforts to\\ncreate unified robot systems in the 1990s led to hybrid architectures,\\nemphasizing a hierarchy in which deliberative plans direct the use of reactive\\nskills. However, since that time there has been significant progress in the\\nlow-level skills available to robots, including manipulation and perception,\\nmaking it newly feasible to accomplish many more tasks in real-world domains.\\nThere is thus renewed optimism that robots will be able to perform a wide array\\nof tasks while maintaining responsiveness to human operators. However, the top\\nlayer in traditional hybrid architectures, designed to achieve long-term goals,\\ncan make it difficult to react quickly to human interactions during goal-driven\\nexecution. To mitigate this difficulty, we propose a novel architecture that\\nsupports such transitions by adding a top-level reactive module which has\\nflexible access to both reactive skills and a deliberative control module. To\\nvalidate this architecture, we present a case study of its application on a\\ndomestic service robot platform.\\n',\n",
       " '  We propose an approach to estimate 3D human pose in real world units from a\\nsingle RGBD image and show that it exceeds performance of monocular 3D pose\\nestimation approaches from color as well as pose estimation exclusively from\\ndepth. Our approach builds on robust human keypoint detectors for color images\\nand incorporates depth for lifting into 3D. We combine the system with our\\nlearning from demonstration framework to instruct a service robot without the\\nneed of markers. Experiments in real world settings demonstrate that our\\napproach enables a PR2 robot to imitate manipulation actions observed from a\\nhuman teacher.\\n',\n",
       " '  We extend the work of Fouvry, Kowalski and Michel on correlation between\\nHecke eigenvalues of modular forms and algebraic trace functions in order to\\nestablish an asymptotic formula for a generalized cubic moment of modular\\nL-functions at the central point s = 1/2 and for prime moduli q. As an\\napplication, we exploit our recent result on the mollification of the fourth\\nmoment of Dirichlet L-functions to derive that for any pair\\n$(\\\\omega_1,\\\\omega_2)$ of multiplicative characters modulo q, there is a\\npositive proportion of $\\\\chi$ (mod q) such that $L(\\\\chi, 1/2 ), L(\\\\chi\\\\omega_1,\\n1/2 )$ and $L(\\\\chi\\\\omega_2, 1/2)$ are simultaneously not too small.\\n',\n",
       " '  Nonclassical states of a quantized light are described in terms of\\nGlauber-Sudarshan P distribution which is not a genuine classical probability\\ndistribution. Despite several attempts, defining a uniform measure of\\nnonclassicality (NC) for the single mode quantum states of light is yet an open\\ntask. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that\\nthe existing well-known measures fail to quantify the NC of single mode states\\nthat are generated under multiple NC-inducing operations. Recently, Ivan et.\\nal. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of\\nnon-Gaussian character of quantum optical states in terms of Wehrl entropy.\\nHere, we adopt this concept in the context of single mode NC. In this paper, we\\npropose a new quantification of NC for the single mode quantum states of light\\nas the difference between the total Wehrl entropy of the state and the maximum\\nWehrl entropy arising due to its classical characteristics. This we achieve by\\nsubtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any\\nclassical state that has same randomness as measured in terms of von-Neumann\\nentropy. We obtain analytic expressions of NC for most of the states, in\\nparticular, all pure states and Gaussian mixed states. However, the evaluation\\nof NC for the non-Gaussian mixed states is subject to extensive numerical\\ncomputation that lies beyond the scope of the current work. We show that, along\\nwith the states generated under single NC-inducing operations, also for the\\nbroader class of states that are generated under multiple NC-inducing\\noperations, our quantification enumerates the NC consistently.\\n',\n",
       " '  Following the recent progress in image classification and captioning using\\ndeep learning, we develop a novel natural language person retrieval system\\nbased on an attention mechanism. More specifically, given the description of a\\nperson, the goal is to localize the person in an image. To this end, we first\\nconstruct a benchmark dataset for natural language person retrieval. To do so,\\nwe generate bounding boxes for persons in a public image dataset from the\\nsegmentation masks, which are then annotated with descriptions and attributes\\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\\nFaster R-CNN as a candidate region generator. The cropped images based on the\\nregion proposals as well as the whole images with attention weights are fed\\ninto Convolutional Neural Networks for visual feature extraction, while the\\nnatural language expression and attributes are input to Bidirectional Long\\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\\ntext features are integrated to score region proposals, and the one with the\\nhighest score is retrieved as the output of our system. The experimental\\nresults show significant improvement over the state-of-the-art method for\\ngeneric object retrieval and this line of research promises to benefit search\\nin surveillance video footage.\\n',\n",
       " '  Real time large scale streaming data pose major challenges to forecasting, in\\nparticular defying the presence of human experts to perform the corresponding\\nanalysis. We present here a class of models and methods used to develop an\\nautomated, scalable and versatile system for large scale forecasting oriented\\ntowards safety and security monitoring. Our system provides short and long term\\nforecasts and uses them to detect safety and security issues in relation with\\nmultiple internet connected devices well in advance they might take place.\\n',\n",
       " '  Machine learning algorithms such as linear regression, SVM and neural network\\nhave played an increasingly important role in the process of scientific\\ndiscovery. However, none of them is both interpretable and accurate on\\nnonlinear datasets. Here we present contextual regression, a method that joins\\nthese two desirable properties together using a hybrid architecture of neural\\nnetwork embedding and dot product layer. We demonstrate its high prediction\\naccuracy and sensitivity through the task of predictive feature selection on a\\nsimulated dataset and the application of predicting open chromatin sites in the\\nhuman genome. On the simulated data, our method achieved high fidelity recovery\\nof feature contributions under random noise levels up to 200%. On the open\\nchromatin dataset, the application of our method not only outperformed the\\nstate of the art method in terms of accuracy, but also unveiled two previously\\nunfound open chromatin related histone marks. Our method can fill the blank of\\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\\n',\n",
       " '  We consider multi-time correlators for output signals from linear detectors,\\ncontinuously measuring several qubit observables at the same time. Using the\\nquantum Bayesian formalism, we show that for unital (symmetric) evolution in\\nthe absence of phase backaction, an $N$-time correlator can be expressed as a\\nproduct of two-time correlators when $N$ is even. For odd $N$, there is a\\nsimilar factorization, which also includes a single-time average. Theoretical\\npredictions agree well with experimental results for two detectors, which\\nsimultaneously measure non-commuting qubit observables.\\n',\n",
       " '  Constraint Handling Rules is an effective concurrent declarative programming\\nlanguage and a versatile computational logic formalism. CHR programs consist of\\nguarded reactive rules that transform multisets of constraints. One of the main\\nfeatures of CHR is its inherent concurrency. Intuitively, rules can be applied\\nto parts of a multiset in parallel. In this comprehensive survey, we give an\\noverview of concurrent and parallel as well as distributed CHR semantics,\\nstandard and more exotic, that have been proposed over the years at various\\nlevels of refinement. These semantics range from the abstract to the concrete.\\nThey are related by formal soundness results. Their correctness is established\\nas correspondence between parallel and sequential computations. We present\\ncommon concise sample CHR programs that have been widely used in experiments\\nand benchmarks. We review parallel CHR implementations in software and\\nhardware. The experimental results obtained show a consistent parallel speedup.\\nMost implementations are available online. The CHR formalism can also be used\\nto implement and reason with models for concurrency. To this end, the Software\\nTransaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\\nhave been faithfully encoded in CHR. Under consideration in Theory and Practice\\nof Logic Programming (TPLP).\\n',\n",
       " '  Many people are suffering from voice disorders, which can adversely affect\\nthe quality of their lives. In response, some researchers have proposed\\nalgorithms for automatic assessment of these disorders, based on voice signals.\\nHowever, these signals can be sensitive to the recording devices. Indeed, the\\nchannel effect is a pervasive problem in machine learning for healthcare. In\\nthis study, we propose a detection system for pathological voice, which is\\nrobust against the channel effect. This system is based on a bidirectional LSTM\\nnetwork. To increase the performance robustness against channel mismatch, we\\nintegrate domain adversarial training (DAT) to eliminate the differences\\nbetween the devices. When we train on data recorded on a high-quality\\nmicrophone and evaluate on smartphone data without labels, our robust detection\\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\\nsample labels). To the best of our knowledge, this is the first study applying\\nunsupervised domain adaptation to pathological voice detection. Notably, our\\nsystem does not need target device sample labels, which allows for\\ngeneralization to many new devices.\\n',\n",
       " '  Computing a basis for the exponent lattice of algebraic numbers is a basic\\nproblem in the field of computational number theory with applications to many\\nother areas. The main cost of a well-known algorithm\\n\\\\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on\\ncomputing the primitive element of the extended field generated by the given\\nalgebraic numbers. When the extended field is of large degree, the problem\\nseems intractable by the tool implementing the algorithm. In this paper, a\\nspecial kind of exponent lattice basis is introduced. An important feature of\\nthe basis is that it can be inductively constructed, which allows us to deal\\nwith the given algebraic numbers one by one when computing the basis. Based on\\nthis, an effective framework for constructing exponent lattice basis is\\nproposed. Through computing a so-called pre-basis first and then solving some\\nlinear Diophantine equations, the basis can be efficiently constructed. A new\\ncertificate for multiplicative independence and some techniques for decreasing\\ndegrees of algebraic numbers are provided to speed up the computation. The new\\nalgorithm has been implemented with Mathematica and its effectiveness is\\nverified by testing various examples. Moreover, the algorithm is applied to\\nprogram verification for finding invariants of linear loops.\\n',\n",
       " '  Investigating the emergence of a particular cell type is a recurring theme in\\nmodels of growing cellular populations. The evolution of resistance to therapy\\nis a classic example. Common questions are: when does the cell type first\\noccur, and via which sequence of steps is it most likely to emerge? For growing\\npopulations, these questions can be formulated in a general framework of\\nbranching processes spreading through a graph from a root to a target vertex.\\nCells have a particular fitness value on each vertex and can transition along\\nedges at specific rates. Vertices represents cell states, say \\\\mic{genotypes\\n}or physical locations, while possible transitions are acquiring a mutation or\\ncell migration. We focus on the setting where cells at the root vertex have the\\nhighest fitness and transition rates are small. Simple formulas are derived for\\nthe time to reach the target vertex and for the probability that it is reached\\nalong a given path in the graph. We demonstrate our results on \\\\mic{several\\nscenarios relevant to the emergence of drug resistance}, including: the\\norderings of resistance-conferring mutations in bacteria and the impact of\\nimperfect drug penetration in cancer.\\n',\n",
       " '  Stimuli-responsive materials that modify their shape in response to changes\\nin environmental conditions -- such as solute concentration, temperature, pH,\\nand stress -- are widespread in nature and technology. Applications include\\nmicro- and nanoporous materials used in filtration and flow control. The\\nphysiochemical mechanisms that induce internal volume modifications have been\\nwidely studies. The coupling between induced volume changes and solute\\ntransport through porous materials, however, is not well understood. Here, we\\nconsider advective and diffusive transport through a small channel linking two\\nlarge reservoirs. A section of stimulus-responsive material regulates the\\nchannel permeability, which is a function of the local solute concentration. We\\nderive an exact solution to the coupled transport problem and demonstrate the\\nexistence of a flow regime in which the steady state is reached via a damped\\noscillation around the equilibrium concentration value. Finally, the\\nfeasibility of an experimental observation of the phenomena is discussed.\\nPlease note that this version of the paper has not been formally peer reviewed,\\nrevised or accepted by a journal.\\n',\n",
       " \"  Today's landscape of robotics is dominated by vertical integration where\\nsingle vendors develop the final product leading to slow progress, expensive\\nproducts and customer lock-in. Opposite to this, an horizontal integration\\nwould result in a rapid development of cost-effective mass-market products with\\nan additional consumer empowerment. The transition of an industry from vertical\\nintegration to horizontal integration is typically catalysed by de facto\\nindustry standards that enable a simplified and seamless integration of\\nproducts. However, in robotics there is currently no leading candidate for a\\nglobal plug-and-play standard.\\nThis paper tackles the problem of incompatibility between robot components\\nthat hinder the reconfigurability and flexibility demanded by the robotics\\nindustry. Particularly, it presents a model to create plug-and-play robot\\nhardware components. Rather than iteratively evolving previous ontologies, our\\nproposed model answers the needs identified by the industry while facilitating\\ninteroperability, measurability and comparability of robotics technology. Our\\napproach differs significantly with the ones presented before as it is\\nhardware-oriented and establishes a clear set of actions towards the\\nintegration of this model in real environments and with real manufacturers.\\n\",\n",
       " '  Machine learning models, especially based on deep architectures are used in\\neveryday applications ranging from self driving cars to medical diagnostics. It\\nhas been shown that such models are dangerously susceptible to adversarial\\nsamples, indistinguishable from real samples to human eye, adversarial samples\\nlead to incorrect classifications with high confidence. Impact of adversarial\\nsamples is far-reaching and their efficient detection remains an open problem.\\nWe propose to use direct density ratio estimation as an efficient model\\nagnostic measure to detect adversarial samples. Our proposed method works\\nequally well with single and multi-channel samples, and with different\\nadversarial sample generation methods. We also propose a method to use density\\nratio estimates for generating adversarial samples with an added constraint of\\npreserving density ratio.\\n',\n",
       " '  We study the query complexity of cake cutting and give lower and upper bounds\\nfor computing approximately envy-free, perfect, and equitable allocations with\\nthe minimum number of cuts. The lower bounds are tight for computing connected\\nenvy-free allocations among n=3 players and for computing perfect and equitable\\nallocations with minimum number of cuts between n=2 players.\\nWe also formalize moving knife procedures and show that a large subclass of\\nthis family, which captures all the known moving knife procedures, can be\\nsimulated efficiently with arbitrarily small error in the Robertson-Webb query\\nmodel.\\n',\n",
       " \"  This paper studies the emotion recognition from musical tracks in the\\n2-dimensional valence-arousal (V-A) emotional space. We propose a method based\\non convolutional (CNN) and recurrent neural networks (RNN), having\\nsignificantly fewer parameters compared with the state-of-the-art method for\\nthe same task. We utilize one CNN layer followed by two branches of RNNs\\ntrained separately for arousal and valence. The method was evaluated using the\\n'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\\narousal and 0.268 for valence, which is the best result reported on this\\ndataset.\\n\",\n",
       " '  We consider previous models of Timed, Probabilistic and Stochastic Timed\\nAutomata, we introduce our model of Timed Automata with Polynomial Delay and we\\ncharacterize the expressiveness of these models relative to each other.\\n',\n",
       " '  We present muon spin rotation measurements on superconducting Cu intercalated\\nBi$_2$Se$_3$, which was suggested as a realization of a topological\\nsuperconductor. We observe a clear evidence of the superconducting transition\\nbelow 4 K, where the width of magnetic field distribution increases as the\\ntemperature is decreased. The measured broadening at mK temperatures suggests a\\nlarge London penetration depth in the $ab$ plane ($\\\\lambda_{\\\\mathrm{eff}}\\\\sim\\n1.6$ $\\\\mathrm{\\\\mu}$m). We show that the temperature dependence of this\\nbroadening follows the BCS prediction, but could be consistent with several gap\\nsymmetries.\\n',\n",
       " '  Here we reveal details of the interaction between human lysozyme proteins,\\nboth native and fibrils, and their water environment by intense terahertz time\\ndomain spectroscopy. With the aid of a rigorous dielectric model, we determine\\nthe amplitude and phase of the oscillating dipole induced by the THz field in\\nthe volume containing the protein and its hydration water. At low\\nconcentrations, the amplitude of this induced dipolar response decreases with\\nincreasing concentration. Beyond a certain threshold, marking the onset of the\\ninteractions between the extended hydration shells, the amplitude remains fixed\\nbut the phase of the induced dipolar response, which is initially in phase with\\nthe applied THz field, begins to change. The changes observed in the THz\\nresponse reveal protein-protein interactions me-diated by extended hydration\\nlayers, which may control fibril formation and may have an important role in\\nchemical recognition phenomena.\\n',\n",
       " \"  We report on experimentally measured light shifts of superconducting flux\\nqubits deep-strongly coupled to LC oscillators, where the coupling constants\\nare comparable to the qubit and oscillator resonance frequencies. By using\\ntwo-tone spectroscopy, the energies of the six lowest levels of each circuit\\nare determined. We find huge Lamb shifts that exceed 90% of the bare qubit\\nfrequencies and inversions of the qubits' ground and excited states when there\\nare a finite number of photons in the oscillator. Our experimental results\\nagree with theoretical predictions based on the quantum Rabi model.\\n\",\n",
       " '  We describe a novel weakly supervised deep learning framework that combines\\nboth the discriminative and generative models to learn meaningful\\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\\nsupervised learning problem where labels are associated with groups of\\ninstances (referred as bags) instead of individual instances. To address the\\nessential challenge in MIL problems raised from the uncertainty of positive\\ninstances label, we use a discriminative model regularized by variational\\nautoencoders (VAEs) to maximize the differences between latent representations\\nof all instances and negative instances. As a result, the hidden layer of the\\nvariational autoencoder learns meaningful representation. This representation\\ncan effectively be used for MIL problems as illustrated by better performance\\non the standard benchmark datasets comparing to the state-of-the-art\\napproaches. More importantly, unlike most related studies, the proposed\\nframework can be easily scaled to large dataset problems, as illustrated by the\\naudio event detection and segmentation task. Visualization also confirms the\\neffectiveness of the latent representation in discriminating positive and\\nnegative classes.\\n',\n",
       " '  We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,\\nconfirming a conjecture of Berman.\\n',\n",
       " '  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary\\n$X$. Assume that $\\\\overline M$ admits a holomorphic $S^1$-action preserving the\\nboundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that\\nthe $\\\\overline\\\\partial$-Neumann Laplacian on $M$ is transversally elliptic and\\nas a consequence, the $m$-th Fourier component of the $q$-th Dolbeault\\ncohomology group $H^q_m(\\\\overline M)$ is finite dimensional, for every\\n$m\\\\in\\\\mathbb Z$ and every $q=0,1,\\\\ldots,n$. This enables us to define\\n$\\\\sum^{n}_{j=0}(-1)^j{\\\\rm dim\\\\,}H^q_m(\\\\overline M)$ the $m$-th Fourier\\ncomponent of the Euler characteristic on $M$ and to study large $m$-behavior of\\n$H^q_m(\\\\overline M)$. In this paper, we establish an index formula for\\n$\\\\sum^{n}_{j=0}(-1)^j{\\\\rm dim\\\\,}H^q_m(\\\\overline M)$ and Morse inequalities for\\n$H^q_m(\\\\overline M)$.\\n',\n",
       " '  Reinforcement learning methods require careful design involving a reward\\nfunction to obtain the desired action policy for a given task. In the absence\\nof hand-crafted reward functions, prior work on the topic has proposed several\\nmethods for reward estimation by using expert state trajectories and action\\npairs. However, there are cases where complete or good action information\\ncannot be obtained from expert demonstrations. We propose a novel reinforcement\\nlearning method in which the agent learns an internal model of observation on\\nthe basis of expert-demonstrated state trajectories to estimate rewards without\\ncompletely learning the dynamics of the external environment from state-action\\npairs. The internal model is obtained in the form of a predictive model for the\\ngiven expert state distribution. During reinforcement learning, the agent\\npredicts the reward as a function of the difference between the actual state\\nand the state predicted by the internal model. We conducted multiple\\nexperiments in environments of varying complexity, including the Super Mario\\nBros and Flappy Bird games. We show our method successfully trains good\\npolicies directly from expert game-play videos.\\n',\n",
       " '  In this paper we are interested in the class of n-ary operations on an\\narbitrary chain that are quasitrivial, symmetric, nondecreasing, and\\nassociative. We first provide a description of these operations. We then prove\\nthat associativity can be replaced with bisymmetry in the definition of this\\nclass. Finally we investigate the special situation where the chain is finite.\\n',\n",
       " '  We propose a new multivariate dependency measure. It is obtained by\\nconsidering a Gaussian kernel based distance between the copula transform of\\nthe given d-dimensional distribution and the uniform copula and then\\nappropriately normalizing it. The resulting measure is shown to satisfy a\\nnumber of desirable properties. A nonparametric estimate is proposed for this\\ndependency measure and its properties (finite sample as well as asymptotic) are\\nderived. Some comparative studies of the proposed dependency measure estimate\\nwith some widely used dependency measure estimates on artificial datasets are\\nincluded. A non-parametric test of independence between two or more random\\nvariables based on this measure is proposed. A comparison of the proposed test\\nwith some existing nonparametric multivariate test for independence is\\npresented.\\n',\n",
       " '  The pyrochlore metal Cd2Re2O7 has been recently investigated by\\nsecond-harmonic generation (SHG) reflectivity. In this paper, we develop a\\ngeneral formalism that allows for the identification of the relevant tensor\\ncomponents of the SHG from azimuthal scans. We demonstrate that the secondary\\norder parameter identified by SHG at the structural phase transition is the\\nx2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2\\nsymmetry of the atomic displacements associated with the I-4m2 crystal\\nstructure that was previously thought to be its origin. Within the same\\nformalism, we suggest that the primary order parameter detected in the SHG\\nexperiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the\\ngeneral mechanism driving the phase transition in our proposed framework, and\\nsuggest experiments, particularly resonant X-ray scattering ones, that could\\nclarify this issue.\\n',\n",
       " '  In evolutionary biology, the speciation history of living organisms is\\nrepresented graphically by a phylogeny, that is, a rooted tree whose leaves\\ncorrespond to current species and branchings indicate past speciation events.\\nPhylogenies are commonly estimated from molecular sequences, such as DNA\\nsequences, collected from the species of interest. At a high level, the idea\\nbehind this inference is simple: the further apart in the Tree of Life are two\\nspecies, the greater is the number of mutations to have accumulated in their\\ngenomes since their most recent common ancestor. In order to obtain accurate\\nestimates in phylogenetic analyses, it is standard practice to employ\\nstatistical approaches based on stochastic models of sequence evolution on a\\ntree. For tractability, such models necessarily make simplifying assumptions\\nabout the evolutionary mechanisms involved. In particular, commonly omitted are\\ninsertions and deletions of nucleotides -- also known as indels.\\nProperly accounting for indels in statistical phylogenetic analyses remains a\\nmajor challenge in computational evolutionary biology. Here we consider the\\nproblem of reconstructing ancestral sequences on a known phylogeny in a model\\nof sequence evolution incorporating nucleotide substitutions, insertions and\\ndeletions, specifically the classical TKF91 process. We focus on the case of\\ndense phylogenies of bounded height, which we refer to as the taxon-rich\\nsetting, where statistical consistency is achievable. We give the first\\npolynomial-time ancestral reconstruction algorithm with provable guarantees\\nunder constant rates of mutation. Our algorithm succeeds when the phylogeny\\nsatisfies the \"big bang\" condition, a necessary and sufficient condition for\\nstatistical consistency in this context.\\n',\n",
       " '  Subject of research is complex networks and network systems. The network\\nsystem is defined as a complex network in which flows are moved. Classification\\nof flows in the network is carried out on the basis of ordering and continuity.\\nIt is shown that complex networks with different types of flows generate\\nvarious network systems. Flow analogues of the basic concepts of the theory of\\ncomplex networks are introduced and the main problems of this theory in terms\\nof flow characteristics are formulated. Local and global flow characteristics\\nof networks bring closer the theory of complex networks to the systems theory\\nand systems analysis. Concept of flow core of network system is introduced and\\ndefined how it simplifies the process of its investigation. Concepts of kernel\\nand flow core of multiplex are determined. Features of operation of multiplex\\ntype systems are analyzed.\\n',\n",
       " '  We study the effect of domain growth on the orientation of striped phases in\\na Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter\\ndependence that allows stripe formation in a half plane, and suppresses\\npatterns in the complement, while the boundary of the pattern-forming region is\\npropagating with fixed normal velocity. We construct front solutions that leave\\nbehind stripes in the pattern-forming region that are parallel to or at a small\\noblique angle to the boundary.\\nTechnically, the construction of stripe formation parallel to the boundary\\nrelies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at\\na small oblique angle are constructed using a functional-analytic, perturbative\\napproach. Here, the main difficulties are the presence of continuous spectrum\\nand the fact that small oblique angles appear as a singular perturbation in a\\ntraveling-wave problem. We resolve the former difficulty using a farfield-core\\ndecomposition and Fredholm theory in weighted spaces. The singular perturbation\\nproblem is resolved using preconditioners and boot-strapping.\\n',\n",
       " '  This paper discusses minimum distance estimation method in the linear\\nregression model with dependent errors which are strongly mixing. The\\nregression parameters are estimated through the minimum distance estimation\\nmethod, and asymptotic distributional properties of the estimators are\\ndiscussed. A simulation study compares the performance of the minimum distance\\nestimator with other well celebrated estimator. This simulation study shows the\\nsuperiority of the minimum distance estimator over another estimator. KoulMde\\n(R package) which was used for the simulation study is available online. See\\nsection 4 for the detail.\\n',\n",
       " '  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,\\nby installing small cloud infrastructures at the network edge. This enables a\\nnew breed of real-time applications, such as instantaneous object recognition\\nand safety assistance in intelligent transportation systems, that require very\\nlow latency. One key issue that comes with proximity is how to ensure that\\nusers always receive good performance as they move across different locations.\\nMigrating services between MECs is seen as the means to achieve this. This\\narticle presents a layered framework for migrating active service applications\\nthat are encapsulated either in virtual machines (VMs) or containers. This\\nlayering approach allows a substantial reduction in service downtime. The\\nframework is easy to implement using readily available technologies, and one of\\nits key advantages is that it supports containers, which is a promising\\nemerging technology that offers tangible benefits over VMs. The migration\\nperformance of various real applications is evaluated by experiments under the\\npresented framework. Insights drawn from the experimentation results are\\ndiscussed.\\n',\n",
       " '  Analog black/white hole pairs, consisting of a region of supersonic flow,\\nhave been achieved in a recent experiment by J. Steinhauer using an elongated\\nBose-Einstein condensate. A growing standing density wave, and a checkerboard\\nfeature in the density-density correlation function, were observed in the\\nsupersonic region. We model the density-density correlation function, taking\\ninto account both quantum fluctuations and the shot-to-shot variation of atom\\nnumber normally present in ultracold-atom experiments. We find that quantum\\nfluctuations alone produce some, but not all, of the features of the\\ncorrelation function, whereas atom-number fluctuation alone can produce all the\\nobserved features, and agreement is best when both are included. In both cases,\\nthe density-density correlation is not intrinsic to the fluctuations, but\\nrather is induced by modulation of the standing wave caused by the\\nfluctuations.\\n',\n",
       " '  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and\\nlet $K_{\\\\infty}/K$ be a geometric extension with Galois group $\\\\mathbb{Z}_p$.\\nLet $K_n$ be the corresponding subextension with Galois group\\n$\\\\mathbb{Z}/p^n\\\\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple\\nexplicit formula $g_n$ in terms of an explicit Witt vector construction of the\\n$\\\\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which\\nis quadratic in $p^n$. Furthermore, we determine all $\\\\mathbb{Z}_p$-towers for\\nwhich the genus sequence is stable, in the sense that there are $a,b,c \\\\in\\n\\\\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus\\nstable towers are expected to have strong stable arithmetic properties for\\ntheir zeta functions. A key technical contribution of this work is a new\\nsimplified formula for the Schmid-Witt symbol coming from local class field\\ntheory.\\n',\n",
       " '  We study the evolution of spin-orbital correlations in an inhomogeneous\\nquantum system with an impurity replacing a doublon by a holon orbital degree\\nof freedom. Spin-orbital entanglement is large when spin correlations are\\nantiferromagnetic, while for a ferromagnetic host we obtain a pure orbital\\ndescription. In this regime the orbital model can be mapped on spinless\\nfermions and we uncover topological phases with zero energy modes at the edge\\nor at the domain between magnetically inequivalent regions.\\n',\n",
       " '  For autonomous agents to successfully operate in the real world, anticipation\\nof future events and states of their environment is a key competence. This\\nproblem has been formalized as a sequence extrapolation problem, where a number\\nof observations are used to predict the sequence into the future. Real-world\\nscenarios demand a model of uncertainty of such predictions, as predictions\\nbecome increasingly uncertain -- in particular on long time horizons. While\\nimpressive results have been shown on point estimates, scenarios that induce\\nmulti-modal distributions over future sequences remain challenging. Our work\\naddresses these challenges in a Gaussian Latent Variable model for sequence\\nprediction. Our core contribution is a \"Best of Many\" sample objective that\\nleads to more accurate and more diverse predictions that better capture the\\ntrue variations in real-world sequence data. Beyond our analysis of improved\\nmodel fit, our models also empirically outperform prior work on three diverse\\ntasks ranging from traffic scenes to weather data.\\n',\n",
       " '  End-to-end approaches have drawn much attention recently for significantly\\nsimplifying the construction of an automatic speech recognition (ASR) system.\\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\\nstudies have shown that RNN-T is difficult to train and a very complex training\\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\\naim to simplify the training process while maintaining performance. First, a\\nnew strategy of learning rate decay is proposed to accelerate the model\\nconvergence. Second, we find that adding convolutional layers at the beginning\\nof the network and using ordered data can discard the pre-training process of\\nthe encoder without loss of performance. Besides, we design experiments to find\\na balance among the usage of GPU memory, training circle and model performance.\\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\\n2% absolute improvement from a strong BLSTM CE system with language model\\ntrained on the same text corpus.\\n',\n",
       " '  Elasticity is a cloud property that enables applications and its execution\\nsystems to dynamically acquire and release shared computational resources on\\ndemand. Moreover, it unfolds the advantage of economies of scale in the cloud\\nthrough a drop in the average costs of these shared resources. However, it is\\nstill an open challenge to achieve a perfect match between resource demand and\\nprovision in autonomous elasticity management. Resource adaptation decisions\\nessentially involve a trade-off between economics and performance, which\\nproduces a gap between the ideal and actual resource provisioning. This gap, if\\nnot properly managed, can negatively impact the aggregate utility of a cloud\\ncustomer in the long run. To address this limitation, we propose a technical\\ndebt-aware learning approach for autonomous elasticity management based on a\\nreinforcement learning of elasticity debts in resource provisioning; the\\nadaptation pursues strategic decisions that trades off economics against\\nperformance. We extend CloudSim and Burlap to evaluate our approach. The\\nevaluation shows that a reinforcement learning of technical debts in elasticity\\nobtains a higher utility for a cloud customer, while conforming expected levels\\nof performance.\\n',\n",
       " \"  This is an exposition of homotopical results on the geometric realization of\\nsemi-simplicial spaces. We then use these to derive basic foundational results\\nabout classifying spaces of topological categories, possibly without units. The\\ntopics considered include: fibrancy conditions on topological categories; the\\neffect on classifying spaces of freely adjoining units; approximate notions of\\nunits; Quillen's Theorems A and B for non-unital topological categories; the\\neffect on classifying spaces of changing the topology on the space of objects;\\nthe Group-Completion Theorem.\\n\",\n",
       " '  Answer Set Programming (ASP) is a well-established declarative paradigm. One\\nof the successes of ASP is the availability of efficient systems.\\nState-of-the-art systems are based on the ground+solve approach. In some\\napplications this approach is infeasible because the grounding of one or few\\nconstraints is expensive. In this paper, we systematically compare alternative\\nstrategies to avoid the instantiation of problematic constraints, that are\\nbased on custom extensions of the solver. Results on real and synthetic\\nbenchmarks highlight some strengths and weaknesses of the different strategies.\\n(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)\\n',\n",
       " \"  The advances in geometric approaches to optical devices due to transformation\\noptics has led to the development of cloaks, concentrators, and other devices.\\nIt has also been shown that transformation optics can be used to gravitational\\nfields from general relativity. However, the technique is currently constrained\\nto linear devices, as a consistent approach to nonlinearity (including both the\\ncase of a nonlinear background medium and a nonlinear transformation) remains\\nan open question. Here we show that nonlinearity can be incorporated into\\ntransformation optics in a consistent way. We use this to illustrate a number\\nof novel effects, including cloaking an optical soliton, modeling nonlinear\\nsolutions to Einstein's field equations, controlling transport in a Debye\\nsolid, and developing a set of constitutive to relations for relativistic\\ncloaks in arbitrary nonlinear backgrounds.\\n\",\n",
       " '  We investigate crack propagation in a simple two-dimensional visco-elastic\\nmodel and find a scaling regime in the relation between the propagation\\nvelocity and energy release rate or fracture energy, together with lower and\\nupper bounds of the scaling regime. On the basis of our result, the existence\\nof the lower and upper bounds is expected to be universal or model-independent:\\nthe present simple simulation model provides generic insight into the physics\\nof crack propagation, and the model will be a first step towards the\\ndevelopment of a more refined coarse-grained model. Relatively abrupt changes\\nof velocity are predicted near the lower and upper bounds for the scaling\\nregime and the positions of the bounds could be good markers for the\\ndevelopment of tough polymers, for which we provide simple views that could be\\nuseful as guiding principles for toughening polymer-based materials.\\n',\n",
       " '  The fundamental group $\\\\pi$ of a Kodaira fibration is, by definition, the\\nextension of a surface group $\\\\Pi_b$ by another surface group $\\\\Pi_g$, i.e. \\\\[\\n1 \\\\rightarrow \\\\Pi_g \\\\rightarrow \\\\pi \\\\rightarrow \\\\Pi_b \\\\rightarrow 1. \\\\]\\nConversely, we can inquire about what conditions need to be satisfied by a\\ngroup of that sort in order to be the fundamental group of a Kodaira fibration.\\nIn this short note we collect some restriction on the image of the classifying\\nmap $m \\\\colon \\\\Pi_b \\\\to \\\\Gamma_g$ in terms of the coinvariant homology of\\n$\\\\Pi_g$. In particular, we observe that if $\\\\pi$ is the fundamental group of a\\nKodaira fibration with relative irregularity $g-s$, then $g \\\\leq 1+ 6s$, and we\\nshow that this effectively constrains the possible choices for $\\\\pi$, namely\\nthat there are group extensions as above that fail to satisfy this bound, hence\\ncannot be the fundamental group of a Kodaira fibration. In particular this\\nprovides examples of symplectic $4$--manifolds that fail to admit a Kähler\\nstructure for reasons that eschew the usual obstructions.\\n',\n",
       " '  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel\\nmaterial are used in a variety of electronics applications. However, a\\ncompetitive CNT-based technology requires the precise placement of CNTs at\\npredefined locations of a substrate. One promising placement approach is to use\\nchemical recognition to bind CNTs from solution at the desired locations on a\\nsurface. Producing the chemical pattern on the substrate is challenging. Here\\nwe describe a one-step patterning approach based on a highly photosensitive\\nsurface monolayer. The monolayer contains chromophopric group as light\\nsensitive body with heteroatoms as high quantum yield photolysis center. As\\ndeposited, the layer will bind CNTs from solution. However, when exposed to\\nultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for\\nconventional photoresists, the monolayer cleaves and no longer binds CNTs.\\nThese features allow standard, wafer-scale UV lithography processes to be used\\nto form a patterned chemical monolayer without the need for complex substrate\\npatterning or monolayer stamping.\\n',\n",
       " '  This paper derives two new optimization-driven Monte Carlo algorithms\\ninspired from variable splitting and data augmentation. In particular, the\\nformulation of one of the proposed approaches is closely related to the\\nalternating direction method of multipliers (ADMM) main steps. The proposed\\nframework enables to derive faster and more efficient sampling schemes than the\\ncurrent state-of-the-art methods and can embed the latter. By sampling\\nefficiently the parameter to infer as well as the hyperparameters of the\\nproblem, the generated samples can be used to approximate Bayesian estimators\\nof the parameters to infer. Additionally, the proposed approach brings\\nconfidence intervals at a low cost contrary to optimization methods.\\nSimulations on two often-studied signal processing problems illustrate the\\nperformance of the two proposed samplers. All results are compared to those\\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\\nsolve these problems.\\n',\n",
       " '  Yes, but only for a parameter value that makes it almost coincide with the\\nstandard model. We reconsider the cosmological dynamics of a generalized\\nChaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a\\ndark energy (DE) component with constant equation of state. This model, which\\nimplies a specific interaction between CDM and DE, has a $\\\\Lambda$CDM limit and\\nprovides the basis for studying deviations from the latter. Including matter\\nand radiation, we use the (modified) CLASS code \\\\cite{class} to construct the\\nCMB and matter power spectra in order to search for a gCg-based concordance\\nmodel that is in agreement with the SNIa data from the JLA sample and with\\nrecent Planck data. The results reveal that the gCg parameter $\\\\alpha$ is\\nrestricted to $|\\\\alpha|\\\\lesssim 0.05$, i.e., to values very close to the\\n$\\\\Lambda$CDM limit $\\\\alpha =0$. This excludes, in particular, models in which\\nDE decays linearly with the Hubble rate.\\n',\n",
       " '  The interest in the extracellular vesicles (EVs) is rapidly growing as they\\nbecame reliable biomarkers for many diseases. For this reason, fast and\\naccurate techniques of EVs size characterization are the matter of utmost\\nimportance. One increasingly popular technique is the Nanoparticle Tracking\\nAnalysis (NTA), in which the diameters of EVs are calculated from their\\ndiffusion constants. The crucial assumption here is that the diffusion in NTA\\nfollows the Stokes-Einstein relation, i.e. that the Mean Square Displacement\\n(MSD) of a particle grows linearly in time (MSD $\\\\propto t$). However, we show\\nthat NTA violates this assumption in both artificial and biological samples,\\ni.e. a large population of particles show a strongly sub-diffusive behaviour\\n(MSD $\\\\propto t^\\\\alpha$, $0<\\\\alpha<1$). To support this observation we present\\na range of experimental results for both polystyrene beads and EVs. This is\\nalso related to another problem: for the same samples there exists a huge\\ndiscrepancy (by the factor of 2-4) between the sizes measured with NTA and with\\nthe direct imaging methods, such as AFM. This can be remedied by e.g. the\\nFinite Track Length Adjustment (FTLA) method in NTA, but its applicability is\\nlimited in the biological and poly-disperse samples. On the other hand, the\\nmodels of sub-diffusion rarely provide the direct relation between the size of\\na particle and the generalized diffusion constant. However, we solve this last\\nproblem by introducing the logarithmic model of sub-diffusion, aimed at\\nretrieving the size data. In result, we propose a novel protocol of NTA data\\nanalysis. The accuracy of our method is on par with FTLA for small\\n($\\\\simeq$200nm) particles. We apply our method to study the EVs samples and\\ncorroborate the results with AFM.\\n',\n",
       " '  The processes of the averaged regression quantiles and of their modifications\\nprovide useful tools in the regression models when the covariates are not fully\\nunder our control. As an application we mention the probabilistic risk\\nassessment in the situation when the return depends on some exogenous\\nvariables. The processes enable to evaluate the expected $\\\\alpha$-shortfall\\n($0\\\\leq\\\\alpha\\\\leq 1$) and other measures of the risk, recently generally\\naccepted in the financial literature, but also help to measure the risk in\\nenvironment analysis and elsewhere.\\n',\n",
       " '  We study primordial perturbations from hyperinflation, proposed recently and\\nbased on a hyperbolic field-space. In the previous work, it was shown that the\\nfield-space angular momentum supported by the negative curvature modifies the\\nbackground dynamics and enhances fluctuations of the scalar fields\\nqualitatively, assuming that the inflationary background is almost de Sitter.\\nIn this work, we confirm and extend the analysis based on the standard approach\\nof cosmological perturbation in multi-field inflation. At the background level,\\nto quantify the deviation from de Sitter, we introduce the slow-varying\\nparameters and show that steep potentials, which usually can not drive\\ninflation, can drive inflation. At the linear perturbation level, we obtain the\\npower spectrum of primordial curvature perturbation and express the spectral\\ntilt and running in terms of the slow-varying parameters. We show that\\nhyperinflation with power-law type potentials has already been excluded by the\\nrecent Planck observations, while exponential-type potential with the exponent\\nof order unity can be made consistent with observations as far as the power\\nspectrum is concerned. We also argue that, in the context of a simple $D$-brane\\ninflation, the hyperinflation requires exponentially large hyperbolic extra\\ndimensions but that masses of Kaluza-Klein gravitons can be kept relatively\\nheavy.\\n',\n",
       " '  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,\\nexhibits interesting semiconductor to metal transition in the temperature range\\nof 530-560 K. The metallic behavior originates because of the reduction of V2O5\\nthrough oxygen vacancies. In the present report, V2O5 nanorods in the\\northorhombic phase with crystal orientation of (001) are grown using vapor\\ntransport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal\\nformula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the\\nformation of metallic phase above the transition temperature is established\\nfrom the temperature-dependent Raman spectroscopic studies. The origin of the\\nmetallic behavior of V2O5 is also understood due to the breakdown of pdpi bond\\nbetween OI and nearest V atom instigated by the formation of vanadyl OI\\nvacancy, confirmed from the downward shift of the bottom most split-off\\nconduction bands in the material with increasing temperature.\\n',\n",
       " '  In this paper, we presented a novel convolutional neural network framework\\nfor graph modeling, with the introduction of two new modules specially designed\\nfor graph-structured data: the $k$-th order convolution operator and the\\nadaptive filtering module. Importantly, our framework of High-order and\\nAdaptive Graph Convolutional Network (HA-GCN) is a general-purposed\\narchitecture that fits various applications on both node and graph centrics, as\\nwell as graph generative models. We conducted extensive experiments on\\ndemonstrating the advantages of our framework. Particularly, our HA-GCN\\noutperforms the state-of-the-art models on node classification and molecule\\nproperty prediction tasks. It also generates 32% more real molecules on the\\nmolecule generation task, both of which will significantly benefit real-world\\napplications such as material design and drug screening.\\n',\n",
       " '  A variety of representation learning approaches have been investigated for\\nreinforcement learning; much less attention, however, has been given to\\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\\nsparse coding representations have been widely used, with non-convex objectives\\nthat result in discriminative representations. In this work, we develop a\\nsupervised sparse coding objective for policy evaluation. Despite the\\nnon-convexity of this objective, we prove that all local minima are global\\nminima, making the approach amenable to simple optimization strategies. We\\nempirically show that it is key to use a supervised objective, rather than the\\nmore straightforward unsupervised sparse coding approach. We compare the\\nlearned representations to a canonical fixed sparse representation, called\\ntile-coding, demonstrating that the sparse coding representation outperforms a\\nwide variety of tilecoding representations.\\n',\n",
       " \"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove\\nthat if a Riemannian manifold has Ricci curvature bounded below in a metric\\nball which moreover has almost maximal volume, then in a smaller ball (in a\\nquantified sense) it holds an almost-euclidean isoperimetric inequality. The\\nresult is actually established in the more general framework of non-smooth\\nspaces satisfying local Ricci curvature lower bounds in a synthetic sense via\\noptimal transportation.\\n\",\n",
       " '  We bound an exponential sum that appears in the study of irregularities of\\ndistribution (the low-frequency Fourier energy of the sum of several Dirac\\nmeasures) by geometric quantities: a special case is that for all $\\\\left\\\\{ x_1,\\n\\\\dots, x_N\\\\right\\\\} \\\\subset \\\\mathbb{T}^2$, $X \\\\geq 1$ and a universal $c>0$ $$\\n\\\\sum_{i,j=1}^{N}{ \\\\frac{X^2}{1 + X^4 \\\\|x_i -x_j\\\\|^4}} \\\\lesssim \\\\sum_{k \\\\in\\n\\\\mathbb{Z}^2 \\\\atop \\\\|k\\\\| \\\\leq X}{ \\\\left| \\\\sum_{n=1}^{N}{ e^{2 \\\\pi i\\n\\\\left\\\\langle k, x_n \\\\right\\\\rangle}}\\\\right|^2} \\\\lesssim \\\\sum_{i,j=1}^{N}{ X^2\\ne^{-c X^2\\\\|x_i -x_j\\\\|^2}}.$$ Since this exponential sum is intimately tied to\\nrather subtle distribution properties of the points, we obtain nonlocal\\nstructural statements for near-minimizers of the Riesz-type energy. In the\\nregime $X \\\\gtrsim N^{1/2}$ both upper and lower bound match for\\nmaximally-separated point sets satisfying $\\\\|x_i -x_j\\\\| \\\\gtrsim N^{-1/2}$.\\n',\n",
       " '  We investigate the effect of dimensional crossover in the ground state of the\\nantiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular\\nlattice that interpolates between the regime of weakly coupled Haldane chains\\n($J^{\\\\prime}\\\\! \\\\!\\\\ll\\\\!\\\\! J$) and the isotropic triangular lattice\\n($J^{\\\\prime}\\\\!\\\\!=\\\\!\\\\!J$). We use the density-matrix renormalization group\\n(DMRG) and Schwinger boson theory performed at the Gaussian correction level\\nabove the saddle-point solution. Our DMRG results show an abrupt transition\\nbetween decoupled spin chains and the spirally ordered regime at\\n$(J^{\\\\prime}/J)_c\\\\sim 0.42$, signaled by the sudden closing of the spin gap.\\nComing from the magnetically ordered side, the computation of the spin\\nstiffness within Schwinger boson theory predicts the instability of the spiral\\nmagnetic order toward a magnetically disordered phase with one-dimensional\\nfeatures at $(J^{\\\\prime}/J)_c \\\\sim 0.43$. The agreement of these complementary\\nmethods, along with the strong difference found between the intra- and the\\ninterchain DMRG short spin-spin correlations; for sufficiently large values of\\nthe interchain coupling, suggests that the interplay between the quantum\\nfluctuations and the dimensional crossover effects gives rise to the\\none-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.\\n',\n",
       " \"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\\noverwritten by new incoming information while important, frequently used\\nknowledge is prevented from being erased. In artificial learning systems,\\nlifelong learning so far has focused mainly on accumulating knowledge over\\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\\ngiven the limited model capacity and the unlimited new information to be\\nlearned, knowledge has to be preserved or erased selectively. Inspired by\\nneuroplasticity, we propose a novel approach for lifelong learning, coined\\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\\nneural network in an unsupervised and online manner. Given a new sample which\\nis fed to the network, MAS accumulates an importance measure for each parameter\\nof the network, based on how sensitive the predicted output function is to a\\nchange in this parameter. When learning a new task, changes to important\\nparameters can then be penalized, effectively preventing important knowledge\\nrelated to previous tasks from being overwritten. Further, we show an\\ninteresting connection between a local version of our method and Hebb's\\nrule,which is a model for the learning process in the brain. We test our method\\non a sequence of object recognition tasks and on the challenging problem of\\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\\nWe show state-of-the-art performance and, for the first time, the ability to\\nadapt the importance of the parameters based on unlabeled data towards what the\\nnetwork needs (not) to forget, which may vary depending on test conditions.\\n\",\n",
       " \"  In this paper, we study the generalized polynomial chaos (gPC) based\\nstochastic Galerkin method for the linear semiconductor Boltzmann equation\\nunder diffusive scaling and with random inputs from an anisotropic collision\\nkernel and the random initial condition. While the numerical scheme and the\\nproof of uniform-in-Knudsen-number regularity of the distribution function in\\nthe random space has been introduced in [Jin-Liu-16'], the main goal of this\\npaper is to first obtain a sharper estimate on the regularity of the\\nsolution-an exponential decay towards its local equilibrium, which then lead to\\nthe uniform spectral convergence of the stochastic Galerkin method for the\\nproblem under study.\\n\",\n",
       " '  Over the last decade, wireless networks have experienced an impressive growth\\nand now play a main role in many telecommunications systems. As a consequence,\\nscarce radio resources, such as frequencies, became congested and the need for\\neffective and efficient assignment methods arose. In this work, we present a\\nGenetic Algorithm for solving large instances of the Power, Frequency and\\nModulation Assignment Problem, arising in the design of wireless networks. To\\nour best knowledge, this is the first Genetic Algorithm that is proposed for\\nsuch problem. Compared to previous works, our approach allows a wider\\nexploration of the set of power solutions, while eliminating sources of\\nnumerical problems. The performance of the algorithm is assessed by tests over\\na set of large realistic instances of a Fixed WiMAX Network.\\n',\n",
       " '  We report on a combined study of the de Haas-van Alphen effect and angle\\nresolved photoemission spectroscopy on single crystals of the metallic\\ndelafossite PdRhO$_2$ rounded off by \\\\textit{ab initio} band structure\\ncalculations. A high sensitivity torque magnetometry setup with SQUID readout\\nand synchrotron-based photoemission with a light spot size of\\n$~50\\\\,\\\\mu\\\\mathrm{m}$ enabled high resolution data to be obtained from samples\\nas small as $150\\\\times100\\\\times20\\\\,(\\\\mu\\\\mathrm{m})^3$. The Fermi surface shape\\nis nearly cylindrical with a rounded hexagonal cross section enclosing a\\nLuttinger volume of 1.00(1) electrons per formula unit.\\n',\n",
       " \"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for\\nexponential integrals of bounded measurable functions in terms of Rényi\\ndivergences. We develop a variational characterization of the Rényi\\ndivergences between two probability distributions on a measurable sace in terms\\nof relative entropies. When combined with the elementary variational formula\\nfor exponential integrals of bounded measurable functions in terms of relative\\nentropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a\\ncorollary. We also develop an analogous variational characterization of the\\nRényi divergence rates between two stationary finite state Markov chains in\\nterms of relative entropy rates. When combined with Varadhan's variational\\ncharacterization of the spectral radius of square matrices with nonnegative\\nentries in terms of relative entropy, this yields an analog of the variational\\nformula of Atar, Chowdary and Dupuis in the framework of finite state Markov\\nchains.\\n\",\n",
       " '  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2\\nhave attracted much attention recently, particularly because of their type II\\nband alignments and the formation of interlayer exciton as the lowest-energy\\nexcitonic state. In this work, we calculate the electronic and optical\\nproperties of such heterostructures with the first-principles GW+Bethe-Salpeter\\nEquation (BSE) method and reveal the important role of interlayer coupling in\\ndeciding the excited-state properties, including the band alignment and\\nexcitonic properties. Our calculation shows that due to the interlayer\\ncoupling, the low energy excitons can be widely tunable by a vertical gate\\nfield. In particular, the dipole oscillator strength and radiative lifetime of\\nthe lowest energy exciton in these bilayer heterostructures is varied by over\\nan order of magnitude within a practical external gate field. We also build a\\nsimple model that captures the essential physics behind this tunability and\\nallows the extension of the ab initio results to a large range of electric\\nfields. Our work clarifies the physical picture of interlayer excitons in\\nbilayer vdW heterostructures and predicts a wide range of gate-tunable\\nexcited-state properties of 2D optoelectronic devices.\\n',\n",
       " \"  We construct the algebraic cobordism theory of bundles and divisors on\\nvarieties. It has a simple basis (over Q) from projective spaces and its rank\\nis equal to the number of Chern numbers. An application of this algebraic\\ncobordism theory is the enumeration of singular subvarieties with give tangent\\nconditions with a fixed smooth divisor, where the subvariety is the zero locus\\nof a section of a vector bundle. We prove that the generating series of numbers\\nof such subvarieties gives a homomorphism from the algebraic cobordism group to\\nthe power series ring. This implies that the enumeration of singular\\nsubvarieties with tangency conditions is governed by universal polynomials of\\nChern numbers, when the vector bundle is sufficiently ample. This result\\ncombines and generalizes the Caporaso-Harris recursive formula, Gottsche's\\nconjecture, classical De Jonquiere's Formula and node polynomials from tropical\\ngeometry.\\n\",\n",
       " \"  People with profound motor deficits could perform useful physical tasks for\\nthemselves by controlling robots that are comparable to the human body. Whether\\nthis is possible without invasive interfaces has been unclear, due to the\\nrobot's complexity and the person's limitations. We developed a novel,\\naugmented reality interface and conducted two studies to evaluate the extent to\\nwhich it enabled people with profound motor deficits to control robotic body\\nsurrogates. 15 novice users achieved meaningful improvements on a clinical\\nmanipulation assessment when controlling the robot in Atlanta from locations\\nacross the United States. Also, one expert user performed 59 distinct tasks in\\nhis own home over seven days, including self-care tasks such as feeding. Our\\nresults demonstrate that people with profound motor deficits can effectively\\ncontrol robotic body surrogates without invasive interfaces.\\n\",\n",
       " '  Object detection in wide area motion imagery (WAMI) has drawn the attention\\nof the computer vision research community for a number of years. WAMI proposes\\na number of unique challenges including extremely small object sizes, both\\nsparse and densely-packed objects, and extremely large search spaces (large\\nvideo frames). Nearly all state-of-the-art methods in WAMI object detection\\nreport that appearance-based classifiers fail in this challenging data and\\ninstead rely almost entirely on motion information in the form of background\\nsubtraction or frame-differencing. In this work, we experimentally verify the\\nfailure of appearance-based classifiers in WAMI, such as Faster R-CNN and a\\nheatmap-based fully convolutional neural network (CNN), and propose a novel\\ntwo-stage spatio-temporal CNN which effectively and efficiently combines both\\nappearance and motion information to significantly surpass the state-of-the-art\\nin WAMI object detection. To reduce the large search space, the first stage\\n(ClusterNet) takes in a set of extremely large video frames, combines the\\nmotion and appearance information within the convolutional architecture, and\\nproposes regions of objects of interest (ROOBI). These ROOBI can contain from\\none to clusters of several hundred objects due to the large video frame size\\nand varying object density in WAMI. The second stage (FoveaNet) then estimates\\nthe centroid location of all objects in that given ROOBI simultaneously via\\nheatmap estimation. The proposed method exceeds state-of-the-art results on the\\nWPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped\\nobjects, as well as being the first proposed method in wide area motion imagery\\nto detect completely stationary objects.\\n',\n",
       " '  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial\\nintelligence (e.g., the game of Go), is a well-known strategy for constructing\\napproximate solutions to sequential decision problems. Its primary innovation\\nis the use of a heuristic, known as a default policy, to obtain Monte Carlo\\nestimates of downstream values for states in a decision tree. This information\\nis used to iteratively expand the tree towards regions of states and actions\\nthat an optimal policy might visit. However, to guarantee convergence to the\\noptimal action, MCTS requires the entire tree to be expanded asymptotically. In\\nthis paper, we propose a new technique called Primal-Dual MCTS that utilizes\\nsampled information relaxation upper bounds on potential actions, creating the\\npossibility of \"ignoring\" parts of the tree that stem from highly suboptimal\\nchoices. This allows us to prove that despite converging to a partial decision\\ntree in the limit, the recommended action from Primal-Dual MCTS is optimal. The\\nnew approach shows significant promise when used to optimize the behavior of a\\nsingle driver navigating a graph while operating on a ride-sharing platform.\\nNumerical experiments on a real dataset of 7,000 trips in New Jersey suggest\\nthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decision\\ntrees and exhibits a reduced sensitivity to the size of the action space.\\n',\n",
       " '  We study the Fermi-edge singularity, describing the response of a degenerate\\nelectron system to optical excitation, in the framework of the functional\\nrenormalization group (fRG). Results for the (interband) particle-hole\\nsusceptibility from various implementations of fRG (one- and two-\\nparticle-irreducible, multi-channel Hubbard-Stratonovich, flowing\\nsusceptibility) are compared to the summation of all leading logarithmic (log)\\ndiagrams, achieved by a (first-order) solution of the parquet equations. For\\nthe (zero-dimensional) special case of the X-ray-edge singularity, we show that\\nthe leading log formula can be analytically reproduced in a consistent way from\\na truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic\\nstructure, we show that this derivation relies on fortuitous partial\\ncancellations special to the form of and accuracy applied to the X-ray-edge\\nsingularity and does not generalize.\\n',\n",
       " '  Retrosynthesis is a technique to plan the chemical synthesis of organic\\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\\nsearch tree is built by analysing molecules recursively and dissecting them\\ninto simpler molecular building blocks until one obtains a set of known\\nbuilding blocks. The search space is intractably large, and it is difficult to\\ndetermine the value of retrosynthetic positions. Here, we propose to model\\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\\nNetwork policy learned from essentially the complete published knowledge of\\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\\nexploratory studies, we demonstrate that MCTS with neural network policies\\noutperforms the traditionally used best-first search with hand-coded\\nheuristics.\\n',\n",
       " '  The class of stochastically self-similar sets contains many famous examples\\nof random sets, e.g. Mandelbrot percolation and general fractal percolation.\\nUnder the assumption of the uniform open set condition and some mild\\nassumptions on the iterated function systems used, we show that the\\nquasi-Assouad dimension of self-similar random recursive sets is almost surely\\nequal to the almost sure Hausdorff dimension of the set. We further comment on\\nrandom homogeneous and $V$-variable sets and the removal of overlap conditions.\\n',\n",
       " '  We report on the influence of spin-orbit coupling (SOC) in the Fe-based\\nsuperconductors (FeSCs) via application of circularly-polarized spin and\\nangle-resolved photoemission spectroscopy. We combine this technique in\\nrepresentative members of both the Fe-pnictides and Fe-chalcogenides with ab\\ninitio density functional theory and tight-binding calculations to establish an\\nubiquitous modification of the electronic structure in these materials imbued\\nby SOC. The influence of SOC is found to be concentrated on the hole pockets\\nwhere the superconducting gap is generally found to be largest. This result\\ncontests descriptions of superconductivity in these materials in terms of pure\\nspin-singlet eigenstates, raising questions regarding the possible pairing\\nmechanisms and role of SOC therein.\\n',\n",
       " '  In this work we examine how the updates addressing Meltdown and Spectre\\nvulnerabilities impact the performance of HPC applications. To study this we\\nuse the application kernel module of XDMoD to test the performance before and\\nafter the application of the vulnerability patches. We tested the performance\\ndifference for multiple application and benchmarks including: NWChem, NAMD,\\nHPCC, IOR, MDTest and IMB. The results show that although some specific\\nfunctions can have performance decreased by as much as 74%, the majority of\\nindividual metrics indicates little to no decrease in performance. The\\nreal-world applications show a 2-3% decrease in performance for single node\\njobs and a 5-11% decrease for parallel multi node jobs.\\n',\n",
       " '  Gene regulatory networks are powerful abstractions of biological systems.\\nSince the advent of high-throughput measurement technologies in biology in the\\nlate 90s, reconstructing the structure of such networks has been a central\\ncomputational problem in systems biology. While the problem is certainly not\\nsolved in its entirety, considerable progress has been made in the last two\\ndecades, with mature tools now available. This chapter aims to provide an\\nintroduction to the basic concepts underpinning network inference tools,\\nattempting a categorisation which highlights commonalities and relative\\nstrengths. While the chapter is meant to be self-contained, the material\\npresented should provide a useful background to the later, more specialised\\nchapters of this book.\\n',\n",
       " '  Glaucoma is the second leading cause of blindness all over the world, with\\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\\ntime, glaucoma causes irreversible damage to the optic nerve leading to\\nblindness. The optic nerve head examination, which involves measurement of\\ncup-to-disc ratio, is considered one of the most valuable methods of structural\\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\\nof optic disc and optic cup on eye fundus images and can be performed by modern\\ncomputer vision algorithms. This work presents universal approach for automatic\\noptic disc and cup segmentation, which is based on deep learning, namely,\\nmodification of U-Net convolutional neural network. Our experiments include\\ncomparison with the best known methods on publicly available databases\\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\\nour method achieves quality comparable to current state-of-the-art methods,\\noutperforming them in terms of the prediction time.\\n',\n",
       " '  The life of the modern world essentially depends on the work of the large\\nartificial homogeneous networks, such as wired and wireless communication\\nsystems, networks of roads and pipelines. The support of their effective\\ncontinuous functioning requires automatic screening and permanent optimization\\nwith processing of the huge amount of data by high-performance distributed\\nsystems. We propose new meta-algorithm of large homogeneous network analysis,\\nits decomposition into alternative sets of loosely connected subnets, and\\nparallel optimization of the most independent elements. This algorithm is based\\non a network-specific correlation function, Simulated Annealing technique, and\\nis adapted to work in the computer cluster. On the example of large wireless\\nnetwork, we show that proposed algorithm essentially increases speed of\\nparallel optimization. The elaborated general approach can be used for analysis\\nand optimization of the wide range of networks, including such specific types\\nas artificial neural networks or organized in networks physiological systems of\\nliving organisms.\\n',\n",
       " '  This paper considers the actor-critic contextual bandit for the mobile health\\n(mHealth) intervention. The state-of-the-art decision-making methods in mHealth\\ngenerally assume that the noise in the dynamic system follows the Gaussian\\ndistribution. Those methods use the least-square-based algorithm to estimate\\nthe expected reward, which is prone to the existence of outliers. To deal with\\nthe issue of outliers, we propose a novel robust actor-critic contextual bandit\\nmethod for the mHealth intervention. In the critic updating, the\\ncapped-$\\\\ell_{2}$ norm is used to measure the approximation error, which\\nprevents outliers from dominating our objective. A set of weights could be\\nachieved from the critic updating. Considering them gives a weighted objective\\nfor the actor updating. It provides the badly noised sample in the critic\\nupdating with zero weights for the actor updating. As a result, the robustness\\nof both actor-critic updating is enhanced. There is a key parameter in the\\ncapped-$\\\\ell_{2}$ norm. We provide a reliable method to properly set it by\\nmaking use of one of the most fundamental definitions of outliers in\\nstatistics. Extensive experiment results demonstrate that our method can\\nachieve almost identical results compared with the state-of-the-art methods on\\nthe dataset without outliers and dramatically outperform them on the datasets\\nnoised by outliers.\\n',\n",
       " \"  In 1933 Kolmogorov constructed a general theory that defines the modern\\nconcept of conditional expectation. In 1955 Renyi fomulated a new axiomatic\\ntheory for probability motivated by the need to include unbounded measures. We\\nintroduce a general concept of conditional expectation in Renyi spaces. In this\\ntheory improper priors are allowed, and the resulting posterior can also be\\nimproper.\\nIn 1965 Lindley published his classic text on Bayesian statistics using the\\ntheory of Renyi, but retracted this idea in 1973 due to the appearance of\\nmarginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes\\nare investigated, and the seemingly conflicting results are explained. The\\ntheory of Renyi can hence be used as an axiomatic basis for statistics that\\nallows use of unbounded priors.\\nKeywords: Haldane's prior; Poisson intensity; Marginalization paradox;\\nMeasure theory; conditional probability space; axioms for statistics;\\nconditioning on a sigma field; improper prior\\n\",\n",
       " '  Recently a new fault tolerant and simple mechanism was designed for solving\\ncommit consensus problem. It is based on replicated validation of messages sent\\nbetween transaction participants and a special dispatcher validator manager\\nnode. This paper presents a correctness, safety proofs and performance analysis\\nof this algorithm.\\n',\n",
       " '  This work presents a new method to quantify connectivity in transportation\\nnetworks. Inspired by the field of topological data analysis, we propose a\\nnovel approach to explore the robustness of road network connectivity in the\\npresence of congestion on the roadway. The robustness of the pattern is\\nsummarized in a congestion barcode, which can be constructed directly from\\ntraffic datasets commonly used for navigation. As an initial demonstration, we\\nillustrate the main technique on a publicly available traffic dataset in a\\nneighborhood in New York City.\\n',\n",
       " \"  The first transiting planetesimal orbiting a white dwarf was recently\\ndetected in K2 data of WD1145+017 and has been followed up intensively. The\\nmultiple, long, and variable transits suggest the transiting objects are dust\\nclouds, probably produced by a disintegrating asteroid. In addition, the system\\ncontains circumstellar gas, evident by broad absorption lines, mostly in the\\nu'-band, and a dust disc, indicated by an infrared excess. Here we present the\\nfirst detection of a change in colour of WD1145+017 during transits, using\\nsimultaneous multi-band fast-photometry ULTRACAM measurements over the\\nu'g'r'i'-bands. The observations reveal what appears to be 'bluing' during\\ntransits; transits are deeper in the redder bands, with a u'-r' colour\\ndifference of up to ~-0.05 mag. We explore various possible explanations for\\nthe bluing. 'Spectral' photometry obtained by integrating over bandpasses in\\nthe spectroscopic data in- and out-of-transit, compared to the photometric\\ndata, shows that the observed colour difference is most likely the result of\\nreduced circumstellar absorption in the spectrum during transits. This\\nindicates that the transiting objects and the gas share the same line-of-sight,\\nand that the gas covers the white dwarf only partially, as would be expected if\\nthe gas, the transiting debris, and the dust emitting the infrared excess, are\\npart of the same general disc structure (although possibly at different radii).\\nIn addition, we present the results of a week-long monitoring campaign of the\\nsystem.\\n\",\n",
       " '  In this review article, we discuss recent studies on drops and bubbles in\\nHele-Shaw cells, focusing on how scaling laws exhibit crossovers from the\\nthree-dimensional counterparts and focusing on topics in which viscosity plays\\nan important role. By virtue of progresses in analytical theory and high-speed\\nimaging, dynamics of drops and bubbles have actively been studied with the aid\\nof scaling arguments. However, compared with three dimensional problems,\\nstudies on the corresponding problems in Hele-Shaw cells are still limited.\\nThis review demonstrates that the effect of confinement in the Hele-Shaw cell\\nintroduces new physics allowing different scaling regimes to appear. For this\\npurpose, we discuss various examples that are potentially important for\\nindustrial applications handling drops and bubbles in confined spaces by\\nshowing agreement between experiments and scaling theories. As a result, this\\nreview provides a collection of problems in hydrodynamics that may be\\nanalytically solved or that may be worth studying numerically in the near\\nfuture.\\n',\n",
       " '  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural\\nnetwork (DNN) resemblance in terms of its very deep, feedforward network\\narchitecture. The typical S-DNN aggregates a variable number of individually\\nlearnable modules in series to assemble a DNN-alike alternative to the targeted\\nobject recognition tasks. This work likewise devises an S-DNN instantiation,\\ndubbed deep analytic network (DAN), on top of the spectral histogram (SH)\\nfeatures. The DAN learning principle relies on ridge regression, and some key\\nDNN constituents, specifically, rectified linear unit, fine-tuning, and\\nnormalization. The DAN aptitude is scrutinized on three repositories of varying\\ndomains, including FERET (faces), MNIST (handwritten digits), and CIFAR10\\n(natural objects). The empirical results unveil that DAN escalates the SH\\nbaseline performance over a sufficiently deep layer.\\n',\n",
       " '  In spite of Anderson\\'s theorem, disorder is known to affect superconductivity\\nin conventional s-wave superconductors. In most superconductors, the degree of\\ndisorder is fixed during sample preparation. Here we report measurements of the\\nsuperconducting properties of the two-dimensional gas that forms at the\\ninterface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal\\norientation, a system that permits \\\\emph{in situ} tuning of carrier density and\\ndisorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO\\ninterface, superconductivity at the (111) LAO/STO interface can be tuned by\\n$V_g$. In contrast to the (001) interface, superconductivity in these (111)\\nsamples is anisotropic, being different along different interface crystal\\ndirections, consistent with the strong anisotropy already observed other\\ntransport properties at the (111) LAO/STO interface. In addition, we find that\\nthe (111) interface samples \"remember\" the backgate voltage $V_F$ at which they\\nare cooled at temperatures near the superconducting transition temperature\\n$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low\\nenergy scale and other characteristics of this memory effect ($<1$ K)\\ndistinguish it from charge-trapping effects previously observed in (001)\\ninterface samples.\\n',\n",
       " '  We investigate beam loading and emittance preservation for a high-charge\\nelectron beam being accelerated in quasi-linear plasma wakefields driven by a\\nshort proton beam. The structure of the studied wakefields are similar to those\\nof a long, modulated proton beam, such as the AWAKE proton driver. We show that\\nby properly choosing the electron beam parameters and exploiting two well known\\neffects, beam loading of the wakefield and full blow out of plasma electrons by\\nthe accelerated beam, the electron beam can gain large amounts of energy with a\\nnarrow final energy spread (%-level) and without significant emittance growth.\\n',\n",
       " '  In this paper, we propose a practical receiver for multicarrier signals\\nsubjected to a strong memoryless nonlinearity. The receiver design is based on\\na generalized approximate message passing (GAMP) framework, and this allows\\nreal-time algorithm implementation in software or hardware with moderate\\ncomplexity. We demonstrate that the proposed receiver can provide more than a\\n2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range\\n$10^{-4}\\\\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to\\nclipping nonlinearity and the crest-factor of the clipped waveform is only\\n1.9dB. Simulation results also demonstrate that the proposed receiver provides\\nsignificant performance gain in frequency-selective multipath channels\\n',\n",
       " '  According to astrophysical observations value of recession velocity in a\\ncertain point is proportional to a distance to this point. The proportionality\\ncoefficient is the Hubble constant measured with 5% accuracy. It is used in\\nmany cosmological theories describing dark energy, dark matter, baryons, and\\ntheir relation with the cosmological constant introduced by Einstein.\\nIn the present work we have determined a limit value of the global Hubble\\nconstant (in a big distance from a point of observations) theoretically without\\nusing any empirical constants on the base of our own fractal model used for the\\ndescription a relation between distance to an observed galaxy and coordinate of\\nits center. The distance has been defined as a nonlinear fractal measure with\\nscale of measurement corresponding to a deviation of the measure from its fixed\\nvalue (zero-gravity radius). We have suggested a model of specific anisotropic\\nfractal for simulation a radial Universe expansion. Our theoretical results\\nhave shown existence of an inverse proportionality between accuracy of\\ndetermination the Hubble constant and accuracy of calculation a coordinates of\\ngalaxies leading to ambiguity results obtained at cosmological observations.\\n',\n",
       " '  Dynamic languages often employ reflection primitives to turn dynamically\\ngenerated text into executable code at run-time. These features make standard\\nstatic analysis extremely hard if not impossible because its essential data\\nstructures, i.e., the control-flow graph and the system of recursive equations\\nassociated with the program to analyse, are themselves dynamically mutating\\nobjects. We introduce SEA, an abstract interpreter for automatic sound string\\nexecutability analysis of dynamic languages employing bounded (i.e, finitely\\nnested) reflection and dynamic code generation. Strings are statically\\napproximated in an abstract domain of finite state automata with basic\\noperations implemented as symbolic transducers. SEA combines standard program\\nanalysis together with string executability analysis. The analysis of a call to\\nreflection determines a call to the same abstract interpreter over a code which\\nis synthesised directly from the result of the static string executability\\nanalysis at that program point. The use of regular languages for approximating\\ndynamically generated code structures allows SEA to soundly approximate safety\\nproperties of self modifying programs yet maintaining efficiency. Soundness\\nhere means that the semantics of the code synthesised by the analyser to\\nresolve reflection over-approximates the semantics of the code dynamically\\nbuilt at run-rime by the program at that point.\\n',\n",
       " '  Reductions for transition systems have been recently introduced as a uniform\\nand principled method for comparing the expressiveness of system models with\\nrespect to a range of properties, especially bisimulations. In this paper we\\nstudy the expressiveness (w.r.t. bisimulations) of models for quantitative\\ncomputations such as weighted labelled transition systems (WLTSs), uniform\\nlabelled transition systems (ULTraSs), and state-to-function transition systems\\n(FuTSs). We prove that there is a trade-off between labels and weights: at one\\nextreme lays the class of (unlabelled) weighted transition systems where\\ninformation is presented using weights only; at the other lays the class of\\nlabelled transition systems (LTSs) where information is shifted on labels.\\nThese categories of systems cannot be further reduced in any significant way\\nand subsume all the aforementioned models.\\n',\n",
       " \"  Poynting's theorem is used to obtain an expression for the turbulent\\npower-spectral density as function of frequency and wavenumber in low-frequency\\nmagnetic turbulence. No reference is made to Elsasser variables as is usually\\ndone in magnetohydrodynamic turbulence mixing mechanical and electromagnetic\\nturbulence. We rather stay with an implicit form of the mechanical part of\\nturbulence as suggested by electromagnetic theory in arbitrary media. All of\\nmechanics and flows is included into a turbulent response function which by\\nappropriate observations can be determined from knowledge of the turbulent\\nfluctuation spectra. This approach is not guided by the wish of developing a\\ncomplete theory of turbulence. It aims on the identification of the response\\nfunction from observations as input into a theory which afterwards attempts its\\ninterpretation. Combination of both the magnetic and electric power spectral\\ndensities leads to a representation of the turbulent response function, i.e.\\nthe turbulent conductivity spectrum $\\\\sigma_{\\\\omega k}$ as function of\\nfrequency $\\\\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to\\nelectric power spectral densities in frequency space. This knowledge allows for\\nformally writing down a turbulent dispersion relation. Power law inertial range\\nspectra result in a power law turbulent conductivity spectrum. These can be\\ncompared with observations in the solar wind. Keywords: MHD turbulence,\\nturbulent dispersion relation, turbulent response function, solar wind\\nturbulence\\n\",\n",
       " '  Let M be a compact Riemannian manifold and let $\\\\mu$,d be the associated\\nmeasure and distance on M. Robert McCann obtained, generalizing results for the\\nEuclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->\\nM pushing forward $\\\\mu$ to a measure $\\\\nu$: each S factors uniquely a.e. into\\nthe composition S = T \\\\circ U, where U : M -> M is volume preserving and T : M\\n-> M is the optimal map transporting $\\\\mu$ to $\\\\nu$ with respect to the cost\\nfunction d^2/2.\\nIn this article we study the polar factorization of conformal and projective\\nmaps of the sphere S^n. For conformal maps, which may be identified with\\nelements of the identity component of O(1,n+1), we prove that the polar\\nfactorization in the sense of optimal mass transport coincides with the\\nalgebraic polar factorization (Cartan decomposition) of this Lie group. For the\\nprojective case, where the group GL_+(n+1) is involved, we find necessary and\\nsufficient conditions for these two factorizations to agree.\\n',\n",
       " '  We examine the representation of numbers as the sum of two squares in\\n$\\\\mathbb{Z}_n$ for a general positive integer $n$. Using this information we\\nmake some comments about the density of positive integers which can be\\nrepresented as the sum of two squares and powers of $2$ in $\\\\mathbb{N}$.\\n',\n",
       " \"  Regression for spatially dependent outcomes poses many challenges, for\\ninference and for computation. Non-spatial models and traditional spatial\\nmixed-effects models each have their advantages and disadvantages, making it\\ndifficult for practitioners to determine how to carry out a spatial regression\\nanalysis. We discuss the data-generating mechanisms implicitly assumed by\\nvarious popular spatial regression models, and discuss the implications of\\nthese assumptions. We propose Bayesian spatial filtering as an approximate\\nmiddle way between non-spatial models and traditional spatial mixed models. We\\nshow by simulation that our Bayesian spatial filtering model has several\\ndesirable properties and hence may be a useful addition to a spatial\\nstatistician's toolkit.\\n\",\n",
       " '  One of the most important parameters in ionospheric plasma research also\\nhaving a wide practical application in wireless satellite telecommunications is\\nthe total electron content (TEC) representing the columnal electron number\\ndensity. The F region with high electron density provides the biggest\\ncontribution to TEC while the relatively weakly ionized plasma of the D region\\n(60 km - 90 km above Earths surface) is often considered as a negligible cause\\nof satellite signal disturbances. However, sudden intensive ionization\\nprocesses like those induced by solar X ray flares can cause relative increases\\nof electron density that are significantly larger in the D-region than in\\nregions at higher altitudes. Therefore, one cannot exclude a priori the D\\nregion from investigations of ionospheric influences on propagation of\\nelectromagnetic signals emitted by satellites. We discuss here this problem\\nwhich has not been sufficiently treated in literature so far. The obtained\\nresults are based on data collected from the D region monitoring by very low\\nfrequency radio waves and on vertical TEC calculations from the Global\\nNavigation Satellite System (GNSS) signal analyses, and they show noticeable\\nvariations in the D region electron content (TECD) during activity of a solar X\\nray flare (it rises by a factor of 136 in the considered case) when TECD\\ncontribution to TEC can reach several percent and which cannot be neglected in\\npractical applications like global positioning procedures by satellites.\\n',\n",
       " '  For the particles undergoing the anomalous diffusion with different waiting\\ntime distributions for different internal states, we derive the Fokker-Planck\\nand Feymann-Kac equations, respectively, describing positions of the particles\\nand functional distributions of the trajectories of particles; in particular,\\nthe equations governing the functional distribution of internal states are also\\nobtained. The dynamics of the stochastic processes are analyzed and the\\napplications, calculating the distribution of the first passage time and the\\ndistribution of the fraction of the occupation time, of the equations are\\ngiven.\\n',\n",
       " '  Stabilizing the magnetic signal of single adatoms is a crucial step towards\\ntheir successful usage in widespread technological applications such as\\nhigh-density magnetic data storage devices. The quantum mechanical nature of\\nthese tiny objects, however, introduces intrinsic zero-point spin-fluctuations\\nthat tend to destabilize the local magnetic moment of interest by dwindling the\\nmagnetic anisotropy potential barrier even at absolute zero temperature. Here,\\nwe elucidate the origins and quantify the effect of the fundamental ingredients\\ndetermining the magnitude of the fluctuations, namely the ($i$) local magnetic\\nmoment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner\\nexcitations. Based on a systematic first-principles study of 3d and 4d adatoms,\\nwe demonstrate that the transverse contribution of the fluctuations is\\ncomparable in size to the magnetic moment itself, leading to a remarkable\\n$\\\\gtrsim$50$\\\\%$ reduction of the magnetic anisotropy energy. Our analysis gives\\nrise to a comprehensible diagram relating the fluctuation magnitude to\\ncharacteristic features of adatoms, providing practical guidelines for\\ndesigning magnetically stable nanomagnets with minimal quantum fluctuations.\\n',\n",
       " '  We study a minimal model for the growth of a phenotypically heterogeneous\\npopulation of cells subject to a fluctuating environment in which they can\\nreplicate (by exploiting available resources) and modify their phenotype within\\na given landscape (thereby exploring novel configurations). The model displays\\nan exploration-exploitation trade-off whose specifics depend on the statistics\\nof the environment. Most notably, the phenotypic distribution corresponding to\\nmaximum population fitness (i.e. growth rate) requires a non-zero exploration\\nrate when the magnitude of environmental fluctuations changes randomly over\\ntime, while a purely exploitative strategy turns out to be optimal in two-state\\nenvironments, independently of the statistics of switching times. We obtain\\nanalytical insight into the limiting cases of very fast and very slow\\nexploration rates by directly linking population growth to the features of the\\nenvironment.\\n',\n",
       " '  Electronic Health Records (EHR) are data generated during routine clinical\\ncare. EHR offer researchers unprecedented phenotypic breadth and depth and have\\nthe potential to accelerate the pace of precision medicine at scale. A main EHR\\nuse-case is creating phenotyping algorithms to define disease status, onset and\\nseverity. Currently, no common machine-readable standard exists for defining\\nphenotyping algorithms which often are stored in human-readable formats. As a\\nresult, the translation of algorithms to implementation code is challenging and\\nsharing across the scientific community is problematic. In this paper, we\\nevaluate openEHR, a formal EHR data specification, for computable\\nrepresentations of EHR phenotyping algorithms.\\n',\n",
       " '  Mission critical data dissemination in massive Internet of things (IoT)\\nnetworks imposes constraints on the message transfer delay between devices. Due\\nto low power and communication range of IoT devices, data is foreseen to be\\nrelayed over multiple device-to-device (D2D) links before reaching the\\ndestination. The coexistence of a massive number of IoT devices poses a\\nchallenge in maximizing the successful transmission capacity of the overall\\nnetwork alongside reducing the multi-hop transmission delay in order to support\\nmission critical applications. There is a delicate interplay between the\\ncarrier sensing threshold of the contention based medium access protocol and\\nthe choice of packet forwarding strategy selected at each hop by the devices.\\nThe fundamental problem in optimizing the performance of such networks is to\\nbalance the tradeoff between conflicting performance objectives such as the\\nspatial frequency reuse, transmission quality, and packet progress towards the\\ndestination. In this paper, we use a stochastic geometry approach to quantify\\nthe performance of multi-hop massive IoT networks in terms of the spatial\\nfrequency reuse and the transmission quality under different packet forwarding\\nschemes. We also develop a comprehensive performance metric that can be used to\\noptimize the system to achieve the best performance. The results can be used to\\nselect the best forwarding scheme and tune the carrier sensing threshold to\\noptimize the performance of the network according to the delay constraints and\\ntransmission quality requirements.\\n',\n",
       " '  We develope a two-species exclusion process with a distinct pair of entry and\\nexit sites for each species of rigid rods. The relatively slower forward\\nstepping of the rods in an extended bottleneck region, located in between the\\ntwo entry sites, controls the extent of interference of the co-directional flow\\nof the two species of rods. The relative positions of the sites of entry of the\\ntwo species of rods with respect to the location of the bottleneck are\\nmotivated by a biological phenomenon. However, the primary focus of the study\\nhere is to explore the effects of the interference of the flow of the two\\nspecies of rods on their spatio-temporal organization and the regulations of\\nthis interference by the extended bottleneck. By a combination of mean-field\\ntheory and computer simulation we calculate the flux of both species of rods\\nand their density profiles as well as the composite phase diagrams of the\\nsystem. If the bottleneck is sufficiently stringent some of the phases become\\npractically unrealizable although not ruled out on the basis of any fundamental\\nphysical principle. Moreover the extent of suppression of flow of the\\ndownstream entrants by the flow of the upstream entrants can also be regulated\\nby the strength of the bottleneck. We speculate on the possible implications of\\nthe results in the context of the biological phenomenon that motivated the\\nformulation of the theoretical model.\\n',\n",
       " '  We introduce a large class of random Young diagrams which can be regarded as\\na natural one-parameter deformation of some classical Young diagram ensembles;\\na deformation which is related to Jack polynomials and Jack characters. We show\\nthat each such a random Young diagram converges asymptotically to some limit\\nshape and that the fluctuations around the limit are asymptotically Gaussian.\\n',\n",
       " '  We explicitly compute the critical exponents associated with logarithmic\\ncorrections (the so-called hatted exponents) starting from the renormalization\\ngroup equations and the mean field behavior for a wide class of models at the\\nupper critical behavior (for short and long range $\\\\phi^n$-theories) and below\\nit. This allows us to check the scaling relations among these critical\\nexponents obtained by analysing the complex singularities (Lee-Yang and Fisher\\nzeroes) of these models. Moreover, we have obtained an explicit method to\\ncompute the $\\\\hat{\\\\coppa}$ exponent [defined by $\\\\xi\\\\sim L (\\\\log\\nL)^{\\\\hat{\\\\coppa}}$] and, finally, we have found a new derivation of the scaling\\nlaw associated with it.\\n',\n",
       " '  We obtain a Bernstein-type inequality for sums of Banach-valued random\\nvariables satisfying a weak dependence assumption of general type and under\\ncertain smoothness assumptions of the underlying Banach norm. We use this\\ninequality in order to investigate in the asymptotical regime the error upper\\nbounds for the broad family of spectral regularization methods for reproducing\\nkernel decision rules, when trained on a sample coming from a $\\\\tau-$mixing\\nprocess.\\n',\n",
       " '  The temperature-dependent evolution of the Kondo lattice is a long-standing\\ntopic of theoretical and experimental investigation and yet it lacks a truly\\nmicroscopic description of the relation of the basic $f$-$d$ hybridization\\nprocesses to the fundamental temperature scales of Kondo screening and\\nFermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$\\nhybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo\\nlattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved\\nphotoemission (ARPES) with sufficient detail to allow direct comparison to\\nfirst principles dynamical mean field theory (DMFT) calculations containing\\nfull realism of crystalline electric field states. The ARPES results, for two\\northogonal (001) and (100) cleaved surfaces and three different $f$-$d$\\nhybridization scenarios, with additional microscopic insight provided by DMFT,\\nreveal $f$ participation in the Fermi surface at temperatures much higher than\\nthe lattice coherence temperature, $T^*\\\\approx$ 45 K, commonly believed to be\\nthe onset for such behavior. The identification of a $T$-dependent crystalline\\nelectric field degeneracy crossover in the DMFT theory $below$ $T^*$ is\\nspecifically highlighted.\\n',\n",
       " '  Hegarty conjectured for $n\\\\neq 2, 3, 5, 7$ that $\\\\mathbb{Z}/n\\\\mathbb{Z}$ has\\na permutation which destroys all arithmetic progressions mod $n$. For $n\\\\ge\\nn_0$, Hegarty and Martinsson demonstrated that $\\\\mathbb{Z}/n\\\\mathbb{Z}$ has an\\narithmetic-progression destroying permutation. However $n_0\\\\approx 1.4\\\\times\\n10^{14}$ and thus resolving the conjecture in full remained out of reach of any\\ncomputational techniques. However, this paper using constructions modeled after\\nthose used by Elkies and Swaminathan for the case of $\\\\mathbb{Z}/p\\\\mathbb{Z}$\\nwith $p$ being prime, establish the conjecture in full. Furthermore our results\\ndo not rely on the fact that it suffices to study when $n<n_0$ and thus our\\nresults completely independent of the proof given by Hegarty and Martinsson.\\n',\n",
       " '  An immersion $f : {\\\\mathcal D} \\\\rightarrow \\\\mathcal C$ between cell complexes\\nis a local homeomorphism onto its image that commutes with the characteristic\\nmaps of the cell complexes. We study immersions between finite-dimensional\\nconnected $\\\\Delta$-complexes by replacing the fundamental group of the base\\nspace by an appropriate inverse monoid. We show how conjugacy classes of the\\nclosed inverse submonoids of this inverse monoid may be used to classify\\nconnected immersions into the complex. This extends earlier results of Margolis\\nand Meakin for immersions between graphs and of Meakin and Szakács on\\nimmersions into $2$-dimensional $CW$-complexes.\\n',\n",
       " '  Resolving the relationship between biodiversity and ecosystem functioning has\\nbeen one of the central goals of modern ecology. Early debates about the\\nrelationship were finally resolved with the advent of a statistical\\npartitioning scheme that decomposed the biodiversity effect into a \"selection\"\\neffect and a \"complementarity\" effect. We prove that both the biodiversity\\neffect and its statistical decomposition into selection and complementarity are\\nfundamentally flawed because these methods use a naïve null expectation based\\non neutrality, likely leading to an overestimate of the net biodiversity\\neffect, and they fail to account for the nonlinear abundance-ecosystem\\nfunctioning relationships observed in nature. Furthermore, under such\\nnonlinearity no statistical scheme can be devised to partition the biodiversity\\neffects. We also present an alternative metric providing a more reasonable\\nestimate of biodiversity effect. Our results suggest that all studies conducted\\nsince the early 1990s likely overestimated the positive effects of biodiversity\\non ecosystem functioning.\\n',\n",
       " \"  The principle of democracy is that the people govern through elected\\nrepresentatives. Therefore, a democracy is healthy as long as the elected\\npoliticians do represent the people. We have analyzed data from the Brazilian\\nelectoral court (Tribunal Superior Eleitoral, TSE) concerning money donations\\nfor the electoral campaigns and the election results. Our work points to two\\ndisturbing conclusions: money is a determining factor on whether a candidate is\\nelected or not (as opposed to representativeness); secondly, the use of\\nBenford's Law to analyze the declared donations received by the parties and\\nelectoral campaigns shows evidence of fraud in the declarations. A better term\\nto define Brazil's government system is what we define as chrimatocracy (govern\\nby money).\\n\",\n",
       " \"  With the increasing commoditization of computer vision, speech recognition\\nand machine translation systems and the widespread deployment of learning-based\\nback-end technologies such as digital advertising and intelligent\\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\\nproduction. These changes have been made possible by unprecedented levels of\\ndata and computation, by methodological advances in machine learning, by\\ninnovations in systems software and architectures, and by the broad\\naccessibility of these technologies.\\nThe next generation of AI systems promises to accelerate these developments\\nand increasingly impact our lives via frequent interactions and making (often\\nmission-critical) decisions on our behalf, often in highly personalized\\ncontexts. Realizing this promise, however, raises daunting challenges. In\\nparticular, we need AI systems that make timely and safe decisions in\\nunpredictable environments, that are robust against sophisticated adversaries,\\nand that can process ever increasing amounts of data across organizations and\\nindividuals without compromising confidentiality. These challenges will be\\nexacerbated by the end of the Moore's Law, which will constrain the amount of\\ndata these technologies can store and process. In this paper, we propose\\nseveral open research directions in systems, architectures, and security that\\ncan address these challenges and help unlock AI's potential to improve lives\\nand society.\\n\",\n",
       " '  We rework and generalize equivariant infinite loop space theory, which shows\\nhow to construct G-spectra from G-spaces with suitable structure. There is a\\nnaive version which gives naive G-spectra for any topological group G, but our\\nfocus is on the construction of genuine G-spectra when G is finite.\\nWe give new information about the Segal and operadic equivariant infinite\\nloop space machines, supplying many details that are missing from the\\nliterature, and we prove by direct comparison that the two machines give\\nequivalent output when fed equivalent input. The proof of the corresponding\\nnonequivariant uniqueness theorem, due to May and Thomason, works for naive\\nG-spectra for general G but fails hopelessly for genuine G-spectra when G is\\nfinite. Even in the nonequivariant case, our comparison theorem is considerably\\nmore precise, giving a direct point-set level comparison.\\nWe have taken the opportunity to update this general area, equivariant and\\nnonequivariant, giving many new proofs, filling in some gaps, and giving some\\ncorrections to results in the literature.\\n',\n",
       " '  We prove that any open subset $U$ of a semi-simple simply connected\\nquasi-split linear algebraic group $G$ with ${codim} (G\\\\setminus U, G)\\\\geq 2$\\nover a number field satisfies strong approximation by establishing a fibration\\nof $G$ over a toric variety. We also prove a similar result of strong\\napproximation with Brauer-Manin obstruction for a partial equivariant smooth\\ncompactification of a homogeneous space where all invertible functions are\\nconstant and the semi-simple part of the linear algebraic group is quasi-split.\\nSome semi-abelian varieties of any given dimension where the complements of a\\nrational point do not satisfy strong approximation with Brauer-Manin\\nobstruction are given.\\n',\n",
       " '  We show that nonlocal minimal cones which are non-singular subgraphs outside\\nthe origin are necessarily halfspaces.\\nThe proof is based on classical ideas of~\\\\cite{DG1} and on the computation of\\nthe linearized nonlocal mean curvature operator, which is proved to satisfy a\\nsuitable maximum principle.\\nWith this, we obtain new, and somehow simpler, proofs of the Bernstein-type\\nresults for nonlocal minimal surfaces which have been recently established\\nin~\\\\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type\\nresult which classifies Lipschitz nonlocal minimal subgraphs outside a ball.\\n',\n",
       " \"  Let $f_1,\\\\ldots,f_k : \\\\mathbb{N} \\\\rightarrow \\\\mathbb{C}$ be multiplicative\\nfunctions taking values in the closed unit disc. Using an analytic approach in\\nthe spirit of Halász' mean value theorem, we compute multidimensional\\naverages of the shape $$x^{-l} \\\\sum_{\\\\mathbf{n} \\\\in [x]^l} \\\\prod_{1 \\\\leq j \\\\leq\\nk} f_j(L_j(\\\\mathbf{n}))$$ as $x \\\\rightarrow \\\\infty$, where $[x] := [1,x]$ and\\n$L_1,\\\\ldots, L_k$ are affine linear forms that satisfy some natural conditions.\\nOur approach gives a new proof of a result of Frantzikinakis and Host that is\\ndistinct from theirs, with \\\\emph{explicit} main and error terms. \\\\\\\\ As an\\napplication of our formulae, we establish a \\\\emph{local-to-global} principle\\nfor Gowers norms of multiplicative functions. We also compute the asymptotic\\ndensities of the sets of integers $n$ such that a given multiplicative function\\n$f: \\\\mathbb{N} \\\\rightarrow \\\\{-1, 1\\\\}$ yields a fixed sign pattern of length 3\\nor 4 on almost all 3- and 4-term arithmetic progressions, respectively, with\\nfirst term $n$.\\n\",\n",
       " \"  The apparent gas permeability of the porous medium is an important parameter\\nin the prediction of unconventional gas production, which was first\\ninvestigated systematically by Klinkenberg in 1941 and found to increase with\\nthe reciprocal mean gas pressure (or equivalently, the Knudsen number).\\nAlthough the underlying rarefaction effects are well-known, the reason that the\\ncorrection factor in Klinkenberg's famous equation decreases when the Knudsen\\nnumber increases has not been fully understood. Most of the studies idealize\\nthe porous medium as a bundle of straight cylindrical tubes, however, according\\nto the gas kinetic theory, this only results in an increase of the correction\\nfactor with the Knudsen number, which clearly contradicts Klinkenberg's\\nexperimental observations. Here, by solving the Bhatnagar-Gross-Krook equation\\nin simplified (but not simple) porous media, we identify, for the first time,\\ntwo key factors that can explain Klinkenberg's experimental results: the\\ntortuous flow path and the non-unitary tangential momentum accommodation\\ncoefficient for the gas-surface interaction. Moreover, we find that\\nKlinkenberg's results can only be observed when the ratio between the apparent\\nand intrinsic permeabilities is $\\\\lesssim30$; at large ratios (or Knudsen\\nnumbers) the correction factor increases with the Knudsen number. Our numerical\\nresults could also serve as benchmarking cases to assess the accuracy of\\nmacroscopic models and/or numerical schemes for the modeling/simulation of\\nrarefied gas flows in complex geometries over a wide range of gas rarefaction.\\n\",\n",
       " '  In previous papers, threshold probabilities for the properties of a random\\ndistance graph to contain strictly balanced graphs were found. We extend this\\nresult to arbitrary graphs and prove that the number of copies of a strictly\\nbalanced graph has asymptotically Poisson distribution at the threshold.\\n',\n",
       " '  Runtime enforcement can be effectively used to improve the reliability of\\nsoftware applications. However, it often requires the definition of ad hoc\\npolicies and enforcement strategies, which might be expensive to identify and\\nimplement. This paper discusses how to exploit lifecycle events to obtain\\nuseful enforcement strategies that can be easily reused across applications,\\nthus reducing the cost of adoption of the runtime enforcement technology. The\\npaper finally sketches how this idea can be used to define libraries that can\\nautomatically overcome problems related to applications misusing them.\\n',\n",
       " '  The atomic norm provides a generalization of the $\\\\ell_1$-norm to continuous\\nparameter spaces. When applied as a sparse regularizer for line spectral\\nestimation the solution can be obtained by solving a convex optimization\\nproblem. This problem is known as atomic norm soft thresholding (AST). It can\\nbe cast as a semidefinite program and solved by standard methods. In the\\nsemidefinite formulation there are $O(N^2)$ dual variables and a standard\\nprimal-dual interior point method requires at least $O(N^6)$ flops per\\niteration. That has lead researcher to consider alternating direction method of\\nmultipliers (ADMM) for the solution of AST, but this method is still somewhat\\nslow for large problem sizes. To obtain a faster algorithm we reformulate AST\\nas a non-symmetric conic program. That has two properties of key importance to\\nits numerical solution: the conic formulation has only $O(N)$ dual variables\\nand the Toeplitz structure inherent to AST is preserved. Based on it we derive\\nFastAST which is a primal-dual interior point method for solving AST. Two\\nvariants are considered with the fastest one requiring only $O(N^2)$ flops per\\niteration. Extensive numerical experiments demonstrate that FastAST solves AST\\nsignificantly faster than a state-of-the-art solver based on ADMM.\\n',\n",
       " '  We study the problem of causal structure learning over a set of random\\nvariables when the experimenter is allowed to perform at most $M$ experiments\\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\\nminimizing the portions of the structure that remains unknown given the limited\\nnumber of experiments in both Bayesian and minimax setting. We characterize the\\ntheoretical optimal solution and propose an algorithm, which designs the\\nexperiments efficiently in terms of time complexity. We show that for bounded\\ndegree graphs, in the minimax case and in the Bayesian case with uniform\\npriors, our proposed algorithm is a $\\\\rho$-approximation algorithm, where\\n$\\\\rho$ is independent of the order of the underlying graph. Simulations on both\\nsynthetic and real data show that the performance of our algorithm is very\\nclose to the optimal solution.\\n',\n",
       " '  We present a novel data-driven nested optimization framework that addresses\\nthe problem of coupling between plant and controller optimization. This\\noptimization strategy is tailored towards instances where a closed-form\\nexpression for the system dynamic response is unobtainable and simulations or\\nexperiments are necessary. Specifically, Bayesian Optimization, which is a\\ndata-driven technique for finding the optimum of an unknown and\\nexpensive-to-evaluate objective function, is employed to solve a nested\\noptimization problem. The underlying objective function is modeled by a\\nGaussian Process (GP); then, Bayesian Optimization utilizes the predictive\\nuncertainty information from the GP to determine the best subsequent control or\\nplant parameters. The proposed framework differs from the majority of co-design\\nliterature where there exists a closed-form model of the system dynamics.\\nFurthermore, we utilize the idea of Batch Bayesian Optimization at the plant\\noptimization level to generate a set of plant designs at each iteration of the\\noverall optimization process, recognizing that there will exist economies of\\nscale in running multiple experiments in each iteration of the plant design\\nprocess. We validate the proposed framework for a Buoyant Airborne Turbine\\n(BAT). We choose the horizontal stabilizer area, longitudinal center of mass\\nrelative to center of buoyancy (plant parameters), and the pitch angle\\nset-point (controller parameter) as our decision variables. Our results\\ndemonstrate that these plant and control parameters converge to their\\nrespective optimal values within only a few iterations.\\n',\n",
       " '  We explore the topological properties of quantum spin-1/2 chains with two\\nIsing symmetries. This class of models does not possess any of the symmetries\\nthat are required to protect the Haldane phase. Nevertheless, we show that\\nthere are 4 symmetry-protected topological phases, in addition to 6 phases that\\nspontaneously break one or both Ising symmetries. By mapping the model to\\none-dimensional interacting fermions with particle-hole and time-reversal\\nsymmetry, we obtain integrable parent Hamiltonians for the conventional and\\ntopological phases of the spin model. We use these Hamiltonians to characterize\\nthe physical properties of all 10 phases, identify their local and nonlocal\\norder parameters, and understand the effects of weak perturbations that respect\\nthe Ising symmetries. Our study provides the first explicit example of a class\\nof spin chains with several topologically non-trivial phases, and binds\\ntogether the topological classifications of interacting bosons and fermions.\\n',\n",
       " '  Most of the codes that have an algebraic decoding algorithm are derived from\\nthe Reed Solomon codes. They are obtained by taking equivalent codes, for\\nexample the generalized Reed Solomon codes, or by using the so-called subfield\\nsubcode method, which leads to Alternant codes and Goppa codes over the\\nunderlying prime field, or over some intermediate subfield. The main advantages\\nof these constructions is to preserve both the minimum distance and the\\ndecoding algorithm of the underlying Reed Solomon code. In this paper, we\\npropose a generalization of the subfield subcode construction by introducing\\nthe notion of subspace subcodes and a generalization of the equivalence of\\ncodes which leads to the notion of generalized subspace subcodes. When the\\ndimension of the selected subspaces is equal to one, we show that our approach\\ngives exactly the family of the codes obtained by equivalence and subfield\\nsubcode technique. However, our approach highlights the links between the\\nsubfield subcode of a code defined over an extension field and the operation of\\npuncturing the $q$-ary image of this code. When the dimension of the subspaces\\nis greater than one, we obtain codes whose alphabet is no longer a finite\\nfield, but a set of r-uples. We explain why these codes are practically as\\nefficient for applications as the codes defined on an extension of degree r. In\\naddition, they make it possible to obtain decodable codes over a large alphabet\\nhaving parameters previously inaccessible. As an application, we give some\\nexamples that can be used in public key cryptosystems such as McEliece.\\n',\n",
       " '  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of\\nGelfand-Cetlin systems over complex partial flag manifolds, we provide a\\ncomplete description of the topology of Gelfand-Cetlin fibers. We prove that\\nall fibers are \\\\emph{smooth} isotropic submanifolds and give a complete\\ndescription of the fiber to be Lagrangian in terms of combinatorics of\\nGelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian\\nfibers. After a few combinatorial and numercal tests for the displaceability,\\nusing the bulk-deformation of Floer cohomology by Schubert cycles, we prove\\nthat every full flag manifold $\\\\mathcal{F}(n)$ ($n \\\\geq 3$) with a monotone\\nKirillov-Kostant-Souriau symplectic form carries a continuum of\\nnon-displaceable Lagrangian tori which degenerates to a non-torus fiber in the\\nHausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\\\\mathcal{F}(3)$\\nis non-displaceable the question of which was raised by Nohara-Ueda who\\ncomputed its Floer cohomology to be vanishing.\\n',\n",
       " '  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\\nare a key component of probabilistic weather forecasting. They represent the\\nuncertainty in the initial conditions by an ensemble which incorporates\\ninformation coming from the physical model with the latest observations.\\nHigh-resolution numerical weather prediction models ran at operational centers\\nare able to resolve non-linear and non-Gaussian physical phenomena such as\\nconvection. There is therefore a growing need to develop ensemble assimilation\\nalgorithms able to deal with non-Gaussianity while staying computationally\\nfeasible. In the present paper we address some of these needs by proposing a\\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\\nformulated in ensemble space and uses a deterministic scheme such that it has\\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\\nlimiting case. A new criterion for choosing the proportion of particle filter\\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\\nframework and numerical experiments in a quasi-operational convective-scale\\nsetup are conducted. The results show the feasibility of the new algorithm in\\npractice and indicate a strong potential for such local hybrid methods, in\\nparticular for forecasting non-Gaussian variables such as wind and hourly\\nprecipitation.\\n',\n",
       " '  In this paper, we consider the Tensor Robust Principal Component Analysis\\n(TRPCA) problem, which aims to exactly recover the low-rank and sparse\\ncomponents from their sum. Our model is based on the recently proposed\\ntensor-tensor product (or t-product) [13]. Induced by the t-product, we first\\nrigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor\\naverage rank, and show that the tensor nuclear norm is the convex envelope of\\nthe tensor average rank within the unit ball of the tensor spectral norm. These\\ndefinitions, their relationships and properties are consistent with matrix\\ncases. Equipped with the new tensor nuclear norm, we then solve the TRPCA\\nproblem by solving a convex program and provide the theoretical guarantee for\\nthe exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA\\nas a special case. Numerical experiments verify our results, and the\\napplications to image recovery and background modeling problems demonstrate the\\neffectiveness of our method.\\n',\n",
       " '  Galaxies in the local Universe are known to follow bimodal distributions in\\nthe global stellar populations properties. We analyze the distribution of the\\nlocal average stellar-population ages of 654,053 sub-galactic regions resolved\\non ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the\\nCALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.\\nWe find a bimodal local-age distribution, with an old and a young peak\\nprimarily due to regions in early-type galaxies and star-forming regions of\\nspirals, respectively. Within spiral galaxies, the older ages of bulges and\\ninter-arm regions relative to spiral arms support an internal age bimodality.\\nAlthough regions of higher stellar-mass surface-density, mu*, are typically\\nolder, mu* alone does not determine the stellar population age and a bimodal\\ndistribution is found at any fixed mu*. We identify an \"old ridge\" of regions\\nof age ~9 Gyr, independent of mu*, and a \"young sequence\" of regions with age\\nincreasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as\\nregions containing only old stars, and the latter as regions where the relative\\ncontamination of old stellar populations by young stars decreases as mu*\\nincreases. The reason why this bimodal age distribution is not inconsistent\\nwith the unimodal shape of the cosmic-averaged star-formation history is that\\ni) the dominating contribution by young stars biases the age low with respect\\nto the average epoch of star formation, and ii) the use of a single average age\\nper region is unable to represent the full time-extent of the star-formation\\nhistory of \"young-sequence\" regions.\\n',\n",
       " '  We introduce a minimal model for the evolution of functional\\nprotein-interaction networks using a sequence-based mutational algorithm, and\\napply the model to study neutral drift in networks that yield oscillatory\\ndynamics. Starting with a functional core module, random evolutionary drift\\nincreases network complexity even in the absence of specific selective\\npressures. Surprisingly, we uncover a hidden order in sequence space that gives\\nrise to long-term evolutionary memory, implying strong constraints on network\\nevolution due to the topology of accessible sequence space.\\n',\n",
       " \"  The handwritten string recognition is still a challengeable task, though the\\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\\nwe proposed an end-to-end system for handwritten string recognition. Compared\\nwith the conventional methods, there is no preprocess nor manually designed\\nrules employed. With enough labelled data, it is easy to apply the proposed\\nmethod to different applications. Although the performance of the proposed\\nmethod may not be comparable with the state-of-the-art approaches, it's\\nusability and robustness are more meaningful for practical applications.\\n\",\n",
       " '  We note that the necessary and sufficient conditions established by Marcel\\nRiesz for the inclusion of regular Nörlund summation methods are in fact\\napplicable quite generally.\\n',\n",
       " \"  These lectures notes were written for a summer school on Mathematics for\\npost-quantum cryptography in Thiès, Senegal. They try to provide a guide for\\nMasters' students to get through the vast literature on elliptic curves,\\nwithout getting lost on their way to learning isogeny based cryptography. They\\nare by no means a reference text on the theory of elliptic curves, nor on\\ncryptography; students are encouraged to complement these notes with some of\\nthe books recommended in the bibliography.\\nThe presentation is divided in three parts, roughly corresponding to the\\nthree lectures given. In an effort to keep the reader interested, each part\\nalternates between the fundamental theory of elliptic curves, and applications\\nin cryptography. We often prefer to have the main ideas flow smoothly, rather\\nthan having a rigorous presentation as one would have in a more classical book.\\nThe reader will excuse us for the inaccuracies and the omissions.\\n\",\n",
       " \"  It has been shown recently that changing the fluidic properties of a drug can\\nimprove its efficacy in ablating solid tumors. We develop a modeling framework\\nfor tumor ablation, and present the simplest possible model for drug diffusion\\nin a spherical tumor with leaky boundaries and assuming cell death eventually\\nleads to ablation of that cell effectively making the two quantities\\nnumerically equivalent. The death of a cell after a given exposure time depends\\non both the concentration of the drug and the amount of oxygen available to the\\ncell. Higher oxygen availability leads to cell death at lower drug\\nconcentrations. It can be assumed that a minimum concentration is required for\\na cell to die, effectively connecting diffusion with efficacy. The\\nconcentration threshold decreases as exposure time increases, which allows us\\nto compute dose-response curves. Furthermore, these curves can be plotted at\\nmuch finer time intervals compared to that of experiments, which is used to\\nproduce a dose-threshold-response surface giving an observer a complete picture\\nof the drug's efficacy for an individual. In addition, since the diffusion,\\nleak coefficients, and the availability of oxygen is different for different\\nindividuals and tumors, we produce artificial replication data through\\nbootstrapping to simulate error. While the usual data-driven model with\\nSigmoidal curves use 12 free parameters, our mechanistic model only has two\\nfree parameters, allowing it to be open to scrutiny rather than forcing\\nagreement with data. Even so, the simplest model in our framework, derived\\nhere, shows close agreement with the bootstrapped curves, and reproduces well\\nestablished relations, such as Haber's rule.\\n\",\n",
       " '  To identify the estimand in missing data problems and observational studies,\\nit is common to base the statistical estimation on the \"missing at random\" and\\n\"no unmeasured confounder\" assumptions. However, these assumptions are\\nunverifiable using empirical data and pose serious threats to the validity of\\nthe qualitative conclusions of the statistical inference. A sensitivity\\nanalysis asks how the conclusions may change if the unverifiable assumptions\\nare violated to a certain degree. In this paper we consider a marginal\\nsensitivity model which is a natural extension of Rosenbaum\\'s sensitivity model\\nthat is widely used for matched observational studies. We aim to construct\\nconfidence intervals based on inverse probability weighting estimators, such\\nthat asymptotically the intervals have at least nominal coverage of the\\nestimand whenever the data generating distribution is in the collection of\\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\\nminimax/maximin inequality to transform this intractable problem to a linear\\nfractional programming problem, which can be solved very efficiently. We\\nillustrate our method using a real dataset to estimate the causal effect of\\nfish consumption on blood mercury level.\\n',\n",
       " '  In this paper, we provide an analysis of self-organized network management,\\nwith an end-to-end perspective of the network. Self-organization as applied to\\ncellular networks is usually referred to Self-organizing Networks (SONs), and\\nit is a key driver for improving Operations, Administration, and Maintenance\\n(OAM) activities. SON aims at reducing the cost of installation and management\\nof 4G and future 5G networks, by simplifying operational tasks through the\\ncapability to configure, optimize and heal itself. To satisfy 5G network\\nmanagement requirements, this autonomous management vision has to be extended\\nto the end to end network. In literature and also in some instances of products\\navailable in the market, Machine Learning (ML) has been identified as the key\\ntool to implement autonomous adaptability and take advantage of experience when\\nmaking decisions. In this paper, we survey how network management can\\nsignificantly benefit from ML solutions. We review and provide the basic\\nconcepts and taxonomy for SON, network management and ML. We analyse the\\navailable state of the art in the literature, standardization, and in the\\nmarket. We pay special attention to 3rd Generation Partnership Project (3GPP)\\nevolution in the area of network management and to the data that can be\\nextracted from 3GPP networks, in order to gain knowledge and experience in how\\nthe network is working, and improve network performance in a proactive way.\\nFinally, we go through the main challenges associated with this line of\\nresearch, in both 4G and in what 5G is getting designed, while identifying new\\ndirections for research.\\n',\n",
       " '  Understanding smart grid cyber attacks is key for developing appropriate\\nprotection and recovery measures. Advanced attacks pursue maximized impact at\\nminimized costs and detectability. This paper conducts risk analysis of\\ncombined data integrity and availability attacks against the power system state\\nestimation. We compare the combined attacks with pure integrity attacks - false\\ndata injection (FDI) attacks. A security index for vulnerability assessment to\\nthese two kinds of attacks is proposed and formulated as a mixed integer linear\\nprogramming problem. We show that such combined attacks can succeed with fewer\\nresources than FDI attacks. The combined attacks with limited knowledge of the\\nsystem model also expose advantages in keeping stealth against the bad data\\ndetection. Finally, the risk of combined attacks to reliable system operation\\nis evaluated using the results from vulnerability assessment and attack impact\\nanalysis. The findings in this paper are validated and supported by a detailed\\ncase study.\\n',\n",
       " '  We propose a family of near-metrics based on local graph diffusion to capture\\nsimilarity for a wide class of data sets. These quasi-metametrics, as their\\nnames suggest, dispense with one or two standard axioms of metric spaces,\\nspecifically distinguishability and symmetry, so that similarity between data\\npoints of arbitrary type and form could be measured broadly and effectively.\\nThe proposed near-metric family includes the forward k-step diffusion and its\\nreverse, typically on the graph consisting of data objects and their features.\\nBy construction, this family of near-metrics is particularly appropriate for\\ncategorical data, continuous data, and vector representations of images and\\ntext extracted via deep learning approaches. We conduct extensive experiments\\nto evaluate the performance of this family of similarity measures and compare\\nand contrast with traditional measures of similarity used for each specific\\napplication and with the ground truth when available. We show that for\\nstructured data including categorical and continuous data, the near-metrics\\ncorresponding to normalized forward k-step diffusion (k small) work as one of\\nthe best performing similarity measures; for vector representations of text and\\nimages including those extracted from deep learning, the near-metrics derived\\nfrom normalized and reverse k-step graph diffusion (k very small) exhibit\\noutstanding ability to distinguish data points from different classes.\\n',\n",
       " '  Recommender system is an important component of many web services to help\\nusers locate items that match their interests. Several studies showed that\\nrecommender systems are vulnerable to poisoning attacks, in which an attacker\\ninjects fake data to a given system such that the system makes recommendations\\nas the attacker desires. However, these poisoning attacks are either agnostic\\nto recommendation algorithms or optimized to recommender systems that are not\\ngraph-based. Like association-rule-based and matrix-factorization-based\\nrecommender systems, graph-based recommender system is also deployed in\\npractice, e.g., eBay, Huawei App Store. However, how to design optimized\\npoisoning attacks for graph-based recommender systems is still an open problem.\\nIn this work, we perform a systematic study on poisoning attacks to graph-based\\nrecommender systems. Due to limited resources and to avoid detection, we assume\\nthe number of fake users that can be injected into the system is bounded. The\\nkey challenge is how to assign rating scores to the fake users such that the\\ntarget item is recommended to as many normal users as possible. To address the\\nchallenge, we formulate the poisoning attacks as an optimization problem,\\nsolving which determines the rating scores for the fake users. We also propose\\ntechniques to solve the optimization problem. We evaluate our attacks and\\ncompare them with existing attacks under white-box (recommendation algorithm\\nand its parameters are known), gray-box (recommendation algorithm is known but\\nits parameters are unknown), and black-box (recommendation algorithm is\\nunknown) settings using two real-world datasets. Our results show that our\\nattack is effective and outperforms existing attacks for graph-based\\nrecommender systems. For instance, when 1% fake users are injected, our attack\\ncan make a target item recommended to 580 times more normal users in certain\\nscenarios.\\n',\n",
       " '  This paper describes the Stockholm University/University of Groningen\\n(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological\\ninflection. Our system is based on an attentional sequence-to-sequence neural\\nnetwork model using Long Short-Term Memory (LSTM) cells, with joint training of\\nmorphological inflection and the inverse transformation, i.e. lemmatization and\\nmorphological analysis. Our system outperforms the baseline with a large\\nmargin, and our submission ranks as the 4th best team for the track we\\nparticipate in (task 1, high-resource).\\n',\n",
       " \"  Neuroscientists classify neurons into different types that perform similar\\ncomputations at different locations in the visual field. Traditional methods\\nfor neural system identification do not capitalize on this separation of 'what'\\nand 'where'. Learning deep convolutional feature spaces that are shared among\\nmany neurons provides an exciting path forward, but the architectural design\\nneeds to account for data limitations: While new experimental techniques enable\\nrecordings from thousands of neurons, experimental time is limited so that one\\ncan sample only a small fraction of each neuron's response space. Here, we show\\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\\nneural data is the estimation of the individual receptive field locations, a\\nproblem that has been scratched only at the surface thus far. We propose a CNN\\narchitecture with a sparse readout layer factorizing the spatial (where) and\\nfeature (what) dimensions. Our network scales well to thousands of neurons and\\nshort recordings and can be trained end-to-end. We evaluate this architecture\\non ground-truth data to explore the challenges and limitations of CNN-based\\nsystem identification. Moreover, we show that our network model outperforms\\ncurrent state-of-the art system identification models of mouse primary visual\\ncortex.\\n\",\n",
       " '  The extremely low efficiency is regarded as the bottleneck of Wireless Power\\nTransfer (WPT) technology. To tackle this problem, either enlarging the\\ntransfer power or changing the infrastructure of WPT system could be an\\nintuitively proposed way. However, the drastically important issue on the user\\nexposure of electromagnetic radiation is rarely considered while we try to\\nimprove the efficiency of WPT. In this paper, a Distributed Antenna Power\\nBeacon (DA-PB) based WPT system where these antennas are uniformly distributed\\non a circle is analyzed and optimized with the safety electromagnetic radiation\\nlevel (SERL) requirement. In this model, three key questions are intended to be\\nanswered: 1) With the SERL, what is the performance of the harvested power at\\nthe users ? 2) How do we configure the parameters to maximize the efficiency of\\nWPT? 3) Under the same constraints, does the DA-PB still have performance gain\\nthan the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of\\nDA-PB is derived to make the radio frequency (RF) electromagnetic radiation\\npower density at any location of the charging cell lower than the SERL\\npublished by the Federal Communications Commission (FCC). Second, the\\nclosed-form expressions of average harvested Direct Current (DC) power per user\\nin the charging cell for pass-loss exponent 2 and 4 are also provided. In order\\nto maximize the average efficiency of WPT, the optimal radii for distributed\\nantennas elements (DAEs) are derived when the pass-loss exponent takes the\\ntypical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a\\nbenchmark. Simulation results verify our derived theoretical results. And it is\\nshown that the proposed DA-PB indeed achieves larger average harvested DC power\\nthan CA-PB and can improve the efficiency of WPT.\\n',\n",
       " '  A numerical method for particle-laden fluids interacting with a deformable\\nsolid domain and mobile rigid parts is proposed and implemented in a full\\nengineering system. The fluid domain is modeled with a lattice Boltzmann\\nrepresentation, the particles and rigid parts are modeled with a discrete\\nelement representation, and the deformable solid domain is modeled using a\\nLagrangian mesh. The main issue of this work, since separately each of these\\nmethods is a mature tool, is to develop coupling and model-reduction approaches\\nin order to efficiently simulate coupled problems of this nature, as occur in\\nvarious geological and engineering applications. The lattice Boltzmann method\\nincorporates a large-eddy simulation technique using the Smagorinsky turbulence\\nmodel. The discrete element method incorporates spherical and polyhedral\\nparticles for stiff contact interactions. A neo-Hookean hyperelastic model is\\nused for the deformable solid. We provide a detailed description of how to\\ncouple the three solvers within a unified algorithm. The technique we propose\\nfor rubber modeling/coupling exploits a simplification that prevents having to\\nsolve a finite-element problem each time step. We also develop a technique to\\nreduce the domain size of the full system by replacing certain zones with\\nquasi-analytic solutions, which act as effective boundary conditions for the\\nlattice Boltzmann method. The major ingredients of the routine are are\\nseparately validated. To demonstrate the coupled method in full, we simulate\\nslurry flows in two kinds of piston-valve geometries. The dynamics of the valve\\nand slurry are studied and reported over a large range of input parameters.\\n',\n",
       " \"  We construct a Schwinger-Keldysh effective field theory for relativistic\\nhydrodynamics for charged matter in a thermal background using a superspace\\nformalism. Superspace allows us to efficiently impose the symmetries of the\\nproblem and to obtain a simple expression for the effective action. We show\\nthat the theory we obtain is compatible with the Kubo-Martin-Schwinger\\ncondition, which in turn implies that Green's functions obey the\\nfluctuation-dissipation theorem. Our approach complements and extends existing\\nformulations found in the literature.\\n\",\n",
       " \"  Observables have a dual nature in both classical and quantum kinematics: they\\nare at the same time \\\\emph{quantities}, allowing to separate states by means of\\ntheir numerical values, and \\\\emph{generators of transformations}, establishing\\nrelations between different states. In this work, we show how this two-fold\\nrole of observables constitutes a key feature in the conceptual analysis of\\nclassical and quantum kinematics, shedding a new light on the distinguishing\\nfeature of the quantum at the kinematical level. We first take a look at the\\nalgebraic description of both classical and quantum observables in terms of\\nJordan-Lie algebras and show how the two algebraic structures are the precise\\nmathematical manifestation of the two-fold role of observables. Then, we turn\\nto the geometric reformulation of quantum kinematics in terms of Kähler\\nmanifolds. A key achievement of this reformulation is to show that the two-fold\\nrole of observables is the constitutive ingredient defining what an observable\\nis. Moreover, it points to the fact that, from the restricted point of view of\\nthe transformational role of observables, classical and quantum kinematics\\nbehave in exactly the same way. Finally, we present Landsman's general\\nframework of Poisson spaces with transition probability, which highlights with\\nunmatched clarity that the crucial difference between the two kinematics lies\\nin the way the two roles of observables are related to each other.\\n\",\n",
       " '  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with\\nsmooth boundary $\\\\partial M$. Suppose that $(M,g)$ admits a scalar-flat\\nconformal metric. We prove that the supremum of the isoperimetric quotient over\\nthe scalar-flat conformal class is strictly larger than the best constant of\\nthe isoperimetric inequality in the Euclidean space, and consequently is\\nachieved, if either (i) $n\\\\ge 12$ and $\\\\partial M$ has a nonumbilic point; or\\n(ii) $n\\\\ge 10$, $\\\\partial M$ is umbilic and the Weyl tensor does not vanish at\\nsome boundary point.\\n',\n",
       " '  Random feature maps are ubiquitous in modern statistical machine learning,\\nwhere they generalize random projections by means of powerful, yet often\\ndifficult to analyze nonlinear operators. In this paper, we leverage the\\n\"concentration\" phenomenon induced by random matrix theory to perform a\\nspectral analysis on the Gram matrix of these random feature maps, here for\\nGaussian mixture models of simultaneously large dimension and size. Our results\\nare instrumental to a deeper understanding on the interplay of the nonlinearity\\nand the statistics of the data, thereby allowing for a better tuning of random\\nfeature-based techniques.\\n',\n",
       " '  The calculation of minimum energy paths for transitions such as atomic and/or\\nspin re-arrangements is an important task in many contexts and can often be\\nused to determine the mechanism and rate of transitions. An important challenge\\nis to reduce the computational effort in such calculations, especially when ab\\ninitio or electron density functional calculations are used to evaluate the\\nenergy since they can require large computational effort. Gaussian process\\nregression is used here to reduce significantly the number of energy\\nevaluations needed to find minimum energy paths of atomic rearrangements. By\\nusing results of previous calculations to construct an approximate energy\\nsurface and then converge to the minimum energy path on that surface in each\\nGaussian process iteration, the number of energy evaluations is reduced\\nsignificantly as compared with regular nudged elastic band calculations. For a\\ntest problem involving rearrangements of a heptamer island on a crystal\\nsurface, the number of energy evaluations is reduced to less than a fifth. The\\nscaling of the computational effort with the number of degrees of freedom as\\nwell as various possible further improvements to this approach are discussed.\\n',\n",
       " '  Social media has changed the ways of communication, where everyone is\\nequipped with the power to express their opinions to others in online\\ndiscussion platforms. Previously, a number of stud- ies have been presented to\\nidentify opinion leaders in online discussion networks. Feng (\"Are you\\nconnected? Evaluating information cascade in online discussion about the\\n#RaceTogether campaign\", Computers in Human Behavior, 2016) identified five\\ntypes of central users and their communication patterns in an online\\ncommunication network of a limited time span. However, to trace the change in\\ncommunication pattern, a long-term analysis is required. In this study, we\\ncritically analyzed framework presented by Feng based on five types of central\\nusers in online communication network and their communication pattern in a\\nlong-term manner. We take another case study presented by Udnor et al.\\n(\"Determining social media impact on the politics of developing countries using\\nsocial network analytics\", Program, 2016) to further understand the dynamics as\\nwell as to perform validation . Results indicate that there may not exist all\\nof these central users in an online communication network in a long-term\\nmanner. Furthermore, we discuss the changing positions of opinion leaders and\\ntheir power to keep isolates interested in an online discussion network.\\n',\n",
       " '  Let $E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ denote the error of best approximation by\\npolynomials of degree at most $n$ in the space\\n$L^2(\\\\varpi_{\\\\alpha,\\\\beta,\\\\gamma})$ on the triangle $\\\\{(x,y): x, y \\\\ge 0, x+y\\n\\\\le 1\\\\}$, where $\\\\varpi_{\\\\alpha,\\\\beta,\\\\gamma}(x,y) := x^\\\\alpha y ^\\\\beta\\n(1-x-y)^\\\\gamma$ for $\\\\alpha,\\\\beta,\\\\gamma > -1$. Our main result gives a sharp\\nestimate of $E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ in terms of the error of best\\napproximation for higher order derivatives of $f$ in appropriate Sobolev\\nspaces. The result also leads to a characterization of\\n$E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ by a weighted $K$-functional.\\n',\n",
       " \"  Due to the increasing dependency of critical infrastructure on synchronized\\nclocks, network time synchronization protocols have become an attractive target\\nfor attackers. We identify data origin authentication as the key security\\nobjective and suggest to employ recently proposed high-performance digital\\nsignature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of\\nsecurity measures to secure multicast time synchronization. We conduct\\nexperiments to verify the computational and communication efficiency for using\\nthese signatures in the standard time synchronization protocols NTP and PTP. We\\npropose additional security measures to prevent replay attacks and to mitigate\\ndelay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we\\nextend our security measures specifically to 2-step mode (PTP) and show that\\nthey have no impact on time synchronization's precision.\\n\",\n",
       " \"  We implement an efficient numerical method to calculate response functions of\\ncomplex impurities based on the Density Matrix Renormalization Group (DMRG) and\\nuse it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This\\nmethod uses the correction vector to obtain precise Green's functions on the\\nreal frequency axis at zero temperature. By using a self-consistent bath\\nconfiguration with very low entanglement, we take full advantage of the DMRG to\\ncalculate dynamical response functions paving the way to treat large effective\\nimpurities such as those corresponding to multi-orbital interacting models and\\nmulti-site or multi-momenta clusters. This method leads to reliable\\ncalculations of non-local self energies at arbitrary dopings and interactions\\nand at any energy scale.\\n\",\n",
       " '  Bulk and surface electronic structures, calculated using density functional\\ntheory and a tight-binding model Hamiltonian, reveal the existence of two\\ntopologically invariant (TI) surface states in the family of cubic Bi\\nperovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI\\nstates, one lying in the valence band (TI-V) and other lying in the conduction\\nband (TI-C) are formed out of bonding and antibonding states of the\\nBi-$\\\\{$s,p$\\\\}$ - O-$\\\\{$p$\\\\}$ coordinated covalent interaction. Below a certain\\ncritical thickness of the film, which varies with A, TI states of top and\\nbottom surfaces couple to destroy the Dirac type linear dispersion and\\nconsequently to open surface energy gaps. The origin of s-p band inversion,\\nnecessary to form a TI state, classifies the family of ABiO$_3$ into two. For\\nclass-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,\\nis induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,\\nSr and Ba) the band inversion is induced through weak but sensitive second\\nneighbor Bi-Bi interactions.\\n',\n",
       " '  It is often recommended that identifiers for ontology terms should be\\nsemantics-free or meaningless. In practice, ontology developers tend to use\\nnumeric identifiers, starting at 1 and working upwards. In this paper we\\npresent a critique of current ontology semantics-free identifiers;\\nmonotonically increasing numbers have a number of significant usability flaws\\nwhich make them unsuitable as a default option, and we present a series of\\nalternatives. We have provide an implementation of these alternatives which can\\nbe freely combined.\\n',\n",
       " \"  Deep learning methods have achieved high performance in sound recognition\\ntasks. Deciding how to feed the training data is important for further\\nperformance improvement. We propose a novel learning method for deep sound\\nrecognition: Between-Class learning (BC learning). Our strategy is to learn a\\ndiscriminative feature space by recognizing the between-class sounds as\\nbetween-class sounds. We generate between-class sounds by mixing two sounds\\nbelonging to different classes with a random ratio. We then input the mixed\\nsound to the model and train the model to output the mixing ratio. The\\nadvantages of BC learning are not limited only to the increase in variation of\\nthe training data; BC learning leads to an enlargement of Fisher's criterion in\\nthe feature space and a regularization of the positional relationship among the\\nfeature distributions of the classes. The experimental results show that BC\\nlearning improves the performance on various sound recognition networks,\\ndatasets, and data augmentation schemes, in which BC learning proves to be\\nalways beneficial. Furthermore, we construct a new deep sound recognition\\nnetwork (EnvNet-v2) and train it with BC learning. As a result, we achieved a\\nperformance surpasses the human level.\\n\",\n",
       " \"  We propose a linear-time, single-pass, top-down algorithm for multiple\\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\\nedges specify a partial ordering in which hypotheses must be tested. The\\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\\n(FDR) while satisfying the logical constraint that a rejected node's parents\\nmust also be rejected. It is designed for sequential testing settings, when the\\nDAG structure is known a priori, but the $p$-values are obtained selectively\\n(such as in a sequence of experiments), but the algorithm is also applicable in\\nnon-sequential settings when all $p$-values can be calculated in advance (such\\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\\nEvolving Rejections on DAGs, provably controls the false discovery rate under\\nindependence, positive dependence or arbitrary dependence of the $p$-values.\\nThe DAGGER procedure specializes to known algorithms in the special cases of\\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\\nprocedure when the DAG has no edges. We explore the empirical performance of\\nDAGGER using simulations, as well as a real dataset corresponding to a gene\\nontology, showing favorable performance in terms of time and power.\\n\",\n",
       " '  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\\\\\"\\nodinger equation (NLS) and an ordinary differential equation (ODE). This system\\nis a simplified model of the NLS around soliton solutions. Following Nakanishi\\n\\\\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.\\nThe proof is based on Nakanishi\\'s framework and Fermi Golden Rule estimates on\\n$L^4$ in time norms.\\n',\n",
       " '  We relate the concepts used in decentralized ledger technology to studies of\\nepisodic memory in the mammalian brain. Specifically, we introduce the standard\\nconcepts of linked list, hash functions, and sharding, from computer science.\\nWe argue that these concepts may be more relevant to studies of the neural\\nmechanisms of memory than has been previously appreciated. In turn, we also\\nhighlight that certain phenomena studied in the brain, namely metacognition,\\nreality monitoring, and how perceptual conscious experiences come about, may\\ninspire development in blockchain technology too, specifically regarding\\nprobabilistic consensus protocols.\\n',\n",
       " '  Time-varying network topologies can deeply influence dynamical processes\\nmediated by them. Memory effects in the pattern of interactions among\\nindividuals are also known to affect how diffusive and spreading phenomena take\\nplace. In this paper we analyze the combined effect of these two ingredients on\\nepidemic dynamics on networks. We study the susceptible-infected-susceptible\\n(SIS) and the susceptible-infected-removed (SIR) models on the recently\\nintroduced activity-driven networks with memory. By means of an activity-based\\nmean-field approach we derive, in the long time limit, analytical predictions\\nfor the epidemic threshold as a function of the parameters describing the\\ndistribution of activities and the strength of the memory effects. Our results\\nshow that memory reduces the threshold, which is the same for SIS and SIR\\ndynamics, therefore favouring epidemic spreading. The theoretical approach\\nperfectly agrees with numerical simulations in the long time asymptotic regime.\\nStrong aging effects are present in the preasymptotic regime and the epidemic\\nthreshold is deeply affected by the starting time of the epidemics. We discuss\\nin detail the origin of the model-dependent preasymptotic corrections, whose\\nunderstanding could potentially allow for epidemic control on correlated\\ntemporal networks.\\n',\n",
       " '  A long-standing obstacle to progress in deep learning is the problem of\\nvanishing and exploding gradients. Although, the problem has largely been\\novercome via carefully constructed initializations and batch normalization,\\narchitectures incorporating skip-connections such as highway and resnets\\nperform much better than standard feedforward architectures despite well-chosen\\ninitialization and batch normalization. In this paper, we identify the\\nshattered gradients problem. Specifically, we show that the correlation between\\ngradients in standard feedforward networks decays exponentially with depth\\nresulting in gradients that resemble white noise whereas, in contrast, the\\ngradients in architectures with skip-connections are far more resistant to\\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\\nsupport of the analysis, on both fully-connected networks and convnets.\\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\\nshattering, with preliminary experiments showing the new initialization allows\\nto train very deep networks without the addition of skip-connections.\\n',\n",
       " '  We study the band structure topology and engineering from the interplay\\nbetween local moments and itinerant electrons in the context of pyrochlore\\niridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction\\nelectrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.\\nWhile the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned\\ninto an ordered spin ice with a finite ordering wavevector, dubbed\\n\"Melko-Hertog-Gingras\" state, by varying Ir and O contents. We point out that\\nthe ordered spin ice of the Pr local moments generates an internal magnetic\\nfield that reconstructs the band structure of the Luttinger semimetal. Besides\\nthe broad existence of Weyl nodes, we predict that the magnetic translation of\\nthe \"Melko-Hertog-Gingras\" state for the Pr moments protects the Dirac band\\ntouching at certain time reversal invariant momenta for the Ir conduction\\nelectrons. We propose the magnetic fields to control the Pr magnetic structure\\nand thereby indirectly influence the topological and other properties of the Ir\\nelectrons. Our prediction may be immediately tested in the ordered\\nPr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed\\nexamination of the band structure, magneto-transport, and other properties of\\nPr$_2$Ir$_2$O$_7$.\\n',\n",
       " '  Boundary value problems for Sturm-Liouville operators with potentials from\\nthe class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the\\npotentials are known on all the edges of the graph except two, and show that\\nthe potentials on the remaining edges can be constructed by fractional parts of\\ntwo spectra. A uniqueness theorem is proved, and an algorithm for the\\nconstructive solution of the partial inverse problem is provided. The main\\ningredient of the proofs is the Riesz-basis property of specially constructed\\nsystems of functions.\\n',\n",
       " '  The topological morphology--order of zeros at the positions of electrons with\\nrespect to a specific electron--of Laughlin state at filling fractions $1/m$\\n($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the\\npositions of other electrons. Although fairly accurate ground state wave\\nfunctions for most of the other quantum Hall states in the lowest Landau level\\nare quite well-known, it had been an open problem in expressing the ground\\nstate wave functions in terms of flux-attachment to particles, {\\\\em a la}, this\\nmorphology of Laughlin state. With a very general consideration of\\nflux-particle relations only, in spherical geometry, we here report a novel\\nmethod for determining morphologies of these states. Based on these, we\\nconstruct almost exact ground state wave-functions for the Coulomb interaction.\\nAlthough the form of interaction may change the ground state wave-function, the\\nsame morphology constructs the latter irrespective of the nature of the\\ninteraction between electrons.\\n',\n",
       " \"  The purpose of this paper is to formulate and study a common refinement of a\\nversion of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's\\n$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued\\nfunctions under a generalization of Yoshida's conjecture on the transcendental\\nparts of CM-periods. Then we conjecture a reciprocity law on their special\\nvalues concerning the absolute Frobenius action. We show that our conjecture\\nimplies a part of Stark's conjecture when the base field is an arbitrary real\\nfield and the splitting place is its real place. It also implies a refinement\\nof the Gross-Stark conjecture under a certain assumption. When the base field\\nis the rational number field, our conjecture follows from Coleman's formula on\\nFermat curves. We also prove some partial results in other cases.\\n\",\n",
       " \"  This paper considers the problem of autonomous multi-agent cooperative target\\nsearch in an unknown environment using a decentralized framework under a\\nno-communication scenario. The targets are considered as static targets and the\\nagents are considered to be homogeneous. The no-communication scenario\\ntranslates as the agents do not exchange either the information about the\\nenvironment or their actions among themselves. We propose an integrated\\ndecision and control theoretic solution for a search problem which generates\\nfeasible agent trajectories. In particular, a perception based algorithm is\\nproposed which allows an agent to estimate the probable strategies of other\\nagents' and to choose a decision based on such estimation. The algorithm shows\\nrobustness with respect to the estimation accuracy to a certain degree. The\\nperformance of the algorithm is compared with random strategies and numerical\\nsimulation shows considerable advantages.\\n\",\n",
       " '  This study explores the validity of chain effects of clean water, which are\\nknown as the \"Mills-Reincke phenomenon,\" in early twentieth-century Japan.\\nRecent studies have reported that water purifications systems are responsible\\nfor huge contributions to human capital. Although a few studies have\\ninvestigated the short-term effects of water-supply systems in pre-war Japan,\\nlittle is known about the benefits associated with these systems. By analyzing\\ncity-level cause-specific mortality data from the years 1922-1940, we found\\nthat eliminating typhoid fever infections decreased the risk of deaths due to\\nnon-waterborne diseases. Our estimates show that for one additional typhoid\\ndeath, there were approximately one to three deaths due to other causes, such\\nas tuberculosis and pneumonia. This suggests that the observed Mills-Reincke\\nphenomenon could have resulted from the prevention typhoid fever in a\\npreviously-developing Asian country.\\n',\n",
       " '  Developing neural network image classification models often requires\\nsignificant architecture engineering. In this paper, we study a method to learn\\nthe model architectures directly on the dataset of interest. As this approach\\nis expensive when the dataset is large, we propose to search for an\\narchitectural building block on a small dataset and then transfer the block to\\na larger dataset. The key contribution of this work is the design of a new\\nsearch space (the \"NASNet search space\") which enables transferability. In our\\nexperiments, we search for the best convolutional layer (or \"cell\") on the\\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\\ntogether more copies of this cell, each with their own parameters to design a\\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\\nnew regularization technique called ScheduledDropPath that significantly\\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\\nreduction of 28% in computational demand from the previous state-of-the-art\\nmodel. When evaluated at different levels of computational cost, accuracies of\\nNASNets exceed those of the state-of-the-art human-designed models. For\\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\\n3.1% better than equivalently-sized, state-of-the-art models for mobile\\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\\ndataset.\\n',\n",
       " '  We propose a new multi-frame method for efficiently computing scene flow\\n(dense depth and optical flow) and camera ego-motion for a dynamic scene\\nobserved from a moving stereo camera rig. Our technique also segments out\\nmoving objects from the rigid scene. In our method, we first estimate the\\ndisparity map and the 6-DOF camera motion using stereo matching and visual\\nodometry. We then identify regions inconsistent with the estimated camera\\nmotion and compute per-pixel optical flow only at these regions. This flow\\nproposal is fused with the camera motion-based flow proposal using fusion moves\\nto obtain the final optical flow and motion segmentation. This unified\\nframework benefits all four tasks - stereo, optical flow, visual odometry and\\nmotion segmentation leading to overall higher accuracy and efficiency. Our\\nmethod is currently ranked third on the KITTI 2015 scene flow benchmark.\\nFurthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3\\norders of magnitude faster than the top six methods. We also report a thorough\\nevaluation on challenging Sintel sequences with fast camera and object motion,\\nwhere our method consistently outperforms OSF [Menze and Geiger, 2015], which\\nis currently ranked second on the KITTI benchmark.\\n',\n",
       " '  Let $\\\\K$ be an algebraically closed field of positive characteristic $p$. We\\nmainly classify pointed Hopf algebras over $\\\\K$ of dimension $p^2q$, $pq^2$ and\\n$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete\\nclassification of such Hopf algebras except two subcases when they are not\\ngenerated by the first terms of coradical filtration. In particular, we obtain\\nmany new examples of non-commutative and non-cocommutative finite-dimensional\\nHopf algebras.\\n',\n",
       " '  We present the mixed Galerkin discretization of distributed parameter\\nport-Hamiltonian systems. On the prototypical example of hyperbolic systems of\\ntwo conservation laws in arbitrary spatial dimension, we derive the main\\ncontributions: (i) A weak formulation of the underlying geometric\\n(Stokes-Dirac) structure with a segmented boundary according to the causality\\nof the boundary ports. (ii) The geometric approximation of the Stokes-Dirac\\nstructure by a finite-dimensional Dirac structure is realized using a mixed\\nGalerkin approach and power-preserving linear maps, which define minimal\\ndiscrete power variables. (iii) With a consistent approximation of the\\nHamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.\\nBy the degrees of freedom in the power-preserving maps, the resulting family of\\nstructure-preserving schemes allows for trade-offs between centered\\napproximations and upwinding. We illustrate the method on the example of\\nWhitney finite elements on a 2D simplicial triangulation and compare the\\neigenvalue approximation in 1D with a related approach.\\n',\n",
       " '  The regularity of earthquakes, their destructive power, and the nuisance of\\nground vibration in urban environments, all motivate designs of defence\\nstructures to lessen the impact of seismic and ground vibration waves on\\nbuildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and\\nup to a few tens of Hz for vibrations generated by human activities, cause a\\nlarge amount of damage, or inconvenience, depending on the geological\\nconditions they can travel considerable distances and may match the resonant\\nfundamental frequency of buildings. The ultimate aim of any seismic\\nmetamaterial, or any other seismic shield, is to protect over this entire range\\nof frequencies, the long wavelengths involved, and low frequency, have meant\\nthis has been unachievable to date.\\nElastic flexural waves, applicable in the mechanical vibrations of thin\\nelastic plates, can be designed to have a broad zero-frequency stop-band using\\na periodic array of very small clamped circles. Inspired by this experimental\\nand theoretical observation, all be it in a situation far removed from seismic\\nwaves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)\\nand body (pressure P and shear S) wave reflectors at very large wavelengths in\\nstructured soils modelled as a fully elastic layer periodically clamped to\\nbedrock.\\nWe identify zero frequency stop-bands that only exist in the limit of columns\\nof concrete clamped at their base to the bedrock. In a realistic configuration\\nof a sedimentary basin 15 meters deep we observe a zero frequency stop-band\\ncovering a broad frequency range of $0$ to $30$ Hz.\\n',\n",
       " \"  In this paper, we prove some difference analogue of second main theorems of\\nmeromorphic mapping from Cm into an algebraic variety V intersecting a finite\\nset of fixed hypersurfaces in subgeneral position. As an application, we prove\\na result on algebraically degenerate of holomorphic curves intersecting\\nhypersurfaces and difference analogue of Picard's theorem on holomorphic\\ncurves. Furthermore, we obtain a second main theorem of meromorphic mappings\\nintersecting hypersurfaces in N-subgeneral position for Veronese embedding in\\nPn(C) and a uniqueness theorem sharing hypersurfaces.\\n\",\n",
       " \"  Large-scale datasets have played a significant role in progress of neural\\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\\ngeneral multi-label video classification. It was created from over 7 million\\nYouTube videos (450,000 hours of video) and includes video labels from a\\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\\npre-extracted audio & visual features from every second of video (3.2 billion\\nfeature vectors in total). Google cloud recently released the datasets and\\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\\nCompetitors are challenged to develop classification algorithms that assign\\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\\nby the competition, we started exploration of audio understanding and\\nclassification using deep learning algorithms and ensemble methods. We built\\nseveral baseline predictions according to the benchmark paper and public github\\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\\nbase level 77% to 80.7% through approaches of ensemble.\\n\",\n",
       " '  Observational data collected during experiments, such as the planned Fire and\\nSmoke Model Evaluation Experiment (FASMEE), are critical for progressing and\\ntransitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM\\ninto operational use. Historical meteorological data, representing typical\\nweather conditions for the anticipated burn locations and times, have been\\nprocessed to initialize and run a set of simulations representing the planned\\nexperimental burns. Based on an analysis of these numerical simulations, this\\npaper provides recommendations on the experimental setup that include the\\nignition procedures, size and duration of the burns, and optimal sensor\\nplacement. New techniques are developed to initialize coupled fire-atmosphere\\nsimulations with weather conditions typical of the planned burn locations and\\ntime of the year. Analysis of variation and sensitivity analysis of simulation\\ndesign to model parameters by repeated Latin Hypercube Sampling are used to\\nassess the locations of the sensors. The simulations provide the locations of\\nthe measurements that maximize the expected variation of the sensor outputs\\nwith the model parameters.\\n',\n",
       " '  For a knot $K$ in a homology $3$-sphere $\\\\Sigma$, let $M$ be the result of\\n$2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our\\nfirst theorem is that if the first homology of $X$ is finite cyclic and $M$ is\\na Seifert fibered space with $N\\\\ge 3$ singular fibers, then $N\\\\ge 4$ if and\\nonly if the first homology of the universal abelian covering of $X$ is\\ninfinite. Our second theorem is that under an appropriate assumption on the\\nAlexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\\\\pm 1$\\n(i.e.\\\\ integral surgery).\\n',\n",
       " \"  Sparse feature selection is necessary when we fit statistical models, we have\\naccess to a large group of features, don't know which are relevant, but assume\\nthat most are not. Alternatively, when the number of features is larger than\\nthe available data the model becomes over parametrized and the sparse feature\\nselection task involves selecting the most informative variables for the model.\\nWhen the model is a simple location model and the number of relevant features\\ndoes not grow with the total number of features, sparse feature selection\\ncorresponds to sparse mean estimation. We deal with a simplified mean\\nestimation problem consisting of an additive model with gaussian noise and mean\\nthat is in a restricted, finite hypothesis space. This restriction simplifies\\nthe mean estimation problem into a selection problem of combinatorial nature.\\nAlthough the hypothesis space is finite, its size is exponential in the\\ndimension of the mean. In limited data settings and when the size of the\\nhypothesis space depends on the amount of data or on the dimension of the data,\\nchoosing an approximation set of hypotheses is a desirable approach. Choosing a\\nset of hypotheses instead of a single one implies replacing the bias-variance\\ntrade off with a resolution-stability trade off. Generalization capacity\\nprovides a resolution selection criterion based on allowing the learning\\nalgorithm to communicate the largest amount of information in the data to the\\nlearner without error. In this work the theory of approximation set coding and\\ngeneralization capacity is explored in order to understand this approach. We\\nthen apply the generalization capacity criterion to the simplified sparse mean\\nestimation problem and detail an importance sampling algorithm which at once\\nsolves the difficulty posed by large hypothesis spaces and the slow convergence\\nof uniform sampling algorithms.\\n\",\n",
       " '  In this letter, we consider the joint power and admission control (JPAC)\\nproblem by assuming that only the channel distribution information (CDI) is\\navailable. Under this assumption, we formulate a new chance (probabilistic)\\nconstrained JPAC problem, where the signal to interference plus noise ratio\\n(SINR) outage probability of the supported links is enforced to be not greater\\nthan a prespecified tolerance. To efficiently deal with the chance SINR\\nconstraint, we employ the sample approximation method to convert them into\\nfinitely many linear constraints. Then, we propose a convex approximation based\\ndeflation algorithm for solving the sample approximation JPAC problem. Compared\\nto the existing works, this letter proposes a novel two-timescale JPAC\\napproach, where admission control is performed by the proposed deflation\\nalgorithm based on the CDI in a large timescale and transmission power is\\nadapted instantly with fast fadings in a small timescale. The effectiveness of\\nthe proposed algorithm is illustrated by simulations.\\n',\n",
       " '  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest\\nstar, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness\\nwere similar to coronally active late-type dwarf members. Later, in 2010, a\\nHubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found\\nfar-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious\\noffset of the ROSAT source, suggested that a late-type companion might be\\nresponsible for the X-rays. Recently, a multi-faceted program tested that\\npremise. Groundbased optical coronography, and near-UV imaging with HST Wide\\nField Camera 3, searched for any close-in faint candidate coronal objects, but\\nwithout success. Then, a Chandra pointing found the X-ray source single and\\ncoincident with the bright star. Significantly, the SiIV emissions of Alpha\\nPersei, in a deeper FUV spectrum collected by HST COS as part of the joint\\nprogram, aligned well with chromospheric atomic oxygen (which must be intrinsic\\nto the luminous star), within the context of cooler late-F and early-G\\nsupergiants, including Cepheid variables. This pointed to the X-rays as the\\nfundamental anomaly. The over-luminous X-rays still support the case for a\\nhyperactive dwarf secondary, albeit now spatially unresolved. However, an\\nalternative is that Alpha Persei represents a novel class of coronal source.\\nResolving the first possibility now has become more difficult, because the easy\\nsolution -- a well separated companion -- has been eliminated. Testing the\\nother possibility will require a broader high-energy census of the early-F\\nsupergiants.\\n',\n",
       " '  Realistic music generation is a challenging task. When building generative\\nmodels of music that are learnt from data, typically high-level representations\\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\\nparticular performance. But these nuances are very important for our perception\\nof musicality and realism, so in this work we embark on modelling music in the\\nraw audio domain. It has been shown that autoregressive models excel at\\ngenerating raw audio waveforms of speech, but when applied to music, we find\\nthem biased towards capturing local signal structure at the expense of\\nmodelling long-range correlations. This is problematic because music exhibits\\nstructure at many different timescales. In this work, we explore autoregressive\\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\\ncapture long-range correlations in waveforms. We find that they allow us to\\nunconditionally generate piano music directly in the raw audio domain, which\\nshows stylistic consistency across tens of seconds.\\n',\n",
       " '  Young asteroid families are unique sources of information about fragmentation\\nphysics and the structure of their parent bodies, since their physical\\nproperties have not changed much since their birth. Families have different\\nproperties such as age, size, taxonomy, collision severity and others, and\\nunderstanding the effect of those properties on our observations of the\\nsize-frequency distribution (SFD) of family fragments can give us important\\ninsights into the hypervelocity collision processes at scales we cannot achieve\\nin our laboratories. Here we take as an example the very young Datura family,\\nwith a small 8-km parent body, and compare its size distribution to other\\nfamilies, with both large and small parent bodies, and created by both\\ncatastrophic and cratering formation events. We conclude that most likely\\nexplanation for the shallower size distribution compared to larger families is\\na more pronounced observational bias because of its small size. Its size\\ndistribution is perfectly normal when its parent body size is taken into\\naccount. We also discuss some other possibilities. In addition, we study\\nanother common feature: an offset or \"bump\" in the distribution occurring for a\\nfew of the larger elements. We hypothesize that it can be explained by a newly\\ndescribed regime of cratering, \"spall cratering\", which controls the majority\\nof impact craters on the surface of small asteroids like Datura.\\n',\n",
       " '  We provide a graph formula which describes an arbitrary monomial in {\\\\omega}\\nclasses (also referred to as stable {\\\\psi} classes) in terms of a simple family\\nof dual graphs (pinwheel graphs) with edges decorated by rational functions in\\n{\\\\psi} classes. We deduce some numerical consequences and in particular a\\ncombinatorial formula expressing top intersections of \\\\k{appa} classes on Mg in\\nterms of top intersections of {\\\\psi} classes.\\n',\n",
       " '  Tomography has made a radical impact on diverse fields ranging from the study\\nof 3D atomic arrangements in matter to the study of human health in medicine.\\nDespite its very diverse applications, the core of tomography remains the same,\\nthat is, a mathematical method must be implemented to reconstruct the 3D\\nstructure of an object from a number of 2D projections. In many scientific\\napplications, however, the number of projections that can be measured is\\nlimited due to geometric constraints, tolerable radiation dose and/or\\nacquisition speed. Thus it becomes an important problem to obtain the\\nbest-possible reconstruction from a limited number of projections. Here, we\\npresent the mathematical implementation of a tomographic algorithm, termed\\nGENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between\\nreal and reciprocal space, GENFIRE searches for a global solution that is\\nconcurrently consistent with the measured data and general physical\\nconstraints. The algorithm requires minimal human intervention and also\\nincorporates angular refinement to reduce the tilt angle error. We demonstrate\\nthat GENFIRE can produce superior results relative to several other popular\\ntomographic reconstruction techniques by numerical simulations, and by\\nexperimentally by reconstructing the 3D structure of a porous material and a\\nfrozen-hydrated marine cyanobacterium. Equipped with a graphical user\\ninterface, GENFIRE is freely available from our website and is expected to find\\nbroad applications across different disciplines.\\n',\n",
       " '  Generative Adversarial Networks (GANs) excel at creating realistic images\\nwith complex models for which maximum likelihood is infeasible. However, the\\nconvergence of GAN training has still not been proved. We propose a two\\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\\nfor both the discriminator and the generator. Using the theory of stochastic\\napproximation, we prove that the TTUR converges under mild assumptions to a\\nstationary local Nash equilibrium. The convergence carries over to the popular\\nAdam optimization, for which we prove that it follows the dynamics of a heavy\\nball with friction and thus prefers flat minima in the objective landscape. For\\nthe evaluation of the performance of GANs at image generation, we introduce the\\n\"Fréchet Inception Distance\" (FID) which captures the similarity of generated\\nimages to real ones better than the Inception Score. In experiments, TTUR\\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\\nBedrooms, and the One Billion Word Benchmark.\\n',\n",
       " '  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we\\npresent new measurements of activity and magnetic field proxies of 442 low-mass\\nK5-M7 dwarfs. The objects were analysed as potential targets to search for\\nplanetary-mass companions with the new spectropolarimeter and high-precision\\nvelocimeter, SPIRou. We have analysed their high-resolution spectra in an\\nhomogeneous way: circular polarisation, chromospheric features, and Zeeman\\nbroadening of the FeH infrared line. The complex relationship between these\\nactivity indicators is analysed: while no strong connection is found between\\nthe large-scale and small-scale magnetic fields, the latter relates with the\\nnon-thermal flux originating in the chromosphere.\\nWe then examine the relationship between various activity diagnostics and the\\noptical radial-velocity jitter available in the literature, especially for\\nplanet host stars. We use this to derive for all stars an activity merit\\nfunction (higher for quieter stars) with the goal of identifying the most\\nfavorable stars where the radial-velocity jitter is low enough for planet\\nsearches. We find that the main contributors to the RV jitter are the\\nlarge-scale magnetic field and the chromospheric non-thermal emission.\\nIn addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed\\nalong their rotation using the spectropolarimetric mode, and we derive their\\nmagnetic topology. These very slow rotators are good representatives of future\\nSPIRou targets. They are compared to other stars where the magnetic topology is\\nalso known. The poloidal component of the magnetic field is predominent in all\\nthree stars.\\n',\n",
       " '  Inferring directional connectivity from point process data of multiple\\nelements is desired in various scientific fields such as neuroscience,\\ngeography, economics, etc. Here, we propose an inference procedure for this\\ngoal based on the kinetic Ising model. The procedure is composed of two steps:\\n(1) determination of the time-bin size for transforming the point-process data\\nto discrete time binary data and (2) screening of relevant couplings from the\\nestimated networks. For these, we develop simple methods based on information\\ntheory and computational statistics. Applications to data from artificial and\\n\\\\textit{in vitro} neuronal networks show that the proposed procedure performs\\nfairly well when identifying relevant couplings, including the discrimination\\nof their signs, with low computational cost. These results highlight the\\npotential utility of the kinetic Ising model to analyze real interacting\\nsystems with event occurrences.\\n',\n",
       " '  Support vector machines (SVMs) are an important tool in modern data analysis.\\nTraditionally, support vector machines have been fitted via quadratic\\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\\nan alternative approach to SVM fitting via the majorization--minimization (MM)\\nparadigm. Algorithms that are derived via MM algorithm constructions can be\\nshown to monotonically decrease their objectives at each iteration, as well as\\nbe globally convergent to stationary points. We demonstrate the construction of\\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\\nfor SVM risk minimization problems involving the hinge, least-square,\\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\\npenalizations. Successful implementations of our algorithms are presented via\\nsome numerical examples.\\n',\n",
       " '  Estimating vaccination uptake is an integral part of ensuring public health.\\nIt was recently shown that vaccination uptake can be estimated automatically\\nfrom web data, instead of slowly collected clinical records or population\\nsurveys. All prior work in this area assumes that features of vaccination\\nuptake collected from the web are temporally regular. We present the first ever\\nmethod to remove this assumption from vaccination uptake estimation: our method\\ndynamically adapts to temporal fluctuations in time series web data used to\\nestimate vaccination uptake. We show our method to outperform the state of the\\nart compared to competitive baselines that use not only web data but also\\ncurated clinical data. This performance improvement is more pronounced for\\nvaccines whose uptake has been irregular due to negative media attention (HPV-1\\nand HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of\\n12 years old (whose vaccination is more irregular compared to younger\\nchildren).\\n',\n",
       " '  We show that every invertible strong mixing transformation on a Lebesgue\\nspace has strictly over-recurrent sets. Also, we give an explicit procedure for\\nconstructing strong mixing transformations with no under-recurrent sets. This\\nanswers both parts of a question of V. Bergelson.\\nWe define $\\\\epsilon$-over-recurrence and show that given $\\\\epsilon > 0$, any\\nergodic measure preserving invertible transformation (including discrete\\nspectrum) has $\\\\epsilon$-over-recurrent sets of arbitrarily small measure.\\nDiscrete spectrum transformations and rotations do not have over-recurrent\\nsets, but we construct a weak mixing rigid transformation with strictly\\nover-recurrent sets.\\n',\n",
       " \"  Development of a mesoscale neural circuitry map of the common marmoset is an\\nessential task due to the ideal characteristics of the marmoset as a model\\norganism for neuroscience research. To facilitate this development there is a\\nneed for new computational tools to cross-register multi-modal data sets\\ncontaining MRI volumes as well as multiple histological series, and to register\\nthe combined data set to a common reference atlas. We present a fully automatic\\npipeline for same-subject-MRI guided reconstruction of image volumes from a\\nseries of histological sections of different modalities, followed by\\ndiffeomorphic mapping to a reference atlas. We show registration results for\\nNissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo\\nMRI as our reference and show that our method achieves accurate registration\\nand eliminates artifactual warping that may be result from the absence of a\\nreference MRI data set. Examination of the determinant of the local metric\\ntensor of the diffeomorphic mapping between each subject's ex-vivo MRI and\\nresultant Nissl reconstruction allows an unprecedented local quantification of\\ngeometrical distortions resulting from the histological processing, showing a\\nslight shrinkage, a median linear scale change of ~-1% in going from the\\nex-vivo MRI to the tape-transfer generated histological image data.\\n\",\n",
       " '  The system that we study in this paper contains a set of users that observe a\\ndiscrete memoryless multiple source and communicate via noise-free channels\\nwith the aim of attaining omniscience, the state that all users recover the\\nentire multiple source. We adopt the concept of successive omniscience (SO),\\ni.e., letting the local omniscience in some user subset be attained before the\\nglobal omniscience in the entire system, and consider the problem of how to\\nefficiently attain omniscience in a successive manner. Based on the existing\\nresults on SO, we propose a CompSetSO algorithm for determining a complimentary\\nset, a user subset in which the local omniscience can be attained first without\\nincreasing the sum-rate, the total number of communications, for the global\\nomniscience. We also derive a sufficient condition for a user subset to be\\ncomplimentary so that running the CompSetSO algorithm only requires a lower\\nbound, instead of the exact value, of the minimum sum-rate for attaining global\\nomniscience. The CompSetSO algorithm returns a complimentary user subset in\\npolynomial time. We show by example how to recursively apply the CompSetSO\\nalgorithm so that the global omniscience can be attained by multi-stages of SO.\\n',\n",
       " '  In this paper we present a novel methodology for identifying scholars with a\\nTwitter account. By combining bibliometric data from Web of Science and Twitter\\nusers identified by Altmetric.com we have obtained the largest set of\\nindividual scholars matched with Twitter users made so far. Our methodology\\nconsists of a combination of matching algorithms, considering different\\nlinguistic elements of both author names and Twitter names; followed by a\\nrule-based scoring system that weights the common occurrence of several\\nelements related with the names, individual elements and activities of both\\nTwitter users and scholars matched. Our results indicate that about 2% of the\\noverall population of scholars in the Web of Science is active on Twitter. By\\ndomain we find a strong presence of researchers from the Social Sciences and\\nthe Humanities. Natural Sciences is the domain with the lowest level of\\nscholars on Twitter. Researchers on Twitter also tend to be younger than those\\nthat are not on Twitter. As this is a bibliometric-based approach, it is\\nimportant to highlight the reliance of the method on the number of publications\\nproduced and tweeted by the scholars, thus the share of scholars on Twitter\\nranges between 1% and 5% depending on their level of productivity. Further\\nresearch is suggested in order to improve and expand the methodology.\\n',\n",
       " '  As a measure for the centrality of a point in a set of multivariate data,\\nstatistical depth functions play important roles in multivariate analysis,\\nbecause one may conveniently construct descriptive as well as inferential\\nprocedures relying on them. Many depth notions have been proposed in the\\nliterature to fit to different applications. However, most of them are mainly\\ndeveloped for the location setting. In this paper, we discuss the possibility\\nof extending some of them into the regression setting. A general concept of\\nregression depth function is also provided.\\n',\n",
       " \"  When a two-dimensional electron gas is exposed to a perpendicular magnetic\\nfield and an in-plane electric field, its conductance becomes quantized in the\\ntransverse in-plane direction: this is known as the quantum Hall (QH) effect.\\nThis effect is a result of the nontrivial topology of the system's electronic\\nband structure, where an integer topological invariant known as the first Chern\\nnumber leads to the quantization of the Hall conductance. Interestingly, it was\\nshown that the QH effect can be generalized mathematically to four spatial\\ndimensions (4D), but this effect has never been realized for the obvious reason\\nthat experimental systems are bound to three spatial dimensions. In this work,\\nwe harness the high tunability and control offered by photonic waveguide arrays\\nto experimentally realize a dynamically-generated 4D QH system using a 2D array\\nof coupled optical waveguides. The inter-waveguide separation is constructed\\nsuch that the propagation of light along the device samples over\\nhigher-dimensional momenta in the directions orthogonal to the two physical\\ndimensions, thus realizing a 2D topological pump. As a result, the device's\\nband structure is associated with 4D topological invariants known as second\\nChern numbers which support a quantized bulk Hall response with a 4D symmetry.\\nIn a finite-sized system, the 4D topological bulk response is carried by\\nlocalized edges modes that cross the sample as a function of of the modulated\\nauxiliary momenta. We directly observe this crossing through photon pumping\\nfrom edge-to-edge and corner-to-corner of our system. These are equivalent to\\nthe pumping of charge across a 4D system from one 3D hypersurface to the\\nopposite one and from one 2D hyperedge to another, and serve as first\\nexperimental realization of higher-dimensional topological physics.\\n\",\n",
       " '  In many applications involving large dataset or online updating, stochastic\\ngradient descent (SGD) provides a scalable way to compute parameter estimates\\nand has gained increasing popularity due to its numerical convenience and\\nmemory efficiency. While the asymptotic properties of SGD-based estimators have\\nbeen established decades ago, statistical inference such as interval estimation\\nremains much unexplored. The traditional resampling method such as the\\nbootstrap is not computationally feasible since it requires to repeatedly draw\\nindependent samples from the entire dataset. The plug-in method is not\\napplicable when there are no explicit formulas for the covariance matrix of the\\nestimator. In this paper, we propose a scalable inferential procedure for\\nstochastic gradient descent, which, upon the arrival of each observation,\\nupdates the SGD estimate as well as a large number of randomly perturbed SGD\\nestimates. The proposed method is easy to implement in practice. We establish\\nits theoretical properties for a general class of models that includes\\ngeneralized linear models and quantile regression models as special cases. The\\nfinite-sample performance and numerical utility is evaluated by simulation\\nstudies and two real data applications.\\n',\n",
       " '  In the work of Peng et al. in 2012, a new measure was proposed for fault\\ndiagnosis of systems: namely, g-good-neighbor conditional diagnosability, which\\nrequires that any fault-free vertex has at least g fault-free neighbors in the\\nsystem. In this paper, we establish the g-good-neighbor conditional\\ndiagnosability of locally twisted cubes under the PMC model and the MM^* model.\\n',\n",
       " \"  Categories of polymorphic lenses in computer science, and of open games in\\ncompositional game theory, have a curious structure that is reminiscent of\\ncompact closed categories, but differs in some crucial ways. Specifically they\\nhave a family of morphisms that behave like the counits of a compact closed\\ncategory, but have no corresponding units; and they have a `partial' duality\\nthat behaves like transposition in a compact closed category when it is\\ndefined. We axiomatise this structure, which we refer to as a `teleological\\ncategory'. We precisely define a diagrammatic language suitable for these\\ncategories, and prove a coherence theorem for them. This underpins the use of\\ndiagrammatic reasoning in compositional game theory, which has previously been\\nused only informally.\\n\",\n",
       " '  We present an efficient algorithm to compute Euler characteristic curves of\\ngray scale images of arbitrary dimension. In various applications the Euler\\ncharacteristic curve is used as a descriptor of an image.\\nOur algorithm is the first streaming algorithm for Euler characteristic\\ncurves. The usage of streaming removes the necessity to store the entire image\\nin RAM. Experiments show that our implementation handles terabyte scale images\\non commodity hardware. Due to lock-free parallelism, it scales well with the\\nnumber of processor cores. Our software---CHUNKYEuler---is available as open\\nsource on Bitbucket.\\nAdditionally, we put the concept of the Euler characteristic curve in the\\nwider context of computational topology. In particular, we explain the\\nconnection with persistence diagrams.\\n',\n",
       " '  We give a new example of an automata group of intermediate growth. It is\\ngenerated by an automaton with 4 states on an alphabet with 8 letters. This\\nautomata group has exponential activity and its limit space is not simply\\nconnected.\\n',\n",
       " '  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to\\nBose-Einstein condensation (BEC) is difficult to realize in quantum materials\\nbecause, unlike in ultracold atoms, one cannot tune the pairing interaction. We\\nrealize the BCS-BEC crossover in a nearly compensated semimetal\\nFe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\\\\epsilon_F$, via\\nchemical doping, which permits us to systematically change $\\\\Delta /\\n\\\\epsilon_F$ from 0.16 to 0.5 were $\\\\Delta$ is the superconducting (SC) gap. We\\nuse angle-resolved photoemission spectroscopy to measure the Fermi energy, the\\nSC gap and characteristic changes in the SC state electronic dispersion as the\\nsystem evolves from a BCS to a BEC regime. Our results raise important\\nquestions about the crossover in multiband superconductors which go beyond\\nthose addressed in the context of cold atoms.\\n',\n",
       " '  Model compression is essential for serving large deep neural nets on devices\\nwith limited resources or applications that require real-time responses. As a\\ncase study, a state-of-the-art neural language model usually consists of one or\\nmore recurrent layers sandwiched between an embedding layer used for\\nrepresenting input tokens and a softmax layer for generating output tokens. For\\nproblems with a very large vocabulary size, the embedding and the softmax\\nmatrices can account for more than half of the model size. For instance, the\\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\\nmethod for neural language models, based on vocabulary-partition (block) based\\nlow-rank matrix approximation and the inherent frequency distribution of tokens\\n(the power-law distribution of words). The experimental results show our method\\ncan significantly outperform traditional compression methods such as low-rank\\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\\ncompression rate for the embedding and softmax matrices, and when combined with\\nquantization, our method can achieve 26 times compression rate, which\\ntranslates to a factor of 12.8 times compression for the entire model with very\\nlittle degradation in perplexity.\\n',\n",
       " '  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy\\nwere implanted into SiO2 matrix with Different fluences. The implanted samples\\nwere annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of\\nimplanted as well as annealed samples were captured by the atomic force\\nmicroscopy (AFM). Two dimension (2D) multifractal detrended fluctuation\\nanalysis (MFDFA) based on the partition function approach has been used to\\nstudy the surfaces of ion implanted and annealed samples. The partition\\nfunction is used to calculate generalized Hurst exponent with the segment size.\\nMoreover, it is seen that the generalized Hurst exponents vary nonlinearly with\\nthe moment, thereby exhibiting the multifractal nature. The multifractality of\\nsurface is pronounced after annealing for the surface implanted with fluence\\n7.5X1016 ions/cm^2.\\n',\n",
       " '  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)\\nmaterial with liquid metal, Lead Lithium ( Pb-Li) has been studied under static\\ncondition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000\\nand 9000 hours. Corrosion rate was calculated from weight loss measurements.\\nMicrostructure analysis was carried out using SEM and chemical composition by\\nSEM-EDX measurements. Micro Vickers hardness and tensile testing were also\\ncarried out. Chromium was found leaching from the near surface regions and\\nsurface hardness was found to decrease in all the three cases. Grain boundaries\\nwere affected. Some grains got detached from the surface giving rise to pebble\\nlike structures in the surface micrographs. There was no significant reduction\\nin the tensile strength, after exposure to liquid metal. This paper discusses\\nthe experimental details and the results obtained.\\n',\n",
       " '  This paper presents an overview and discussion of magnetocapillary\\nself-assemblies. New results are presented, in particular concerning the\\npossible development of future applications. These self-organizing structures\\npossess the notable ability to move along an interface when powered by an\\noscillatory, uniform magnetic field. The system is constructed as follows. Soft\\nmagnetic particles are placed on a liquid interface, and submitted to a\\nmagnetic induction field. An attractive force due to the curvature of the\\ninterface around the particles competes with an interaction between magnetic\\ndipoles. Ordered structures can spontaneously emerge from these conditions.\\nFurthermore, time-dependent magnetic fields can produce a wide range of dynamic\\nbehaviours, including non-time-reversible deformation sequences that produce\\ntranslational motion at low Reynolds number. In other words, due to a\\nspontaneous breaking of time-reversal symmetry, the assembly can turn into a\\nsurface microswimmer. Trajectories have been shown to be precisely\\ncontrollable. As a consequence, this system offers a way to produce microrobots\\nable to perform different tasks. This is illustrated in this paper by the\\ncapture, transport and release of a floating cargo, and the controlled mixing\\nof fluids at low Reynolds number.\\n',\n",
       " '  For the problem of nonparametric detection of signal in Gaussian white noise\\nwe point out strong asymptotically minimax tests. The sets of alternatives are\\na ball in Besov space $B^r_{2\\\\infty}$ with \"small\" balls in $L_2$ removed.\\n',\n",
       " '  Metabolic flux balance analyses are a standard tool in analysing metabolic\\nreaction rates compatible with measurements, steady-state and the metabolic\\nreaction network stoichiometry. Flux analysis methods commonly place\\nunrealistic assumptions on fluxes due to the convenience of formulating the\\nproblem as a linear programming model, and most methods ignore the notable\\nuncertainty in flux estimates. We introduce a novel paradigm of Bayesian\\nmetabolic flux analysis that models the reactions of the whole genome-scale\\ncellular system in probabilistic terms, and can infer the full flux vector\\ndistribution of genome-scale metabolic systems based on exchange and\\nintracellular (e.g. 13C) flux measurements, steady-state assumptions, and\\ntarget function assumptions. The Bayesian model couples all fluxes jointly\\ntogether in a simple truncated multivariate posterior distribution, which\\nreveals informative flux couplings. Our model is a plug-in replacement to\\nconventional metabolic balance methods, such as flux balance analysis (FBA).\\nOur experiments indicate that we can characterise the genome-scale flux\\ncovariances, reveal flux couplings, and determine more intracellular unobserved\\nfluxes in C. acetobutylicum from 13C data than flux variability analysis. The\\nCOBRA compatible software is available at github.com/markusheinonen/bamfa\\n',\n",
       " '  We introduce a robust estimator of the location parameter for the\\nchange-point in the mean based on the Wilcoxon statistic and establish its\\nconsistency for $L_1$ near epoch dependent processes. It is shown that the\\nconsistency rate depends on the magnitude of change. A simulation study is\\nperformed to evaluate finite sample properties of the Wilcoxon-type estimator\\nin standard cases, as well as under heavy-tailed distributions and disturbances\\nby outliers, and to compare it with a CUSUM-type estimator. It shows that the\\nWilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard\\ncases, but outperforms the CUSUM-type estimator in presence of heavy tails or\\noutliers in the data.\\n',\n",
       " '  In glass forming liquids close to the glass transition point, even a very\\nslight increase in the macroscopic density results in a dramatic slowing down\\nof the macroscopic relaxation. Concomitantly, the local density itself\\nfluctuates in space. Therefore, one can imagine that even very small local\\ndensity variations control the local glassy nature. Based on this perspective,\\na model for describing growing length scale accompanying the vitrification is\\nintroduced, in which we assume that in a subsystem whose density is above a\\ncertain threshold value, $\\\\rho_{\\\\rm c}$, owing to steric constraints, particle\\nrearrangements are highly suppressed for a sufficiently long time period\\n($\\\\sim$ structural relaxation time). We regard such a subsystem as a glassy\\ncluster. Then, based on the statistics of the subsystem-density, we predict\\nthat with compression (increasing average density $\\\\rho$) at a fixed\\ntemperature $T$ in supercooled states, the characteristic length of the\\nclusters, $\\\\xi$, diverges as $\\\\xi\\\\sim(\\\\rho_{\\\\rm c}-\\\\rho)^{-2/d}$, where $d$ is\\nthe spatial dimensionality. This $\\\\xi$ measures the average persistence length\\nof the steric constraints in blocking the rearrangement motions and is\\ndetermined by the subsystem density. Additionally, with decreasing $T$ at a\\nfixed $\\\\rho$, the length scale diverges in the same manner as $\\\\xi\\\\sim(T-T_{\\\\rm\\nc})^{-2/d}$, for which $\\\\rho$ is identical to $\\\\rho_{\\\\rm c}$ at $T=T_{\\\\rm c}$.\\nThe exponent describing the diverging length scale is the same as the one\\npredicted by some theoretical models and indeed has been observed in some\\nsimulations and experiments. However, the basic mechanism for this divergence\\nis different; that is, we do not invoke thermodynamic anomalies associated with\\nthe thermodynamic phase transition as the origin of the growing length scale.\\nWe further present arguements for the cooperative properties based on the\\nclusters.\\n',\n",
       " '  We propose a new Pareto Local Search Algorithm for the many-objective\\ncombinatorial optimization. Pareto Local Search proved to be a very effective\\ntool in the case of the bi-objective combinatorial optimization and it was used\\nin a number of the state-of-the-art algorithms for problems of this kind. On\\nthe other hand, the standard Pareto Local Search algorithm becomes very\\ninefficient for problems with more than two objectives. We build an effective\\nMany-Objective Pareto Local Search algorithm using three new mechanisms: the\\nefficient update of large Pareto archives with ND-Tree data structure, a new\\nmechanism for the selection of the promising solutions for the neighborhood\\nexploration, and a partial exploration of the neighborhoods. We apply the\\nproposed algorithm to the instances of two different problems, i.e. the\\ntraveling salesperson problem and the traveling salesperson problem with\\nprofits with up to 5 objectives showing high effectiveness of the proposed\\nalgorithm.\\n',\n",
       " '  We identify the components of bio-inspired artificial camouflage systems\\nincluding actuation, sensing, and distributed computation. After summarizing\\nrecent results in understanding the physiology and system-level performance of\\na variety of biological systems, we describe computational algorithms that can\\ngenerate similar patterns and have the potential for distributed\\nimplementation. We find that the existing body of work predominately treats\\ncomponent technology in an isolated manner that precludes a material-like\\nimplementation that is scale-free and robust. We conclude with open research\\nchallenges towards the realization of integrated camouflage solutions.\\n',\n",
       " '  In the present work we study Bayesian nonparametric inference for the\\ncontinuous-time M/G/1 queueing system. In the focus of the study is the\\nunobservable service time distribution. We assume that the only available data\\nof the system are the marked departure process of customers with the marks\\nbeing the queue lengths just after departure instants. These marks constitute\\nan embedded Markov chain whose distribution may be parametrized by stochastic\\nmatrices of a special delta form. We develop the theory in order to obtain\\nintegral mixtures of Markov measures with respect to suitable prior\\ndistributions. We have found a sufficient statistic with a distribution of a\\nso-called S-structure sheding some new light on the inner statistical structure\\nof the M/G/1 queue. Moreover, it allows to update suitable prior distributions\\nto the posterior. Our inference methods are validated by large sample results\\nas posterior consistency and posterior normality.\\n',\n",
       " '  We will show that $(1-q)(1-q^2)\\\\dots (1-q^m)$ is a polynomial in $q$ with\\ncoefficients from $\\\\{-1,0,1\\\\}$ iff $m=1,\\\\ 2,\\\\ 3,$ or $5$ and explore some\\ninteresting consequences of this result. We find explicit formulas for the\\n$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\\\\dots$ and\\n$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\\\\dots$. In doing so, we extend certain\\nobservations made by Sudler in 1964. We also discuss the classification of the\\nproducts $(1-q)(1-q^2)\\\\dots (1-q^m)$ and some related series with respect to\\ntheir absolute largest coefficients.\\n',\n",
       " '  In this paper, we develop a position estimation system for Unmanned Aerial\\nVehicles formed by hardware and software. It is based on low-cost devices: GPS,\\ncommercial autopilot sensors and dense optical flow algorithm implemented in an\\nonboard microcomputer. Comparative tests were conducted using our approach and\\nthe conventional one, where only fusion of GPS and inertial sensors are used.\\nExperiments were conducted using a quadrotor in two flying modes: hovering and\\ntrajectory tracking in outdoor environments. Results demonstrate the\\neffectiveness of the proposed approach in comparison with the conventional\\napproaches presented in the vast majority of commercial drones.\\n',\n",
       " '  We study the decomposition of a multivariate Hankel matrix H\\\\_$\\\\sigma$ as a\\nsum of Hankel matrices of small rank in correlation with the decomposition of\\nits symbol $\\\\sigma$ as a sum of polynomial-exponential series. We present a new\\nalgorithm to compute the low rank decomposition of the Hankel operator and the\\ndecomposition of its symbol exploiting the properties of the associated\\nArtinian Gorenstein quotient algebra A\\\\_$\\\\sigma$. A basis of A\\\\_$\\\\sigma$ is\\ncomputed from the Singular Value Decomposition of a sub-matrix of the Hankel\\nmatrix H\\\\_$\\\\sigma$. The frequencies and the weights are deduced from the\\ngeneralized eigenvectors of pencils of shifted sub-matrices of H $\\\\sigma$.\\nExplicit formula for the weights in terms of the eigenvectors avoid us to solve\\na Vandermonde system. This new method is a multivariate generalization of the\\nso-called Pencil method for solving Prony-type decomposition problems. We\\nanalyse its numerical behaviour in the presence of noisy input moments, and\\ndescribe a rescaling technique which improves the numerical quality of the\\nreconstruction for frequencies of high amplitudes. We also present a new Newton\\niteration, which converges locally to the closest multivariate Hankel matrix of\\nlow rank and show its impact for correcting errors on input moments.\\n',\n",
       " '  Linear time-periodic (LTP) dynamical systems frequently appear in the\\nmodeling of phenomena related to fluid dynamics, electronic circuits, and\\nstructural mechanics via linearization centered around known periodic orbits of\\nnonlinear models. Such LTP systems can reach orders that make repeated\\nsimulation or other necessary analysis prohibitive, motivating the need for\\nmodel reduction.\\nWe develop here an algorithmic framework for constructing reduced models that\\nretains the linear time-periodic structure of the original LTP system. Our\\napproach generalizes optimal approaches that have been established previously\\nfor linear time-invariant (LTI) model reduction problems. We employ an\\nextension of the usual H2 Hardy space defined for the LTI setting to\\ntime-periodic systems and within this broader framework develop an a posteriori\\nerror bound expressible in terms of related LTI systems. Optimization of this\\nbound motivates our algorithm. We illustrate the success of our method on two\\nnumerical examples.\\n',\n",
       " '  Broad efforts are underway to capture metadata about research software and\\nretain it across services; notable in this regard is the CodeMeta project. What\\nmetadata are important to have about (research) software? What metadata are\\nuseful for searching for codes? What would you like to learn about astronomy\\nsoftware? This BoF sought to gather information on metadata most desired by\\nresearchers and users of astro software and others interested in registering,\\nindexing, capturing, and doing research on this software. Information from this\\nBoF could conceivably result in changes to the Astrophysics Source Code Library\\n(ASCL) or other resources for the benefit of the community or provide input\\ninto other projects concerned with software metadata.\\n',\n",
       " \"  Recently, digital music libraries have been developed and can be plainly\\naccessed. Latest research showed that current organization and retrieval of\\nmusic tracks based on album information are inefficient. Moreover, they\\ndemonstrated that people use emotion tags for music tracks in order to search\\nand retrieve them. In this paper, we discuss separability of a set of emotional\\nlabels, proposed in the categorical emotion expression, using Fisher's\\nseparation theorem. We determine a set of adjectives to tag music parts: happy,\\nsad, relaxing, exciting, epic and thriller. Temporal, frequency and energy\\nfeatures have been extracted from the music parts. It could be seen that the\\nmaximum separability within the extracted features occurs between relaxing and\\nepic music parts. Finally, we have trained a classifier using Support Vector\\nMachines to automatically recognize and generate emotional labels for a music\\npart. Accuracy for recognizing each label has been calculated; where the\\nresults show that epic music can be recognized more accurately (77.4%),\\ncomparing to the other types of music.\\n\",\n",
       " '  One key requirement for effective supply chain management is the quality of\\nits inventory management. Various inventory management methods are typically\\nemployed for different types of products based on their demand patterns,\\nproduct attributes, and supply network. In this paper, our goal is to develop\\nrobust demand prediction methods for weather sensitive products at retail\\nstores. We employ historical datasets from Walmart, whose customers and markets\\nare often exposed to extreme weather events which can have a huge impact on\\nsales regarding the affected stores and products. We want to accurately predict\\nthe sales of 111 potentially weather-sensitive products around the time of\\nmajor weather events at 45 of Walmart retails locations in the U.S.\\nIntuitively, we may expect an uptick in the sales of umbrellas before a big\\nthunderstorm, but it is difficult for replenishment managers to predict the\\nlevel of inventory needed to avoid being out-of-stock or overstock during and\\nafter that storm. While they rely on a variety of vendor tools to predict sales\\naround extreme weather events, they mostly employ a time-consuming process that\\nlacks a systematic measure of effectiveness. We employ all the methods critical\\nto any analytics project and start with data exploration. Critical features are\\nextracted from the raw historical dataset for demand forecasting accuracy and\\nrobustness. In particular, we employ Artificial Neural Network for forecasting\\ndemand for each product sold around the time of major weather events. Finally,\\nwe evaluate our model to evaluate their accuracy and robustness.\\n',\n",
       " '  We propose a deformable generator model to disentangle the appearance and\\ngeometric information from images into two independent latent vectors. The\\nappearance generator produces the appearance information, including color,\\nillumination, identity or category, of an image. The geometric generator\\nproduces displacement of the coordinates of each pixel and performs geometric\\nwarping, such as stretching and rotation, on the appearance generator to obtain\\nthe final synthesized image. The proposed model can learn both representations\\nfrom image data in an unsupervised manner. The learned geometric generator can\\nbe conveniently transferred to the other image datasets to facilitate\\ndownstream AI tasks.\\n',\n",
       " '  The Gaussian kernel is a very popular kernel function used in many\\nmachine-learning algorithms, especially in support vector machines (SVM). For\\nnonlinear training instances in machine learning, it often outperforms\\npolynomial kernels in model accuracy. We use Gaussian kernel profoundly in\\nformulating nonlinear classical SVM. In the recent research, P. Rebentrost\\net.al. discuss a very elegant quantum version of least square support vector\\nmachine using the quantum version of polynomial kernel, which is exponentially\\nfaster than the classical counterparts. In this paper, we have demonstrated a\\nquantum version of the Gaussian kernel and analyzed its complexity in the\\ncontext of quantum SVM. Our analysis shows that the computational complexity of\\nthe quantum Gaussian kernel is O(\\\\epsilon^(-1)logN) with N-dimensional\\ninstances and \\\\epsilon with a Taylor remainder error term |R_m (\\\\epsilon^(-1)\\nlogN)|.\\n',\n",
       " '  Security, privacy, and fairness have become critical in the era of data\\nscience and machine learning. More and more we see that achieving universally\\nsecure, private, and fair systems is practically impossible. We have seen for\\nexample how generative adversarial networks can be used to learn about the\\nexpected private training data; how the exploitation of additional data can\\nreveal private information in the original one; and how what looks like\\nunrelated features can teach us about each other. Confronted with this\\nchallenge, in this paper we open a new line of research, where the security,\\nprivacy, and fairness is learned and used in a closed environment. The goal is\\nto ensure that a given entity (e.g., the company or the government), trusted to\\ninfer certain information with our data, is blocked from inferring protected\\ninformation from it. For example, a hospital might be allowed to produce\\ndiagnosis on the patient (the positive task), without being able to infer the\\ngender of the subject (negative task). Similarly, a company can guarantee that\\ninternally it is not using the provided data for any undesired task, an\\nimportant goal that is not contradicting the virtually impossible challenge of\\nblocking everybody from the undesired task. We design a system that learns to\\nsucceed on the positive task while simultaneously fail at the negative one, and\\nillustrate this with challenging cases where the positive task is actually\\nharder than the negative one being blocked. Fairness, to the information in the\\nnegative task, is often automatically obtained as a result of this proposed\\napproach. The particular framework and examples open the door to security,\\nprivacy, and fairness in very important closed scenarios, ranging from private\\ndata accumulation companies like social networks to law-enforcement and\\nhospitals.\\n',\n",
       " '  The difficulty of modeling energy consumption in communication systems leads\\nto challenges in energy harvesting (EH) systems, in which nodes scavenge energy\\nfrom their environment. An EH receiver must harvest enough energy for\\ndemodulating and decoding. The energy required depends upon factors, like code\\nrate and signal-to-noise ratio, which can be adjusted dynamically. We consider\\na receiver which harvests energy from ambient sources and the transmitter,\\nmeaning the received signal is used for both EH and information decoding.\\nAssuming a generalized function for energy consumption, we maximize the total\\nnumber of information bits decoded, under both average and peak power\\nconstraints at the transmitter, by carefully optimizing the power used for EH,\\npower used for information transmission, fraction of time for EH, and code\\nrate. For transmission over a single block, we find there exist problem\\nparameters for which either maximizing power for information transmission or\\nmaximizing power for EH is optimal. In the general case, the optimal solution\\nis a tradeoff of the two. For transmission over multiple blocks, we give an\\nupper bound on performance and give sufficient and necessary conditions to\\nachieve this bound. Finally, we give some numerical results to illustrate our\\nresults and analysis.\\n',\n",
       " \"  This paper studies a recently proposed continuous-time distributed\\nself-appraisal model with time-varying interactions among a network of $n$\\nindividuals which are characterized by a sequence of time-varying relative\\ninteraction matrices. The model describes the evolution of the\\nsocial-confidence levels of the individuals via a reflected appraisal mechanism\\nin real time. We first show by example that when the relative interaction\\nmatrices are stochastic (not doubly stochastic), the social-confidence levels\\nof the individuals may not converge to a steady state. We then show that when\\nthe relative interaction matrices are doubly stochastic, the $n$ individuals'\\nself-confidence levels will all converge to $1/n$, which indicates a democratic\\nstate, exponentially fast under appropriate assumptions, and provide an\\nexplicit expression of the convergence rate.\\n\",\n",
       " '  When the brain receives input from multiple sensory systems, it is faced with\\nthe question of whether it is appropriate to process the inputs in combination,\\nas if they originated from the same event, or separately, as if they originated\\nfrom distinct events. Furthermore, it must also have a mechanism through which\\nit can keep sensory inputs calibrated to maintain the accuracy of its internal\\nrepresentations. We have developed a neural network architecture capable of i)\\napproximating optimal multisensory spatial integration, based on Bayesian\\ncausal inference, and ii) recalibrating the spatial encoding of sensory\\nsystems. The architecture is based on features of the dorsal processing\\nhierarchy, including the spatial tuning properties of unisensory neurons and\\nthe convergence of different sensory inputs onto multisensory neurons.\\nFurthermore, we propose that these unisensory and multisensory neurons play\\ndual roles in i) encoding spatial location as separate or integrated estimates\\nand ii) accumulating evidence for the independence or relatedness of\\nmultisensory stimuli. We further propose that top-down feedback connections\\nspanning the dorsal pathway play key a role in recalibrating spatial encoding\\nat the level of early unisensory cortices. Our proposed architecture provides\\npossible explanations for a number of human electrophysiological and\\nneuroimaging results and generates testable predictions linking neurophysiology\\nwith behaviour.\\n',\n",
       " \"  A common problem in large-scale data analysis is to approximate a matrix\\nusing a combination of specifically sampled rows and columns, known as CUR\\ndecomposition. Unfortunately, in many real-world environments, the ability to\\nsample specific individual rows or columns of the matrix is limited by either\\nsystem constraints or cost. In this paper, we consider matrix approximation by\\nsampling predefined \\\\emph{blocks} of columns (or rows) from the matrix. We\\npresent an algorithm for sampling useful column blocks and provide novel\\nguarantees for the quality of the approximation. This algorithm has application\\nin problems as diverse as biometric data analysis to distributed computing. We\\ndemonstrate the effectiveness of the proposed algorithms for computing the\\nBlock CUR decomposition of large matrices in a distributed setting with\\nmultiple nodes in a compute cluster, where such blocks correspond to columns\\n(or rows) of the matrix stored on the same node, which can be retrieved with\\nmuch less overhead than retrieving individual columns stored across different\\nnodes. In the biometric setting, the rows correspond to different users and\\ncolumns correspond to users' biometric reaction to external stimuli, {\\\\em\\ne.g.,}~watching video content, at a particular time instant. There is\\nsignificant cost in acquiring each user's reaction to lengthy content so we\\nsample a few important scenes to approximate the biometric response. An\\nindividual time sample in this use case cannot be queried in isolation due to\\nthe lack of context that caused that biometric reaction. Instead, collections\\nof time segments ({\\\\em i.e.,} blocks) must be presented to the user. The\\npractical application of these algorithms is shown via experimental results\\nusing real-world user biometric data from a content testing environment.\\n\",\n",
       " '  The unusually high surface tension of room temperature liquid metal is\\nmolding it as unique material for diverse newly emerging areas. However, unlike\\nits practices on earth, such metal fluid would display very different behaviors\\nwhen working in space where gravity disappears and surface property dominates\\nthe major physics. So far, few direct evidences are available to understand\\nsuch effect which would impede further exploration of liquid metal use for\\nspace. Here to preliminarily probe into this intriguing issue, a low cost\\nexperimental strategy to simulate microgravity environment on earth was\\nproposed through adopting bridges with high enough free falling distance as the\\ntest platform. Then using digital cameras amounted along x, y, z directions on\\noutside wall of the transparent container with liquid metal and allied solution\\ninside, synchronous observations on the transient flow and transformational\\nactivities of liquid metal were performed. Meanwhile, an unmanned aerial\\nvehicle was adopted to record the whole free falling dynamics of the test\\ncapsule from the far end which can help justify subsequent experimental\\nprocedures. A series of typical fundamental phenomena were thus observed as:\\n(a) A relatively large liquid metal object would spontaneously transform from\\nits original planar pool state into a sphere and float in the container if\\ninitiating the free falling; (b) The liquid metal changes its three-dimensional\\nshape due to dynamic microgravity strength due to free falling and rebound of\\nthe test capsule; and (c) A quick spatial transformation of liquid metal\\nimmersed in the solution can easily be induced via external electrical fields.\\nThe mechanisms of the surface tension driven liquid metal actuation in space\\nwere interpreted. All these findings indicated that microgravity effect should\\nbe fully treated in developing future generation liquid metal space\\ntechnologies.\\n',\n",
       " '  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)\\nmethod for performing approximate inference in complex probabilistic models of\\ncontinuous variables. In common with many MCMC methods, however, the standard\\nHMC approach performs poorly in distributions with multiple isolated modes. We\\npresent a method for augmenting the Hamiltonian system with an extra continuous\\ntemperature control variable which allows the dynamic to bridge between\\nsampling a complex target distribution and a simpler unimodal base\\ndistribution. This augmentation both helps improve mixing in multimodal targets\\nand allows the normalisation constant of the target distribution to be\\nestimated. The method is simple to implement within existing HMC code,\\nrequiring only a standard leapfrog integrator. We demonstrate experimentally\\nthat the method is competitive with annealed importance sampling and simulating\\ntempering methods at sampling from challenging multimodal distributions and\\nestimating their normalising constants.\\n',\n",
       " \"  We present a new method for the automated synthesis of digital controllers\\nwith formal safety guarantees for systems with nonlinear dynamics, noisy output\\nmeasurements, and stochastic disturbances. Our method derives digital\\ncontrollers such that the corresponding closed-loop system, modeled as a\\nsampled-data stochastic control system, satisfies a safety specification with\\nprobability above a given threshold. The proposed synthesis method alternates\\nbetween two steps: generation of a candidate controller pc, and verification of\\nthe candidate. pc is found by maximizing a Monte Carlo estimate of the safety\\nprobability, and by using a non-validated ODE solver for simulating the system.\\nSuch a candidate is therefore sub-optimal but can be generated very rapidly. To\\nrule out unstable candidate controllers, we prove and utilize Lyapunov's\\nindirect method for instability of sampled-data nonlinear systems. In the\\nsubsequent verification step, we use a validated solver based on SMT\\n(Satisfiability Modulo Theories) to compute a numerically and statistically\\nvalid confidence interval for the safety probability of pc. If the probability\\nso obtained is not above the threshold, we expand the search space for\\ncandidates by increasing the controller degree. We evaluate our technique on\\nthree case studies: an artificial pancreas model, a powertrain control model,\\nand a quadruple-tank process.\\n\",\n",
       " '  In the present paper we consider numerical methods to solve the discrete\\nSchrödinger equation with a time dependent Hamiltonian (motivated by problems\\nencountered in the study of spin systems). We will consider both short-range\\ninteractions, which lead to evolution equations involving sparse matrices, and\\nlong-range interactions, which lead to dense matrices. Both of these settings\\nshow very different computational characteristics. We use Magnus integrators\\nfor time integration and employ a framework based on Leja interpolation to\\ncompute the resulting action of the matrix exponential. We consider both\\ntraditional Magnus integrators (which are extensively used for these types of\\nproblems in the literature) as well as the recently developed commutator-free\\nMagnus integrators and implement them on modern CPU and GPU (graphics\\nprocessing unit) based systems.\\nWe find that GPUs can yield a significant speed-up (up to a factor of $10$ in\\nthe dense case) for these types of problems. In the sparse case GPUs are only\\nadvantageous for large problem sizes and the achieved speed-ups are more\\nmodest. In most cases the commutator-free variant is superior but especially on\\nthe GPU this advantage is rather small. In fact, none of the advantage of\\ncommutator-free methods on GPUs (and on multi-core CPUs) is due to the\\nelimination of commutators. This has important consequences for the design of\\nmore efficient numerical methods.\\n',\n",
       " '  This paper re-investigates the estimation of multiple factor models relaxing\\nthe convention that the number of factors is small and using a new approach for\\nidentifying factors. We first obtain the collection of all possible factors and\\nthen provide a simultaneous test, security by security, of which factors are\\nsignificant. Since the collection of risk factors is large and highly\\ncorrelated, high-dimension methods (including the LASSO and prototype\\nclustering) have to be used. The multi-factor model is shown to have a\\nsignificantly better fit than the Fama-French 5-factor model. Robustness tests\\nare also provided.\\n',\n",
       " '  We experimentally confirmed the threshold behavior and scattering length\\nscaling law of the three-body loss coefficients in an ultracold spin-polarized\\ngas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the\\nthree-body loss coefficients as functions of temperature and scattering volume,\\nand found that the threshold law and the scattering length scaling law hold in\\nlimited temperature and magnetic field regions. We also found that the\\nbreakdown of the scaling laws is due to the emergence of the effective-range\\nterm. This work is an important first step toward full understanding of the\\nloss of identical fermions with $p$-wave interactions.\\n',\n",
       " \"  The paper proposes an expanded version of the Local Variance Gamma model of\\nCarr and Nadtochiy by adding drift to the governing underlying process. Still\\nin this new model it is possible to derive an ordinary differential equation\\nfor the option price which plays a role of Dupire's equation for the standard\\nlocal volatility model. It is shown how calibration of multiple smiles (the\\nwhole local volatility surface) can be done in such a case. Further, assuming\\nthe local variance to be a piecewise linear function of strike and piecewise\\nconstant function of time this ODE is solved in closed form in terms of\\nConfluent hypergeometric functions. Calibration of the model to market smiles\\ndoes not require solving any optimization problem and, in contrast, can be done\\nterm-by-term by solving a system of non-linear algebraic equations for each\\nmaturity, which is fast.\\n\",\n",
       " '  In processing human produced text using natural language processing (NLP)\\ntechniques, two fundamental subtasks that arise are (i) segmentation of the\\nplain text into meaningful subunits (e.g., entities), and (ii) dependency\\nparsing, to establish relations between subunits. In this paper, we develop a\\nrelatively simple and effective neural joint model that performs both\\nsegmentation and dependency parsing together, instead of one after the other as\\nin most state-of-the-art works. We will focus in particular on the real estate\\nad setting, aiming to convert an ad to a structured description, which we name\\nproperty tree, comprising the tasks of (1) identifying important entities of a\\nproperty (e.g., rooms) from classifieds and (2) structuring them into a tree\\nformat. In this work, we propose a new joint model that is able to tackle the\\ntwo tasks simultaneously and construct the property tree by (i) avoiding the\\nerror propagation that would arise from the subtasks one after the other in a\\npipelined fashion, and (ii) exploiting the interactions between the subtasks.\\nFor this purpose, we perform an extensive comparative study of the pipeline\\nmethods and the new proposed joint model, reporting an improvement of over\\nthree percentage points in the overall edge F1 score of the property tree.\\nAlso, we propose attention methods, to encourage our model to focus on salient\\ntokens during the construction of the property tree. Thus we experimentally\\ndemonstrate the usefulness of attentive neural architectures for the proposed\\njoint model, showcasing a further improvement of two percentage points in edge\\nF1 score for our application.\\n',\n",
       " '  The asymptotic variance of the maximum likelihood estimate is proved to\\ndecrease when the maximization is restricted to a subspace that contains the\\ntrue parameter value. Maximum likelihood estimation allows a systematic fitting\\nof covariance models to the sample, which is important in data assimilation.\\nThe hierarchical maximum likelihood approach is applied to the spectral\\ndiagonal covariance model with different parameterizations of eigenvalue decay,\\nand to the sparse inverse covariance model with specified parameter values on\\ndifferent sets of nonzero entries. It is shown computationally that using\\nsmaller sets of parameters can decrease the sampling noise in high dimension\\nsubstantially.\\n',\n",
       " '  Fully automating machine learning pipelines is one of the key challenges of\\ncurrent artificial intelligence research, since practical machine learning\\noften requires costly and time-consuming human-powered processes such as model\\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\\nverify that automated architecture search synergizes with the effect of\\ngradient-based meta learning. We adopt the progressive neural architecture\\nsearch \\\\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\\narchitectures for meta-learners. The gradient based meta-learner whose\\narchitecture was automatically found achieved state-of-the-art results on the\\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\\\%$ accuracy,\\nwhich is $11.54\\\\%$ improvement over the result obtained by the first\\ngradient-based meta-learner called MAML\\n\\\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\\nthe first successful neural architecture search implementation in the context\\nof meta learning.\\n',\n",
       " '  We provide a comprehensive study of the convergence of forward-backward\\nalgorithm under suitable geometric conditions leading to fast rates. We present\\nseveral new results and collect in a unified view a variety of results\\nscattered in the literature, often providing simplified proofs. Novel\\ncontributions include the analysis of infinite dimensional convex minimization\\nproblems, allowing the case where minimizers might not exist. Further, we\\nanalyze the relation between different geometric conditions, and discuss novel\\nconnections with a priori conditions in linear inverse problems, including\\nsource conditions, restricted isometry properties and partial smoothness.\\n',\n",
       " '  Magnetic Particle Imaging (MPI) is a novel imaging modality with important\\napplications such as angiography, stem cell tracking, and cancer imaging.\\nRecently, there have been efforts to increase the functionality of MPI via\\nmulti-color imaging methods that can distinguish the responses of different\\nnanoparticles, or nanoparticles in different environmental conditions. The\\nproposed techniques typically rely on extensive calibrations that capture the\\ndifferences in the harmonic responses of the nanoparticles. In this work, we\\npropose a method to directly estimate the relaxation time constant of the\\nnanoparticles from the MPI signal, which is then used to generate a multi-color\\nrelaxation map. The technique is based on the underlying mirror symmetry of the\\nadiabatic MPI signal when the same region is scanned back and forth. We\\nvalidate the proposed method via extensive simulations, and via experiments on\\nour in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our\\nin-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be\\nsuccessfully distinguished with the proposed technique, without any calibration\\nor prior knowledge about the nanoparticles.\\n',\n",
       " '  Draft of textbook chapter on neural machine translation. a comprehensive\\ntreatment of the topic, ranging from introduction to neural networks,\\ncomputation graphs, description of the currently dominant attentional\\nsequence-to-sequence model, recent refinements, alternative architectures and\\nchallenges. Written as chapter for the textbook Statistical Machine\\nTranslation. Used in the JHU Fall 2017 class on machine translation.\\n',\n",
       " '  Let $D$ be a bounded domain $D$ in $\\\\mathbb R^n $ with infinitely smooth\\nboundary and $n$ is odd. We prove that if the volume cut off from the domain by\\na hyperplane is an algebraic function of the hyperplane, free of real singular\\npoints, then the domain is an ellipsoid. This partially answers a question of\\nV.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically\\nintegrable domains?\\n',\n",
       " '  The novel unseen classes can be formulated as the extreme values of known\\nclasses. This inspired the recent works on open-set recognition\\n\\\\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no\\nway of naming the novel unseen classes. To solve this problem, we propose the\\nExtreme Value Learning (EVL) formulation to learn the mapping from visual\\nfeature to semantic space. To model the margin and coverage distributions of\\neach class, the Vocabulary-informed Learning (ViL) is adopted by using vast\\nopen vocabulary in the semantic space. Essentially, by incorporating the EVL\\nand ViL, we for the first time propose a novel semantic embedding paradigm --\\nVocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual\\nfeatures into semantic space in a probabilistic way. The learned embedding can\\nbe directly used to solve supervised learning, zero-shot and open set\\nrecognition simultaneously. Experiments on two benchmark datasets demonstrate\\nthe effectiveness of proposed frameworks.\\n',\n",
       " \"  In this paper, we study the performance of two cross-layer optimized dynamic\\nrouting techniques for radio interference mitigation across multiple coexisting\\nwireless body area networks (BANs), based on real-life measurements. At the\\nnetwork layer, the best route is selected according to channel state\\ninformation from the physical layer, associated with low duty cycle TDMA at the\\nMAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel\\ncooperative multi-path routing (CMR) incorporating 3-branch selection\\ncombining) perform real-time and reliable data transfer across BANs operating\\nnear the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'\\nmixed-activities is used for analyzing the proposed cross-layer optimization.\\nWe show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and\\neven 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage\\nprobability at a realistic signal-to-interference-plus-noise ratio (SINR).\\nAcceptable packet delivery ratios (PDR) and spectral efficiencies are obtained\\nfrom SPR and CMR with reasonably sensitive receivers across a range of TDMA low\\nduty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The\\ndistribution fits for received SINR through routing are also derived and\\nvalidated with theoretical analysis.\\n\",\n",
       " '  In recent years, the proliferation of online resumes and the need to evaluate\\nlarge populations of candidates for on-site and virtual teams have led to a\\ngrowing interest in automated team-formation. Given a large pool of candidates,\\nthe general problem requires the selection of a team of experts to complete a\\ngiven task. Surprisingly, while ongoing research has studied numerous\\nvariations with different constraints, it has overlooked a factor with a\\nwell-documented impact on team cohesion and performance: team faultlines.\\nAddressing this gap is challenging, as the available measures for faultlines in\\nexisting teams cannot be efficiently applied to faultline optimization. In this\\nwork, we meet this challenge with a new measure that can be efficiently used\\nfor both faultline measurement and minimization. We then use the measure to\\nsolve the problem of automatically partitioning a large population into\\nlow-faultline teams. By introducing faultlines to the team-formation\\nliterature, our work creates exciting opportunities for algorithmic work on\\nfaultline optimization, as well as on work that combines and studies the\\nconnection of faultlines with other influential team characteristics.\\n',\n",
       " '  Deep convolutional neural networks (CNNs) have recently achieved great\\nsuccess in many visual recognition tasks. However, existing deep neural network\\nmodels are computationally expensive and memory intensive, hindering their\\ndeployment in devices with low memory resources or in applications with strict\\nlatency requirements. Therefore, a natural thought is to perform model\\ncompression and acceleration in deep networks without significantly decreasing\\nthe model performance. During the past few years, tremendous progress has been\\nmade in this area. In this paper, we survey the recent advanced techniques for\\ncompacting and accelerating CNNs model developed. These techniques are roughly\\ncategorized into four schemes: parameter pruning and sharing, low-rank\\nfactorization, transferred/compact convolutional filters, and knowledge\\ndistillation. Methods of parameter pruning and sharing will be described at the\\nbeginning, after that the other techniques will be introduced. For each scheme,\\nwe provide insightful analysis regarding the performance, related applications,\\nadvantages, and drawbacks etc. Then we will go through a few very recent\\nadditional successful methods, for example, dynamic capacity networks and\\nstochastic depths networks. After that, we survey the evaluation matrix, the\\nmain datasets used for evaluating the model performance and recent benchmarking\\nefforts. Finally, we conclude this paper, discuss remaining challenges and\\npossible directions on this topic.\\n',\n",
       " '  The task of calibration is to retrospectively adjust the outputs from a\\nmachine learning model to provide better probability estimates on the target\\nvariable. While calibration has been investigated thoroughly in classification,\\nit has not yet been well-established for regression tasks. This paper considers\\nthe problem of calibrating a probabilistic regression model to improve the\\nestimated probability densities over the real-valued targets. We propose to\\ncalibrate a regression model through the cumulative probability density, which\\ncan be derived from calibrating a multi-class classifier. We provide three\\nnon-parametric approaches to solve the problem, two of which provide empirical\\nestimates and the third providing smooth density estimates. The proposed\\napproaches are experimentally evaluated to show their ability to improve the\\nperformance of regression models on the predictive likelihood.\\n',\n",
       " \"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of\\nX-ray photons off quantized plasma electrons in the strong magnetic field (of\\nthe order 10^12 G) close to the surface of an accreting X-ray pulsar. The line\\nprofiles of CRSFs cannot be described by an analytic expression. Numerical\\nmethods such as Monte Carlo (MC) simulations of the scattering processes are\\nrequired in order to predict precise line shapes for a given physical setup,\\nwhich can be compared to observations to gain information about the underlying\\nphysics in these systems.\\nA versatile simulation code is needed for the generation of synthetic\\ncyclotron lines. Sophisticated geometries should be investigatable by making\\ntheir simulation possible for the first time.\\nThe simulation utilizes the mean free path tables described in the first\\npaper of this series for the fast interpolation of propagation lengths. The\\ncode is parallelized to make the very time consuming simulations possible on\\nconvenient time scales. Furthermore, it can generate responses to\\nmono-energetic photon injections, producing Green's functions, which can be\\nused later to generate spectra for arbitrary continua.\\nWe develop a new simulation code to generate synthetic cyclotron lines for\\ncomplex scenarios, allowing for unprecedented physical interpretation of the\\nobserved data. An associated XSPEC model implementation is used to fit\\nsynthetic line profiles to NuSTAR data of Cep X-4. The code has been developed\\nwith the main goal of overcoming previous geometrical constraints in MC\\nsimulations of CRSFs. By applying this code also to more simple, classic\\ngeometries used in previous works, we furthermore address issues of code\\nverification and cross-comparison of various models. The XSPEC model and the\\nGreen's function tables are available online at\\nthis http URL .\\n\",\n",
       " '  This paper is devoted to the study of the construction of new quantum MDS\\ncodes. Based on constacyclic codes over Fq2 , we derive four new families of\\nquantum MDS codes, one of which is an explicit generalization of the\\nconstruction given in Theorem 7 in [22]. We also extend the result of Theorem\\n3:3 given in [17].\\n',\n",
       " '  We present a unified categorical treatment of completeness theorems for\\nseveral classical and intuitionistic infinitary logics with a proposed\\naxiomatization. This provides new completeness theorems and subsumes previous\\nones by Gödel, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an\\napplication we prove, using large cardinals assumptions, the disjunction and\\nexistence properties for infinitary intuitionistic first-order logics.\\n',\n",
       " '  Recent advances in stochastic gradient techniques have made it possible to\\nestimate posterior distributions from large datasets via Markov Chain Monte\\nCarlo (MCMC). However, when the target posterior is multimodal, mixing\\nperformance is often poor. This results in inadequate exploration of the\\nposterior distribution. A framework is proposed to improve the sampling\\nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A\\ngeneralized kinetic function is leveraged, delivering superior stationary\\nmixing, especially for multimodal distributions. Techniques are also discussed\\nto overcome the practical issues introduced by this generalization. It is shown\\nthat the proposed approach is better at exploring complex multimodal posterior\\ndistributions, as demonstrated on multiple applications and in comparison with\\nother stochastic gradient MCMC methods.\\n',\n",
       " '  We study the problems related to the estimation of the Gini index in presence\\nof a fat-tailed data generating process, i.e. one in the stable distribution\\nclass with finite mean but infinite variance (i.e. with tail index\\n$\\\\alpha\\\\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be\\nreliably estimated using conventional nonparametric methods, because of a\\ndownward bias that emerges under fat tails. This has important implications for\\nthe ongoing discussion about economic inequality.\\nWe start by discussing how the nonparametric estimator of the Gini index\\nundergoes a phase transition in the symmetry structure of its asymptotic\\ndistribution, as the data distribution shifts from the domain of attraction of\\na light-tailed distribution to that of a fat-tailed one, especially in the case\\nof infinite variance. We also show how the nonparametric Gini bias increases\\nwith lower values of $\\\\alpha$. We then prove that maximum likelihood estimation\\noutperforms nonparametric methods, requiring a much smaller sample size to\\nreach efficiency.\\nFinally, for fat-tailed data, we provide a simple correction mechanism to the\\nsmall sample bias of the nonparametric estimator based on the distance between\\nthe mode and the mean of its asymptotic distribution.\\n',\n",
       " '  Training a neural network using backpropagation algorithm requires passing\\nerror gradients sequentially through the network. The backward locking prevents\\nus from updating network layers in parallel and fully leveraging the computing\\nresources. Recently, there are several works trying to decouple and parallelize\\nthe backpropagation algorithm. However, all of them suffer from severe accuracy\\nloss or memory explosion when the neural network is deep. To address these\\nchallenging issues, we propose a novel parallel-objective formulation for the\\nobjective function of the neural network. After that, we introduce features\\nreplay algorithm and prove that it is guaranteed to converge to critical points\\nfor the non-convex problem under certain conditions. Finally, we apply our\\nmethod to training deep convolutional neural networks, and the experimental\\nresults show that the proposed method achieves {faster} convergence, {lower}\\nmemory consumption, and {better} generalization error than compared methods.\\n',\n",
       " '  We study a diagrammatic categorification (the \"anti-spherical category\") of\\nthe anti-spherical module for any Coxeter group. We deduce that Deodhar\\'s\\n(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,\\nand that a monotonicity conjecture of Brenti\\'s holds. The main technical\\nobservation is a localisation procedure for the anti-spherical category, from\\nwhich we construct a \"light leaves\" basis of morphisms. Our techniques may be\\nused to calculate many new elements of the $p$-canonical basis in the\\nanti-spherical module. The results use generators and relations for Soergel\\nbimodules (\"Soergel calculus\") in a crucial way.\\n',\n",
       " '  Recently, we have predicted that the modulation instability of optical vortex\\nsolitons propagating in nonlinear colloidal suspensions with exponential\\nsaturable nonlinearity leads to formation of necklace beams (NBs)\\n[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \\\\textbf{40},\\n5714 (2015)]. Here, we investigate the dynamics of NB formation and\\npropagation, and show that the distance at which the NB is formed depends on\\nthe input power of the vortex beam. Moreover, we show that the NB trajectories\\nare not necessarily tangent to the initial vortex ring, and that their\\nvelocities have components stemming both from the beam diffraction and from the\\nbeam orbital angular momentum. We also demonstrate the generation of twisted\\nsolitons and analyze the influence of losses on their propagation. Finally, we\\ninvestigate the conservation of the orbital angular momentum in necklace and\\ntwisted beams. Our studies, performed in ideal lossless media and in realistic\\ncolloidal suspensions with losses, provide a detailed description of NB\\ndynamics and may be useful in studies of light propagation in highly scattering\\ncolloids and biological samples.\\n',\n",
       " '  A three-dimensional spin current solver based on a generalised spin\\ndrift-diffusion description, including the spin Hall effect, is integrated with\\na magnetisation dynamics solver. The resulting model is shown to simultaneously\\nreproduce the spin-orbit torques generated using the spin Hall effect, spin\\npumping torques generated by magnetisation dynamics in multilayers, as well as\\nthe spin transfer torques acting on magnetisation regions with spatial\\ngradients, whilst field-like and spin-like torques are reproduced in a spin\\nvalve geometry. Two approaches to modelling interfaces are analysed, one based\\non the spin mixing conductance and the other based on continuity of spin\\ncurrents where the spin dephasing length governs the absorption of transverse\\nspin components. In both cases analytical formulas are derived for the\\nspin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in\\ngeneral both field-like and damping-like torques are generated. The limitations\\nof the analytical approach are discussed, showing that even in a simple bilayer\\ngeometry, due to the non-uniformity of the spin currents, a full\\nthree-dimensional treatment is required. Finally the model is applied to the\\nquantitative analysis of the spin Hall angle in Pt by reproducing published\\nexperimental data on the ferromagnetic resonance linewidth in the bilayer\\ngeometry.\\n',\n",
       " '  This paper aims to explore models based on the extreme gradient boosting\\n(XGBoost) approach for business risk classification. Feature selection (FS)\\nalgorithms and hyper-parameter optimizations are simultaneously considered\\nduring model training. The five most commonly used FS methods including weight\\nby Gini, weight by Chi-square, hierarchical variable clustering, weight by\\ncorrelation, and weight by information are applied to alleviate the effect of\\nredundant features. Two hyper-parameter optimization approaches, random search\\n(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in\\nXGBoost. The effect of different FS and hyper-parameter optimization methods on\\nthe model performance are investigated by the Wilcoxon Signed Rank Test. The\\nperformance of XGBoost is compared to the traditionally utilized logistic\\nregression (LR) model in terms of classification accuracy, area under the curve\\n(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results\\nshow that hierarchical clustering is the optimal FS method for LR while weight\\nby Chi-square achieves the best performance in XG-Boost. Both TPE and RS\\noptimization in XGBoost outperform LR significantly. TPE optimization shows a\\nsuperiority over RS since it results in a significantly higher accuracy and a\\nmarginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE\\ntuning shows a lower variability than the RS method. Finally, the ranking of\\nfeature importance based on XGBoost enhances the model interpretation.\\nTherefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an\\noperative while powerful approach for business risk modeling.\\n',\n",
       " '  Immiscible fluids flowing at high capillary numbers in porous media may be\\ncharacterized by an effective viscosity. We demonstrate that the effective\\nviscosity is well described by the Lichtenecker-Rother equation. The exponent\\n$\\\\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in\\nthree-dimensional systems depending on the pore geometry. Our arguments are\\nbased on analytical and numerical methods.\\n',\n",
       " \"  Quantum charge pumping phenomenon connects band topology through the dynamics\\nof a one-dimensional quantum system. In terms of a microscopic model, the\\nSu-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful\\nstarting point for many considerations of topological physics. Here we present\\na generalized Creutz scheme as a distinct two-band quantum pump model. By\\nnoting that it undergoes two kinds of topological band transitions accompanying\\nwith a Zak-phase-difference of $\\\\pi$ and $2\\\\pi$, respectively, various charge\\npumping schemes are studied by applying an elaborate Peierl's phase\\nsubstitution. Translating into real space, the transportation of quantized\\ncharges is a result of cooperative quantum interference effect. In particular,\\nan all-flux quantum pump emerges which operates with time-varying fluxes only\\nand transports two charge units. This puts cold atoms with artificial gauge\\nfields as an unique system where this kind of phenomena can be realized.\\n\",\n",
       " '  Heart disease is the leading cause of death, and experts estimate that\\napproximately half of all heart attacks and strokes occur in people who have\\nnot been flagged as \"at risk.\" Thus, there is an urgent need to improve the\\naccuracy of heart disease diagnosis. To this end, we investigate the potential\\nof using data analysis, and in particular the design and use of deep neural\\nnetworks (DNNs) for detecting heart disease based on routine clinical data. Our\\nmain contribution is the design, evaluation, and optimization of DNN\\narchitectures of increasing depth for heart disease diagnosis. This work led to\\nthe discovery of a novel five layer DNN architecture - named Heart Evaluation\\nfor Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields\\nbest prediction accuracy. HEARO-5\\'s design employs regularization optimization\\nand automatically deals with missing data and/or data outliers. To evaluate and\\ntune the architectures we use k-way cross-validation as well as Matthews\\ncorrelation coefficient (MCC) to measure the quality of our classifications.\\nThe study is performed on the publicly available Cleveland dataset of medical\\ninformation, and we are making our developments open source, to further\\nfacilitate openness and research on the use of DNNs in medicine. The HEARO-5\\narchitecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms\\ncurrently published research in the area.\\n',\n",
       " '  The theory of integral quadratic constraints (IQCs) allows verification of\\nstability and gain-bound properties of systems containing nonlinear or\\nuncertain elements. Gain bounds often imply exponential stability, but it can\\nbe challenging to compute useful numerical bounds on the exponential decay\\nrate. This work presents a generalization of the classical IQC results of\\nMegretski and Rantzer that leads to a tractable computational procedure for\\nfinding exponential rate certificates that are far less conservative than ones\\ncomputed from $L_2$ gain bounds alone. An expanded library of IQCs for\\ncertifying exponential stability is also provided and the effectiveness of the\\ntechnique is demonstrated via numerical examples.\\n',\n",
       " \"  Foreshock transients upstream of Earth's bow shock have been recently\\nobserved to accelerate electrons to many times their thermal energy. How such\\nacceleration occurs is unknown, however. Using THEMIS case studies, we examine\\na subset of acceleration events (31 of 247 events) in foreshock transients with\\ncores that exhibit gradual electron energy increases accompanied by low\\nbackground magnetic field strength and large-amplitude magnetic fluctuations.\\nUsing the evolution of electron distributions and the energy increase rates at\\nmultiple spacecraft, we suggest that Fermi acceleration between a converging\\nforeshock transient's compressional boundary and the bow shock is responsible\\nfor the observed electron acceleration. We then show that a one-dimensional\\ntest particle simulation of an ideal Fermi acceleration model in fluctuating\\nfields prescribed by the observations can reproduce the observed evolution of\\nelectron distributions, energy increase rate, and pitch-angle isotropy,\\nproviding further support for our hypothesis. Thus, Fermi acceleration is\\nlikely the principal electron acceleration mechanism in at least this subset of\\nforeshock transient cores.\\n\",\n",
       " '  The quantum speed limit (QSL), or the energy-time uncertainty relation,\\ndescribes the fundamental maximum rate for quantum time evolution and has been\\nregarded as being unique in quantum mechanics. In this study, we obtain a\\nclassical speed limit corresponding to the QSL using the Hilbert space for the\\nclassical Liouville equation. Thus, classical mechanics has a fundamental speed\\nlimit, and QSL is not a purely quantum phenomenon but a universal dynamical\\nproperty of the Hilbert space. Furthermore, we obtain similar speed limits for\\nthe imaginary-time Schroedinger equations such as the master equation.\\n',\n",
       " '  This paper mainly discusses the diffusion on complex networks with\\ntime-varying couplings. We propose a model to describe the adaptive diffusion\\nprocess of local topological and dynamical information, and find that the\\nBarabasi-Albert scale-free network (BA network) is beneficial to the diffusion\\nand leads nodes to arrive at a larger state value than other networks do. The\\nability of diffusion for a node is related to its own degree. Specifically,\\nnodes with smaller degrees are more likely to change their states and reach\\nlarger values, while those with larger degrees tend to stick to their original\\nstates. We introduce state entropy to analyze the thermodynamic mechanism of\\nthe diffusion process, and interestingly find that this kind of diffusion\\nprocess is a minimization process of state entropy. We use the inequality\\nconstrained optimization method to reveal the restriction function of the\\nminimization and find that it has the same form as the Gibbs free energy. The\\nthermodynamical concept allows us to understand dynamical processes on complex\\nnetworks from a brand-new perspective. The result provides a convenient means\\nof optimizing relevant dynamical processes on practical circuits as well as\\nrelated complex systems.\\n',\n",
       " '  This paper proposes a non-parallel many-to-many voice conversion (VC) method\\nusing a variant of the conditional variational autoencoder (VAE) called an\\nauxiliary classifier VAE (ACVAE). The proposed method has three key features.\\nFirst, it adopts fully convolutional architectures to construct the encoder and\\ndecoder networks so that the networks can learn conversion rules that capture\\ntime dependencies in the acoustic feature sequences of source and target\\nspeech. Second, it uses an information-theoretic regularization for the model\\ntraining to ensure that the information in the attribute class label will not\\nbe lost in the conversion process. With regular CVAEs, the encoder and decoder\\nare free to ignore the attribute class label input. This can be problematic\\nsince in such a situation, the attribute class label will have little effect on\\ncontrolling the voice characteristics of input speech at test time. Such\\nsituations can be avoided by introducing an auxiliary classifier and training\\nthe encoder and decoder so that the attribute classes of the decoder outputs\\nare correctly predicted by the classifier. Third, it avoids producing\\nbuzzy-sounding speech at test time by simply transplanting the spectral details\\nof the input speech into its converted version. Subjective evaluation\\nexperiments revealed that this simple method worked reasonably well in a\\nnon-parallel many-to-many speaker identity conversion task.\\n',\n",
       " '  Informed by LES data and resolvent analysis of the mean flow, we examine the\\nstructure of turbulence in jets in the subsonic, transonic, and supersonic\\nregimes. Spectral (frequency-space) proper orthogonal decomposition is used to\\nextract energy spectra and decompose the flow into energy-ranked coherent\\nstructures. The educed structures are generally well predicted by the resolvent\\nanalysis. Over a range of low frequencies and the first few azimuthal mode\\nnumbers, these jets exhibit a low-rank response characterized by\\nKelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer\\nup to the end of the potential core and that are excited by forcing in the\\nvery-near-nozzle shear layer. These modes too the have been experimentally\\nobserved before and predicted by quasi-parallel stability theory and other\\napproximations--they comprise a considerable portion of the total turbulent\\nenergy. At still lower frequencies, particularly for the axisymmetric mode, and\\nagain at high frequencies for all azimuthal wavenumbers, the response is not\\nlow rank, but consists of a family of similarly amplified modes. These modes,\\nwhich are primarily active downstream of the potential core, are associated\\nwith the Orr mechanism. They occur also as sub-dominant modes in the range of\\nfrequencies dominated by the KH response. Our global analysis helps tie\\ntogether previous observations based on local spatial stability theory, and\\nexplains why quasi-parallel predictions were successful at some frequencies and\\nazimuthal wavenumbers, but failed at others.\\n',\n",
       " '  One of the popular approaches for low-rank tensor completion is to use the\\nlatent trace norm regularization. However, most existing works in this\\ndirection learn a sparse combination of tensors. In this work, we fill this gap\\nby proposing a variant of the latent trace norm that helps in learning a\\nnon-sparse combination of tensors. We develop a dual framework for solving the\\nlow-rank tensor completion problem. We first show a novel characterization of\\nthe dual solution space with an interesting factorization of the optimal\\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\\noptimization framework for proposing computationally efficient trust region\\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\\nseveral real-world datasets across applications.\\n',\n",
       " \"  Nous tentons dans cet article de proposer une thèse cohérente concernant\\nla formation de la notion d'involution dans le Brouillon Project de Desargues.\\nPour cela, nous donnons une analyse détaillée des dix premières pages\\ndudit Brouillon, comprenant les développements de cas particuliers qui aident\\nà comprendre l'intention de Desargues. Nous mettons cette analyse en regard\\nde la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis\\nCharitables.\\nThe purpose of this article is to propose a coherent thesis on how Girard\\nDesargues arrived at the notion of involution in his Brouillon Project of 1639.\\nTo this purpose we give a detailed analysis of the ten first pages of the\\nBrouillon, including developments of particular cases which help to understand\\nthe goal of Desargues, as well as to clarify the links between the notion of\\ninvolution and that of harmonic division. We compare the conclusions of this\\nanalysis with the very critical reading Jean de Beaugrand made of the Brouillon\\nProject in the Advis Charitables of 1640.\\n\",\n",
       " '  X-ray computed tomography (CT) using sparse projection views is a recent\\napproach to reduce the radiation dose. However, due to the insufficient\\nprojection views, an analytic reconstruction approach using the filtered back\\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\\napproaches using large receptive field neural networks such as U-Net have\\ndemonstrated impressive performance for sparse- view CT reconstruction.\\nHowever, theoretical justification is still lacking. Inspired by the recent\\ntheory of deep convolutional framelets, the main goal of this paper is,\\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\\ndeep learning schemes. In particular, we show that the alternative U- Net\\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\\nframe condition which make them better for effective recovery of high frequency\\nedges in sparse view- CT. Using extensive experiments with real patient data\\nset, we demonstrate that the new network architectures provide better\\nreconstruction performance.\\n',\n",
       " '  A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a\\nsubsheaf of the sheaf $\\\\mathfrak{X}$ of vector fields on $M$. We show that if\\nthis singular foliation admits a resolution (in the sense of sheaves)\\nconsisting of sections of a graded vector bundle of finite type, then one can\\nlift the Lie bracket of vector fields to a Lie $\\\\infty$-algebroid structure on\\nthis resolution, that we call a universal Lie $\\\\infty$-algebroid associated to\\nthe foliation. The name is justified because it is isomorphic (up to homotopy)\\nto any other Lie $\\\\infty$-algebroid structure built on any other resolution of\\nthe given singular foliation.\\n',\n",
       " '  The Weyl semimetal phase is a recently discovered topological quantum state\\nof matter characterized by the presence of topologically protected degeneracies\\nnear the Fermi level. These degeneracies are the source of exotic phenomena,\\nincluding the realization of chiral Weyl fermions as quasiparticles in the bulk\\nand the formation of Fermi arc states on the surfaces. Here, we demonstrate\\nthat these two key signatures show distinct evolutions with the bulk band\\ntopology by performing angle-resolved photoemission spectroscopy, supported by\\nfirst-principle calculations, on transition-metal monophosphides. While Weyl\\nfermion quasiparticles exist only when the chemical potential is located\\nbetween two saddle points of the Weyl cone features, the Fermi arc states\\nextend in a larger energy scale and are robust across the bulk Lifshitz\\ntransitions associated with the recombination of two non-trivial Fermi surfaces\\nenclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl\\npoints of opposite chirality. Therefore, in some systems (e.g. NbP),\\ntopological Fermi arc states are preserved even if Weyl fermion quasiparticles\\nare absent in the bulk. Our findings not only provide insight into the\\nrelationship between the exotic physical phenomena and the intrinsic bulk band\\ntopology in Weyl semimetals, but also resolve the apparent puzzle of the\\ndifferent magneto-transport properties observed in TaAs, TaP and NbP, where the\\nFermi arc states are similar.\\n',\n",
       " \"  A sequence of pathological changes takes place in Alzheimer's disease, which\\ncan be assessed in vivo using various brain imaging methods. Currently, there\\nis no appropriate statistical model available that can easily integrate\\nmultiple imaging modalities, being able to utilize the additional information\\nprovided from the combined data. We applied Gaussian graphical models (GGMs)\\nfor analyzing the conditional dependency networks of multimodal neuroimaging\\ndata and assessed alterations of the network structure in mild cognitive\\nimpairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy\\ncontrols.\\nData from N=667 subjects were obtained from the Alzheimer's Disease\\nNeuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism\\n(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.\\nSeparate GGMs were estimated using a Bayesian framework for the combined\\nmultimodal data for each diagnostic category. Graph-theoretical statistics were\\ncalculated to determine network alterations associated with disease severity.\\nNetwork measures clustering coefficient, path length and small-world\\ncoefficient were significantly altered across diagnostic groups, with a\\nbiphasic u-shape trajectory, i.e. increased small-world coefficient in early\\nMCI, intermediate values in late MCI, and decreased values in AD patients\\ncompared to controls. In contrast, no group differences were found for\\nclustering coefficient and small-world coefficient when estimating conditional\\ndependency networks on single imaging modalities.\\nGGMs provide a useful methodology to analyze the conditional dependency\\nnetworks of multimodal neuroimaging data.\\n\",\n",
       " '  This work bridges the technical concepts underlying distributed computing and\\nblockchain technologies with their profound socioeconomic and sociopolitical\\nimplications, particularly on academic research and the healthcare industry.\\nSeveral examples from academia, industry, and healthcare are explored\\nthroughout this paper. The limiting factor in contemporary life sciences\\nresearch is often funding: for example, to purchase expensive laboratory\\nequipment and materials, to hire skilled researchers and technicians, and to\\nacquire and disseminate data through established academic channels. In the case\\nof the U.S. healthcare system, hospitals generate massive amounts of data, only\\na small minority of which is utilized to inform current and future medical\\npractice. Similarly, corporations too expend large amounts of money to collect,\\nsecure and transmit data from one centralized source to another. In all three\\nscenarios, data moves under the traditional paradigm of centralization, in\\nwhich data is hosted and curated by individuals and organizations and of\\nbenefit to only a small subset of people.\\n',\n",
       " '  Fragility curves are commonly used in civil engineering to assess the\\nvulnerability of structures to earthquakes. The probability of failure\\nassociated with a prescribed criterion (e.g. the maximal inter-storey drift of\\na building exceeding a certain threshold) is represented as a function of the\\nintensity of the earthquake ground motion (e.g. peak ground acceleration or\\nspectral acceleration). The classical approach relies on assuming a lognormal\\nshape of the fragility curves; it is thus parametric. In this paper, we\\nintroduce two non-parametric approaches to establish the fragility curves\\nwithout employing the above assumption, namely binned Monte Carlo simulation\\nand kernel density estimation. As an illustration, we compute the fragility\\ncurves for a three-storey steel frame using a large number of synthetic ground\\nmotions. The curves obtained with the non-parametric approaches are compared\\nwith respective curves based on the lognormal assumption. A similar comparison\\nis presented for a case when a limited number of recorded ground motions is\\navailable. It is found that the accuracy of the lognormal curves depends on the\\nground motion intensity measure, the failure criterion and most importantly, on\\nthe employed method for estimating the parameters of the lognormal shape.\\n',\n",
       " '  We consider continuous-time Markov chains which display a family of wells at\\nthe same depth. We provide sufficient conditions which entail the convergence\\nof the finite-dimensional distributions of the order parameter to the ones of a\\nfinite state Markov chain. We also show that the state of the process can be\\nrepresented as a time-dependent convex combination of metastable states, each\\nof which is supported on one well.\\n',\n",
       " '  We construct embedded minimal surfaces which are $n$-periodic in\\n$\\\\mathbb{R}^n$. They are new for codimension $n-2\\\\ge 2$. We start with a Jordan\\ncurve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk\\nwhich Schwarz reflection extends to a complete minimal surface. Studying the\\ngroup of Schwarz reflections, we can characterize those Jordan curves for which\\nthe complete surface is embedded. For example, for $n=4$ exactly five such\\nJordan curves generate embedded surfaces. Our results apply to surface classes\\nother than minimal as well, for instance polygonal surfaces.\\n',\n",
       " '  We report the discovery of three small transiting planets orbiting GJ 9827, a\\nbright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\\\\pm0.11$\\n$R_{\\\\rm \\\\oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$\\n$R_{\\\\rm \\\\oplus}$ super Earth on a 3.6 day period, and a $2.07\\\\pm0.14$ $R_{\\\\rm\\n\\\\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting\\nGJ 9827 span the transition between predominantly rocky and gaseous planets,\\nand GJ 9827 b and c fall in or close to the known gap in the radius\\ndistribution of small planets between these populations. At a distance of 30\\nparsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making\\nthese planets well-suited for atmospheric studies with the upcoming James Webb\\nSpace Telescope. The GJ 9827 system provides a valuable opportunity to\\ncharacterize interior structure and atmospheric properties of coeval planets\\nspanning the rocky to gaseous transition.\\n',\n",
       " '  We define a family of quantum invariants of closed oriented $3$-manifolds\\nusing spherical multi-fusion categories. The state sum nature of this invariant\\nleads directly to $(2+1)$-dimensional topological quantum field theories\\n($\\\\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury\\n($\\\\text{TVBW}$) $\\\\text{TQFT}$s from spherical fusion categories. The invariant\\nis given as a state sum over labeled triangulations, which is mostly parallel\\nto, but richer than the $\\\\text{TVBW}$ approach in that here the labels live not\\nonly on $1$-simplices but also on $0$-simplices. It is shown that a\\nmulti-fusion category in general cannot be a spherical fusion category in the\\nusual sense. Thus we introduce the concept of a spherical multi-fusion category\\nby imposing a weakened version of sphericity. Besides containing the\\n$\\\\text{TVBW}$ theory, our construction also includes the recent higher gauge\\ntheory $(2+1)$-$\\\\text{TQFT}$s given by Kapustin and Thorngren, which was not\\nknown to have a categorical origin before.\\n',\n",
       " '  We propose Sparse Neural Network architectures that are based on random or\\nstructured bipartite graph topologies. Sparse architectures provide compression\\nof the models learned and speed-ups of computations, they can also surpass\\ntheir unstructured or fully connected counterparts. As we show, even more\\ncompact topologies of the so-called SNN (Sparse Neural Network) can be achieved\\nwith the use of structured graphs of connections between consecutive layers of\\nneurons. In this paper, we investigate how the accuracy and training speed of\\nthe models depend on the topology and sparsity of the neural network. Previous\\napproaches using sparcity are all based on fully connected neural network\\nmodels and create sparcity during training phase, instead we explicitly define\\na sparse architectures of connections before the training. Building compact\\nneural network models is coherent with empirical observations showing that\\nthere is much redundancy in learned neural network models. We show\\nexperimentally that the accuracy of the models learned with neural networks\\ndepends on expander-like properties of the underlying topologies such as the\\nspectral gap and algebraic connectivity rather than the density of the graphs\\nof connections.\\n',\n",
       " '  Computer vision has made remarkable progress in recent years. Deep neural\\nnetwork (DNN) models optimized to identify objects in images exhibit\\nunprecedented task-trained accuracy and, remarkably, some generalization\\nability: new visual problems can now be solved more easily based on previous\\nlearning. Biological vision (learned in life and through evolution) is also\\naccurate and general-purpose. Is it possible that these different learning\\nregimes converge to similar problem-dependent optimal computations? We\\ntherefore asked whether the human system-level computation of visual perception\\nhas DNN correlates and considered several anecdotal test cases. We found that\\nperceptual sensitivity to image changes has DNN mid-computation correlates,\\nwhile sensitivity to segmentation, crowding and shape has DNN end-computation\\ncorrelates. Our results quantify the applicability of using DNN computation to\\nestimate perceptual loss, and are consistent with the fascinating theoretical\\nview that properties of human perception are a consequence of\\narchitecture-independent visual learning.\\n',\n",
       " '  A numerical analysis of heat conduction through the cover plate of a heat\\npipe is carried out to determine the temperature of the working substance,\\naverage temperature of heating and cooling surfaces, heat spread in the\\ntransmitter, and the heat bypass through the cover plate. Analysis has been\\nextended for the estimation of heat transfer requirements at the outer surface\\nof the con- denser under different heat load conditions using Genetic\\nAlgorithm. This paper also presents the estimation of an average heat transfer\\ncoefficient for the boiling and condensation of the working substance inside\\nthe microgrooves corresponding to a known temperature of the heat source. The\\nequation of motion of the working fluid in the meniscus of an equilateral\\ntriangular groove has been presented from which a new term called the minimum\\nsurface tension required for avoiding the dry out condition is defined.\\nQuantitative results showing the effect of thickness of cover plate, heat load,\\nangle of inclination and viscosity of the working fluid on the different\\naspects of the heat transfer, minimum surface tension required to avoid dry\\nout, velocity distribution of the liquid, and radius of liquid meniscus inside\\nthe micro-grooves have been presented and discussed.\\n',\n",
       " \"  This paper provides short proofs of two fundamental theorems of finite\\nsemigroup theory whose previous proofs were significantly longer, namely the\\ntwo-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike\\ntheorem, using a new algebraic technique that we call the merge decomposition.\\nA prototypical application of this technique decomposes a semigroup $T$ into a\\ntwo-sided semidirect product whose components are built from two subsemigroups\\n$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their\\nsetwise product $T_1T_2$. In this sense we decompose $T$ by merging the\\nsubsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup\\nhomomorphisms from free semigroups.\\n\",\n",
       " \"  Nefarious actors on social media and other platforms often spread rumors and\\nfalsehoods through images whose metadata (e.g., captions) have been modified to\\nprovide visual substantiation of the rumor/falsehood. This type of modification\\nis referred to as image repurposing, in which often an unmanipulated image is\\npublished along with incorrect or manipulated metadata to serve the actor's\\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\\ndataset, a substantially challenging dataset over that which has been\\npreviously available to support research into image repurposing detection. The\\nnew dataset includes location, person, and organization manipulations on\\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\\nmultimodal learning model for assessing the integrity of an image by combining\\ninformation extracted from the image with related information from a knowledge\\nbase. The proposed method is compared against state-of-the-art techniques on\\nexisting datasets as well as MEIR, where it outperforms existing methods across\\nthe board, with AUC improvement up to 0.23.\\n\",\n",
       " '  Recent advances in adversarial Deep Learning (DL) have opened up a largely\\nunexplored surface for malicious attacks jeopardizing the integrity of\\nautonomous DL systems. With the wide-spread usage of DL in critical and\\ntime-sensitive applications, including unmanned vehicles, drones, and video\\nsurveillance systems, online detection of malicious inputs is of utmost\\nimportance. We propose DeepFense, the first end-to-end automated framework that\\nsimultaneously enables efficient and safe execution of DL models. DeepFense\\nformalizes the goal of thwarting adversarial attacks as an optimization problem\\nthat minimizes the rarely observed regions in the latent feature space spanned\\nby a DL network. To solve the aforementioned minimization problem, a set of\\ncomplementary but disjoint modular redundancies are trained to validate the\\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\\nleverages hardware/software/algorithm co-design and customized acceleration to\\nachieve just-in-time performance in resource-constrained settings. The proposed\\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\\nto train modular redundancies. We further provide an accompanying API to reduce\\nthe non-recurring engineering cost and ensure automated adaptation to various\\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\\nof magnitude performance improvement while enabling online adversarial sample\\ndetection.\\n',\n",
       " '  Users form information trails as they browse the web, checkin with a\\ngeolocation, rate items, or consume media. A common problem is to predict what\\na user might do next for the purposes of guidance, recommendation, or\\nprefetching. First-order and higher-order Markov chains have been widely used\\nmethods to study such sequences of data. First-order Markov chains are easy to\\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\\nin contrast, have too many parameters and suffer from overfitting the training\\ndata. Fitting these parameters with regularization and smoothing only offers\\nmild improvements. In this paper we propose the retrospective higher-order\\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\\nis a special case of a higher-order Markov chain where the transitions depend\\nretrospectively on a single history state instead of an arbitrary combination\\nof history states. There are two immediate computational advantages: the number\\nof parameters is linear in the order of the Markov chain and the model can be\\nfit to large state spaces. Furthermore, by providing a specific structure to\\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\\nutilizing history states without risks of overfitting the data. We demonstrate\\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\\nmethod on various real application datasets spanning geolocation data, review\\nsequences, and business locations. The RHOMP model uniformly outperforms\\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\\nfactorizations in terms of prediction accuracy.\\n',\n",
       " '  We analyze two novel randomized variants of the Frank-Wolfe (FW) or\\nconditional gradient algorithm. While classical FW algorithms require solving a\\nlinear minimization problem over the domain at each iteration, the proposed\\nmethod only requires to solve a linear minimization problem over a small\\n\\\\emph{subset} of the original domain. The first algorithm that we propose is a\\nrandomized variant of the original FW algorithm and achieves a\\n$\\\\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic\\ncounterpart. The second algorithm is a randomized variant of the Away-step FW\\nalgorithm, and again as its deterministic counterpart, reaches linear (i.e.,\\nexponential) convergence rate making it the first provably convergent\\nrandomized variant of Away-step FW. In both cases, while subsampling reduces\\nthe convergence rate by a constant factor, the linear minimization step can be\\na fraction of the cost of that of the deterministic versions, especially when\\nthe data is streamed. We illustrate computational gains of the algorithms on\\nregression problems, involving both $\\\\ell_1$ and latent group lasso penalties.\\n',\n",
       " '  In this paper, we prove a mean value formula for bounded subharmonic\\nHermitian matrix valued function on a complete Riemannian manifold with\\nnonnegative Ricci curvature. As its application, we obtain a Liouville type\\ntheorem for the complex Monge-Ampère equation on product manifolds.\\n',\n",
       " '  In this work, we investigate the value of uncertainty modeling in 3D\\nsuper-resolution with convolutional neural networks (CNNs). Deep learning has\\nshown success in a plethora of medical image transformation problems, such as\\nsuper-resolution (SR) and image synthesis. However, the highly ill-posed nature\\nof such problems results in inevitable ambiguity in the learning of networks.\\nWe propose to account for intrinsic uncertainty through a per-patch\\nheteroscedastic noise model and for parameter uncertainty through approximate\\nBayesian inference in the form of variational dropout. We show that the\\ncombined benefits of both lead to the state-of-the-art performance SR of\\ndiffusion MR brain images in terms of errors compared to ground truth. We\\nfurther show that the reduced error scores produce tangible benefits in\\ndownstream tractography. In addition, the probabilistic nature of the methods\\nnaturally confers a mechanism to quantify uncertainty over the super-resolved\\noutput. We demonstrate through experiments on both healthy and pathological\\nbrains the potential utility of such an uncertainty measure in the risk\\nassessment of the super-resolved images for subsequent clinical use.\\n',\n",
       " '  Superconductor-Ferromagnet (SF) heterostructures are of interest due to\\nnumerous phenomena related to the spin-dependent interaction of Cooper pairs\\nwith the magnetization. Here we address the effects of a magnetic insulator on\\nthe density of states of a superconductor based on a recently developed\\nboundary condition for strongly spin-dependent interfaces. We show that the\\nboundary to a magnetic insulator has a similar effect like the presence of\\nmagnetic impurities. In particular we find that the impurity effects of\\nstrongly scattering localized spins leading to the formation of Shiba bands can\\nbe mapped onto the boundary problem.\\n',\n",
       " '  We present the first general purpose framework for marginal maximum a\\nposteriori estimation of probabilistic program variables. By using a series of\\ncode transformations, the evidence of any probabilistic program, and therefore\\nof any graphical model, can be optimized with respect to an arbitrary subset of\\nits sampled variables. To carry out this optimization, we develop the first\\nBayesian optimization package to directly exploit the source code of its\\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\\noptimization, and implicit constraint satisfaction; delivering significant\\nperformance improvements over prominent existing packages. We present\\napplications of our method to a number of tasks including engineering design\\nand parameter optimization.\\n',\n",
       " '  Long-term load forecasting plays a vital role for utilities and planners in\\nterms of grid development and expansion planning. An overestimate of long-term\\nelectricity load will result in substantial wasted investment in the\\nconstruction of excess power facilities, while an underestimate of future load\\nwill result in insufficient generation and unmet demand. This paper presents\\nfirst-of-its-kind approach to use multiplicative error model (MEM) in\\nforecasting load for long-term horizon. MEM originates from the structure of\\nautoregressive conditional heteroscedasticity (ARCH) model where conditional\\nvariance is dynamically parameterized and it multiplicatively interacts with an\\ninnovation term of time-series. Historical load data, accessed from a U.S.\\nregional transmission operator, and recession data for years 1993-2016 is used\\nin this study. The superiority of considering volatility is proven by\\nout-of-sample forecast results as well as directional accuracy during the great\\neconomic recession of 2008. To incorporate future volatility, backtesting of\\nMEM model is performed. Two performance indicators used to assess the proposed\\nmodel are mean absolute percentage error (for both in-sample model fit and\\nout-of-sample forecasts) and directional accuracy.\\n',\n",
       " '  Many empirical studies document power law behavior in size distributions of\\neconomic interest such as cities, firms, income, and wealth. One mechanism for\\ngenerating such behavior combines independent and identically distributed\\nGaussian additive shocks to log-size with a geometric age distribution. We\\ngeneralize this mechanism by allowing the shocks to be non-Gaussian (but\\nlight-tailed) and dependent upon a Markov state variable. Our main results\\nprovide sharp bounds on tail probabilities and simple formulas for Pareto\\nexponents. We present two applications: (i) we show that the tails of the\\nwealth distribution in a heterogeneous-agent dynamic general equilibrium model\\nwith idiosyncratic endowment risk decay exponentially, unlike models with\\ninvestment risk where the tails may be Paretian, and (ii) we show that a random\\ngrowth model for the population dynamics of Japanese prefectures is consistent\\nwith the observed Pareto exponent but only after allowing for Markovian\\ndynamics.\\n',\n",
       " '  In topological quantum computing, information is encoded in \"knotted\" quantum\\nstates of topological phases of matter, thus being locked into topology to\\nprevent decay. Topological precision has been confirmed in quantum Hall liquids\\nby experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum\\nmemory. In this survey, we discuss the conceptual development of this\\ninterdisciplinary field at the juncture of mathematics, physics and computer\\nscience. Our focus is on computing and physical motivations, basic mathematical\\nnotions and results, open problems and future directions related to and/or\\ninspired by topological quantum computing.\\n',\n",
       " '  $ \\\\def\\\\vecc#1{\\\\boldsymbol{#1}} $We design a polynomial time algorithm that\\nfor any weighted undirected graph $G = (V, E,\\\\vecc w)$ and sufficiently large\\n$\\\\delta > 1$, partitions $V$ into subsets $V_1, \\\\ldots, V_h$ for some $h\\\\geq\\n1$, such that\\n$\\\\bullet$ at most $\\\\delta^{-1}$ fraction of the weights are between clusters,\\ni.e. \\\\[ w(E - \\\\cup_{i = 1}^h E(V_i)) \\\\lesssim \\\\frac{w(E)}{\\\\delta};\\\\]\\n$\\\\bullet$ the effective resistance diameter of each of the induced subgraphs\\n$G[V_i]$ is at most $\\\\delta^3$ times the average weighted degree, i.e. \\\\[\\n\\\\max_{u, v \\\\in V_i} \\\\mathsf{Reff}_{G[V_i]}(u, v) \\\\lesssim \\\\delta^3 \\\\cdot\\n\\\\frac{|V|}{w(E)} \\\\quad \\\\text{ for all } i=1, \\\\ldots, h.\\\\]\\nIn particular, it is possible to remove one percent of weight of edges of any\\ngiven graph such that each of the resulting connected components has effective\\nresistance diameter at most the inverse of the average weighted degree.\\nOur proof is based on a new connection between effective resistance and low\\nconductance sets. We show that if the effective resistance between two vertices\\n$u$ and $v$ is large, then there must be a low conductance cut separating $u$\\nfrom $v$. This implies that very mildly expanding graphs have constant\\neffective resistance diameter. We believe that this connection could be of\\nindependent interest in algorithm design.\\n',\n",
       " '  Self-supervised learning (SSL) is a reliable learning mechanism in which a\\nrobot enhances its perceptual capabilities. Typically, in SSL a trusted,\\nprimary sensor cue provides supervised training data to a secondary sensor cue.\\nIn this article, a theoretical analysis is performed on the fusion of the\\nprimary and secondary cue in a minimal model of SSL. A proof is provided that\\ndetermines the specific conditions under which it is favorable to perform\\nfusion. In short, it is favorable when (i) the prior on the target value is\\nstrong or (ii) the secondary cue is sufficiently accurate. The theoretical\\nfindings are validated with computational experiments. Subsequently, a\\nreal-world case study is performed to investigate if fusion in SSL is also\\nbeneficial when assumptions of the minimal model are not met. In particular, a\\nflying robot learns to map pressure measurements to sonar height measurements\\nand then fuses the two, resulting in better height estimation. Fusion is also\\nbeneficial in the opposite case, when pressure is the primary cue. The analysis\\nand results are encouraging to study SSL fusion also for other robots and\\nsensors.\\n',\n",
       " '  Deep reinforcement learning on Atari games maps pixel directly to actions;\\ninternally, the deep neural network bears the responsibility of both extracting\\nuseful information and making decisions based on it. Aiming at devoting entire\\ndeep networks to decision making alone, we propose a new method for learning\\npolicies and compact state representations separately but simultaneously for\\npolicy approximation in reinforcement learning. State representations are\\ngenerated by a novel algorithm based on Vector Quantization and Sparse Coding,\\ntrained online along with the network, and capable of growing its dictionary\\nsize over time. We also introduce new techniques allowing both the neural\\nnetwork and the evolution strategy to cope with varying dimensions. This\\nenables networks of only 6 to 18 neurons to learn to play a selection of Atari\\ngames with performance comparable---and occasionally superior---to\\nstate-of-the-art techniques using evolution strategies on deep networks two\\norders of magnitude larger.\\n',\n",
       " '  We consider the problem of isotonic regression, where the underlying signal\\n$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the\\ncone $\\\\{ x\\\\in\\\\mathbb{R}^n : x_1 \\\\leq \\\\dots \\\\leq x_n\\\\}$. We study the isotonic\\nprojection operator (projection to this cone), and find a necessary and\\nsufficient condition characterizing all norms with respect to which this\\nprojection is contractive. This enables a simple and non-asymptotic analysis of\\nthe convergence properties of isotonic regression, yielding uniform confidence\\nbands that adapt to the local Lipschitz properties of the signal.\\n',\n",
       " '  Heating, Ventilation, and Cooling (HVAC) systems are often the most\\nsignificant contributor to the energy usage, and the operational cost, of large\\noffice buildings. Therefore, to understand the various factors affecting the\\nenergy usage, and to optimize the operational efficiency of building HVAC\\nsystems, energy analysts and architects often create simulations (e.g.,\\nEnergyPlus or DOE-2), of buildings prior to construction or renovation to\\ndetermine energy savings and quantify the Return-on-Investment (ROI). While\\nuseful, these simulations usually use static HVAC control strategies such as\\nlowering room temperature at night, or reactive control based on simulated room\\noccupancy. Recently, advances have been made in HVAC control algorithms that\\npredict room occupancy. However, these algorithms depend on costly sensor\\ninstallations and the tradeoffs between predictive accuracy, energy savings,\\ncomfort and expenses are not well understood. Current simulation frameworks do\\nnot support easy analysis of these tradeoffs. Our contribution is a simulation\\nframework that can be used to explore this design space by generating objective\\nestimates of the energy savings and occupant comfort for different levels of\\nHVAC prediction and control performance. We validate our framework on a\\nreal-world occupancy dataset spanning 6 months for 235 rooms in a large\\nuniversity office building. Using the gold standard of energy use modeling and\\nsimulation (Revit and Energy Plus), we compare the energy consumption and\\noccupant comfort in 29 independent simulations that explore our parameter\\nspace. Our results highlight a number of potentially useful tradeoffs with\\nrespect to energy savings, comfort, and algorithmic performance among\\npredictive, reactive, and static schedules, for a stakeholder of our building.\\n',\n",
       " '  In this paper, we consider the problem of identifying the type (local\\nminimizer, maximizer or saddle point) of a given isolated real critical point\\n$c$, which is degenerate, of a multivariate polynomial function $f$. To this\\nend, we introduce the definition of faithful radius of $c$ by means of the\\ncurve of tangency of $f$. We show that the type of $c$ can be determined by the\\nglobal extrema of $f$ over the Euclidean ball centered at $c$ with a faithful\\nradius.We propose algorithms to compute a faithful radius of $c$ and determine\\nits type.\\n',\n",
       " '  Identification of patients at high risk for readmission could help reduce\\nmorbidity and mortality as well as healthcare costs. Most of the existing\\nstudies on readmission prediction did not compare the contribution of data\\ncategories. In this study we analyzed relative contribution of 90,101 variables\\nacross 398,884 admission records corresponding to 163,468 patients, including\\npatient demographics, historical hospitalization information, discharge\\ndisposition, diagnoses, procedures, medications and laboratory test results. We\\nestablished an interpretable readmission prediction model based on Logistic\\nRegression in scikit-learn, and added the available variables to the model one\\nby one in order to analyze the influences of individual data categories on\\nreadmission prediction accuracy. Diagnosis related groups (c-statistic\\nincrement of 0.0933) and discharge disposition (c-statistic increment of\\n0.0269) were the strongest contributors to model accuracy. Additionally, we\\nalso identified the top ten contributing variables in every data category.\\n',\n",
       " '  Tropical recurrent sequences are introduced satisfying a given vector (being\\na tropical counterpart of classical linear recurrent sequences). We consider\\nthe case when Newton polygon of the vector has a single (bounded) edge. In this\\ncase there are periodic tropical recurrent sequences which are similar to\\nclassical linear recurrent sequences. A question is studied when there exists a\\nnon-periodic tropical recurrent sequence satisfying a given vector, and partial\\nanswers are provided to this question. Also an algorithm is designed which\\ntests existence of non-periodic tropical recurrent sequences satisfying a given\\nvector with integer coordinates. Finally, we introduce a tropical entropy of a\\nvector and provide some bounds on it.\\n',\n",
       " '  An interesting approach to analyzing neural networks that has received\\nrenewed attention is to examine the equivalent kernel of the neural network.\\nThis is based on the fact that a fully connected feedforward network with one\\nhidden layer, a certain weight distribution, an activation function, and an\\ninfinite number of neurons can be viewed as a mapping into a Hilbert space. We\\nderive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for\\nall rotationally-invariant weight distributions, generalizing a previous result\\nthat required Gaussian weight distributions. Additionally, the Central Limit\\nTheorem is used to show that for certain activation functions, kernels\\ncorresponding to layers with weight distributions having $0$ mean and finite\\nabsolute third moment are asymptotically universal, and are well approximated\\nby the kernel corresponding to layers with spherical Gaussian weights. In deep\\nnetworks, as depth increases the equivalent kernel approaches a pathological\\nfixed point, which can be used to argue why training randomly initialized\\nnetworks can be difficult. Our results also have implications for weight\\ninitialization.\\n',\n",
       " '  We explore whether useful temporal neural generative models can be learned\\nfrom sequential data without back-propagation through time. We investigate the\\nviability of a more neurocognitively-grounded approach in the context of\\nunsupervised generative modeling of sequences. Specifically, we build on the\\nconcept of predictive coding, which has gained influence in cognitive science,\\nin a neural framework. To do so we develop a novel architecture, the Temporal\\nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The\\nunderlying directed generative model is fully recurrent, meaning that it\\nemploys structural feedback connections and temporal feedback connections,\\nyielding information propagation cycles that create local learning signals.\\nThis facilitates a unified bottom-up and top-down approach for information\\ntransfer inside the architecture. Our proposed algorithm shows promise on the\\nbouncing balls generative modeling problem. Further experiments could be\\nconducted to explore the strengths and weaknesses of our approach.\\n',\n",
       " '  Kitaev quantum spin liquid is a topological magnetic quantum state\\ncharacterized by Majorana fermions of fractionalized spin excitations, which\\nare identical to their own antiparticles. Here, we demonstrate emergence of\\nMajorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice\\n{\\\\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage\\nrelease of magnetic entropy involving localized and itinerant Majorana\\nfermions. The inelastic neutron scattering results further corroborate these\\ntwo distinct fermions by exhibiting quasielastic excitations at low energies\\naround the Brillouin zone center and Y-shaped magnetic continuum at high\\nenergies, which are evident for the ferromagnetic Kitaev model. Our results\\nprovide an opportunity to build a unified conceptual framework of\\nfractionalized excitations, applicable also for the quantum Hall states,\\nsuperconductors, and frustrated magnets.\\n',\n",
       " '  Identifying the mechanism by which high energy Lyman continuum (LyC) photons\\nescaped from early galaxies is one of the most pressing questions in cosmic\\nevolution. Haro 11 is the best known local LyC leaking galaxy, providing an\\nimportant opportunity to test our understanding of LyC escape. The observed LyC\\nemission in this galaxy presumably originates from one of the three bright,\\nphotoionizing knots known as A, B, and C. It is known that Knot C has strong\\nLy$\\\\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray\\nsource, which may be a low-luminosity AGN. To clarify the LyC source, we carry\\nout ionization-parameter mapping (IPM) by obtaining narrow-band imaging from\\nthe Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved\\nratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization\\nstructure of the interstellar medium and allows us to identify optically thin\\nregions. To optimize the continuum subtraction, we introduce a new method for\\ndetermining the best continuum scale factor derived from the mode of the\\ncontinuum-subtracted, image flux distribution. We find no conclusive evidence\\nof LyC escape from Knots B or C, but instead, we identify a high-ionization\\nregion extending over at least 1 kpc from Knot A. Knot A shows evidence of an\\nextremely young age ($\\\\lesssim 1$ Myr), perhaps containing very massive stars\\n($>100$ M$_\\\\odot$). It is weak in Ly$\\\\alpha$, so if it is confirmed as the LyC\\nsource, our results imply that LyC emission may be independent of Ly$\\\\alpha$\\nemission.\\n',\n",
       " '  Dependently typed languages such as Coq are used to specify and verify the\\nfull functional correctness of source programs. Type-preserving compilation can\\nbe used to preserve these specifications and proofs of correctness through\\ncompilation into the generated target-language programs. Unfortunately,\\ntype-preserving compilation of dependent types is hard. In essence, the problem\\nis that dependent type systems are designed around high-level compositional\\nabstractions to decide type checking, but compilation interferes with the\\ntype-system rules for reasoning about run-time terms.\\nWe develop a type-preserving closure-conversion translation from the Calculus\\nof Constructions (CC) with strong dependent pairs ($\\\\Sigma$ types)---a subset\\nof the core language of Coq---to a type-safe, dependently typed compiler\\nintermediate language named CC-CC. The central challenge in this work is how to\\ntranslate the source type-system rules for reasoning about functions into\\ntarget type-system rules for reasoning about closures. To justify these rules,\\nwe prove soundness of CC-CC by giving a model in CC. In addition to type\\npreservation, we prove correctness of separate compilation.\\n',\n",
       " '  Any generic closed curve in the plane can be transformed into a simple closed\\ncurve by a finite sequence of local transformations called homotopy moves. We\\nprove that simplifying a planar closed curve with $n$ self-crossings requires\\n$\\\\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the\\nbest previous upper bound $O(n^2)$, which is already implicit in the classical\\nwork of Steinitz; the matching lower bound follows from the construction of\\nclosed curves with large defect, a topological invariant of generic closed\\ncurves introduced by Aicardi and Arnold. Our lower bound also implies that\\n$\\\\Omega(n^{3/2})$ facial electrical transformations are required to reduce any\\nplane graph with treewidth $\\\\Omega(\\\\sqrt{n})$ to a single vertex, matching\\nknown upper bounds for rectangular and cylindrical grid graphs. More generally,\\nwe prove that transforming one immersion of $k$ circles with at most $n$\\nself-crossings into another requires $\\\\Theta(n^{3/2} + nk + k^2)$ homotopy\\nmoves in the worst case. Finally, we prove that transforming one\\nnoncontractible closed curve to another on any orientable surface requires\\n$\\\\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if\\nthe curve is homotopic to a simple closed curve.\\n',\n",
       " '  Consider a channel with a given input distribution. Our aim is to degrade it\\nto a channel with at most L output letters. One such degradation method is the\\nso called \"greedy-merge\" algorithm. We derive an upper bound on the reduction\\nin mutual information between input and output. For fixed input alphabet size\\nand variable L, the upper bound is within a constant factor of an\\nalgorithm-independent lower bound. Thus, we establish that greedy-merge is\\noptimal in the power-law sense.\\n',\n",
       " '  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let\\n${\\\\mathcal F}$ be the family of all functions holomorphic in the unit disk\\n${\\\\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on\\n$L_1$. It is shown that ${\\\\mathcal F}$ is normal in ${\\\\mathbb\\nD}\\\\backslash\\\\{0\\\\}$. The case where $L_0$ is the positive real axis and $L_1$ is\\nthe negative real axis is studied in more detail.\\n',\n",
       " '  A habitable exoplanet is a world that can maintain stable liquid water on its\\nsurface. Techniques and approaches to characterizing such worlds are essential,\\nas performing a census of Earth-like planets that may or may not have life will\\ninform our understanding of how frequently life originates and is sustained on\\nworlds other than our own. Observational techniques like high contrast imaging\\nand transit spectroscopy can reveal key indicators of habitability for\\nexoplanets. Both polarization measurements and specular reflectance from oceans\\n(also known as \"glint\") can provide direct evidence for surface liquid water,\\nwhile constraining surface pressure and temperature (from moderate resolution\\nspectra) can indicate liquid water stability. Indirect evidence for\\nhabitability can come from a variety of sources, including observations of\\nvariability due to weather, surface mapping studies, and/or measurements of\\nwater vapor or cloud profiles that indicate condensation near a surface.\\nApproaches to making the types of measurements that indicate habitability are\\ndiverse, and have different considerations for the required wavelength range,\\nspectral resolution, maximum noise levels, stellar host temperature, and\\nobserving geometry.\\n',\n",
       " '  In this paper, we evaluate the accuracy of deep learning approaches on\\ngeospatial vector geometry classification tasks. The purpose of this evaluation\\nis to investigate the ability of deep learning models to learn from geometry\\ncoordinates directly. Previous machine learning research applied to geospatial\\npolygon data did not use geometries directly, but derived properties thereof.\\nThese are produced by way of extracting geometry properties such as Fourier\\ndescriptors. Instead, our introduced deep neural net architectures are able to\\nlearn on sequences of coordinates mapped directly from polygons. In three\\nclassification tasks we show that the deep learning architectures are\\ncompetitive with common learning algorithms that require extracted features.\\n',\n",
       " \"  The distance standard deviation, which arises in distance correlation\\nanalysis of multivariate data, is studied as a measure of spread. New\\nrepresentations for the distance standard deviation are obtained in terms of\\nGini's mean difference and in terms of the moments of spacings of order\\nstatistics. Inequalities for the distance variance are derived, proving that\\nthe distance standard deviation is bounded above by the classical standard\\ndeviation and by Gini's mean difference. Further, it is shown that the distance\\nstandard deviation satisfies the axiomatic properties of a measure of spread.\\nExplicit closed-form expressions for the distance variance are obtained for a\\nbroad class of parametric distributions. The asymptotic distribution of the\\nsample distance variance is derived.\\n\",\n",
       " '  One of the most basic skills a robot should possess is predicting the effect\\nof physical interactions with objects in the environment. This enables optimal\\naction selection to reach a certain goal state. Traditionally, dynamics are\\napproximated by physics-based analytical models. These models rely on specific\\nstate representations that may be hard to obtain from raw sensory data,\\nespecially if no knowledge of the object shape is assumed. More recently, we\\nhave seen learning approaches that can predict the effect of complex physical\\ninteractions directly from sensory input. It is however an open question how\\nfar these models generalize beyond their training data. In this work, we\\ninvestigate the advantages and limitations of neural network based learning\\napproaches for predicting the effects of actions based on sensory input and\\nshow how analytical and learned models can be combined to leverage the best of\\nboth worlds. As physical interaction task, we use planar pushing, for which\\nthere exists a well-known analytical model and a large real-world dataset. We\\npropose to use a convolutional neural network to convert raw depth images or\\norganized point clouds into a suitable representation for the analytical model\\nand compare this approach to using neural networks for both, perception and\\nprediction. A systematic evaluation of the proposed approach on a very large\\nreal-world dataset shows two main advantages of the hybrid architecture.\\nCompared to a pure neural network, it significantly (i) reduces required\\ntraining data and (ii) improves generalization to novel physical interaction.\\n',\n",
       " '  In this paper, we give a complete characterization of Leavitt path algebras\\nwhich are graded $\\\\Sigma $-$V$ rings, that is, rings over which a direct sum of\\narbitrary copies of any graded simple module is graded injective. Specifically,\\nwe show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded\\n$\\\\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of\\narbitrary size but with finitely many non-zero entries over $K$ or\\n$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical\\ncharacterization of such a graded $\\\\Sigma $-$V$ ring $L$% . When the graph $E$\\nis finite, we show that $L$ is a graded $\\\\Sigma $-$V$ ring $\\\\Longleftrightarrow\\nL$ is graded directly-finite $\\\\Longleftrightarrow L $ has bounded index of\\nnilpotence $\\\\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that\\nthe equivalence of these properties in the preceding statement no longer holds\\nwhen the graph $E$ is infinite. Following this, we also characterize Leavitt\\npath algebras $L$ which are non-graded $\\\\Sigma $-$V$ rings. Graded rings which\\nare graded directly-finite are explored and it is shown that if a Leavitt path\\nalgebra $L$ is a graded $\\\\Sigma$-$V$ ring, then $L$ is always graded\\ndirectly-finite. Examples show the subtle differences between graded and\\nnon-graded directly-finite rings. Leavitt path algebras which are graded\\ndirectly-finite are shown to be directed unions of graded semisimple rings.\\nUsing this, we give an alternative proof of a theorem of Vaš \\\\cite{V} on\\ndirectly-finite Leavitt path algebras.\\n',\n",
       " '  We derive the mean squared error convergence rates of kernel density-based\\nplug-in estimators of mutual information measures between two multidimensional\\nrandom variables $\\\\mathbf{X}$ and $\\\\mathbf{Y}$ for two cases: 1) $\\\\mathbf{X}$\\nand $\\\\mathbf{Y}$ are both continuous; 2) $\\\\mathbf{X}$ is continuous and\\n$\\\\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble\\nestimator of these information measures for the second case by taking a\\nweighted sum of the plug-in estimators with varied bandwidths. The resulting\\nensemble estimator achieves the $1/N$ parametric convergence rate when the\\nconditional densities of the continuous variables are sufficiently smooth. To\\nthe best of our knowledge, this is the first nonparametric mutual information\\nestimator known to achieve the parametric convergence rate for this case, which\\nfrequently arises in applications (e.g. variable selection in classification).\\nThe estimator is simple to implement as it uses the solution to an offline\\nconvex optimization problem and simple plug-in estimators. A central limit\\ntheorem is also derived for the ensemble estimator. Ensemble estimators that\\nachieve the parametric rate are also derived for the first case ($\\\\mathbf{X}$\\nand $\\\\mathbf{Y}$ are both continuous) and another case 3) $\\\\mathbf{X}$ and\\n$\\\\mathbf{Y}$ may have any mixture of discrete and continuous components.\\n',\n",
       " '  Widespread use of social media has led to the generation of substantial\\namounts of information about individuals, including health-related information.\\nSocial media provides the opportunity to study health-related information about\\nselected population groups who may be of interest for a particular study. In\\nthis paper, we explore the possibility of utilizing social media to perform\\ntargeted data collection and analysis from a particular population group --\\npregnant women. We hypothesize that we can use social media to identify cohorts\\nof pregnant women and follow them over time to analyze crucial health-related\\ninformation. To identify potentially pregnant women, we employ simple\\nrule-based searches that attempt to detect pregnancy announcements with\\nmoderate precision. To further filter out false positives and noise, we employ\\na supervised classifier using a small number of hand-annotated data. We then\\ncollect their posts over time to create longitudinal health timelines and\\nattempt to divide the timelines into different pregnancy trimesters. Finally,\\nwe assess the usefulness of the timelines by performing a preliminary analysis\\nto estimate drug intake patterns of our cohort at different trimesters. Our\\nrule-based cohort identification technique collected 53,820 users over thirty\\nmonths from Twitter. Our pregnancy announcement classification technique\\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\\ntimelines. Analysis of the timelines revealed that pertinent health-related\\ninformation, such as drug-intake and adverse reactions can be mined from the\\ndata. Our approach to using user timelines in this fashion has produced very\\nencouraging results and can be employed for other important tasks where\\ncohorts, for which health-related information may not be available from other\\nsources, are required to be followed over time to derive population-based\\nestimates.\\n',\n",
       " '  The vortex method is a common numerical and theoretical approach used to\\nimplement the motion of an ideal flow, in which the vorticity is approximated\\nby a sum of point vortices, so that the Euler equations read as a system of\\nordinary differential equations. Such a method is well justified in the full\\nplane, thanks to the explicit representation formulas of Biot and Savart. In an\\nexterior domain, we also replace the impermeable boundary by a collection of\\npoint vortices generating the circulation around the obstacle. The density of\\nthese point vortices is chosen in order that the flow remains tangent at\\nmidpoints between adjacent vortices. In this work, we provide a rigorous\\njustification for this method in exterior domains. One of the main mathematical\\ndifficulties being that the Biot-Savart kernel defines a singular integral\\noperator when restricted to a curve. For simplicity and clarity, we only treat\\nthe case of the unit disk in the plane approximated by a uniformly distributed\\nmesh of point vortices. The complete and general version of our work is\\navailable in [arXiv:1707.01458].\\n',\n",
       " \"  Let $R$ be an associative ring with unit and denote by $K({\\\\rm R\\n\\\\mbox{-}Proj})$ the homotopy category of complexes of projective left\\n$R$-modules. Neeman proved the theorem that $K({\\\\rm R \\\\mbox{-}Proj})$ is\\n$\\\\aleph_1$-compactly generated, with the category $K^+ ({\\\\rm R \\\\mbox{-}proj})$\\nof left bounded complexes of finitely generated projective $R$-modules\\nproviding an essentially small class of such generators. Another proof of\\nNeeman's theorem is explained, using recent ideas of Christensen and Holm, and\\nEmmanouil. The strategy of the proof is to show that every complex in $K({\\\\rm R\\n\\\\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\\\\rm R\\n\\\\mbox{-}Flat})/\\\\langle K^+ ({\\\\rm R \\\\mbox{-}proj}) \\\\rangle.$\\n\",\n",
       " '  For an arbitrary finite family of semi-algebraic/definable functions, we\\nconsider the corresponding inequality constraint set and we study qualification\\nconditions for perturbations of this set. In particular we prove that all\\npositive diagonal perturbations, save perhaps a finite number of them, ensure\\nthat any point within the feasible set satisfies Mangasarian-Fromovitz\\nconstraint qualification. Using the Milnor-Thom theorem, we provide a bound for\\nthe number of singular perturbations when the constraints are polynomial\\nfunctions. Examples show that the order of magnitude of our exponential bound\\nis relevant. Our perturbation approach provides a simple protocol to build\\nsequences of \"regular\" problems approximating an arbitrary\\nsemi-algebraic/definable problem. Applications to sequential quadratic\\nprogramming methods and sum of squares relaxation are provided.\\n',\n",
       " '  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits\\nof the hamiltonian $H=\\\\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a\\nsymplectic manifold (\\\\Gamma,\\\\omega), a hamiltonian $H:\\\\Gamma\\\\to\\\\mathbb{R}$ and\\na Lagrangian sub-manifold $M\\\\subset\\\\Gamma$ we find a generalization of the\\nnotion of curvature. The particular case\\n$H=\\\\frac{1}{2}g^{ij}\\\\left[p_{i}-A_{i}\\\\right]\\\\left[p_{j}-A_{j}\\\\right]+\\\\phi $ of\\na particle moving in a gravitational, electromagnetic and scalar fields is\\nstudied in more detail. The integral of the generalized Ricci tensor w.r.t. the\\nBoltzmann weight reduces to the action principle\\n$\\\\int\\\\left[R+\\\\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\\\\partial_{i}\\\\phi\\\\partial_{j}\\\\phi\\\\right]e^{-\\\\phi}\\\\sqrt{g}d^{n}q$\\nfor the scalar, vector and tensor fields.\\n',\n",
       " '  We investigate how a neural network can learn perception actions loops for\\nnavigation in unknown environments. Specifically, we consider how to learn to\\nnavigate in environments populated with cul-de-sacs that represent convex local\\nminima that the robot could fall into instead of finding a set of feasible\\nactions that take it to the goal. Traditional methods rely on maintaining a\\nglobal map to solve the problem of over coming a long cul-de-sac. However, due\\nto errors induced from local and global drift, it is highly challenging to\\nmaintain such a map for long periods of time. One way to mitigate this problem\\nis by using learning techniques that do not rely on hand engineered map\\nrepresentations and instead output appropriate control policies directly from\\ntheir sensory input. We first demonstrate that such a problem cannot be solved\\ndirectly by deep reinforcement learning due to the sparse reward structure of\\nthe environment. Further, we demonstrate that deep supervised learning also\\ncannot be used directly to solve this problem. We then investigate network\\nmodels that offer a combination of reinforcement learning and supervised\\nlearning and highlight the significance of adding fully differentiable memory\\nunits to such networks. We evaluate our networks on their ability to generalize\\nto new environments and show that adding memory to such networks offers huge\\njumps in performance\\n',\n",
       " '  In this paper, we present a combinatorial approach to the opposite 2-variable\\nbi-free partial $S$-transforms where the opposite multiplication is used on the\\nright. In addition, extensions of this partial $S$-transforms to the\\nconditional bi-free and operator-valued bi-free settings are discussed.\\n',\n",
       " '  Many giant exoplanets are found near their Roche limit and in mildly\\neccentric orbits. In this study we examine the fate of such planets through\\nRoche-lobe overflow as a function of the physical properties of the binary\\ncomponents, including the eccentricity and the asynchronicity of the rotating\\nplanet. We use a direct three-body integrator to compute the trajectories of\\nthe lost mass in the ballistic limit and investigate the possible outcomes. We\\nfind three different outcomes for the mass transferred through the Lagrangian\\npoint $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the\\nstellar surface, (iii) disk formation around the star. We explore the parameter\\nspace of the three different regimes and find that at low eccentricities,\\n$e\\\\lesssim 0.2$, mass overflow leads to disk formation for most systems, while\\nfor higher eccentricities or retrograde orbits self-accretion is the only\\npossible outcome. We conclude that the assumption often made in previous work\\nthat when a planet overflows its Roche lobe it is quickly disrupted and\\naccreted by the star is not always valid.\\n',\n",
       " '  A new Short-Orbit Spectrometer (SOS) has been constructed and installed\\nwithin the experimental facility of the A1 collaboration at Mainz Microtron\\n(MAMI), with the goal to detect low-energy pions. It is equipped with a\\nBrowne-Buechner magnet and a detector system consisting of two helium-ethane\\nbased drift chambers and a scintillator telescope made of five layers. The\\ndetector system allows detection of pions in the momentum range of 50 - 147\\nMeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can\\nbe placed at a distance range of 54 - 66 cm from the target center. Two\\ncollimators are available for the measurements, one having 1.8 msr aperture and\\nthe other having 7 msr aperture. The Short-Orbit Spectrometer has been\\nsuccessfully calibrated and used in coincidence measurements together with the\\nstandard magnetic spectrometers of the A1 collaboration.\\n',\n",
       " '  Capacitive deionization (CDI) is a fast-emerging water desalination\\ntechnology in which a small cell voltage of ~1 V across porous carbon\\nelectrodes removes salt from feedwaters via electrosorption. In flow-through\\nelectrode (FTE) CDI cell architecture, feedwater is pumped through macropores\\nor laser perforated channels in porous electrodes, enabling highly compact\\ncells with parallel flow and electric field, as well as rapid salt removal. We\\nhere present a one-dimensional model describing water desalination by FTE CDI,\\nand a comparison to data from a custom-built experimental cell. The model\\nemploys simple cell boundary conditions derived via scaling arguments. We show\\ngood model-to-data fits with reasonable values for fitting parameters such as\\nthe Stern layer capacitance, micropore volume, and attraction energy. Thus, we\\ndemonstrate that from an engineering modeling perspective, an FTE CDI cell may\\nbe described with simpler one-dimensional models, unlike more typical\\nflow-between electrodes architecture where 2D models are required.\\n',\n",
       " '  We present bilateral teleoperation system for task learning and robot motion\\ngeneration. Our system includes a bilateral teleoperation platform and a deep\\nlearning software. The deep learning software refers to human demonstration\\nusing the bilateral teleoperation platform to collect visual images and robotic\\nencoder values. It leverages the datasets of images and robotic encoder\\ninformation to learn about the inter-modal correspondence between visual images\\nand robot motion. In detail, the deep learning software uses a combination of\\nDeep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent\\nNeural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor\\nangles, to learn motion taught be human teleoperation. The learnt models are\\nused to predict new motion trajectories for similar tasks. Experimental results\\nshow that our system has the adaptivity to generate motion for similar scooping\\ntasks. Detailed analysis is performed based on failure cases of the\\nexperimental results. Some insights about the cans and cannots of the system\\nare summarized.\\n',\n",
       " '  We propose two multimodal deep learning architectures that allow for\\ncross-modal dataflow (XFlow) between the feature extractors, thereby extracting\\nmore interpretable features and obtaining a better representation than through\\nunimodal learning, for the same amount of training data. These models can\\nusefully exploit correlations between audio and visual data, which have a\\ndifferent dimensionality and are therefore nontrivially exchangeable. Our work\\nimproves on existing multimodal deep learning metholodogies in two essential\\nways: (1) it presents a novel method for performing cross-modality (before\\nfeatures are learned from individual modalities) and (2) extends the previously\\nproposed cross-connections, which only transfer information between streams\\nthat process compatible data. Both cross-modal architectures outperformed their\\nbaselines (by up to 7.5%) when evaluated on the AVletters dataset.\\n',\n",
       " '  Educational research has shown that narratives are useful tools that can help\\nyoung students make sense of scientific phenomena. Based on previous research,\\nI argue that narratives can also become tools for high school students to make\\nsense of concepts such as the electric field. In this paper I examine high\\nschool students visual and oral narratives in which they describe the\\ninteraction among electric charges as if they were characters of a cartoon\\nseries. The study investigates: given the prompt to produce narratives for\\nelectrostatic phenomena during a classroom activity prior to receiving formal\\ninstruction, (1) what ideas of electrostatics do students attend to in their\\nnarratives?; (2) what role do students narratives play in their understanding\\nof electrostatics? The participants were a group of high school students\\nengaged in an open-ended classroom activity prior to receiving formal\\ninstruction about electrostatics. During the activity, the group was asked to\\ndraw comic strips for electric charges. In addition to individual work,\\nstudents shared their work within small groups as well as with the whole group.\\nPost activity, six students from a small group were interviewed individually\\nabout their work. In this paper I present two cases in which students produced\\nnarratives to express their ideas about electrostatics in different ways. In\\neach case, I present student work for the comic strip activity (visual\\nnarratives), their oral descriptions of their work (oral narratives) during the\\ninterview and/or to their peers during class, and the their ideas of the\\nelectric interactions expressed through their narratives.\\n',\n",
       " \"  Hospital acquired infections (HAI) are infections acquired within the\\nhospital from healthcare workers, patients or from the environment, but which\\nhave no connection to the initial reason for the patient's hospital admission.\\nHAI are a serious world-wide problem, leading to an increase in mortality\\nrates, duration of hospitalisation as well as significant economic burden on\\nhospitals. Although clear preventive guidelines exist, studies show that\\ncompliance to them is frequently poor. This paper details the software\\nperspective for an innovative, business process software based cyber-physical\\nsystem that will be implemented as part of a European Union-funded research\\nproject. The system is composed of a network of sensors mounted in different\\nsites around the hospital, a series of wearables used by the healthcare workers\\nand a server side workflow engine. For better understanding, we describe the\\nsystem through the lens of a single, simple clinical workflow that is\\nresponsible for a significant portion of all hospital infections. The goal is\\nthat when completed, the system will be configurable in the sense of\\nfacilitating the creation and automated monitoring of those clinical workflows\\nthat when combined, account for over 90\\\\% of hospital infections.\\n\",\n",
       " '  We advocate the use of curated, comprehensive benchmark suites of machine\\nlearning datasets, backed by standardized OpenML-based interfaces and\\ncomplementary software toolkits written in Python, Java and R. Major\\ndistinguishing features of OpenML benchmark suites are (a) ease of use through\\nstandardized data formats, APIs, and existing client libraries; (b)\\nmachine-readable meta-information regarding the contents of the suite; and (c)\\nonline sharing of results, enabling large scale comparisons. As a first such\\nsuite, we propose the OpenML100, a machine learning benchmark suite of\\n100~classification datasets carefully curated from the thousands of datasets\\navailable on OpenML.org.\\n',\n",
       " '  In this article, we study orbifold constructions associated with the Leech\\nlattice vertex operator algebra. As an application, we prove that the structure\\nof a strongly regular holomorphic vertex operator algebra of central charge\\n$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra\\nhas the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,\\n$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse\\norbifold construction. Our result also provides alternative constructions of\\nthese vertex operator algebras (except for the case $A_{6,7}$) from the Leech\\nlattice vertex operator algebra.\\n',\n",
       " '  Deconstruction of the theme of the 2017 FQXi essay contest is already an\\ninteresting exercise in its own right: Teleology is rarely useful in physics\\n--- the only known mainstream physics example (black hole event horizons) has a\\nvery mixed score-card --- so the \"goals\" and \"aims and intentions\" alluded to\\nin the theme of the 2017 FQXi essay contest are already somewhat pushing the\\nlimits. Furthermore, \"aims and intentions\" certainly carries the implication of\\nconsciousness, and opens up a whole can of worms related to the mind-body\\nproblem. As for \"mindless mathematical laws\", that allusion is certainly in\\ntension with at least some versions of the \"mathematical universe hypothesis\".\\nFinally \"wandering towards a goal\" again carries the implication of\\nconsciousness, with all its attendant problems.\\nIn this essay I will argue, simply because we do not yet have any really good\\nmathematical or physical theory of consciousness, that the theme of this essay\\ncontest is premature, and unlikely to lead to any resolution that would be\\nwidely accepted in the mathematics or physics communities.\\n',\n",
       " '  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water\\nvapour radiometers (WVR), which monitor the atmospheric water vapour line at\\n183 GHz along the line of sight above each antenna to correct for phase delays\\nintroduced by the wet component of the troposphere. The application of WVR\\nderived phase corrections improve the image quality and facilitate successful\\nobservations in weather conditions that were classically marginal or poor. We\\npresent work to indicate that a scaling factor applied to the WVR solutions can\\nact to further improve the phase stability and image quality of ALMA data. We\\nfind reduced phase noise statistics for 62 out of 75 datasets from the\\nlong-baseline science verification campaign after a WVR scaling factor is\\napplied. The improvement of phase noise translates to an expected coherence\\nimprovement in 39 datasets. When imaging the bandpass source, we find 33 of the\\n39 datasets show an improvement in the signal-to-noise ratio (S/N) between a\\nfew to ~30 percent. There are 23 datasets where the S/N of the science image is\\nimproved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies\\nstudied (band 6 and band 7) are those most improved, specifically datasets with\\nlow precipitable water vapour (PWV), <1mm, where the dominance of the wet\\ncomponent is reduced. Although these improvements are not profound, phase\\nstability improvements via the WVR scaling factor come into play for the higher\\nfrequency (>450 GHz) and long-baseline (>5km) observations. These inherently\\nhave poorer phase stability and are taken in low PWV (<1mm) conditions for\\nwhich we find the scaling to be most effective. A promising explanation for the\\nscaling factor is the mixing of dry and wet air components, although other\\norigins are discussed. We have produced a python code to allow ALMA users to\\nundertake WVR scaling tests and make improvements to their data.\\n',\n",
       " '  In this paper, we study the $\\\\mu$-ordinary locus of a Shimura variety with\\nparahoric level structure. Under the axioms in \\\\cite{HR}, we show that\\n$\\\\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport\\nstrata introduced in \\\\cite{HR} and we give criteria on the density of the\\n$\\\\mu$-ordinary locus.\\n',\n",
       " '  Charts are an excellent way to convey patterns and trends in data, but they\\ndo not facilitate further modeling of the data or close inspection of\\nindividual data points. We present a fully automated system for extracting the\\nnumerical values of data points from images of scatter plots. We use deep\\nlearning techniques to identify the key components of the chart, and optical\\ncharacter recognition together with robust regression to map from pixels to the\\ncoordinate system of the chart. We focus on scatter plots with linear scales,\\nwhich already have several interesting challenges. Previous work has done fully\\nautomatic extraction for other types of charts, but to our knowledge this is\\nthe first approach that is fully automatic for scatter plots. Our method\\nperforms well, achieving successful data extraction on 89% of the plots in our\\ntest set.\\n',\n",
       " '  We consider a modification to the standard cosmological history consisting of\\nintroducing a new species $\\\\phi$ whose energy density red-shifts with the scale\\nfactor $a$ like $\\\\rho_\\\\phi \\\\propto a^{-(4+n)}$. For $n>0$, such a red-shift is\\nfaster than radiation, hence the new species dominates the energy budget of the\\nuniverse at early times while it is completely negligible at late times. If\\nequality with the radiation energy density is achieved at low enough\\ntemperatures, dark matter can be produced as a thermal relic during the new\\ncosmological phase. Dark matter freeze-out then occurs at higher temperatures\\ncompared to the standard case, implying that reproducing the observed abundance\\nrequires significantly larger annihilation rates. Here, we point out a\\ncompletely new phenomenon, which we refer to as $\\\\textit{relentless}$ dark\\nmatter: for large enough $n$, unlike the standard case where annihilation ends\\nshortly after the departure from thermal equilibrium, dark matter particles\\nkeep annihilating long after leaving chemical equilibrium, with a significant\\ndepletion of the final relic abundance. Relentless annihilation occurs for $n\\n\\\\geq 2$ and $n \\\\geq 4$ for s-wave and p-wave annihilation, respectively, and it\\nthus occurs in well motivated scenarios such as a quintessence with a kination\\nphase. We discuss a few microscopic realizations for the new cosmological\\ncomponent and highlight the phenomenological consequences of our calculations\\nfor dark matter searches.\\n',\n",
       " '  We describe the configuration space $\\\\mathbf{S}$ of polygons with prescribed\\nedge slopes, and study the perimeter $\\\\mathcal{P}$ as a Morse function on\\n$\\\\mathbf{S}$. We characterize critical points of $\\\\mathcal{P}$ (these are\\n\\\\textit{tangential} polygons) and compute their Morse indices. This setup is\\nmotivated by a number of results about critical points and Morse indices of the\\noriented area function defined on the configuration space of polygons with\\nprescribed edge lengths (flexible polygons). As a by-product, we present an\\nindependent computation of the Morse index of the area function (obtained\\nearlier by G. Panina and A. Zhukova).\\n',\n",
       " '  This paper discusses a Metropolis-Hastings algorithm developed by\\n\\\\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is\\nproven that the algorithm becomes more efficient with more data and meets the\\ngrowing demands of large scale educational measurement.\\n',\n",
       " '  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National\\nUniversity of Singapore. In this contribution he describes the power of\\noptofluidics as a research tool and reviews new insights within the areas of\\nsingle cell analysis, microphysiological analysis, and integrated systems.\\n',\n",
       " '  Topology has appeared in different physical contexts. The most prominent\\napplication is topologically protected edge transport in condensed matter\\nphysics. The Chern number, the topological invariant of gapped Bloch\\nHamiltonians, is an important quantity in this field. Another example of\\ntopology, in polarization physics, are polarization singularities, called L\\nlines and C points. By establishing a connection between these two theories, we\\ndevelop a novel technique to visualize and potentially measure the Chern\\nnumber: it can be expressed either as the winding of the polarization azimuth\\nalong L lines in reciprocal space, or in terms of the handedness and the index\\nof the C points. For mechanical systems, this is directly connected to the\\nvisible motion patterns.\\n',\n",
       " '  We study the scale and tidy subgroups of an endomorphism of a totally\\ndisconnected locally compact group using a geometric framework. This leads to\\nnew interpretations of tidy subgroups and the scale function. Foremost, we\\nobtain a geometric tidying procedure which applies to endomorphisms as well as\\na geometric proof of the fact that tidiness is equivalent to being minimizing\\nfor a given endomorphism. Our framework also yields an endomorphism version of\\nthe Baumgartner-Willis tree representation theorem. We conclude with a\\nconstruction of new endomorphisms of totally disconnected locally compact\\ngroups from old via HNN-extensions.\\n',\n",
       " '  The coupled exciton-vibrational dynamics of a three-site model of the FMO\\ncomplex is investigated using the Multi-layer Multi-configuration\\nTime-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of\\nthe spectral density on the exciton state populations as well as on the\\nvibrational and vibronic non-equilibrium excitations. Models which use either a\\nsingle or site-specific spectral densities are contrasted to a spectral density\\nadapted from experiment. For the transfer efficiency, the total integrated\\nHuang-Rhys factor is found to be more important than details of the spectral\\ndistributions. However, the latter are relevant for the obtained\\nnon-equilibrium vibrational and vibronic distributions and thus influence the\\nactual pattern of population relaxation.\\n',\n",
       " '  In the first part of this work we show the convergence with respect to an\\nasymptotic parameter {\\\\epsilon} of a delayed heat equation. It represents a\\nmathematical extension of works considered previously by the authors [Milisic\\net al. 2011, Milisic et al. 2016]. Namely, this is the first result involving\\ndelay operators approximating protein linkages coupled with a spatial elliptic\\nsecond order operator. For the sake of simplicity we choose the Laplace\\noperator, although more general results could be derived. The main arguments\\nare (i) new energy estimates and (ii) a stability result extended from the\\nprevious work to this more involved context. They allow to prove convergence of\\nthe delay operator to a friction term together with the Laplace operator in the\\nsame asymptotic regime considered without the space dependence in [Milisic et\\nal, 2011]. In a second part we extend fixed-point results for the fully\\nnon-linear model introduced in [Milisic et al, 2016] and prove global existence\\nin time. This shows that the blow-up scenario observed previously does not\\noccur. Since the latter result was interpreted as a rupture of adhesion forces,\\nwe discuss the possibility of bond breaking both from the analytic and\\nnumerical point of view.\\n',\n",
       " '  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\\nestimating Gaussian location mixture densities in $d$-dimensions from\\nindependent observations. Unlike usual likelihood-based methods for fitting\\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\\nmultiplicative factors) when the true density is a discrete Gaussian mixture\\nwithout any prior information on the number of mixture components. NPMLEs can\\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\\nestimator. Here our results imply that the empirical Bayes estimator performs\\nat nearly the optimal level (up to logarithmic multiplicative factors) for\\ndenoising in clustering situations without any prior knowledge of the number of\\nclusters.\\n',\n",
       " '  Parametric resonance is among the most efficient phenomena generating\\ngravitational waves (GWs) in the early Universe. The dynamics of parametric\\nresonance, and hence of the GWs, depend exclusively on the resonance parameter\\n$q$. The latter is determined by the properties of each scenario: the initial\\namplitude and potential curvature of the oscillating field, and its coupling to\\nother species. Previous works have only studied the GW production for fixed\\nvalue(s) of $q$. We present an analytical derivation of the GW amplitude\\ndependence on $q$, valid for any scenario, which we confront against numerical\\nresults. By running lattice simulations in an expanding grid, we study for a\\nwide range of $q$ values, the production of GWs in post-inflationary preheating\\nscenarios driven by parametric resonance. We present simple fits for the final\\namplitude and position of the local maxima in the GW spectrum. Our\\nparametrization allows to predict the location and amplitude of the GW\\nbackground today, for an arbitrary $q$. The GW signal can be rather large, as\\n$h^2\\\\Omega_{\\\\rm GW}(f_p) \\\\lesssim 10^{-11}$, but it is always peaked at high\\nfrequencies $f_p \\\\gtrsim 10^{7}$ Hz. We also discuss the case of\\nspectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,\\nor the Standard Model Higgs.\\n',\n",
       " '  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed\\nbands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a\\ntentative assignment is given to the 2-photon transition from $X$ to the\\nn=12-13 $[X^{2}{\\\\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate\\nthe IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been\\nidentified as a molecular candidate amenable to laser control. Our work allows\\nus to identify an efficient method for loading cold SiO$^{+}$ from an ablated\\nsample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.\\n',\n",
       " '  Robots and automated systems are increasingly being introduced to unknown and\\ndynamic environments where they are required to handle disturbances, unmodeled\\ndynamics, and parametric uncertainties. Robust and adaptive control strategies\\nare required to achieve high performance in these dynamic environments. In this\\npaper, we propose a novel adaptive model predictive controller that combines\\nmodel predictive control (MPC) with an underlying $\\\\mathcal{L}_1$ adaptive\\ncontroller to improve trajectory tracking of a system subject to unknown and\\nchanging disturbances. The $\\\\mathcal{L}_1$ adaptive controller forces the\\nsystem to behave in a predefined way, as specified by a reference model. A\\nhigher-level model predictive controller then uses this reference model to\\ncalculate the optimal reference input based on a cost function, while taking\\ninto account input and state constraints. We focus on the experimental\\nvalidation of the proposed approach and demonstrate its effectiveness in\\nexperiments on a quadrotor. We show that the proposed approach has a lower\\ntrajectory tracking error compared to non-predictive, adaptive approaches and a\\npredictive, non-adaptive approach, even when external wind disturbances are\\napplied.\\n',\n",
       " '  With the use of ontologies in several domains such as semantic web,\\ninformation retrieval, artificial intelligence, the concept of similarity\\nmeasuring has become a very important domain of research. Therefore, in the\\ncurrent paper, we propose our method of similarity measuring which uses the\\nDijkstra algorithm to define and compute the shortest path. Then, we use this\\none to compute the semantic distance between two concepts defined in the same\\nhierarchy of ontology. Afterward, we base on this result to compute the\\nsemantic similarity. Finally, we present an experimental comparison between our\\nmethod and other methods of similarity measuring.\\n',\n",
       " '  In this short note, we present a novel method for computing exact lower and\\nupper bounds of eigenvalues of a symmetric tridiagonal interval matrix.\\nCompared to the known methods, our approach is fast, simple to present and to\\nimplement, and avoids any assumptions. Our construction explicitly yields those\\nmatrices for which particular lower and upper bounds are attained.\\n',\n",
       " '  Visualizing a complex network is computationally intensive process and\\ndepends heavily on the number of components in the network. One way to solve\\nthis problem is not to render the network in real time. PRE-render Content\\nUsing Tiles (PRECUT) is a process to convert any complex network into a\\npre-rendered network. Tiles are generated from pre-rendered images at different\\nzoom levels, and navigating the network simply becomes delivering relevant\\ntiles. PRECUT is exemplified by performing large-scale compound-target\\nrelationship analyses. Matched molecular pair (MMP) networks were created using\\ncompounds and the target class description found in the ChEMBL database. To\\nvisualize MMP networks, the MMP network viewer has been implemented in COMBINE\\nand as a web application, hosted at this http URL.\\n',\n",
       " '  We prove that every triangle-free graph with maximum degree $\\\\Delta$ has list\\nchromatic number at most $(1+o(1))\\\\frac{\\\\Delta}{\\\\ln \\\\Delta}$. This matches the\\nbest-known bound for graphs of girth at least 5. We also provide a new proof\\nthat for any $r\\\\geq 4$ every $K_r$-free graph has list-chromatic number at most\\n$200r\\\\frac{\\\\Delta\\\\ln\\\\ln\\\\Delta}{\\\\ln\\\\Delta}$.\\n',\n",
       " '  We study the two-dimensional topology of the galactic distribution when\\nprojected onto two-dimensional spherical shells. Using the latest Horizon Run 4\\nsimulation data, we construct the genus of the two-dimensional field and\\nconsider how this statistic is affected by late-time nonlinear effects --\\nprincipally gravitational collapse and redshift space distortion (RSD). We also\\nconsider systematic and numerical artifacts such as shot noise, galaxy bias,\\nand finite pixel effects. We model the systematics using a Hermite polynomial\\nexpansion and perform a comprehensive analysis of known effects on the\\ntwo-dimensional genus, with a view toward using the statistic for cosmological\\nparameter estimation. We find that the finite pixel effect is dominated by an\\namplitude drop and can be made less than $1\\\\%$ by adopting pixels smaller than\\n$1/3$ of the angular smoothing length. Nonlinear gravitational evolution\\nintroduces time-dependent coefficients of the zeroth, first, and second Hermite\\npolynomials, but the genus amplitude changes by less than $1\\\\%$ between $z=1$\\nand $z=0$ for smoothing scales $R_{\\\\rm G} > 9 {\\\\rm Mpc/h}$. Non-zero terms are\\nmeasured up to third order in the Hermite polynomial expansion when studying\\nRSD. Differences in shapes of the genus curves in real and redshift space are\\nsmall when we adopt thick redshift shells, but the amplitude change remains a\\nsignificant $\\\\sim {\\\\cal O}(10\\\\%)$ effect. The combined effects of galaxy\\nbiasing and shot noise produce systematic effects up to the second Hermite\\npolynomial. It is shown that, when sampling, the use of galaxy mass cuts\\nsignificantly reduces the effect of shot noise relative to random sampling.\\n',\n",
       " '  The new index of the author\\'s popularity estimation is represented in the\\npaper. The index is calculated on the basis of Wikipedia encyclopedia analysis\\n(Wikipedia Index - WI). Unlike the conventional existed citation indices, the\\nsuggested mark allows to evaluate not only the popularity of the author, as it\\ncan be done by means of calculating the general citation number or by the\\nHirsch index, which is often used to measure the author\\'s research rate. The\\nindex gives an opportunity to estimate the author\\'s popularity, his/her\\ninfluence within the sought-after area \"knowledge area\" in the Internet - in\\nthe Wikipedia. The suggested index is supposed to be calculated in frames of\\nthe subject domain, and it, on the one hand, avoids the mistaken computation of\\nthe homonyms, and on the other hand - provides the entirety of the subject\\narea. There are proposed algorithms and the technique of the Wikipedia Index\\ncalculation through the network encyclopedia sounding, the exemplified\\ncalculations of the index for the prominent researchers, and also the methods\\nof the information networks formation - models of the subject domains by the\\nautomatic monitoring and networks information reference resources analysis. The\\nconsidered in the paper notion network corresponds the terms-heads of the\\nWikipedia articles.\\n',\n",
       " '  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree\\n266 and describe the applied method. This yields explicit polynomials having J1\\nas a Galois group over K(t), [K:Q] = 7.\\n',\n",
       " '  Our goal is to find classes of convolution semigroups on Lie groups $G$ that\\ngive rise to interesting processes in symmetric spaces $G/K$. The\\n$K$-bi-invariant convolution semigroups are a well-studied example. An\\nappealing direction for the next step is to generalise to right $K$-invariant\\nconvolution semigroups, but recent work of Liao has shown that these are in\\none-to-one correspondence with $K$-bi-invariant convolution semigroups. We\\ninvestigate a weaker notion of right $K$-invariance, but show that this is, in\\nfact, the same as the usual notion. Another possible approach is to use\\ngeneralised notions of negative definite functions, but this also leads to\\nnothing new. We finally find an interesting class of convolution semigroups\\nthat are obtained by making use of the Cartan decomposition of a semisimple Lie\\ngroup, and the solution of certain stochastic differential equations. Examples\\nsuggest that these are well-suited for generating random motion along geodesics\\nin symmetric spaces.\\n',\n",
       " '  CMO Council reports that 71\\\\% of internet users in the U.S. were influenced\\nby coupons and discounts when making their purchase decisions. It has also been\\nshown that offering coupons to a small fraction of users (called seed users)\\nmay affect the purchase decisions of many other users in a social network. This\\nmotivates us to study the optimal coupon allocation problem, and our objective\\nis to allocate coupons to a set of users so as to maximize the expected\\ncascade. Different from existing studies on influence maximizaton (IM), our\\nframework allows a general utility function and a more complex set of\\nconstraints. In particular, we formulate our problem as an approximate\\nsubmodular maximization problem subject to matroid and knapsack constraints.\\nExisting techniques relying on the submodularity of the utility function, such\\nas greedy algorithm, can not work directly on a non-submodular function. We use\\n$\\\\epsilon$ to measure the difference between our function and its closest\\nsubmodular function and propose a novel approximate algorithm with\\napproximation ratio $\\\\beta(\\\\epsilon)$ with $\\\\lim_{\\\\epsilon\\\\rightarrow\\n0}\\\\beta(\\\\epsilon)=1-1/e$. This is the best approximation guarantee for\\napproximate submodular maximization subject to a partition matroid and knapsack\\nconstraints, our results apply to a broad range of optimization problems that\\ncan be formulated as an approximate submodular maximization problem.\\n',\n",
       " '  We prove the Lefschetz duality for intersection (co)homology in the framework\\nof $\\\\partial$-pesudomanifolds. We work with general perversities and without\\nrestriction on the coefficient ring.\\n',\n",
       " '  All possible removals of $n=5$ nodes from networks of size $N=100$ are\\nperformed in order to find the optimal set of nodes which fragments the\\noriginal network into the smallest largest connected component. The resulting\\nattacks are ordered according to the size of the largest connected component\\nand compared with the state of the art methods of network attacks. We chose\\nattacks of size $5$ on relative small networks of size $100$ because the number\\nof all possible attacks, ${100}\\\\choose{5}$ $\\\\approx 10^8$, is at the verge of\\nthe possible to compute with the available standard computers. Besides, we\\napplied the procedure in a series of networks with controlled and varied\\nmodularity, comparing the resulting statistics with the effect of removing the\\nsame amount of vertices, according to the known most efficient disruption\\nstrategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index\\nattack (CI), and Modular Based Attack (MBA). Results show that modularity has\\nan inverse relation with robustness, with $Q_c \\\\approx 0.7$ being the critical\\nvalue. For modularities lower than $Q_c$, all heuristic method gives mostly the\\nsame results than with random attacks, while for bigger $Q$, networks are less\\nrobust and highly vulnerable to malicious attacks.\\n',\n",
       " \"  In this paper, we study stochastic non-convex optimization with non-convex\\nrandom functions. Recent studies on non-convex optimization revolve around\\nestablishing second-order convergence, i.e., converging to a nearly\\nsecond-order optimal stationary points. However, existing results on stochastic\\nnon-convex optimization are limited, especially with a high probability\\nsecond-order convergence. We propose a novel updating step (named NCG-S) by\\nleveraging a stochastic gradient and a noisy negative curvature of a stochastic\\nHessian, where the stochastic gradient and Hessian are based on a proper\\nmini-batch of random functions. Building on this step, we develop two\\nalgorithms and establish their high probability second-order convergence. To\\nthe best of our knowledge, the proposed stochastic algorithms are the first\\nwith a second-order convergence in {\\\\it high probability} and a time complexity\\nthat is {\\\\it almost linear} in the problem's dimensionality.\\n\",\n",
       " \"  Lower bounds on the smallest eigenvalue of a symmetric positive definite\\nmatrices $A\\\\in\\\\mathbb{R}^{m\\\\times m}$ play an important role in condition\\nnumber estimation and in iterative methods for singular value computation. In\\nparticular, the bounds based on ${\\\\rm Tr}(A^{-1})$ and ${\\\\rm Tr}(A^{-2})$\\nattract attention recently because they can be computed in $O(m)$ work when $A$\\nis tridiagonal. In this paper, we focus on these bounds and investigate their\\nproperties in detail. First, we consider the problem of finding the optimal\\nbound that can be computed solely from ${\\\\rm Tr}(A^{-1})$ and ${\\\\rm\\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\\nbecomes smallest when ${\\\\rm Tr}(A^{-2})/\\\\{{\\\\rm Tr}(A^{-1})\\\\}^2$ approaches 1 or\\n$\\\\frac{1}{m}$. These results will be useful, for example, in designing\\nefficient shift strategies for singular value computation algorithms.\\n\",\n",
       " \"  This paper describes the development of a magnetic attitude control subsystem\\nfor a 2U cubesat. Due to the presence of gravity gradient torques, the\\nsatellite dynamics are open-loop unstable near the desired pointing\\nconfiguration. Nevertheless the linearized time-varying system is completely\\ncontrollable, under easily verifiable conditions, and the system's disturbance\\nrejection capabilities can be enhanced by adding air drag panels exemplifying a\\nbeneficial interplay between hardware design and control. In the paper,\\nconditions for the complete controllability for the case of a magnetically\\ncontrolled satellite with passive air drag panels are developed, and simulation\\ncase studies with the LQR and MPC control designs applied in combination with a\\nnonlinear time-varying input transformation are presented to demonstrate the\\nability of the closed-loop system to satisfy mission objectives despite\\ndisturbance torques.\\n\",\n",
       " '  One advantage of decision tree based methods like random forests is their\\nability to natively handle categorical predictors without having to first\\ntransform them (e.g., by using feature engineering techniques). However, in\\nthis paper, we show how this capability can lead to an inherent \"absent levels\"\\nproblem for decision tree based methods that has never been thoroughly\\ndiscussed, and whose consequences have never been carefully explored. This\\nproblem occurs whenever there is an indeterminacy over how to handle an\\nobservation that has reached a categorical split which was determined when the\\nobservation in question\\'s level was absent during training. Although these\\nincidents may appear to be innocuous, by using Leo Breiman and Adele Cutler\\'s\\nrandom forests FORTRAN code and the randomForest R package (Liaw and Wiener,\\n2002) as motivating case studies, we examine how overlooking the absent levels\\nproblem can systematically bias a model. Furthermore, by using three real data\\nexamples, we illustrate how absent levels can dramatically alter a model\\'s\\nperformance in practice, and we empirically demonstrate how some simple\\nheuristics can be used to help mitigate the effects of the absent levels\\nproblem until a more robust theoretical solution is found.\\n',\n",
       " '  For decades, conventional computers based on the von Neumann architecture\\nhave performed computation by repeatedly transferring data between their\\nprocessing and their memory units, which are physically separated. As\\ncomputation becomes increasingly data-centric and as the scalability limits in\\nterms of performance and power are being reached, alternative computing\\nparadigms are searched for in which computation and storage are collocated. A\\nfascinating new approach is that of computational memory where the physics of\\nnanoscale memory devices are used to perform certain computational tasks within\\nthe memory unit in a non-von Neumann manner. Here we present a large-scale\\nexperimental demonstration using one million phase-change memory devices\\norganized to perform a high-level computational primitive by exploiting the\\ncrystallization dynamics. Also presented is an application of such a\\ncomputational memory to process real-world data-sets. The results show that\\nthis co-existence of computation and storage at the nanometer scale could be\\nthe enabler for new, ultra-dense, low power, and massively parallel computing\\nsystems.\\n',\n",
       " '  We generalise some well-known graph parameters to operator systems by\\nconsidering their underlying quantum channels. In particular, we introduce the\\nquantum complexity as the dimension of the smallest co-domain Hilbert space a\\nquantum channel requires to realise a given operator system as its\\nnon-commutative confusability graph. We describe quantum complexity as a\\ngeneralised minimum semidefinite rank and, in the case of a graph operator\\nsystem, as a quantum intersection number. The quantum complexity and a closely\\nrelated quantum version of orthogonal rank turn out to be upper bounds for the\\nShannon zero-error capacity of a quantum channel, and we construct examples for\\nwhich these bounds beat the best previously known general upper bound for the\\ncapacity of quantum channels, given by the quantum Lovász theta number.\\n',\n",
       " '  During the ionization of atoms irradiated by linearly polarized intense laser\\nfields, we find for the first time that the transverse momentum distribution of\\nphotoelectrons can be well fitted by a squared zeroth-order Bessel function\\nbecause of the quantum interference effect of Glory rescattering. The\\ncharacteristic of the Bessel function is determined by the common angular\\nmomentum of a bunch of semiclassical paths termed as Glory trajectories, which\\nare launched with different nonzero initial transverse momenta distributed on a\\nspecific circle in the momentum plane and finally deflected to the same\\nasymptotic momentum, which is along the polarization direction, through\\npost-tunneling rescattering. Glory rescattering theory (GRT) based on the\\nsemiclassical path-integral formalism is developed to address this effect\\nquantitatively. Our theory can resolve the long-standing discrepancies between\\nexisting theories and experiments on the fringe location, predict the sudden\\ntransition of the fringe structure in holographic patterns, and shed light on\\nthe quantum interference aspects of low-energy structures in strong-field\\natomic ionization.\\n',\n",
       " '  We are interested in extending operators defined on positive measures, called\\nhere transfunctions, to signed measures and vector measures. Our methods use a\\nsomewhat nonstandard approach to measures and vector measures. The necessary\\nbackground, including proofs of some auxiliary results, is included.\\n',\n",
       " \"  Convolutional Neural Networks (CNNs) can learn effective features, though\\nhave been shown to suffer from a performance drop when the distribution of the\\ndata changes from training to test data. In this paper we analyze the internal\\nrepresentations of CNNs and observe that the representations of unseen data in\\neach class, spread more (with higher variance) in the embedding space of the\\nCNN compared to representations of the training data. More importantly, this\\ndifference is more extreme if the unseen data comes from a shifted\\ndistribution. Based on this observation, we objectively evaluate the degree of\\nrepresentation's variance in each class via eigenvalue decomposition on the\\nwithin-class covariance of the internal representations of CNNs and observe the\\nsame behaviour. This can be problematic as larger variances might lead to\\nmis-classification if the sample crosses the decision boundary of its class. We\\napply nearest neighbor classification on the representations and empirically\\nshow that the embeddings with the high variance actually have significantly\\nworse KNN classification performances, although this could not be foreseen from\\ntheir end-to-end classification results. To tackle this problem, we propose\\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\\nsignificantly reduces the within-class covariance of a DNN's representation,\\nimproving performance on unseen test data from a shifted distribution. We\\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\\nsignificantly improve the network's internal representation, it also increases\\nthe end-to-end classification accuracy, especially when the test set exhibits a\\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\\npercentage points improvement in the case of a distribution mismatch.\\n\",\n",
       " '  We present an efficient second-order algorithm with\\n$\\\\tilde{O}(\\\\frac{1}{\\\\eta}\\\\sqrt{T})$ regret for the bandit online multiclass\\nproblem. The regret bound holds simultaneously with respect to a family of loss\\nfunctions parameterized by $\\\\eta$, for a range of $\\\\eta$ restricted by the norm\\nof the competitor. The family of loss functions ranges from hinge loss\\n($\\\\eta=0$) to squared hinge loss ($\\\\eta=1$). This provides a solution to the\\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\\n$\\\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\\nalgorithm experimentally, showing that it also performs favorably against\\nearlier algorithms.\\n',\n",
       " '  Swarm systems constitute a challenging problem for reinforcement learning\\n(RL) as the algorithm needs to learn decentralized control policies that can\\ncope with limited local sensing and communication abilities of the agents.\\nWhile it is often difficult to directly define the behavior of the agents,\\nsimple communication protocols can be defined more easily using prior knowledge\\nabout the given task. In this paper, we propose a number of simple\\ncommunication protocols that can be exploited by deep reinforcement learning to\\nfind decentralized control policies in a multi-robot swarm environment. The\\nprotocols are based on histograms that encode the local neighborhood relations\\nof the agents and can also transmit task-specific information, such as the\\nshortest distance and direction to a desired target. In our framework, we use\\nan adaptation of Trust Region Policy Optimization to learn complex\\ncollaborative tasks, such as formation building and building a communication\\nlink. We evaluate our findings in a simulated 2D-physics environment, and\\ncompare the implications of different communication protocols.\\n',\n",
       " '  We describe the design and implementation of an extremely scalable real-time\\nRFI mitigation method, based on the offline AOFlagger. All algorithms scale\\nlinearly in the number of samples. We describe how we implemented the flagger\\nin the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we\\nintroduce a novel simple history-based flagger that helps reduce the impact of\\nour small window on the data.\\nBy examining an observation of a known pulsar, we demonstrate that our\\nflagger can achieve much higher quality than a simple thresholder, even when\\nrunning in real time, on a distributed system. The flagger works on visibility\\ndata, but also on raw voltages, and beam formed data. The algorithms are\\nscale-invariant, and work on microsecond to second time scales. We are\\ncurrently implementing a prototype for the time domain pipeline of the SKA\\ncentral signal processor.\\n',\n",
       " '  Controlling embodied agents with many actuated degrees of freedom is a\\nchallenging task. We propose a method that can discover and interpolate between\\ncontext dependent high-level actions or body-affordances. These provide an\\nabstract, low-dimensional interface indexing high-dimensional and time-\\nextended action policies. Our method is related to recent ap- proaches in the\\nmachine learning literature but is conceptually simpler and easier to\\nimplement. More specifically our method requires the choice of a n-dimensional\\ntarget sensor space that is endowed with a distance metric. The method then\\nlearns an also n-dimensional embedding of possibly reactive body-affordances\\nthat spread as far as possible throughout the target sensor space.\\n',\n",
       " '  Let $n >3$ and $ 0< k < \\\\frac{n}{2} $ be integers. In this paper, we\\ninvestigate some algebraic properties of the line graph of the graph $\\n{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$\\nwhich is induced by the set of vertices of weights $k$ and $k+1$. In the first\\nstep, we determine the automorphism groups of these graphs for all values of\\n$k$. In the second step, we study Cayley properties of the line graph of these\\ngraphs. In particular, we show that for $ k>2, $ if $ 2k+1 \\\\neq n$, then the\\nline graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley\\ngraph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a\\nCayley graph if and only if $ n$ is a power of a prime $p$.\\n',\n",
       " '  Recently software development companies started to embrace Machine Learning\\n(ML) techniques for introducing a series of advanced functionality in their\\nproducts such as personalisation of the user experience, improved search,\\ncontent recommendation and automation. The technical challenges for tackling\\nthese problems are heavily researched in literature. A less studied area is a\\npragmatic approach to the role of humans in a complex modern industrial\\nenvironment where ML based systems are developed. Key stakeholders affect the\\nsystem from inception and up to operation and maintenance. Product managers\\nwant to embed \"smart\" experiences for their users and drive the decisions on\\nwhat should be built next; software engineers are challenged to build or\\nutilise ML software tools that require skills that are well outside of their\\ncomfort zone; legal and risk departments may influence design choices and data\\naccess; operations teams are requested to maintain ML systems which are\\nnon-stationary in their nature and change behaviour over time; and finally ML\\npractitioners should communicate with all these stakeholders to successfully\\nbuild a reliable system. This paper discusses some of the challenges we faced\\nin Atlassian as we started investing more in the ML space.\\n',\n",
       " '  Generative Adversarial Networks (GANs) produce systematically better quality\\nsamples when class label information is provided., i.e. in the conditional GAN\\nsetup. This is still observed for the recently proposed Wasserstein GAN\\nformulation which stabilized adversarial training and allows considering high\\ncapacity network architectures such as ResNet. In this work we show how to\\nboost conditional GAN by augmenting available class labels. The new classes\\ncome from clustering in the representation space learned by the same GAN model.\\nThe proposed strategy is also feasible when no class information is available,\\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\\nunsupervised setup.\\n',\n",
       " '  We study the phase space dynamics of cosmological models in the theoretical\\nformulations of non-minimal metric-torsion couplings with a scalar field, and\\ninvestigate in particular the critical points which yield stable solutions\\nexhibiting cosmic acceleration driven by the {\\\\em dark energy}. The latter is\\ndefined in a way that it effectively has no direct interaction with the\\ncosmological fluid, although in an equivalent scalar-tensor cosmological setup\\nthe scalar field interacts with the fluid (which we consider to be the\\npressureless dust). Determining the conditions for the existence of the stable\\ncritical points we check their physical viability, in both Einstein and Jordan\\nframes. We also verify that in either of these frames, the evolution of the\\nuniverse at the corresponding stable points matches with that given by the\\nrespective exact solutions we have found in an earlier work (arXiv: 1611.00654\\n[gr-qc]). We not only examine the regions of physical relevance for the\\ntrajectories in the phase space when the coupling parameter is varied, but also\\ndemonstrate the evolution profiles of the cosmological parameters of interest\\nalong fiducial trajectories in the effectively non-interacting scenarios, in\\nboth Einstein and Jordan frames.\\n',\n",
       " '  In this work, we propose an end-to-end deep architecture that jointly learns\\nto detect obstacles and estimate their depth for MAV flight applications. Most\\nof the existing approaches either rely on Visual SLAM systems or on depth\\nestimation models to build 3D maps and detect obstacles. However, for the task\\nof avoiding obstacles this level of complexity is not required. Recent works\\nhave proposed multi task architectures to both perform scene understanding and\\ndepth estimation. We follow their track and propose a specific architecture to\\njointly estimate depth and obstacles, without the need to compute a global map,\\nbut maintaining compatibility with a global SLAM system if needed. The network\\narchitecture is devised to exploit the joint information of the obstacle\\ndetection task, that produces more reliable bounding boxes, with the depth\\nestimation one, increasing the robustness of both to scenario changes. We call\\nthis architecture J-MOD$^{2}$. We test the effectiveness of our approach with\\nexperiments on sequences with different appearance and focal lengths and\\ncompare it to SotA multi task methods that jointly perform semantic\\nsegmentation and depth estimation. In addition, we show the integration in a\\nfull system using a set of simulated navigation experiments where a MAV\\nexplores an unknown scenario and plans safe trajectories by using our detection\\nmodel.\\n',\n",
       " '  In this paper, we prove that there exists a dimensional constant $\\\\delta > 0$\\nsuch that given any background Kähler metric $\\\\omega$, the Calabi flow with\\ninitial data $u_0$ satisfying \\\\begin{equation*} \\\\partial \\\\bar \\\\partial u_0 \\\\in\\nL^\\\\infty (M) \\\\text{ and } (1- \\\\delta )\\\\omega < \\\\omega_{u_0} < (1+\\\\delta\\n)\\\\omega, \\\\end{equation*} admits a unique short time solution and it becomes\\nsmooth immediately, where $\\\\omega_{u_0} : = \\\\omega +\\\\sqrt{-1}\\\\partial\\n\\\\bar\\\\partial u_0$. The existence time depends on initial data $u_0$ and the\\nmetric $\\\\omega$. As a corollary, we get that Calabi flow has short time\\nexistence for any initial data satisfying \\\\begin{equation*} \\\\partial \\\\bar\\n\\\\partial u_0 \\\\in C^0(M) \\\\text{ and } \\\\omega_{u_0} > 0, \\\\end{equation*} which\\nshould be interpreted as a \"continuous Kähler metric\". A main technical\\ningredient is Schauder-type estimates for biharmonic heat equation on\\nRiemannian manifolds with time weighted Hölder norms.\\n',\n",
       " '  To probe the star-formation (SF) processes, we present results of an analysis\\nof the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency\\nobservations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.\\nAn almost horseshoe-like structure embedded within the MCG35.2 is evident in\\nthe infrared and millimeter images and harbors the previously known sites,\\nultra-compact/hyper-compact G35.20$-$0.74N H\\\\,{\\\\sc ii} region, Ap2-1, and\\nMercer 14 at its base. The site, Ap2-1 is found to be excited by a radio\\nspectral type of B0.5V star where the distribution of 20 cm and H$\\\\alpha$\\nemission is surrounded by the extended molecular hydrogen emission. Using the\\n{\\\\it Herschel} 160-500 $\\\\mu$m and photometric 1-24 $\\\\mu$m data analysis,\\nseveral embedded clumps and clusters of young stellar objects (YSOs) are\\ninvestigated within the MCG35.2, revealing the SF activities. Majority of the\\nYSOs clusters and massive clumps (500-4250 M$_{\\\\odot}$) are seen toward the\\nhorseshoe-like structure. The position-velocity analysis of $^{13}$CO emission\\nshows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km\\ns$^{-1}$) interconnected by lower intensity intermediated velocity emission,\\ntracing a broad bridge feature. The presence of such broad bridge feature\\nsuggests the onset of a collision between molecular components in the MCG35.2.\\nA noticeable change in the H-band starlight mean polarization angles has also\\nbeen observed in the MCG35.2, probably tracing the interaction between\\nmolecular components. Taken together, it seems that the cloud-cloud collision\\nprocess has influenced the birth of massive stars and YSOs clusters in the\\nMCG35.2.\\n',\n",
       " \"  We present novel oblivious routing algorithms for both splittable and\\nunsplittable multicommodity flow. Our algorithm for minimizing congestion for\\n\\\\emph{unsplittable} multicommodity flow is the first oblivious routing\\nalgorithm for this setting. As an intermediate step towards this algorithm, we\\npresent a novel generalization of Valiant's classical load balancing scheme for\\npacket-switched networks to arbitrary graphs, which is of independent interest.\\nOur algorithm for minimizing congestion for \\\\emph{splittable} multicommodity\\nflow improves upon the state-of-the-art, in terms of both running time and\\nperformance, for graphs that exhibit good expansion guarantees. Our algorithms\\nrely on diffusing traffic via iterative applications of the random walk\\noperator. Consequently, the performance guarantees of our algorithms are\\nderived from the convergence of the random walk operator to the stationary\\ndistribution and are expressed in terms of the spectral gap of the graph (which\\ndominates the mixing time).\\n\",\n",
       " '  We study functional graphs generated by quadratic polynomials over prime\\nfields. We introduce efficient algorithms for methodical computations and\\nprovide the values of various direct and cumulative statistical parameters of\\ninterest. These include: the number of connected functional graphs, the number\\nof graphs having a maximal cycle, the number of cycles of fixed size, the\\nnumber of components of fixed size, as well as the shape of trees extracted\\nfrom functional graphs. We particularly focus on connected functional graphs,\\nthat is, the graphs which contain only one component (and thus only one cycle).\\nBased on the results of our computations, we formulate several conjectures\\nhighlighting the similarities and differences between these functional graphs\\nand random mappings.\\n',\n",
       " \"  Helmholtz decomposition theorem for vector fields is usually presented with\\ntoo strong restrictions on the fields and only for time independent fields.\\nBlumenthal showed in 1905 that decomposition is possible for any asymptotically\\nweakly decreasing vector field. He used a regularization method in his proof\\nwhich can be extended to prove the theorem even for vector fields\\nasymptotically increasing sublinearly. Blumenthal's result is then applied to\\nthe time-dependent fields of the dipole radiation and an artificial sublinearly\\nincreasing field.\\n\",\n",
       " \"  We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy\\ndecomposition of the fibre $\\\\Omega^3S^{17}\\\\{2\\\\}$ of the $H$-space squaring map\\non the triple loop space of the $17$-sphere. This induces a splitting of the\\nmod-$2$ homotopy groups $\\\\pi_\\\\ast(S^{17}; \\\\mathbb{Z}/2\\\\mathbb{Z})$ in terms of\\nthe integral homotopy groups of the fibre of the double suspension\\n$E^2:S^{2n-1} \\\\to \\\\Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,\\nwho gave similar decompositions for $S^5$ and $S^9$. We relate these\\ndecompositions to various Whitehead products in the homotopy groups of mod-$2$\\nMoore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},\\ni_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$\\nis divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.\\n\",\n",
       " '  We show that certain orderable groups admit no isolated left orders. The\\ngroups we consider are cyclic amalgamations of a free group with a general\\norderable group, the HNN extensions of free groups over cyclic subgroups, and a\\nparticular class of one-relator groups. In order to prove the results about\\norders, we develop perturbation techniques for actions of these groups on the\\nline.\\n',\n",
       " '  Machine learning classifiers are known to be vulnerable to inputs maliciously\\nconstructed by adversaries to force misclassification. Such adversarial\\nexamples have been extensively studied in the context of computer vision\\napplications. In this work, we show adversarial attacks are also effective when\\ntargeting neural network policies in reinforcement learning. Specifically, we\\nshow existing adversarial example crafting techniques can be used to\\nsignificantly degrade test-time performance of trained policies. Our threat\\nmodel considers adversaries capable of introducing small perturbations to the\\nraw input of the policy. We characterize the degree of vulnerability across\\ntasks and training algorithms, for a subclass of adversarial-example attacks in\\nwhite-box and black-box settings. Regardless of the learned task or training\\nalgorithm, we observe a significant drop in performance, even with small\\nadversarial perturbations that do not interfere with human perception. Videos\\nare available at this http URL.\\n',\n",
       " '  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy\\noffer a unique way to constrain the shape of galactic gravitational potentials.\\nSuch streams can be used as leaning tower gravitational experiments on galactic\\nscales. The most well motivated modification of gravity proposed as an\\nalternative to dark matter on galactic scales is Milgromian dynamics (MOND),\\nand we present here the first ever N-body simulations of the dynamical\\nevolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a\\nrealistic baryonic mass model for the Milky Way, we attempt to reproduce the\\npresent-day spatial and kinematic structure of the Sagittarius dwarf and its\\nimmense tidal stream that wraps around the Milky Way. With very little freedom\\non the original structure of the progenitor, constrained by the total\\nluminosity of the Sagittarius structure and by the observed stellar mass-size\\nrelation for isolated dwarf galaxies, we find reasonable agreement between our\\nsimulations and observations of this system. The observed stellar velocities in\\nthe leading arm can be reproduced if we include a massive hot gas corona around\\nthe Milky Way that is flattened in the direction of the principal plane of its\\nsatellites. This is the first time that tidal dissolution in MOND has been\\ntested rigorously at these mass and acceleration scales.\\n',\n",
       " '  The response of an electron system to electromagnetic fields with sharp\\nspatial variations is strongly dependent on quantum electronic properties, even\\nin ambient conditions, but difficult to access experimentally. We use\\npropagating graphene plasmons, together with an engineered dielectric-metallic\\nenvironment, to probe the graphene electron liquid and unveil its detailed\\nelectronic response at short wavelengths.The near-field imaging experiments\\nreveal a parameter-free match with the full theoretical quantum description of\\nthe massless Dirac electron gas, in which we identify three types of quantum\\neffects as keys to understanding the experimental response of graphene to\\nshort-ranged terahertz electric fields. The first type is of single-particle\\nnature and is related to shape deformations of the Fermi surface during a\\nplasmon oscillations. The second and third types are a many-body effect\\ncontrolled by the inertia and compressibility of the interacting electron\\nliquid in graphene. We demonstrate how, in principle, our experimental approach\\ncan determine the full spatiotemporal response of an electron system.\\n',\n",
       " \"  A new generation of solar instruments provides improved spectral, spatial,\\nand temporal resolution, thus facilitating a better understanding of dynamic\\nprocesses on the Sun. High-resolution observations often reveal\\nmultiple-component spectral line profiles, e.g., in the near-infrared He I\\n10830 \\\\AA\\\\ triplet, which provides information about the chromospheric velocity\\nand magnetic fine structure. We observed an emerging flux region, including two\\nsmall pores and an arch filament system, on 2015 April 17 with the 'very fast\\nspectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the\\n1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We\\ndiscuss this method of obtaining fast (one per minute) spectral scans of the\\nsolar surface and its potential to follow dynamic processes on the Sun. We\\ndemonstrate the performance of the 'very fast spectroscopic mode' by tracking\\nchromospheric high-velocity features in the arch filament system.\\n\",\n",
       " '  In the past decade, the information security and threat landscape has grown\\nsignificantly making it difficult for a single defender to defend against all\\nattacks at the same time. This called for introduc- ing information sharing, a\\nparadigm in which threat indicators are shared in a community of trust to\\nfacilitate defenses. Standards for representation, exchange, and consumption of\\nindicators are pro- posed in the literature, although various issues are\\nundermined. In this paper, we rethink information sharing for actionable\\nintelli- gence, by highlighting various issues that deserve further explo-\\nration. We argue that information sharing can benefit from well- defined use\\nmodels, threat models, well-understood risk by mea- surement and robust\\nscoring, well-understood and preserved pri- vacy and quality of indicators and\\nrobust mechanism to avoid free riding behavior of selfish agent. We call for\\nusing the differential nature of data and community structures for optimizing\\nsharing.\\n',\n",
       " '  Permutation polynomials over finite fields have wide applications in many\\nareas of science and engineering. In this paper, we present six new classes of\\npermutation trinomials over $\\\\mathbb{F}_{2^n}$ which have explicit forms by\\ndetermining the solutions of some equations.\\n',\n",
       " '  Ben-David and Shelah proved that if $\\\\lambda$ is a singular strong-limit\\ncardinal and $2^\\\\lambda=\\\\lambda^+$, then $\\\\square^*_\\\\lambda$ entails the\\nexistence of a normal $\\\\lambda$-distributive $\\\\lambda^+$-Aronszajn tree. Here,\\nit is proved that the same conclusion remains valid after replacing the\\nhypothesis $\\\\square^*_\\\\lambda$ by $\\\\square(\\\\lambda^+,{<}\\\\lambda)$.\\nAs $\\\\square(\\\\lambda^+,{<}\\\\lambda)$ does not impose a bound on the order-type\\nof the witnessing clubs, our construction is necessarily different from that of\\nBen-David and Shelah, and instead uses walks on ordinals augmented with club\\nguessing.\\nA major component of this work is the study of postprocessing functions and\\ntheir effect on square sequences. A byproduct of this study is the finding that\\nfor $\\\\kappa$ regular uncountable, $\\\\square(\\\\kappa)$ entails the existence of a\\npartition of $\\\\kappa$ into $\\\\kappa$ many fat sets. When contrasted with a\\nclassic model of Magidor, this shows that it is equiconsistent with the\\nexistence of a weakly compact cardinal that $\\\\omega_2$ cannot be split into two\\nfat sets.\\n',\n",
       " '  The real Scarf II potential is discussed as a radial problem. This potential\\nhas been studied extensively as a one-dimensional problem, and now these\\nresults are used to construct its bound and resonance solutions for $l=0$ by\\nsetting the origin at some arbitrary value of the coordinate. The solutions\\nwith appropriate boundary conditions are composed as the linear combination of\\nthe two independent solutions of the Schrödinger equation. The asymptotic\\nexpression of these solutions is used to construct the $S_0(k)$ s-wave\\n$S$-matrix, the poles of which supply the $k$ values corresponding to the\\nbound, resonance and anti-bound solutions. The location of the discrete energy\\neigenvalues is analyzed, and the relation of the solutions of the radial and\\none-dimensional Scarf II potentials is discussed. It is shown that the\\ngeneralized Woods--Saxon potential can be generated from the Rosen--Morse II\\npotential in the same way as the radial Scarf II potential is obtained from its\\none-dimensional correspondent. Based on this analogy, possible applications are\\nalso pointed out.\\n',\n",
       " '  This paper presents a novel model for multimodal learning based on gated\\nneural networks. The Gated Multimodal Unit (GMU) model is intended to be used\\nas an internal unit in a neural network architecture whose purpose is to find\\nan intermediate representation based on a combination of data from different\\nmodalities. The GMU learns to decide how modalities influence the activation of\\nthe unit using multiplicative gates. It was evaluated on a multilabel scenario\\nfor genre classification of movies using the plot and the poster. The GMU\\nimproved the macro f-score performance of single-modality approaches and\\noutperformed other fusion strategies, including mixture of experts models.\\nAlong with this work, the MM-IMDb dataset is released which, to the best of our\\nknowledge, is the largest publicly available multimodal dataset for genre\\nprediction on movies.\\n',\n",
       " '  In a single winner election with several candidates and ranked choice or\\nrating scale ballots, a Condorcet winner is one who wins all their two way\\nraces by majority rule or MR. A voting system has Condorcet consistency or CC\\nif it names any Condorcet winner the winner. Many voting systems lack CC, but a\\nthree step line of reasoning is used here to show why it is necessary. In step\\n1 we show that we can dismiss all the electoral criteria which conflict with\\nCC. In step 2 we point out that CC follows almost automatically if we can agree\\nthat MR is the only acceptable system for elections with two candidates. In\\nstep 3 we make that argument for MR. This argument itself has three parts.\\nFirst, in races with two candidates, the only well known alternatives to MR can\\nsometimes name as winner a candidate who is preferred over their opponent by\\nonly one voter, with all others preferring the opponent. That is unacceptable.\\nSecond, those same systems are also extremely susceptible to strategic\\ninsincere voting. Third, in simulation studies using spatial models with two\\ncandidates, the best known alternative to MR picks the best or most centrist\\ncandidate significantly less often than MR does.\\n',\n",
       " '  In the framework of the Einstein-Maxwell-aether theory we study the\\nbirefringence effect, which can occur in the pp-wave symmetric dynamic aether.\\nThe dynamic aether is considered to be latently birefringent quasi-medium,\\nwhich displays this hidden property if and only if the aether motion is\\nnon-uniform, i.e., when the aether flow is characterized by the non-vanishing\\nexpansion, shear, vorticity or acceleration. In accordance with the\\ndynamo-optical scheme of description of the interaction between electromagnetic\\nwaves and the dynamic aether, we shall model the susceptibility tensors by the\\nterms linear in the covariant derivative of the aether velocity four-vector.\\nWhen the pp-wave modes appear in the dynamic aether, we deal with a\\ngravitationally induced degeneracy removal with respect to hidden\\nsusceptibility parameters. As a consequence, the phase velocities of\\nelectromagnetic waves possessing orthogonal polarizations do not coincide, thus\\ndisplaying the birefringence effect. Two electromagnetic field configurations\\nare studied in detail: longitudinal and transversal with respect to the aether\\npp-wave front. For both cases the solutions are found, which reveal anomalies\\nin the electromagnetic response on the action of the pp-wave aether mode.\\n',\n",
       " \"  The $p$-set, which is in a simple analytic form, is well distributed in unit\\ncubes. The well-known Weil's exponential sum theorem presents an upper bound of\\nthe exponential sum over the $p$-set. Based on the result, one shows that the\\n$p$-set performs well in numerical integration, in compressed sensing as well\\nas in UQ. However, $p$-set is somewhat rigid since the cardinality of the\\n$p$-set is a prime $p$ and the set only depends on the prime number $p$. The\\npurpose of this paper is to present generalizations of $p$-sets, say\\n$\\\\mathcal{P}_{d,p}^{{\\\\mathbf a},\\\\epsilon}$, which is more flexible.\\nParticularly, when a prime number $p$ is given, we have many different choices\\nof the new $p$-sets. Under the assumption that Goldbach conjecture holds, for\\nany even number $m$, we present a point set, say ${\\\\mathcal L}_{p,q}$, with\\ncardinality $m-1$ by combining two different new $p$-sets, which overcomes a\\nmajor bottleneck of the $p$-set. We also present the upper bounds of the\\nexponential sums over $\\\\mathcal{P}_{d,p}^{{\\\\mathbf a},\\\\epsilon}$ and ${\\\\mathcal\\nL}_{p,q}$, which imply these sets have many potential applications.\\n\",\n",
       " '  This paper presents the design and implementation of a Human Interface for a\\nhousekeeper robot. It bases on the idea of making the robot understand the\\nhuman needs without making the human go through the details of robots work, for\\nexample, the way that the robot implements the work or the method that the\\nrobot uses to plan the path in order to reach the work area. The interface\\ncommands based on idioms of the natural human language and designed in a manner\\nthat the user gives the robot several commands with their execution date/time.\\n',\n",
       " '  This work proposes a new algorithm for training a re-weighted L2 Support\\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès\\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\\nparticular, the margin required for each training vector is set independently,\\ndefining a new weighted SVM model. These weights are selected to be binary, and\\nthey are automatically adapted during the training of the model, resulting in a\\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\\ncomputational complexity as the original algorithm. As shown experimentally,\\nthis algorithm is computationally cheaper to apply since it requires less\\niterations to converge, and it produces models with a sparser representation in\\nterms of support vectors and which are more stable with respect to the\\nselection of the regularization hyper-parameter.\\n',\n",
       " '  Learning-based approaches to robotic manipulation are limited by the\\nscalability of data collection and accessibility of labels. In this paper, we\\npresent a multi-task domain adaptation framework for instance grasping in\\ncluttered scenes by utilizing simulated robot experiments. Our neural network\\ntakes monocular RGB images and the instance segmentation mask of a specified\\ntarget object as inputs, and predicts the probability of successfully grasping\\nthe specified object for each candidate motor command. The proposed transfer\\nlearning framework trains a model for instance grasping in simulation and uses\\na domain-adversarial loss to transfer the trained model to real robots using\\nindiscriminate grasping data, which is available both in simulation and the\\nreal world. We evaluate our model in real-world robot experiments, comparing it\\nwith alternative model architectures as well as an indiscriminate grasping\\nbaseline.\\n',\n",
       " \"  A 1-ended finitely presented group has semistable fundamental group at\\n$\\\\infty$ if it acts geometrically on some (equivalently any) simply connected\\nand locally finite complex $X$ with the property that any two proper rays in\\n$X$ are properly homotopic. If $G$ has semistable fundamental group at $\\\\infty$\\nthen one can unambiguously define the fundamental group at $\\\\infty$ for $G$.\\nThe problem, asking if all finitely presented groups have semistable\\nfundamental group at $\\\\infty$ has been studied for over 40 years. If $G$ is an\\nascending HNN extension of a finitely presented group then indeed, $G$ has\\nsemistable fundamental group at $\\\\infty$, but since the early 1980's it has\\nbeen suggested that the finitely presented groups that are ascending HNN\\nextensions of {\\\\it finitely generated} groups may include a group with\\nnon-semistable fundamental group at $\\\\infty$. Ascending HNN extensions\\nnaturally break into two classes, those with bounded depth and those with\\nunbounded depth. Our main theorem shows that bounded depth finitely presented\\nascending HNN extensions of finitely generated groups have semistable\\nfundamental group at $\\\\infty$. Semistability is equivalent to two weaker\\nasymptotic conditions on the group holding simultaneously. We show one of these\\nconditions holds for all ascending HNN extensions, regardless of depth. We give\\na technique for constructing ascending HNN extensions with unbounded depth.\\nThis work focuses attention on a class of groups that may contain a group with\\nnon-semistable fundamental group at $\\\\infty$.\\n\",\n",
       " '  Developing and testing algorithms for autonomous vehicles in real world is an\\nexpensive and time consuming process. Also, in order to utilize recent advances\\nin machine intelligence and deep learning we need to collect a large amount of\\nannotated training data in a variety of conditions and environments. We present\\na new simulator built on Unreal Engine that offers physically and visually\\nrealistic simulations for both of these goals. Our simulator includes a physics\\nengine that can operate at a high frequency for real-time hardware-in-the-loop\\n(HITL) simulations with support for popular protocols (e.g. MavLink). The\\nsimulator is designed from the ground up to be extensible to accommodate new\\ntypes of vehicles, hardware platforms and software protocols. In addition, the\\nmodular design enables various components to be easily usable independently in\\nother projects. We demonstrate the simulator by first implementing a quadrotor\\nas an autonomous vehicle and then experimentally comparing the software\\ncomponents with real-world flights.\\n',\n",
       " '  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power\\nseries. The associated metric and Hausdorff dimension function give rise to the\\nHausdorff spectrum, which consists of the Hausdorff dimensions of closed\\nsubgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff\\ndimension function is well understood; in particular, the Hausdorff spectrum\\nconsists of finitely many rational numbers closely linked to the analytic\\ndimensions of subgroups of $G$.\\nConversely, it is a long-standing open question whether the finiteness of the\\nHausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the\\nanswer is yes, in a strong sense, under the extra condition that $G$ is\\nsoluble.\\nFurthermore, we explore the problem and related questions also for other\\nfiltration series, such as the lower $p$-series, the Frattini series, the\\nmodular dimension subgroup series and quite general filtration series. For\\ninstance, we prove, for odd primes $p$, that every countably based pro-$p$\\ngroup $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers\\nadmits a filtration series such that the corresponding Hausdorff spectrum\\ncontains an infinite real interval.\\n',\n",
       " \"  Brain-Machine Interaction (BMI) system motivates interesting and promising\\nresults in forward/feedback control consistent with human intention. It holds\\ngreat promise for advancements in patient care and applications to\\nneurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic\\nplatform using a personalized social robot in order to assist patients having\\ncognitive deficits through bilateral rehabilitation and mental training. For\\ninitial testing of the platform, electroencephalography (EEG) brainwaves of a\\nhuman user were collected in real time during tasks of imaginary movements.\\nFirst, the brainwaves associated with imagined body kinematics parameters were\\ndecoded to control a cursor on a computer screen in training protocol. Then,\\nthe experienced subject was able to interact with a social robot via our\\nreal-time BMI robotic platform. Corresponding to subject's imagery performance,\\nhe/she received specific gesture movements and eye color changes as\\nneural-based feedback from the robot. This hands-free neurofeedback interaction\\nnot only can be used for mind control of a social robot's movements, but also\\nsets the stage for application to enhancing and recovering mental abilities\\nsuch as attention via training in humans by providing real-time neurofeedback\\nfrom a social robot.\\n\",\n",
       " '  Road networks in cities are massive and is a critical component of mobility.\\nFast response to defects, that can occur not only due to regular wear and tear\\nbut also because of extreme events like storms, is essential. Hence there is a\\nneed for an automated system that is quick, scalable and cost-effective for\\ngathering information about defects. We propose a system for city-scale road\\naudit, using some of the most recent developments in deep learning and semantic\\nsegmentation. For building and benchmarking the system, we curated a dataset\\nwhich has annotations required for road defects. However, many of the labels\\nrequired for road audit have high ambiguity which we overcome by proposing a\\nlabel hierarchy. We also propose a multi-step deep learning model that segments\\nthe road, subdivide the road further into defects, tags the frame for each\\ndefect and finally localizes the defects on a map gathered using GPS. We\\nanalyze and evaluate the models on image tagging as well as segmentation at\\ndifferent levels of the label hierarchy.\\n',\n",
       " \"  In this Letter, we study the motion and wake-patterns of freely rising and\\nfalling cylinders in quiescent fluid. We show that the amplitude of oscillation\\nand the overall system-dynamics are intricately linked to two parameters: the\\nparticle's mass-density relative to the fluid $m^* \\\\equiv \\\\rho_p/\\\\rho_f$ and\\nits relative moment-of-inertia $I^* \\\\equiv {I}_p/{I}_f$. This supersedes the\\ncurrent understanding that a critical mass density ($m^*\\\\approx$ 0.54) alone\\ntriggers the sudden onset of vigorous vibrations. Using over 144 combinations\\nof ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering\\nvery heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire\\ndata collapses into two scaling regimes demarcated by a transitional Strouhal\\nnumber, $St_t \\\\approx 0.17$. $St_t$ separates a mass-dominated regime from a\\nregime dominated by the particle's moment of inertia. A shift from one regime\\nto the other also marks a gradual transition in the wake-shedding pattern: from\\nthe classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.\\nThus, auto-rotation can have a significant influence on the trajectories and\\nwakes of freely rising isotropic bodies.\\n\",\n",
       " \"  Theory of Mind is the ability to attribute mental states (beliefs, intents,\\nknowledge, perspectives, etc.) to others and recognize that these mental states\\nmay differ from one's own. Theory of Mind is critical to effective\\ncommunication and to teams demonstrating higher collective performance. To\\neffectively leverage the progress in Artificial Intelligence (AI) to make our\\nlives more productive, it is important for humans and AI to work well together\\nin a team. Traditionally, there has been much emphasis on research to make AI\\nmore accurate, and (to a lesser extent) on having it better understand human\\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\\nmore human-like and having it develop a theory of our minds. In this work, we\\nargue that for human-AI teams to be effective, humans must also develop a\\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\\nand quirks. We instantiate these ideas within the domain of Visual Question\\nAnswering (VQA). We find that using just a few examples (50), lay people can be\\ntrained to better predict responses and oncoming failures of a complex VQA\\nmodel. We further evaluate the role existing explanation (or interpretability)\\nmodalities play in helping humans build ToAIM. Explainable AI has received\\nconsiderable scientific and popular attention in recent times. Surprisingly, we\\nfind that having access to the model's internal states - its confidence in its\\ntop-k predictions, explicit or implicit attention maps which highlight regions\\nin the image (and words in the question) the model is looking at (and listening\\nto) while answering a question about an image - do not help people better\\npredict its behavior.\\n\",\n",
       " '  We study families of varieties endowed with polarized canonical eigensystems\\nof several maps, inducing canonical heights on the dominating variety as well\\nas on the \"good\" fibers of the family. We show explicitely the dependence on\\nthe parameter for global and local canonical heights defined by Kawaguchi when\\nthe fibers change, extending previous works of J. Silverman and others.\\nFinally, fixing an absolute value $v \\\\in K$ and a variety $V/K$, we descript\\nthe Kawaguchi`s canonical local height $\\\\hat{\\\\lambda}_{V,E,\\\\mathcal{Q},}(.,v)$\\nas an intersection number, provided that the polarized system $(V,\\\\mathcal{Q})$\\nhas a certain weak Néron model over Spec$(\\\\mathcal{O}_v)$ to be defined and\\nunder some conditions depending on the special fiber. With this we extend\\nNéron\\'s work strengthening Silverman\\'s results, which were for systems\\nhaving only one map.\\n',\n",
       " '  Large-scale extragalactic magnetic fields may induce conversions between\\nvery-high-energy photons and axionlike particles (ALPs), thereby shielding the\\nphotons from absorption on the extragalactic background light. However, in\\nsimplified \"cell\" models, used so far to represent extragalactic magnetic\\nfields, this mechanism would be strongly suppressed by current astrophysical\\nbounds. Here we consider a recent model of extragalactic magnetic fields\\nobtained from large-scale cosmological simulations. Such simulated magnetic\\nfields would have large enhancement in the filaments of matter. As a result,\\nphoton-ALP conversions would produce a significant spectral hardening for\\ncosmic TeV photons. This effect would be probed with the upcoming Cherenkov\\nTelescope Array detector. This possible detection would give a unique chance to\\nperform a tomography of the magnetized cosmic web with ALPs.\\n',\n",
       " '  Predicting the future state of a system has always been a natural motivation\\nfor science and practical applications. Such a topic, beyond its obvious\\ntechnical and societal relevance, is also interesting from a conceptual point\\nof view. This owes to the fact that forecasting lends itself to two equally\\nradical, yet opposite methodologies. A reductionist one, based on the first\\nprinciples, and the naive inductivist one, based only on data. This latter view\\nhas recently gained some attention in response to the availability of\\nunprecedented amounts of data and increasingly sophisticated algorithmic\\nanalytic techniques. The purpose of this note is to assess critically the role\\nof big data in reshaping the key aspects of forecasting and in particular the\\nclaim that bigger data leads to better predictions. Drawing on the\\nrepresentative example of weather forecasts we argue that this is not generally\\nthe case. We conclude by suggesting that a clever and context-dependent\\ncompromise between modelling and quantitative analysis stands out as the best\\nforecasting strategy, as anticipated nearly a century ago by Richardson and von\\nNeumann.\\n',\n",
       " '  We show that for an elliptic curve E defined over a number field K, the group\\nE(A) of points of E over the adele ring A of K is a topological group that can\\nbe analyzed in terms of the Galois representation associated to the torsion\\npoints of E. An explicit description of E(A) is given, and we prove that for K\\nof degree n, almost all elliptic curves over K have an adelic point group\\ntopologically isomorphic to a universal group depending on n. We also show that\\nthere exist infinitely many elliptic curves over K having a different adelic\\npoint group.\\n',\n",
       " '  Wireless backhaul communication has been recently realized with large\\nantennas operating in the millimeter wave (mmWave) frequency band and\\nimplementing highly directional beamforming. In this paper, we focus on the\\nalignment problem of narrow beams between fixed position network nodes in\\nmmWave backhaul systems that are subject to small displacements due to wind\\nflow or ground vibration. We consider nodes equipped with antenna arrays that\\nare capable of performing only analog processing and communicate through\\nwireless channels including a line-of-sight component. Aiming at minimizing the\\ntime needed to achieve beam alignment, we present an efficient method that\\ncapitalizes on the exchange of position information between the nodes to design\\ntheir beamforming and combining vectors. Some numerical results on the outage\\nprobability with the proposed beam alignment method offer useful preliminary\\ninsights on the impact of some system and operation parameters.\\n',\n",
       " '  Feature engineering has been the key to the success of many prediction\\nmodels. However, the process is non-trivial and often requires manual feature\\nengineering or exhaustive searching. DNNs are able to automatically learn\\nfeature interactions; however, they generate all the interactions implicitly,\\nand are not necessarily efficient in learning all types of cross features. In\\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\\nof a DNN model, and beyond that, it introduces a novel cross network that is\\nmore efficient in learning certain bounded-degree feature interactions. In\\nparticular, DCN explicitly applies feature crossing at each layer, requires no\\nmanual feature engineering, and adds negligible extra complexity to the DNN\\nmodel. Our experimental results have demonstrated its superiority over the\\nstate-of-art algorithms on the CTR prediction dataset and dense classification\\ndataset, in terms of both model accuracy and memory usage.\\n',\n",
       " '  We investigate the spin structure of a uni-axial chiral magnet near the\\ntransition temperatures in low fields perpendicular to the helical axis. We\\nfind a fan-type modulation structure where the clockwise and counterclockwise\\nwindings appear alternatively along the propagation direction of the modulation\\nstructure. This structure is often realized in a Yoshimori-type (non-chiral)\\nhelimagnet but it is rarely realized in a chiral helimagnet. To discuss\\nunderlying physics of this structure, we reconsider the phase diagram (phase\\nboundary and crossover lines) through the free energy and asymptotic behaviors\\nof isolated solitons. The fan structure appears slightly below the phase\\nboundary of the continuous transition of instability-type. In this region,\\nthere are no solutions containing any types of isolated solitons to the mean\\nfield equations.\\n',\n",
       " '  Several natural satellites of the giant planets have shown evidence of a\\nglobal internal ocean, coated by a thin, icy crust. This crust is probably\\nviscoelastic, which would alter its rotational response. This response would\\ntranslate into several rotational quantities, i.e. the obliquity, and the\\nlibrations at different frequencies, for which the crustal elasticity reacts\\ndifferently. This study aims at modelling the global response of the\\nviscoelastic crust. For that, I derive the time-dependency of the tensor of\\ninertia, which I combine with the time evolution of the rotational quantities,\\nthanks to an iterative algorithm. This algorithm combines numerical simulations\\nof the rotation with a digital filtering of the resulting tensor of inertia.\\nThe algorithm works very well in the elastic case, provided the problem is not\\nresonant. However, considering tidal dissipation adds different phase lags to\\nthe oscillating contributions, which challenge the convergence of the\\nalgorithm.\\n',\n",
       " '  A number of fundamental quantities in statistical signal processing and\\ninformation theory can be expressed as integral functions of two probability\\ndensity functions. Such quantities are called density functionals as they map\\ndensity functions onto the real line. For example, information divergence\\nfunctions measure the dissimilarity between two probability density functions\\nand are useful in a number of applications. Typically, estimating these\\nquantities requires complete knowledge of the underlying distribution followed\\nby multi-dimensional integration. Existing methods make parametric assumptions\\nabout the data distribution or use non-parametric density estimation followed\\nby high-dimensional integration. In this paper, we propose a new alternative.\\nWe introduce the concept of \"data-driven basis functions\" - functions of\\ndistributions whose value we can estimate given only samples from the\\nunderlying distributions without requiring distribution fitting or direct\\nintegration. We derive a new data-driven complete basis that is similar to the\\ndeterministic Bernstein polynomial basis and develop two methods for performing\\nbasis expansions of functionals of two distributions. We also show that the new\\nbasis set allows us to approximate functions of distributions as closely as\\ndesired. Finally, we evaluate the methodology by developing data driven\\nestimators for the Kullback-Leibler divergences and the Hellinger distance and\\nby constructing empirical estimates of tight bounds on the Bayes error rate.\\n',\n",
       " '  Let $E/\\\\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.\\nLet $p$ be a prime of ordinary reduction. We experimentally study conjecture\\n$4$ of B. Mazur and J. Tate in his article \"Refined Conjectures of the Birch\\nand Swinnerton-Dyer Type\". We report the computational evidence.\\n',\n",
       " '  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in\\nbase~$2$ (the Hamming weight of the binary expansion of $n$) and states the\\nfollowing: assume that $k$ is a positive integer and $1\\\\leq t<2^k-1$. Then\\n\\\\[\\\\Bigl \\\\lvert\\\\Bigl\\\\{(a,b)\\\\in\\\\bigl\\\\{0,\\\\ldots,2^k-2\\\\bigr\\\\}^2:a+b\\\\equiv t\\\\bmod\\n2^k-1, w(a)+w(b)<k\\\\Bigr\\\\}\\\\Bigr \\\\rvert\\\\leq 2^{k-1}.\\\\]\\nWe prove that the Tu--Deng Conjecture holds almost surely in the following\\nsense: the proportion of $t\\\\in[1,2^k-2]$ such that the above inequality holds\\napproaches $1$ as $k\\\\rightarrow\\\\infty$.\\nMoreover, we prove that the Tu--Deng Conjecture implies a conjecture due to\\nT.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.\\n',\n",
       " '  In this paper, we made an extension to the convergence analysis of the\\ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into\\nconsideration two popular regularization terms: the $\\\\ell_1$ and $\\\\ell_2$ norm\\nof the parameter vector $w$, and added it to the square loss function with\\ncoefficient $\\\\lambda/2$. We proved that when $\\\\lambda$ is small, the weight\\nvector $w$ converges to the optimal solution $\\\\hat{w}$ (with respect to the new\\nloss function) with probability $\\\\geq (1-\\\\varepsilon)(1-A_d)/2$ under random\\ninitiations in a sphere centered at the origin, where $\\\\varepsilon$ is a small\\nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams\\nand repeated simulations verified our theory.\\n',\n",
       " '  We further progress along the line of Ref. [Phys. Rev. {\\\\bf A 94}, 043614\\n(2016)] where a functional for Fermi systems with anomalously large $s$-wave\\nscattering length $a_s$ was proposed that has no free parameters. The\\nfunctional is designed to correctly reproduce the unitary limit in Fermi gases\\ntogether with the leading-order contributions in the s- and p-wave channels at\\nlow density. The functional is shown to be predictive up to densities\\n$\\\\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang\\nfunctional, valid for $\\\\rho < 10^{-6}$ fm$^{-3}$. The form of the functional\\nretained in this work is further motivated. It is shown that the new functional\\ncorresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all\\norders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One\\nconclusion from the present work is that, except in the extremely low--density\\nregime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with\\nrespect to the unitary limit. Starting from the functional, we introduce\\ndensity--dependent scales and show that scales associated to the bare\\ninteraction are strongly renormalized by medium effects. As a consequence, some\\nof the scales at play around saturation are dominated by the unitary gas\\nproperties and not directly to low-energy constants. For instance, we show that\\nthe scale in the s-wave channel around saturation is proportional to the\\nso-called Bertsch parameter $\\\\xi_0$ and becomes independent of $a_s$. We also\\npoint out that these scales are of the same order of magnitude than those\\nempirically obtained in the Skyrme energy density functional. We finally\\npropose a slight modification of the functional such that it becomes accurate\\nup to the saturation density $\\\\rho\\\\simeq 0.16$ fm$^{-3}$.\\n',\n",
       " '  Marker-based and marker-less optical skeletal motion-capture methods use an\\noutside-in arrangement of cameras placed around a scene, with viewpoints\\nconverging on the center. They often create discomfort by possibly needed\\nmarker suits, and their recording volume is severely restricted and often\\nconstrained to indoor scenes with controlled backgrounds. We therefore propose\\na new method for real-time, marker-less and egocentric motion capture which\\nestimates the full-body skeleton pose from a lightweight stereo pair of fisheye\\ncameras that are attached to a helmet or virtual-reality headset. It combines\\nthe strength of a new generative pose estimation framework for fisheye views\\nwith a ConvNet-based body-part detector trained on a new automatically\\nannotated and augmented dataset. Our inside-in method captures full-body motion\\nin general indoor and outdoor scenes, and also crowded scenes.\\n',\n",
       " '  Diffusion maps are an emerging data-driven technique for non-linear\\ndimensionality reduction, which are especially useful for the analysis of\\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\\ncomputational complexity of the diffusion maps algorithm scales with the number\\nof observations. Thus, long time-series data presents a significant challenge\\nfor fast and efficient embedding. We propose integrating the Nyström method\\nwith diffusion maps in order to ease the computational demand. We achieve a\\nspeedup of roughly two to four times when approximating the dominant diffusion\\nmap components.\\n',\n",
       " \"  We present a set of effective outflow/open boundary conditions and an\\nassociated algorithm for simulating the dynamics of multiphase flows consisting\\nof $N$ ($N\\\\geqslant 2$) immiscible incompressible fluids in domains involving\\noutflows or open boundaries. These boundary conditions are devised based on the\\nproperties of energy stability and reduction consistency. The energy stability\\nproperty ensures that the contributions of these boundary conditions to the\\nenergy balance will not cause the total energy of the N-phase system to\\nincrease over time. Therefore, these open/outflow boundary conditions are very\\neffective in overcoming the backflow instability in multiphase systems. The\\nreduction consistency property ensures that if some fluid components are absent\\nfrom the N-phase system then these N-phase boundary conditions will reduce to\\nthose corresponding boundary conditions for the equivalent smaller system. Our\\nnumerical algorithm for the proposed boundary conditions together with the\\nN-phase governing equations involves only the solution of a set of de-coupled\\nindividual Helmholtz-type equations within each time step, and the resultant\\nlinear algebraic systems after discretization involve only constant and\\ntime-independent coefficient matrices which can be pre-computed. Therefore, the\\nalgorithm is computationally very efficient and attractive. We present\\nextensive numerical experiments for flow problems involving multiple fluid\\ncomponents and inflow/outflow boundaries to test the proposed method. In\\nparticular, we compare in detail the simulation results of a three-phase\\ncapillary wave problem with Prosperetti's exact physical solution and\\ndemonstrate that the method developed herein produces physically accurate\\nresults.\\n\",\n",
       " '  The recent detection of two faint and extended star clusters in the central\\nregions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,\\nraises the question of whether clusters with such low densities can survive the\\ntidal field of cold dark matter haloes with central density cusps. Using both\\nanalytic arguments and a suite of collisionless N-body simulations, I show that\\nthese clusters are extremely fragile and quickly disrupted in the presence of\\ncentral cusps $\\\\rho\\\\sim r^{-\\\\alpha}$ with $\\\\alpha\\\\gtrsim 0.2$. Furthermore, the\\nscenario in which the clusters where originally more massive and sank to the\\ncenter of the halo requires extreme fine tuning and does not naturally\\nreproduce the observed systems. In turn, these clusters are long lived in cored\\nhaloes, whose central regions are safe shelters for $\\\\alpha\\\\lesssim 0.2$. The\\nonly viable scenario for hosts that have preserved their primoridal cusp to the\\npresent time is that the clusters formed at rest at the bottom of the\\npotential, which is easily tested by measurement of the clusters proper\\nvelocity within the host. This offers means to readily probe the central\\ndensity profile of two dwarf galaxies as faint as $L_V\\\\sim5\\\\times 10^5 L_\\\\odot$\\nand $L_V\\\\sim6\\\\times10^4 L_\\\\odot$, in which stellar feedback is unlikely to be\\neffective.\\n',\n",
       " '  Fundamental relations between information and estimation have been\\nestablished in the literature for the continuous-time Gaussian and Poisson\\nchannels, in a long line of work starting from the classical representation\\ntheorems by Duncan and Kabanov respectively. In this work, we demonstrate that\\nsuch relations hold for a much larger family of continuous-time channels. We\\nintroduce the family of semi-martingale channels where the channel output is a\\nsemi-martingale stochastic process, and the channel input modulates the\\ncharacteristics of the semi-martingale. For these channels, which includes as a\\nspecial case the continuous time Gaussian and Poisson models, we establish new\\nrepresentations relating the mutual information between the channel input and\\noutput to an optimal causal filtering loss, thereby unifying and considerably\\nextending results from the Gaussian and Poisson settings. Extensions to the\\nsetting of mismatched estimation are also presented where the relative entropy\\nbetween the laws governing the output of the channel under two different input\\ndistributions is equal to the cumulative difference between the estimation loss\\nincurred by using the mismatched and optimal causal filters respectively. The\\nmain tool underlying these results is the Doob--Meyer decomposition of a class\\nof likelihood ratio sub-martingales. The results in this work can be viewed as\\nthe continuous-time analogues of recent generalizations for relations between\\ninformation and estimation for discrete-time Lévy channels.\\n',\n",
       " '  Galaxy cluster centring is a key issue for precision cosmology studies using\\ngalaxy surveys. Mis-identification of central galaxies causes systematics in\\nvarious studies such as cluster lensing, satellite kinematics, and galaxy\\nclustering. The red-sequence Matched-filter Probabilistic Percolation\\n(redMaPPer) estimates the probability that each member galaxy is central from\\nphotometric information rather than specifying one central galaxy. The\\nredMaPPer estimates can be used for calibrating the off-centring effect,\\nhowever, the centring algorithm has not previously been well-tested. We test\\nthe centring probabilities of redMaPPer cluster catalog using the projected\\ncross correlation between redMaPPer clusters with photometric red galaxies and\\ngalaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which\\nthe redMaPPer central galaxies (RMCGs) are not the brightest member galaxies\\n(BMEM) and both of them have spectroscopic redshift. This subsample represents\\nnearly 10% of the whole cluster sample. We find a clear difference in the\\ncross-correlation measurements between RMCGs and BMEMs, and the estimated\\ncentring probability is 74$\\\\pm$10% for RMCGs and 13$\\\\pm$4% for BMEMs in the\\nGaussian offset model and 78$\\\\pm$9% for RMCGs and 5$\\\\pm$5% for BMEMs in the NFW\\noffset model. These values are in agreement with the centring probability\\nvalues reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\\\\sigma$.\\nOur analysis provides a strong consistency test of the redMaPPer centring\\nprobabilities. Our results suggest that redMaPPer centring probabilities are\\nreliably estimated. We confirm that the brightest galaxy in the cluster is not\\nalways the central galaxy as has been shown in previous works.\\n',\n",
       " '  The goal of this article is to provide an useful criterion of positivity and\\nwell-posedness for a wide range of infinite dimensional semilinear abstract\\nCauchy problems. This criterion is based on some weak assumptions on the\\nnon-linear part of the semilinear problem and on the existence of a strongly\\ncontinuous semigroup generated by the differential operator. To illustrate a\\nlarge variety of applications, we exhibit the feasibility of this criterion\\nthrough three examples in mathematical biology: epidemiology, predator-prey\\ninteractions and oncology.\\n',\n",
       " '  The ongoing progress in quantum theory emphasizes the crucial role of the\\nvery basic principles of quantum theory. However, this is not properly followed\\nin teaching quantum mechanics on the graduate and undergraduate levels of\\nphysics studies. The existing textbooks typically avoid the axiomatic\\npresentation of the theory. We emphasize usefulness of the systematic,\\naxiomatic approach to the basics of quantum theory as well as its importance in\\nthe light of the modern scientific-research context.\\n',\n",
       " \"  Non-conding RNAs play a key role in the post-transcriptional regulation of\\nmRNA translation and turnover in eukaryotes. miRNAs, in particular, interact\\nwith their target RNAs through protein-mediated, sequence-specific binding,\\ngiving rise to extended and highly heterogeneous miRNA-RNA interaction\\nnetworks. Within such networks, competition to bind miRNAs can generate an\\neffective positive coupling between their targets. Competing endogenous RNAs\\n(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.\\nAlbeit potentially weak, ceRNA interactions can occur both dynamically,\\naffecting e.g. the regulatory clock, and at stationarity, in which case ceRNA\\nnetworks as a whole can be implicated in the composition of the cell's\\nproteome. Many features of ceRNA interactions, including the conditions under\\nwhich they become significant, can be unraveled by mathematical and in silico\\nmodels. We review the understanding of the ceRNA effect obtained within such\\nframeworks, focusing on the methods employed to quantify it, its role in the\\nprocessing of gene expression noise, and how network topology can determine its\\nreach.\\n\",\n",
       " \"  We show that in Grayson's model of higher algebraic $K$-theory using binary\\nacyclic complexes, the complexes of length two suffice to generate the whole\\ngroup. Moreover, we prove that the comparison map from Nenashev's model for\\n$K_1$ to Grayson's model for $K_1$ is an isomorphism. It follows that algebraic\\n$K$-theory of exact categories commutes with infinite products.\\n\",\n",
       " '  We study the min-cost seed selection problem in online social networks, where\\nthe goal is to select a set of seed nodes with the minimum total cost such that\\nthe expected number of influenced nodes in the network exceeds a predefined\\nthreshold. We propose several algorithms that outperform the previous studies\\nboth on the theoretical approximation ratios and on the experimental\\nperformance. Under the case where the nodes have heterogeneous costs, our\\nalgorithms are the first bi- criteria approximation algorithms with polynomial\\nrunning time and provable logarithmic performance bounds using a general\\ncontagion model. Under the case where the users have uniform costs, our\\nalgorithms achieve logarithmic approximation ratio and provable time complexity\\nwhich is smaller than that of existing algorithms in orders of magnitude. We\\nconduct extensive experiments using real social networks. The experimental\\nresults show that, our algorithms significantly outperform the existing\\nalgorithms both on the total cost and on the running time, and also scale well\\nto billion-scale networks.\\n',\n",
       " '  We propose a new splitting criterion for a meta-learning approach to\\nmulticlass classifier design that adaptively merges the classes into a\\ntree-structured hierarchy of increasingly difficult binary classification\\nproblems. The classification tree is constructed from empirical estimates of\\nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that\\nrank the binary subproblems in terms of difficulty of classification. The\\nproposed empirical estimates of the Bayes error rate are computed from the\\nminimal spanning tree (MST) of the samples from each pair of classes. Moreover,\\na meta-learning technique is presented for quantifying the one-vs-rest Bayes\\nerror rate for each individual class from a single MST on the entire dataset.\\nExtensive simulations on benchmark datasets show that the proposed hierarchical\\nmethod can often be learned much faster than competing methods, while achieving\\ncompetitive accuracy.\\n',\n",
       " '  Variational approaches for the calculation of vibrational wave functions and\\nenergies are a natural route to obtain highly accurate results with\\ncontrollable errors. However, the unfavorable scaling and the resulting high\\ncomputational cost of standard variational approaches limit their application\\nto small molecules with only few vibrational modes. Here, we demonstrate how\\nthe density matrix renormalization group (DMRG) can be exploited to optimize\\nvibrational wave functions (vDMRG) expressed as matrix product states. We study\\nthe convergence of these calculations with respect to the size of the local\\nbasis of each mode, the number of renormalized block states, and the number of\\nDMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for\\nsmall molecules that were intensively studied in the literature. We then\\nproceed to show that the complete fingerprint region of the sarcosyn-glycin\\ndipeptide can be calculated with vDMRG.\\n',\n",
       " '  In this work, we present a methodology that enables an agent to make\\nefficient use of its exploratory actions by autonomously identifying possible\\nobjectives in its environment and learning them in parallel. The identification\\nof objectives is achieved using an online and unsupervised adaptive clustering\\nalgorithm. The identified objectives are learned (at least partially) in\\nparallel using Q-learning. Using a simulated agent and environment, it is shown\\nthat the converged or partially converged value function weights resulting from\\noff-policy learning can be used to accumulate knowledge about multiple\\nobjectives without any additional exploration. We claim that the proposed\\napproach could be useful in scenarios where the objectives are initially\\nunknown or in real world scenarios where exploration is typically a time and\\nenergy intensive process. The implications and possible extensions of this work\\nare also briefly discussed.\\n',\n",
       " '  Recent studies on diffusion-based sampling methods have shown that Langevin\\nMonte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and\\nrigorous theoretical guarantees have been proven for both asymptotic and\\nfinite-time regimes. Algorithmically, LMC-based algorithms resemble the\\nwell-known gradient descent (GD) algorithm, where the GD recursion is perturbed\\nby an additive Gaussian noise whose variance has a particular form. Fractional\\nLangevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the\\nGaussian noise is replaced by a heavy-tailed {\\\\alpha}-stable noise. As opposed\\nto its Gaussian counterpart, these heavy-tailed perturbations can incur large\\njumps and it has been empirically demonstrated that the choice of\\n{\\\\alpha}-stable noise can provide several advantages in modern machine learning\\nproblems, both in optimization and sampling contexts. However, as opposed to\\nLMC, only asymptotic convergence properties of FLMC have been yet established.\\nIn this study, we analyze the non-asymptotic behavior of FLMC for non-convex\\noptimization and prove finite-time bounds for its expected suboptimality. Our\\nresults show that the weak-error of FLMC increases faster than LMC, which\\nsuggests using smaller step-sizes in FLMC. We finally extend our results to the\\ncase where the exact gradients are replaced by stochastic gradients and show\\nthat similar results hold in this setting as well.\\n',\n",
       " \"  We use automatic speech recognition to assess spoken English learner\\npronunciation based on the authentic intelligibility of the learners' spoken\\nresponses determined from support vector machine (SVM) classifier or deep\\nlearning neural network model predictions of transcription correctness. Using\\nnumeric features produced by PocketSphinx alignment mode and many recognition\\npasses searching for the substitution and deletion of each expected phoneme and\\ninsertion of unexpected phonemes in sequence, the SVM models achieve 82 percent\\nagreement with the accuracy of Amazon Mechanical Turk crowdworker\\ntranscriptions, up from 75 percent reported by multiple independent\\nresearchers. Using such features with SVM classifier probability prediction\\nmodels can help computer-aided pronunciation teaching (CAPT) systems provide\\nintelligibility remediation.\\n\",\n",
       " '  We consider bilinear optimal control problems, whose objective functionals do\\nnot depend on the controls. Hence, bang-bang solutions will appear. We\\ninvestigate sufficient second-order conditions for bang-bang controls, which\\nguarantee local quadratic growth of the objective functional in $L^1$. In\\naddition, we prove that for controls that are not bang-bang, no such growth can\\nbe expected. Finally, we study the finite-element discretization, and prove\\nerror estimates of bang-bang controls in $L^1$-norms.\\n',\n",
       " '  We carry out a comprehensive analysis of letter frequencies in contemporary\\nwritten Marathi. We determine sets of letters which statistically predominate\\nany large generic Marathi text, and use these sets to estimate the entropy of\\nMarathi.\\n',\n",
       " \"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,\\ncommunication, and storage capabilities of mobile and fixed devices in the\\nfield as well as those of computing and storage servers in remote datacenters\\nis envisioned. Mobile device clouds can be harnessed to enable innovative\\npervasive applications that rely on real-time, in-situ processing of sensor\\ndata collected in the field. To support concurrent mobile applications on the\\ndevice cloud, a robust and secure distributed computing framework, called\\nMaestro, is proposed. The key components of Maestro are (i) a task scheduling\\nmechanism that employs controlled task replication in addition to task\\nreallocation for robustness and (ii) Dedup for task deduplication among\\nconcurrent pervasive workflows. An architecture-based solution that relies on\\ntask categorization and authorized access to the categories of tasks is\\nproposed for different levels of protection. Experimental evaluation through\\nprototype testbed of Android- and Linux-based mobile devices as well as\\nsimulations is performed to demonstrate Maestro's capabilities.\\n\",\n",
       " '  Despite numerous studies the exact nature of the order parameter in\\nsuperconducting Sr2RuO4 remains unresolved. We have extended previous\\nsmall-angle neutron scattering studies of the vortex lattice in this material\\nto a wider field range, higher temperatures, and with the field applied close\\nto both the <100> and <110> basal plane directions. Measurements at high field\\nwere made possible by the use of both spin polarization and analysis to improve\\nthe signal-to-noise ratio. Rotating the field towards the basal plane causes a\\ndistortion of the square vortex lattice observed for H // <001>, and also a\\nsymmetry change to a distorted triangular symmetry for fields close to <100>.\\nThe vortex lattice distortion allows us to determine the intrinsic\\nsuperconducting anisotropy between the c-axis and the Ru-O basal plane,\\nyielding a value of ~60 at low temperature and low to intermediate fields. This\\ngreatly exceeds the upper critical field anisotropy of ~20 at low temperature,\\nreminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects\\non the unpaired quasiparticles in the vortex cores are observed, but a direct\\ndetection lies below the measurement sensitivity. The superconducting\\nanisotropy is found to be independent of temperature but increases for fields >\\n1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature\\ndependence of the scattered intensity provides further support for gap nodes or\\ndeep minima in the superconducting gap.\\n',\n",
       " \"  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$\\ndegenerate fundamental quarks with mass $m$ and a $\\\\theta$-parameter. For\\ngeneric $m$ and $\\\\theta$ the theory has a single gapped vacuum. However, as\\n$\\\\theta$ is varied through $\\\\theta=\\\\pi$ for large $m$ there is a first order\\ntransition. For $N_f=1$ the first order transition line ends at a point with a\\nmassless $\\\\eta'$ particle (for all $N$) and for $N_f>1$ the first order\\ntransition ends at $m=0$, where, depending on the value of $N_f$, the IR theory\\nhas free Nambu-Goldstone bosons, an interacting conformal field theory, or a\\nfree gauge theory. Even when the $4d$ bulk is smooth, domain walls and\\ninterfaces can have interesting phase transitions separating different $3d$\\nphases. These turn out to be the phases of the recently studied $3d$\\nChern-Simons matter theories, thus relating the dynamics of QCD$_4$ and\\nQCD$_3$, and, in particular, making contact with the recently discussed\\ndualities in 2+1 dimensions. For example, when the massless $4d$ theory has an\\n$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a\\n$3d$ massless $CP^{N_f-1}$ nonlinear $\\\\sigma$-model with a Wess-Zumino term, in\\nagreement with the conjectured dynamics in 2+1 dimensions.\\n\",\n",
       " '  Investigation of the autoignition delay of the butanol isomers has been\\nperformed at elevated pressures of 15 bar and 30 bar and low to intermediate\\ntemperatures of 680-860 K. The reactivity of the stoichiometric isomers of\\nbutanol, in terms of inverse ignition delay, was ranked as n-butanol >\\nsec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but\\nchanged to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For\\nthe temperature and pressure conditions in this study, no NTC or two-stage\\nignition behavior were observed. However, for both of the compressed pressures\\nstudied in this work, tert-butanol exhibited unique pre-ignition heat release\\ncharacteristics. As such, tert-butanol was further studied at two additional\\nequivalence ratios ($\\\\phi$ = 0.5 and 2.0) to help determine the cause of the\\nheat release.\\n',\n",
       " '  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method\\ncalled CLGrouping, for constructing tree-structured probabilistic graphical\\nmodels, a statistical framework that is commonly used for inferring\\nphylogenetic trees. While CLGrouping works correctly if there is a unique MST,\\nwe observe an indeterminacy in the method in the case that there are multiple\\nMSTs. In this work we remove this indeterminacy by introducing so-called\\nvertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely\\nrelated to the number of leaves in the MST. This motivates the problem of\\nfinding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We\\nprovide a polynomial time algorithm for the MLVRMST problem, and prove its\\ncorrectness for graphs whose edges are weighted with tree-additive distances.\\n',\n",
       " \"  Variational Bayesian neural nets combine the flexibility of deep learning\\nwith Bayesian uncertainty estimation. Unfortunately, there is a tradeoff\\nbetween cheap but simple variational families (e.g.~fully factorized) or\\nexpensive and complicated inference procedures. We show that natural gradient\\nascent with adaptive weight noise implicitly fits a variational posterior to\\nmaximize the evidence lower bound (ELBO). This insight allows us to train\\nfull-covariance, fully factorized, or matrix-variate Gaussian variational\\nposteriors using noisy versions of natural gradient, Adam, and K-FAC,\\nrespectively, making it possible to scale up to modern-size ConvNets. On\\nstandard regression benchmarks, our noisy K-FAC algorithm makes better\\npredictions and matches Hamiltonian Monte Carlo's predictive variances better\\nthan existing methods. Its improved uncertainty estimates lead to more\\nefficient exploration in active learning, and intrinsic motivation for\\nreinforcement learning.\\n\",\n",
       " '  We define rules for cellular automata played on quasiperiodic tilings of the\\nplane arising from the multigrid method in such a way that these cellular\\nautomata are isomorphic to Conway\\'s Game of Life. Although these tilings are\\nnonperiodic, determining the next state of each tile is a local computation,\\nrequiring only knowledge of the local structure of the tiling and the states of\\nfinitely many nearby tiles. As an example, we show a version of a \"glider\"\\nmoving through a region of a Penrose tiling. This constitutes a potential\\ntheoretical framework for a method of executing computations in\\nnon-periodically structured substrates such as quasicrystals.\\n',\n",
       " '  In the present work, we explore the existence, stability and dynamics of\\nsingle and multiple vortex ring states that can arise in Bose-Einstein\\ncondensates. Earlier works have illustrated the bifurcation of such states, in\\nthe vicinity of the linear limit, for isotropic or anisotropic\\nthree-dimensional harmonic traps. Here, we extend these states to the regime of\\nlarge chemical potentials, the so-called Thomas-Fermi limit, and explore their\\nproperties such as equilibrium radii and inter-ring distance, for multi-ring\\nstates, as well as their vibrational spectra and possible instabilities. In\\nthis limit, both the existence and stability characteristics can be partially\\ntraced to a particle picture that considers the rings as individual particles\\noscillating within the trap and interacting pairwise with one another. Finally,\\nwe examine some representative instability scenarios of the multi-ring dynamics\\nincluding breakup and reconnections, as well as the transient formation of\\nvortex lines.\\n',\n",
       " '  For a class of partially observed diffusions, sufficient conditions are given\\nfor the map from initial condition of the signal to filtering distribution to\\nbe contractive with respect to Wasserstein distances, with rate which has no\\ndependence on the dimension of the state-space and is stable under tensor\\nproducts of the model. The main assumptions are that the signal has affine\\ndrift and constant diffusion coefficient, and that the likelihood functions are\\nlog-concave. Contraction estimates are obtained from an $h$-process\\nrepresentation of the transition probabilities of the signal reweighted so as\\nto condition on the observations.\\n',\n",
       " '  We study the motion of an electron bubble in the zero temperature limit where\\nneither phonons nor rotons provide a significant contribution to the drag\\nexerted on an ion moving within the superfluid. By using the Gross-Clark model,\\nin which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled\\nto a Schrödinger equation for the electron wavefunction, we study how\\nvortex nucleation affects the measured drift velocity of the ion. We use\\nparameters that give realistic values of the ratio of the radius of the bubble\\nwith respect to the healing length in superfluid $^4$He at a pressure of one\\nbar. By performing fully 3D spatio-temporal simulations of the superfluid\\ncoupled to an electron, that is modelled within an adiabatic approximation and\\nmoving under the influence of an applied electric field, we are able to recover\\nthe key dynamics of the ion-vortex interactions that arise and the subsequent\\nion-vortex complexes that can form. Using the numerically computed drift\\nvelocity of the ion as a function of the applied electric field, we determine\\nthe vortex-nucleation limited mobility of the ion to recover values in\\nreasonable agreement with measured data.\\n',\n",
       " '  We present new JVLA multi-frequency measurements of a set of stars in\\ntransition from the post-AGB to the Planetary Nebula phase monitored in the\\nradio range over several years. Clear variability is found for five sources.\\nTheir light curves show increasing and decreasing patterns. New radio\\nobservations at high angular resolution are also presented for two sources.\\nAmong these is IRAS 18062+2410, whose radio structure is compared to\\nnear-infrared images available in the literature. With these new maps, we can\\nestimate inner and outer radii of 0.03$\"$ and 0.08$\"$ for the ionised shell, an\\nionised mass of $3.2\\\\times10^{-4}$ M$_\\\\odot$, and a density at the inner radius\\nof $7.7\\\\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with\\nthe new morphological constraints. The combination of multi-frequency data and,\\nwhere available, spectral-index maps leads to the detection of spectral indices\\nnot due to thermal emission, contrary to what one would expect in planetary\\nnebulae. Our results allow us to hypothesise the existence of a link between\\nradio variability and non-thermal emission mechanisms in the nebulae. This link\\nseems to hold for IRAS 22568+6141 and may generally hold for those nebulae\\nwhere the radio flux decreases over time.\\n',\n",
       " '  We develop an on-line monitoring procedure to detect a change in a large\\napproximate factor model. Our statistics are based on a well-known property of\\nthe $% \\\\left( r+1\\\\right) $-th eigenvalue of the sample covariance matrix of the\\ndata (having defined $r$ as the number of common factors): whilst under the\\nnull the $\\\\left( r+1\\\\right) $-th eigenvalue is bounded, under the alternative\\nof a change (either in the loadings, or in the number of factors itself) it\\nbecomes spiked. Given that the sample eigenvalue cannot be estimated\\nconsistently under the null, we regularise the problem by randomising the test\\nstatistic in conjunction with sample conditioning, obtaining a sequence of\\n\\\\textit{i.i.d.}, asymptotically chi-square statistics which are then employed\\nto build the monitoring scheme. Numerical evidence shows that our procedure\\nworks very well in finite samples, with a very small probability of false\\ndetections and tight detection times in presence of a genuine change-point.\\n',\n",
       " '  A susceptibility propagation that is constructed by combining a belief\\npropagation and a linear response method is used for approximate computation\\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\\npropagation by using the concept of a diagonal matching method that is based on\\nmean-field approaches to inverse Ising problems. The proposed susceptibility\\npropagation is robust for various network structures, and it is reduced to the\\nordinary susceptibility propagation and to the adaptive\\nThouless-Anderson-Palmer equation in special cases.\\n',\n",
       " '  This paper analyzes the downlink performance of ultra-dense networks with\\nelevated base stations (BSs). We consider a general dual-slope pathloss model\\nwith distance-dependent probability of line-of-sight (LOS) transmission between\\nBSs and receivers. Specifically, we consider the scenario where each link may\\nbe obstructed by randomly placed buildings. Using tools from stochastic\\ngeometry, we show that both coverage probability and area spectral efficiency\\ndecay to zero as the BS density grows large. Interestingly, we show that the BS\\nheight alone has a detrimental effect on the system performance even when the\\nstandard single-slope pathloss model is adopted.\\n',\n",
       " '  We demonstrate the first application of deep reinforcement learning to\\nautonomous driving. From randomly initialised parameters, our model is able to\\nlearn a policy for lane following in a handful of training episodes using a\\nsingle monocular image as input. We provide a general and easy to obtain\\nreward: the distance travelled by the vehicle without the safety driver taking\\ncontrol. We use a continuous, model-free deep reinforcement learning algorithm,\\nwith all exploration and optimisation performed on-vehicle. This demonstrates a\\nnew framework for autonomous driving which moves away from reliance on defined\\nlogical rules, mapping, and direct supervision. We discuss the challenges and\\nopportunities to scale this approach to a broader range of autonomous driving\\ntasks.\\n',\n",
       " '  Strong-coupling of monolayer metal dichalcogenide semiconductors with light\\noffers encouraging prospects for realistic exciton devices at room temperature.\\nHowever, the nature of this coupling depends extremely sensitively on the\\noptical confinement and the orientation of electronic dipoles and fields. Here,\\nwe show how plasmon strong coupling can be achieved in compact robust\\neasily-assembled gold nano-gap resonators at room temperature. We prove that\\nstrong coupling is impossible with monolayers due to the large exciton\\ncoherence size, but resolve clear anti-crossings for 8 layer devices with Rabi\\nsplittings exceeding 135 meV. We show that such structures improve on prospects\\nfor nonlinear exciton functionalities by at least 10^4, while retaining quantum\\nefficiencies above 50%.\\n',\n",
       " '  Positioning data offer a remarkable source of information to analyze crowds\\nurban dynamics. However, discovering urban activity patterns from the emergent\\nbehavior of crowds involves complex system modeling. An alternative approach is\\nto adopt computational techniques belonging to the emergent paradigm, which\\nenables self-organization of data and allows adaptive analysis. Specifically,\\nour approach is based on stigmergy. By using stigmergy each sample position is\\nassociated with a digital pheromone deposit, which progressively evaporates and\\naggregates with other deposits according to their spatiotemporal proximity.\\nBased on this principle, we exploit positioning data to identify high density\\nareas (hotspots) and characterize their activity over time. This\\ncharacterization allows the comparison of dynamics occurring in different days,\\nproviding a similarity measure exploitable by clustering techniques. Thus, we\\ncluster days according to their activity behavior, discovering unexpected urban\\nactivity patterns. As a case study, we analyze taxi traces in New York City\\nduring 2015.\\n',\n",
       " '  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with\\ntwo commuting multiplicative linear maps. The main purpose of this paper is to\\ndefine representations and a cohomology of BiHom-Lie colour algebras and to\\nstudy some key constructions and properties.\\nMoreover, we discuss $\\\\alpha^{k}\\\\beta^l$-generalized derivations,\\n$\\\\alpha^{k}\\\\beta^l$-quasi-derivations and $\\\\alpha^{k}\\\\beta^l$-quasi-centroid.\\nWe provide some properties and their relationships with BiHom-Jordan colour\\nalgebra.\\n',\n",
       " '  We consider the problem related to clustering of gamma-ray bursts (from\\n\"BATSE\" catalogue) through kernel principal component analysis in which our\\nproposed kernel outperforms results of other competent kernels in terms of\\nclustering accuracy and we obtain three physically interpretable groups of\\ngamma-ray bursts. The effectivity of the suggested kernel in combination with\\nkernel principal component analysis in revealing natural clusters in noisy and\\nnonlinear data while reducing the dimension of the data is also explored in two\\nsimulated data sets.\\n',\n",
       " '  Baker, Harman, and Pintz showed that a weak form of the Prime Number Theorem\\nholds in intervals of the form $[x-x^{0.525},x]$ for large $x$. In this paper,\\nwe extend a result of Maynard and Tao concerning small gaps between primes to\\nintervals of this length. More precisely, we prove that for any $\\\\delta\\\\in\\n[0.525,1]$ there exist positive integers $k,d$ such that for sufficiently large\\n$x$, the interval $[x-x^\\\\delta,x]$ contains $\\\\gg_{k} \\\\frac{x^\\\\delta}{(\\\\log\\nx)^k}$ pairs of consecutive primes differing by at most $d$. This confirms a\\nspeculation of Maynard that results on small gaps between primes can be refined\\nto the setting of short intervals of this length.\\n',\n",
       " '  Improved Phantom cell is a new scenario which has been introduced recently to\\nenhance the capacity of Heterogeneous Networks (HetNets). The main trait of\\nthis scenario is that, besides maximizing the total network capacity in both\\nindoor and outdoor environments, it claims to reduce the handover number\\ncompared to the conventional scenarios. In this paper, by a comprehensive\\nreview of the Improved Phantom cells structure, an appropriate algorithm will\\nbe introduced for the handover procedure of this scenario. To reduce the number\\nof handover in the proposed algorithm, various parameters such as the received\\nSignal to Interference plus Noise Ratio (SINR) at the user equipment (UE),\\nusers access conditions to the phantom cells, and users staying time in the\\ntarget cell based on its velocity, has been considered. Theoretical analyses\\nand simulation results show that applying the suggested algorithm the improved\\nphantom cell structure has a much better performance than conventional HetNets\\nin terms of the number of handover.\\n',\n",
       " '  We address the problem of localisation of objects as bounding boxes in images\\nwith weak labels. This weakly supervised object localisation problem has been\\ntackled in the past using discriminative models where each object class is\\nlocalised independently from other classes. We propose a novel framework based\\non Bayesian joint topic modelling. Our framework has three distinctive\\nadvantages over previous works: (1) All object classes and image backgrounds\\nare modelled jointly together in a single generative model so that \"explaining\\naway\" inference can resolve ambiguity and lead to better learning and\\nlocalisation. (2) The Bayesian formulation of the model enables easy\\nintegration of prior knowledge about object appearance to compensate for\\nlimited supervision. (3) Our model can be learned with a mixture of weakly\\nlabelled and unlabelled data, allowing the large volume of unlabelled images on\\nthe Internet to be exploited for learning. Extensive experiments on the\\nchallenging VOC dataset demonstrate that our approach outperforms the\\nstate-of-the-art competitors.\\n',\n",
       " '  All people have to make risky decisions in everyday life. And we do not know\\nhow true they are. But is it possible to mathematically assess the correctness\\nof our choice? This article discusses the model of decision making under risk\\non the example of project management. This is a game with two players, one of\\nwhich is Investor, and the other is the Project Manager. Each player makes a\\nrisky decision for himself, based on his past experience. With the help of a\\nmathematical model, the players form a level of confidence, depending on who\\nthe player accepts the strategy or does not accept. The project manager\\nassesses the costs and compares them with the level of confidence. An investor\\nevaluates past results. Also visit the case where the strategy of the player\\naccepts the part.\\n',\n",
       " \"  Modeling the interior of exoplanets is essential to go further than the\\nconclusions provided by mean density measurements. In addition to the still\\nlimited precision on the planets' fundamental parameters, models are limited by\\nthe existence of degeneracies on their compositions. Here we present a model of\\ninternal structure dedicated to the study of solid planets up to ~10 Earth\\nmasses, i.e. Super-Earths. When the measurement is available, the assumption\\nthat the bulk Fe/Si ratio of a planet is similar to that of its host star\\nallows us to significantly reduce the existing degeneracy and more precisely\\nconstrain the planet's composition. Based on our model, we provide an update of\\nthe mass-radius relationships used to provide a first estimate of a planet's\\ncomposition from density measurements. Our model is also applied to the cases\\nof two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently\\nupdated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are\\nfound to lie within the 10-37% and 10-33% ranges, respectively, allowing both\\nplanets to be compatible with an Earth-like composition. We also extend the\\nrecent study of Proxima Centauri b, and show that its radius may reach 1.94\\nEarth radii in the case of a 5 Earth masses planet, as there is a 96.7%\\nprobability that the real mass of Proxima Centauri b is below this value.\\n\",\n",
       " '  In this paper we discuss the characteristics and operation of Astro Space\\nCenter (ASC) software FX correlator that is an important component of\\nspace-ground interferometer for Radioastron project. This project performs\\njoint observations of compact radio sources using 10 meter space radio\\ntelescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm\\nwavelengths. In this paper we describe the main features of space-ground VLBI\\ndata processing of Radioastron project using ASC correlator. Quality of\\nimplemented fringe search procedure provides positive results without\\nsignificant losses in correlated amplitude. ASC Correlator has a computational\\npower close to real time operation. The correlator has a number of processing\\nmodes: \"Continuum\", \"Spectral Line\", \"Pulsars\", \"Giant Pulses\",\"Coherent\".\\nSpecial attention is paid to peculiarities of Radioastron space-ground VLBI\\ndata processing. The algorithms of time delay and delay rate calculation are\\nalso discussed, which is a matter of principle for data correlation of\\nspace-ground interferometers. During 5 years of Radioastron space radio\\ntelescope (SRT) successful operation, ASC correlator showed high potential of\\nsatisfying steady growing needs of current and future ground and space VLBI\\nscience. Results of ASC software correlator operation are demonstrated.\\n',\n",
       " \"  Schoof's classic algorithm allows point-counting for elliptic curves over\\nfinite fields in polynomial time. This algorithm was subsequently improved by\\nAtkin, using factorizations of modular polynomials, and by Elkies, using a\\ntheory of explicit isogenies. Moving to Jacobians of genus-2 curves, the\\ncurrent state of the art for point counting is a generalization of Schoof's\\nalgorithm. While we are currently missing the tools we need to generalize\\nElkies' methods to genus 2, recently Martindale and Milio have computed\\nanalogues of modular polynomials for genus-2 curves whose Jacobians have real\\nmultiplication by maximal orders of small discriminant. In this article, we\\nprove Atkin-style results for genus-2 Jacobians with real multiplication by\\nmaximal orders, with a view to using these new modular polynomials to improve\\nthe practicality of point-counting algorithms for these curves.\\n\",\n",
       " '  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It\\nis well-known that any ambiguous ideal in $L$ is locally free over\\n$\\\\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free\\nclass group of $\\\\mathcal{O}_KG$, where $\\\\mathcal{O}_K$ denotes the ring of\\nintegers of $K$. In this paper, we shall study the relationship among the\\nclasses arising from the ring of integers $\\\\mathcal{O}_L$ of $L$, the inverse\\ndifferent $\\\\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the\\ninverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is\\nabelian. They are naturally related because $A_{L/K}^2 =\\n\\\\mathfrak{D}_{L/K}^{-1} = \\\\mathcal{O}_L^*$, and $A_{L/K}$ is special because\\n$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of\\n$L/K$.\\n',\n",
       " '  Transformative AI technologies have the potential to reshape critical aspects\\nof society in the near future. However, in order to properly prepare policy\\ninitiatives for the arrival of such technologies accurate forecasts and\\ntimelines are necessary. A survey was administered to attendees of three AI\\nconferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).\\nThe survey included questions for estimating AI capabilities over the next\\ndecade, questions for forecasting five scenarios of transformative AI and\\nquestions concerning the impact of computational resources in AI research.\\nRespondents indicated a median of 21.5% of human tasks (i.e., all tasks that\\nhumans are currently paid to do) can be feasibly automated now, and that this\\nfigure would rise to 40% in 5 years and 60% in 10 years. Median forecasts\\nindicated a 50% probability of AI systems being capable of automating 90% of\\ncurrent human tasks in 25 years and 99% of current human tasks in 50 years. The\\nconference of attendance was found to have a statistically significant impact\\non all forecasts, with attendees of HLAI providing more optimistic timelines\\nwith less uncertainty. These findings suggest that AI experts expect major\\nadvances in AI technology to continue over the next decade to a degree that\\nwill likely have profound transformative impacts on society.\\n',\n",
       " '  Let $G,H$ be groups, $\\\\phi: G \\\\rightarrow H$ a group morphism, and $A$ a\\n$G$-graded algebra. The morphism $\\\\phi$ induces an $H$-grading on $A$, and on\\nany $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given\\nan injective $G$-graded $A$-module, we give bounds for its injective dimension\\nwhen seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give\\nan application of our results to the stability of dualizing complexes through\\nchange of grading.\\n',\n",
       " '  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts\\nas an approximate isometry when restricted to sparse vectors. Previous work has\\nshown it to be NP-hard to determine whether a matrix possess this property, but\\nonly in a narrow range of parameters. In this work, we show that it is NP-hard\\nto make this determination for any accuracy parameter, even when we restrict\\nourselves to instances which are either RIP or far from being RIP. This result\\nimplies that it is NP-hard to approximate the range of parameters for which a\\nmatrix possesses the Restricted Isometry Property with accuracy better than\\nsome constant. Ours is the first work to prove such a claim without any\\nadditional assumptions.\\n',\n",
       " '  We present a compact design for a velocity-map imaging spectrometer for\\nenergetic electrons and ions. The standard geometry by Eppink and Parker [A. T.\\nJ. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented\\nby just two extended electrodes so as to realize an additional einzel lens. In\\nthis way, for a maximum electrode voltage of 7 kV we experimentally demonstrate\\nimaging of electrons with energies up to 65 eV. Simulations show that energy\\nacceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are\\nachievable for electrode voltages of <20 kV when using diameters of the\\nposition-sensitive detector of 42 and 78 mm, respectively.\\n',\n",
       " '  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,\\nmimic a distributed storage of a set of $n$ data items on $m$ servers, in such\\na way that any batch of $k$ data items can be retrieved by reading at most some\\n$t$ symbols from each server. Combinatorial batch codes, are replication-based\\nbatch codes in which each server stores a subset of the data items.\\nIn this paper, we propose a generalization of combinatorial batch codes,\\ncalled multiset combinatorial batch codes (MCBC), in which $n$ data items are\\nstored in $m$ servers, such that any multiset request of $k$ items, where any\\nitem is requested at most $r$ times, can be retrieved by reading at most $t$\\nitems from each server. The setup of this new family of codes is motivated by\\nrecent work on codes which enable high availability and parallel reads in\\ndistributed storage systems. The main problem under this paradigm is to\\nminimize the number of items stored in the servers, given the values of\\n$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and\\nsufficient condition for the existence of MCBCs. Then, we present several\\nbounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we\\ndetermine the value of $N(n,k,m,1;r)$ for any $n\\\\geq\\n\\\\left\\\\lfloor\\\\frac{k-1}{r}\\\\right\\\\rfloor{m\\\\choose k-1}-(m-k+1)A(m,4,k-2)$, where\\n$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length\\n$m$, distance four and weight $k-2$. We also determine the exact value of\\n$N(n,k,m,1;r)$ when $r\\\\in\\\\{k,k-1\\\\}$ or $k=m$.\\n',\n",
       " '  The energy efficiency and power of a three-terminal thermoelectric nanodevice\\nare studied by considering elastic tunneling through a single quantum dot.\\nFacilitated by the three-terminal geometry, the nanodevice is able to generate\\nsimultaneously two electrical powers by utilizing only one temperature bias.\\nThese two electrical powers can add up constructively or destructively,\\ndepending on their signs. It is demonstrated that the constructive addition\\nleads to the enhancement of both energy efficiency and output power for various\\nsystem parameters. In fact, such enhancement, dubbed as thermoelectric\\ncooperative effect, can lead to maximum efficiency and power no less than when\\nonly one of the electrical power is harvested.\\n',\n",
       " '  In recent years, a number of methods for verifying DNNs have been developed.\\nBecause the approaches of the methods differ and have their own limitations, we\\nthink that a number of verification methods should be applied to a developed\\nDNN. To apply a number of methods to the DNN, it is necessary to translate\\neither the implementation of the DNN or the verification method so that one\\nruns in the same environment as the other. Since those translations are\\ntime-consuming, a utility tool, named DeepSaucer, which helps to retain and\\nreuse implementations of DNNs, verification methods, and their environments, is\\nproposed. In DeepSaucer, code snippets of loading DNNs, running verification\\nmethods, and creating their environments are retained and reused as software\\nassets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer\\nis confirmed by implementing it on the basis of Anaconda, which provides\\nvirtual environment for loading a DNN and running a verification method. In\\naddition, the effectiveness of DeepSaucer is demonstrated by usecase examples.\\n',\n",
       " '  During exploratory testing sessions the tester simultaneously learns, designs\\nand executes tests. The activity is iterative and utilizes the skills of the\\ntester and provides flexibility and creativity.Test charters are used as a\\nvehicle to support the testers during the testing. The aim of this study is to\\nsupport practitioners in the design of test charters through checklists. We\\naimed to identify factors allowing practitioners to critically reflect on their\\ndesigns and contents of test charters to support practitioners in making\\ninformed decisions of what to include in test charters. The factors and\\ncontents have been elicited through interviews. Overall, 30 factors and 35\\ncontent elements have been elicited.\\n',\n",
       " \"  We demonstrate the parallel and non-destructive readout of the hyperfine\\nstate for optically trapped $^{87}$Rb atoms. The scheme is based on\\nstate-selective fluorescence imaging and achieves detection fidelities $>$98%\\nwithin 10$\\\\,$ms, while keeping 99% of the atoms trapped. For the read-out of\\ndense arrays of neutral atoms in optical lattices, where the fluorescence\\nimages of neighboring atoms overlap, we apply a novel image analysis technique\\nusing Bayesian inference to determine the internal state of multiple atoms. Our\\nmethod is scalable to large neutral atom registers relevant for future quantum\\ninformation processing tasks requiring fast and non-destructive readout and can\\nalso be used for the simultaneous read-out of quantum information stored in\\ninternal qubit states and in the atoms' positions.\\n\",\n",
       " '  In this work we apply Amplitude Modulation Spectrum (AMS) features to the\\nsource localization problem. Our approach computes 36 bilateral features for 2s\\nlong signal segments and estimates the azimuthal directions of a sound source\\nthrough a binaurally trained classifier. This directional information of a\\nsound source could be e.g. used to steer the beamformer in a hearing aid to the\\nsource of interest in order to increase the SNR. We evaluated our approach on\\nthe development set of the IEEE-AASP Challenge on sound source localization and\\ntracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline\\napproach. Additionally, our approach is computationally less complex.\\n',\n",
       " '  We propose a novel class of dynamic shrinkage processes for Bayesian time\\nseries and regression analysis. Building upon a global-local framework of prior\\nconstruction, in which continuous scale mixtures of Gaussian distributions are\\nemployed for both desirable shrinkage properties and computational\\ntractability, we model dependence among the local scale parameters. The\\nresulting processes inherit the desirable shrinkage behavior of popular\\nglobal-local priors, such as the horseshoe prior, but provide additional\\nlocalized adaptivity, which is important for modeling time series data or\\nregression functions with local features. We construct a computationally\\nefficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture\\nrepresentation of the proposed process. Using dynamic shrinkage processes, we\\ndevelop a Bayesian trend filtering model that produces more accurate estimates\\nand tighter posterior credible intervals than competing methods, and apply the\\nmodel for irregular curve-fitting of minute-by-minute Twitter CPU usage data.\\nIn addition, we develop an adaptive time-varying parameter regression model to\\nassess the efficacy of the Fama-French five-factor asset pricing model with\\nmomentum added as a sixth factor. Our dynamic analysis of manufacturing and\\nhealthcare industry data shows that with the exception of the market risk, no\\nother risk factors are significant except for brief periods.\\n',\n",
       " '  The monitoring of the lifestyles may be performed based on a system for the\\nrecognition of Activities of Daily Living (ADL) and their environments,\\ncombining the results obtained with the user agenda. The system may be\\ndeveloped with the use of the off-the-shelf mobile devices commonly used,\\nbecause they have several types of sensors available, including motion,\\nmagnetic, acoustic, and location sensors. Data acquisition, data processing,\\ndata fusion, and artificial intelligence methods are applied in different\\nstages of the system developed, which recognizes the ADL with pattern\\nrecognition methods. The motion and magnetic sensors allow the recognition of\\nactivities with movement, but the acoustic sensors allow the recognition of the\\nenvironments. The fusion of the motion, magnetic and acoustic sensors allows\\nthe differentiation of other ADL. On the other hand, the location sensors\\nallows the recognition of ADL with large movement, and the combination of these\\nsensors with the other sensors increases the number of ADL recognized by the\\nsystem. This study consists on the comparison of different types of ANN for\\nchoosing the best methods for the recognition of several ADL, which they are\\nimplemented in a system for the recognition of ADL that combines the sensors\\ndata with the users agenda for the monitoring of the lifestyles. Conclusions\\npoint to the use of Deep Neural Networks (DNN) with normalized data for the\\nidentification of ADL with 85.89% of accuracy, the use of Feedforward neural\\nnetworks with non-normalized data for the identification of the environments\\nwith 86.50% of accuracy, and the use of DNN with normalized data for the\\nidentification of standing activities with 100% of accuracy, proving the\\nreliability of the framework presented in this study.\\n',\n",
       " \"  The extreme value index is a fundamental parameter in univariate Extreme\\nValue Theory (EVT). It captures the tail behavior of a distribution and is\\ncentral in the extrapolation beyond observed data. Among other semi-parametric\\nmethods (such as the popular Hill's estimator), the Block Maxima (BM) and\\nPeaks-Over-Threshold (POT) methods are widely used for assessing the extreme\\nvalue index and related normalizing constants. We provide asymptotic theory for\\nthe maximum likelihood estimators (MLE) based on the BM method. Our main result\\nis the asymptotic normality of the MLE with a non-trivial bias depending on the\\nextreme value index and on the so-called second order parameter. Our approach\\ncombines asymptotic expansions of the likelihood process and of the empirical\\nquantile process of block maxima. The results permit to complete the comparison\\nof most common semi-parametric estimators in EVT (MLE and probability weighted\\nmoment estimators based on the POT or BM methods) through their asymptotic\\nvariances, biases and optimal mean square errors.\\n\",\n",
       " '  The spread of opinions, memes, diseases, and \"alternative facts\" in a\\npopulation depends both on the details of the spreading process and on the\\nstructure of the social and communication networks on which they spread. In\\nthis paper, we explore how \\\\textit{anti-establishment} nodes (e.g.,\\n\\\\textit{hipsters}) influence the spreading dynamics of two competing products.\\nWe consider a model in which spreading follows a deterministic rule for\\nupdating node states (which describe which product has been adopted) in which\\nan adjustable fraction $p_{\\\\rm Hip}$ of the nodes in a network are hipsters,\\nwho choose to adopt the product that they believe is the less popular of the\\ntwo. The remaining nodes are conformists, who choose which product to adopt by\\nconsidering which products their immediate neighbors have adopted. We simulate\\nour model on both synthetic and real networks, and we show that the hipsters\\nhave a major effect on the final fraction of people who adopt each product:\\neven when only one of the two products exists at the beginning of the\\nsimulations, a very small fraction of hipsters in a network can still cause the\\nother product to eventually become the more popular one. To account for this\\nbehavior, we construct an approximation for the steady-state adoption fraction\\non $k$-regular trees in the limit of few hipsters. Additionally, our\\nsimulations demonstrate that a time delay $\\\\tau$ in the knowledge of the\\nproduct distribution in a population, as compared to immediate knowledge of\\nproduct adoption among nearest neighbors, can have a large effect on the final\\ndistribution of product adoptions. Our simple model and analysis may help shed\\nlight on the road to success for anti-establishment choices in elections, as\\nsuch success can arise rather generically in our model from a small number of\\nanti-establishment individuals and ordinary processes of social influence on\\nnormal individuals.\\n',\n",
       " '  We study the problem of testing identity against a given distribution with a\\nfocus on the high confidence regime. More precisely, given samples from an\\nunknown distribution $p$ over $n$ elements, an explicitly given distribution\\n$q$, and parameters $0< \\\\epsilon, \\\\delta < 1$, we wish to distinguish, {\\\\em\\nwith probability at least $1-\\\\delta$}, whether the distributions are identical\\nversus $\\\\varepsilon$-far in total variation distance. Most prior work focused\\non the case that $\\\\delta = \\\\Omega(1)$, for which the sample complexity of\\nidentity testing is known to be $\\\\Theta(\\\\sqrt{n}/\\\\epsilon^2)$. Given such an\\nalgorithm, one can achieve arbitrarily small values of $\\\\delta$ via black-box\\namplification, which multiplies the required number of samples by\\n$\\\\Theta(\\\\log(1/\\\\delta))$.\\nWe show that black-box amplification is suboptimal for any $\\\\delta = o(1)$,\\nand give a new identity tester that achieves the optimal sample complexity. Our\\nnew upper and lower bounds show that the optimal sample complexity of identity\\ntesting is \\\\[\\n\\\\Theta\\\\left( \\\\frac{1}{\\\\epsilon^2}\\\\left(\\\\sqrt{n \\\\log(1/\\\\delta)} +\\n\\\\log(1/\\\\delta) \\\\right)\\\\right) \\\\] for any $n, \\\\varepsilon$, and $\\\\delta$. For\\nthe special case of uniformity testing, where the given distribution is the\\nuniform distribution $U_n$ over the domain, our new tester is surprisingly\\nsimple: to test whether $p = U_n$ versus $d_{\\\\mathrm TV}(p, U_n) \\\\geq\\n\\\\varepsilon$, we simply threshold $d_{\\\\mathrm TV}(\\\\widehat{p}, U_n)$, where\\n$\\\\widehat{p}$ is the empirical probability distribution. The fact that this\\nsimple \"plug-in\" estimator is sample-optimal is surprising, even in the\\nconstant $\\\\delta$ case. Indeed, it was believed that such a tester would not\\nattain sublinear sample complexity even for constant values of $\\\\varepsilon$\\nand $\\\\delta$.\\n',\n",
       " '  Component-based design is a different way of constructing systems which\\noffers numerous benefits, in particular, decreasing the complexity of system\\ndesign. However, deploying components into a system is a challenging and\\nerror-prone task. Model checking is one of the reliable methods that\\nautomatically and systematically analyse the correctness of a given system. Its\\nbrute-force check of the state space significantly expands the level of\\nconfidence in the system. Nevertheless, model checking is limited by a critical\\nproblem so-called State Space Explosion (SSE). To benefit from model checking,\\nappropriate methods to reduce SSE, is required. In two last decades, a great\\nnumber of methods to mitigate the state space explosion have been proposed\\nwhich have many similarities, dissimilarities, and unclear concepts in some\\ncases. This research, firstly, aims at present a review and brief discussion of\\nthe methods of handling SSE problem and classify them based on their\\nsimilarities, principle and characteristics. Second, it investigates the\\nmethods for handling SSE problem in verifying Component-based system (CBS) and\\nprovides insight into CBS verification limitations that have not been addressed\\nyet. The analysis in this research has revealed the patterns, specific\\nfeatures, and gaps in the state-of-the-art methods. In addition, we identified\\nand discussed suitable methods to soften SSE problem in CBS and underlined the\\nkey challenges for future research efforts.\\n',\n",
       " '  Dust devils are likely the dominant source of dust for the martian\\natmosphere, but the amount and frequency of dust-lifting depend on the\\nstatistical distribution of dust devil parameters. Dust devils exhibit pressure\\nperturbations and, if they pass near a barometric sensor, they may register as\\na discernible dip in a pressure time-series. Leveraging this fact, several\\nsurveys using barometric sensors on landed spacecraft have revealed dust devil\\nstructures and occurrence rates. However powerful they are, though, such\\nsurveys suffer from non-trivial biases that skew the inferred dust devil\\nproperties. For example, such surveys are most sensitive to dust devils with\\nthe widest and deepest pressure profiles, but the recovered profiles will be\\ndistorted, broader and shallow than the actual profiles. In addition, such\\nsurveys often do not provide wind speed measurements alongside the pressure\\ntime series, and so the durations of the dust devil signals in the time series\\ncannot be directly converted to profile widths. Fortunately, simple statistical\\nand geometric considerations can de-bias these surveys, allowing conversion of\\nthe duration of dust devil signals into physical widths, given only a\\ndistribution of likely translation velocities, and the recovery of the\\nunderlying distributions of physical parameters. In this study, we develop a\\nscheme for de-biasing such surveys. Applying our model to an in-situ survey\\nusing data from the Phoenix lander suggests a larger dust flux and a dust devil\\noccurrence rate about ten times larger than previously inferred. Comparing our\\nresults to dust devil track surveys suggests only about one in five\\nlow-pressure cells lifts sufficient dust to leave a visible track.\\n',\n",
       " '  With the help of first principles calculation method based on the density\\nfunctional theory we have investigated the structural, elastic, mechanical\\nproperties and Debye temperature of Fe2ScM (M = P and As) compounds under\\npressure up to 60 GPa. The optical properties have been investigated under zero\\npressure. Our calculated optimized structural parameters of both the compounds\\nare in good agreement with the other theoretical results. The calculated\\nelastic constants show that Fe2ScM (M = P and As) compounds are mechanically\\nstable up to 60 GPa.\\n',\n",
       " \"  Lowpass envelope approximation of smooth continuous-variable signals are\\nintroduced in this work. Envelope approximations are necessary when a given\\nsignal has to be approximated always to a larger value (such as in TV white\\nspace protection regions). In this work, a near-optimal approximate algorithm\\nfor finding a signal's envelope, while minimizing a mean-squared cost function,\\nis detailed. The sparse (lowpass) signal approximation is obtained in the\\nlinear Fourier series basis. This approximate algorithm works by discretizing\\nthe envelope property from an infinite number of points to a large (but finite)\\nnumber of points. It is shown that this approximate algorithm is near-optimal\\nand can be solved by using efficient convex optimization programs available in\\nthe literature. Simulation results are provided towards the end to gain more\\ninsights into the analytical results presented.\\n\",\n",
       " '  We study the spectral properties of curl, a linear differential operator of\\nfirst order acting on differential forms of appropriate degree on an\\nodd-dimensional closed oriented Riemannian manifold. In three dimensions its\\neigenvalues are the electromagnetic oscillation frequencies in vacuum without\\nexternal sources. In general, the spectrum consists of the eigenvalue 0 with\\ninfinite multiplicity and further real discrete eigenvalues of finite\\nmultiplicity. We compute the Weyl asymptotics and study the zeta-function. We\\ngive a sharp lower eigenvalue bound for positively curved manifolds and analyze\\nthe equality case. Finally, we compute the spectrum for flat tori, round\\nspheres and 3-dimensional spherical space forms.\\n',\n",
       " '  This paper presents a topology optimization framework for structural problems\\nsubjected to transient loading. The mechanical model assumes a linear elastic\\nisotropic material, infinitesimal strains, and a dynamic response. The\\noptimization problem is solved using the gradient-based optimizer Method of\\nMoving Asymptotes (MMA) with time-dependent sensitivities provided via the\\nadjoint method. The stiffness of materials is interpolated using the Solid\\nIsotropic Material with Penalization (SIMP) method and the Heaviside Projection\\nMethod (HPM) is used to stabilize the problem numerically and improve the\\nmanufacturability of the topology-optimized designs. Both static and dynamic\\noptimization examples are considered here. The resulting optimized designs\\ndemonstrate the ability of topology optimization to tailor the transient\\nresponse of structures.\\n',\n",
       " \"  It is pointed out that the generalized Lambert series\\n$\\\\displaystyle\\\\sum_{n=1}^{\\\\infty}\\\\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by\\nKanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's\\nLost Notebook in a slightly more general form. We extend an important\\ntransformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by\\nremoving restrictions on the parameters $N$ and $h$ that they impose. From our\\nextension we deduce a beautiful new generalization of Ramanujan's famous\\nformula for odd zeta values which, for $N$ odd and $m>0$, gives a relation\\nbetween $\\\\zeta(2m+1)$ and $\\\\zeta(2Nm+1)$. A result complementary to the\\naforementioned generalization is obtained for any even $N$ and\\n$m\\\\in\\\\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded\\nas a formula for $\\\\zeta\\\\left(2m+1-\\\\frac{1}{N}\\\\right)$. Applications of these\\ntransformations include a generalization of the transformation for the\\nlogarithm of Dedekind eta-function $\\\\eta(z)$, Zudilin- and Rivoal-type results\\non transcendence of certain values, and a transcendence criterion for Euler's\\nconstant $\\\\gamma$.\\n\",\n",
       " \"  Motivated by applications in cancer genomics and following the work of\\nHajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, to\\nappear) introduced the minimum conflict-free row split (MCRS) problem: split\\neach row of a given binary matrix into a bitwise OR of a set of rows so that\\nthe resulting matrix corresponds to a perfect phylogeny and has the minimum\\npossible number of rows among all matrices with this property. Hajirasouliha\\nand Raphael also proposed the study of a similar problem, in which the task is\\nto minimize the number of distinct rows of the resulting matrix. Hujdurović\\net al. proved that both problems are NP-hard, gave a related characterization\\nof transitively orientable graphs, and proposed a polynomial-time heuristic\\nalgorithm for the MCRS problem based on coloring cocomparability graphs.\\nWe give new, more transparent formulations of the two problems, showing that\\nthe problems are equivalent to two optimization problems on branchings in a\\nderived directed acyclic graph. Building on these formulations, we obtain new\\nresults on the two problems, including: (i) a strengthening of the heuristic by\\nHujdurović et al. via a new min-max result in digraphs generalizing\\nDilworth's theorem, which may be of independent interest, (ii) APX-hardness\\nresults for both problems, (iii) approximation algorithms, and (iv)\\nexponential-time algorithms solving the two problems to optimality faster than\\nthe naïve brute-force approach. Our work relates to several well studied\\nnotions in combinatorial optimization: chain partitions in partially ordered\\nsets, laminar hypergraphs, and (classical and weighted) colorings of graphs.\\n\",\n",
       " '  Smart cities are a growing trend in many cities in Argentina. In particular,\\nthe so-called intermediate cities present a context and requirements different\\nfrom those of large cities with respect to smart cities. One aspect of\\nrelevance is to encourage the development of applications (generally for mobile\\ndevices) that enable citizens to take advantage of data and services normally\\nassociated with the city, for example, in the urban mobility domain. In this\\nwork, a platform is proposed for intermediate cities that provide \"high level\"\\nservices and that allow the construction of software applications that consume\\nthose services. Our platform-centric strategy focused aims to integrate systems\\nand heterogeneous data sources, and provide \"intelligent\" services to different\\napplications. Examples of these services include: construction of user\\nprofiles, recommending local events, and collaborative sensing based on data\\nmining techniques, among others. In this work, the design of this platform\\n(currently in progress) is described, and experiences of applications for urban\\nmobility are discussed, which are being migrated in the form of reusable\\nservices provided by the platform\\n',\n",
       " '  Bayesian estimation is increasingly popular for performing model based\\ninference to support policymaking. These data are often collected from surveys\\nunder informative sampling designs where subject inclusion probabilities are\\ndesigned to be correlated with the response variable of interest. Sampling\\nweights constructed from marginal inclusion probabilities are typically used to\\nform an exponentiated pseudo likelihood that adjusts the population likelihood\\nfor estimation on the sample due to ease-of-estimation. We propose an\\nalternative adjustment based on a Bayes rule construction that simultaneously\\nperforms weight smoothing and estimates the population model parameters in a\\nfully Bayesian construction. We formulate conditions on known marginal and\\npairwise inclusion probabilities that define a class of sampling designs where\\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\\nperformances between the two approaches on synthetic data, which reveals that\\nour fully Bayesian approach better estimates posterior uncertainty without a\\nrequirement to calibrate the normalization of the sampling weights. We\\ndemonstrate our method on an application concerning the National Health and\\nNutrition Examination Survey exploring the relationship between caffeine\\nconsumption and systolic blood pressure.\\n',\n",
       " '  Reaction-diffusion equations appear in biology and chemistry, and combine\\nlinear diffusion with different kind of reaction terms. Some of them are\\nremarkable from the mathematical point of view, since they admit families of\\ntravelling waves that describe the asymptotic behaviour of a larger class of\\nsolutions $0\\\\leq u(x,t)\\\\leq 1$ of the problem posed in the real line. We\\ninvestigate here the existence of waves with constant propagation speed, when\\nthe linear diffusion is replaced by the \"slow\" doubly nonlinear diffusion. In\\nthe present setting we consider bistable reaction terms, which present\\ninteresting differences w.r.t. the Fisher-KPP framework recently studied in\\n\\\\cite{AA-JLV:art}. We find different families of travelling waves that are\\nemployed to describe the wave propagation of more general solutions and to\\nstudy the stability/instability of the steady states, even when we extend the\\nstudy to several space dimensions. A similar study is performed in the critical\\ncase that we call \"pseudo-linear\", i.e., when the operator is still nonlinear\\nbut has homogeneity one. With respect to the classical model and the\\n\"pseudo-linear\" case, the travelling waves of the \"slow\" diffusion setting\\nexhibit free boundaries. \\\\\\\\ Finally, as a complement of \\\\cite{AA-JLV:art}, we\\nstudy the asymptotic behaviour of more general solutions in the presence of a\\n\"heterozygote superior\" reaction function and doubly nonlinear diffusion\\n(\"slow\" and \"pseudo-linear\").\\n',\n",
       " \"  Drivable free space information is vital for autonomous vehicles that have to\\nplan evasive maneuvers in real-time. In this paper, we present a new efficient\\nmethod for environmental free space detection with laser scanner based on 2D\\noccupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems\\n(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced\\ninverse sensor model tailored for high-resolution laser scanners for building\\nOGM. It compensates the unreflected beams and deals with the ray casting to\\ngrid cells accuracy and computational effort problems. Secondly, we introduce\\nthe 'vehicle on a circle for grid maps' map alignment algorithm that allows\\nbuilding more accurate local maps by avoiding the computationally expensive\\ninaccurate operations of image sub-pixel shifting and rotation. The resulted\\ngrid map is more convenient for ADAS features than existing methods, as it\\nallows using less memory sizes, and hence, results into a better real-time\\nperformance. Thirdly, we present an algorithm to detect what we call the\\n'in-sight edges'. These edges guarantee modeling the free space area with a\\nsingle polygon of a fixed number of vertices regardless the driving situation\\nand map complexity. The results from real world experiments show the\\neffectiveness of our approach.\\n\",\n",
       " \"  In this paper, the fundamental problem of distribution and proactive caching\\nof computing tasks in fog networks is studied under latency and reliability\\nconstraints. In the proposed scenario, computing can be executed either locally\\nat the user device or offloaded to an edge cloudlet. Moreover, cloudlets\\nexploit both their computing and storage capabilities by proactively caching\\npopular task computation results to minimize computing latency. To this end, a\\nclustering method to group spatially proximate user devices with mutual task\\npopularity interests and their serving cloudlets is proposed. Then, cloudlets\\ncan proactively cache the popular tasks' computations of their cluster members\\nto minimize computing latency. Additionally, the problem of distributing tasks\\nto cloudlets is formulated as a matching game in which a cost function of\\ncomputing delay is minimized under latency and reliability constraints.\\nSimulation results show that the proposed scheme guarantees reliable\\ncomputations with bounded latency and achieves up to 91% decrease in computing\\nlatency as compared to baseline schemes.\\n\",\n",
       " '  High-mass stars are expected to form from dense prestellar cores. Their\\nprecise formation conditions are widely discussed, including their virial\\ncondition, which results in slow collapse for super-virial cores with strong\\nsupport by turbulence or magnetic fields, or fast collapse for sub-virial\\nsources. To disentangle their formation processes, measurements of the\\ndeuterium fractions are frequently employed to approximately estimate the ages\\nof these cores and to obtain constraints on their dynamical evolution. We here\\npresent 3D magneto-hydrodynamical simulations including for the first time an\\naccurate non-equilibrium chemical network with 21 gas-phase species plus dust\\ngrains and 213 reactions. With this network we model the deuteration process in\\nfully depleted prestellar cores in great detail and determine its response to\\nvariations in the initial conditions. We explore the dependence on the initial\\ngas column density, the turbulent Mach number, the mass-to-magnetic flux ratio\\nand the distribution of the magnetic field, as well as the initial\\northo-to-para ratio of H2. We find excellent agreement with recent observations\\nof deuterium fractions in quiescent sources. Our results show that deuteration\\nis rather efficient, even when assuming a conservative ortho-to-para ratio of 3\\nand highly sub-virial initial conditions, leading to large deuterium fractions\\nalready within roughly a free-fall time. We discuss the implications of our\\nresults and give an outlook to relevant future investigations.\\n',\n",
       " '  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)\\nsimulations and among the most widely used computational kernels in science.\\nThe potential models atomistic attraction and repulsion with century old\\nprescribed parameters ($q=6, \\\\; p=12$, respectively), originally related by a\\nfactor of two for simplicity of calculations. We re-examine the value of the\\nrepulsion exponent through data driven uncertainty quantification. We perform\\nHierarchical Bayesian inference on MD simulations of argon using experimental\\ndata of the radial distribution function (RDF) for a range of thermodynamic\\nconditions, as well as dimer interaction energies from quantum mechanics\\nsimulations. The experimental data suggest a repulsion exponent ($p \\\\approx\\n6.5$), in contrast to the quantum simulations data that support values closer\\nto the original ($p=12$) exponent. Most notably, we find that predictions of\\nRDF, diffusion coefficient and density of argon are more accurate and robust in\\nproducing the correct argon phase around its triple point, when using the\\nvalues inferred from experimental data over those from quantum mechanics\\nsimulations. The present results suggest the need for data driven recalibration\\nof the LJ potential across MD simulations.\\n',\n",
       " '  Thompson sampling has emerged as an effective heuristic for a broad range of\\nonline decision problems. In its basic form, the algorithm requires computing\\nand sampling from a posterior distribution over models, which is tractable only\\nfor simple special cases. This paper develops ensemble sampling, which aims to\\napproximate Thompson sampling while maintaining tractability even in the face\\nof complex models such as neural networks. Ensemble sampling dramatically\\nexpands on the range of applications for which Thompson sampling is viable. We\\nestablish a theoretical basis that supports the approach and present\\ncomputational results that offer further insight.\\n',\n",
       " '  In recent years, Deep Learning has become the go-to solution for a broad\\nrange of applications, often outperforming state-of-the-art. However, it is\\nimportant, for both theoreticians and practitioners, to gain a deeper\\nunderstanding of the difficulties and limitations associated with common\\napproaches and algorithms. We describe four types of simple problems, for which\\nthe gradient-based algorithms commonly used in deep learning either fail or\\nsuffer from significant difficulties. We illustrate the failures through\\npractical experiments, and provide theoretical insights explaining their\\nsource, and how they might be remedied.\\n',\n",
       " '  Proxima Centauri is known as the closest star from the Sun. Recently, radial\\nvelocity observations revealed the existence of an Earth-mass planet around it.\\nWith an orbital period of ~11 days, the surface of Proxima Centauri b is\\ntemperate and might be habitable. We took a photometric monitoring campaign to\\nsearch for its transit, using the Bright Star Survey Telescope at the Zhongshan\\nStation in Antarctica. A transit-like signal appearing on 2016 September 8th,\\nis identified tentatively. Its midtime, $T_{C}=2,457,640.1990\\\\pm0.0017$ HJD, is\\nconsistent with the predicted ephemeris based on RV orbit in a 1$\\\\sigma$\\nconfidence interval. Time-correlated noise is pronounced in the light curve of\\nProxima Centauri, affecting detection of transits. We develop a technique, in a\\nGaussian process framework, to gauge the statistical significance of potential\\ntransit detection. The tentative transit signal reported here, has a confidence\\nlevel of $2.5\\\\sigma$. Further detection of its periodic signals is necessary to\\nconfirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima\\nCentauri in next Polar night at Dome A in Antarctica, taking the advantage of\\ncontinuous darkness. \\\\citet{Kipping17} reported two tentative transit-like\\nsignals of Proxima Centauri b, observed by the Microvariability and Oscillation\\nof Stars space Telescope in 2014 and 2015, respectively. The midtransit time of\\nour detection is 138 minutes later than that predicted by their transit\\nephemeris. If all the signals are real transits, the misalignment of the epochs\\nplausibly suggests transit timing variations of Proxima Centauri b induced by\\nan outer planet in this system.\\n',\n",
       " '  In this paper, we propose an optimization-based sparse learning approach to\\nidentify the set of most influential reactions in a chemical reaction network.\\nThis reduced set of reactions is then employed to construct a reduced chemical\\nreaction mechanism, which is relevant to chemical interaction network modeling.\\nThe problem of identifying influential reactions is first formulated as a\\nmixed-integer quadratic program, and then a relaxation method is leveraged to\\nreduce the computational complexity of our approach. Qualitative and\\nquantitative validation of the sparse encoding approach demonstrates that the\\nmodel captures important network structural properties with moderate\\ncomputational load.\\n',\n",
       " '  Using density-functional theory calculations, we analyze the optical\\nabsorption properties of lead (Pb)-free metal halide perovskites\\n(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or\\nmonovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent\\nmetal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is\\nnot Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions\\nbecause of their indirect bandgap nature. Among the nine possible types of\\nPb-free metal halide double perovskites, six have direct bandgaps. Of these six\\ntypes, four show inversion symmetry-induced parity-forbidden or weak\\ntransitions between band edges, making them not ideal for thin-film solar cell\\napplication. Only one type of Pb-free double perovskite shows optical\\nabsorption and electronic properties suitable for solar cell applications,\\nnamely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide\\nimportant insights for designing new metal halide perovskites and double\\nperovskites for optoelectronic applications.\\n',\n",
       " '  MicroRNAs play important roles in many biological processes. Their aberrant\\nexpression can have oncogenic or tumor suppressor function directly\\nparticipating to carcinogenesis, malignant transformation, invasiveness and\\nmetastasis. Indeed, miRNA profiles can distinguish not only between normal and\\ncancerous tissue but they can also successfully classify different subtypes of\\na particular cancer. Here, we focus on a particular class of transcripts\\nencoding polycistronic miRNA genes that yields multiple miRNA components. We\\ndescribe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully\\nredesigned release of the MMRA computational pipeline (MiRNA Master Regulator\\nAnalysis), developed to search for clustered miRNAs potentially driving cancer\\nmolecular subtyping. Genomically clustered miRNAs are frequently co-expressed\\nto target different components of pro-tumorigenic signalling pathways. By\\napplying ClustMMRA to breast cancer patient data, we identified key miRNA\\nclusters driving the phenotype of different tumor subgroups. The pipeline was\\napplied to two independent breast cancer datasets, providing statistically\\nconcordant results between the two analysis. We validated in cell lines the\\nmiR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative\\nsubtype phenotype through its control of proliferation and EMT.\\n',\n",
       " '  We present a simultaneous localization and mapping (SLAM) algorithm that is\\nbased on radio signals and the association of specular multipath components\\n(MPCs) with geometric features. Especially in indoor scenarios, robust\\nlocalization from radio signals is challenging due to diffuse multipath\\npropagation, unknown MPC-feature association, and limited visibility of\\nfeatures. In our approach, specular reflections at flat surfaces are described\\nin terms of virtual anchors (VAs) that are mirror images of the physical\\nanchors (PAs). The positions of these VAs and possibly also of the PAs are\\nunknown. We develop a Bayesian model of the SLAM problem including the unknown\\nMPC-VA/PA association. We represent this model by a factor graph, which enables\\nthe use of the belief propagation (BP) scheme for efficient marginalization of\\nthe joint posterior distribution. The resulting BP-based SLAM algorithm detects\\nthe VAs associated with the PAs and estimates jointly the time-varying position\\nof the mobile agent and the positions of the VAs and possibly also of the PAs,\\nthereby leveraging the MPCs in the radio signal for improved accuracy and\\nrobustness of agent localization. A core aspect of the algorithm is BP-based\\nprobabilistic MPC-VA/PA association. Moreover, for improved initialization of\\nnew VA positions, the states of unobserved potential VAs are modeled as a\\nrandom finite set and propagated in time by means of a \"zero-measurement\"\\nprobability hypothesis density filter. The proposed BP-based SLAM algorithm has\\na low computational complexity and scales well in all relevant system\\nparameters. Experimental results using both synthetically generated\\nmeasurements and real ultra-wideband radio signals demonstrate the excellent\\nperformance of the algorithm in challenging indoor environments.\\n',\n",
       " '  This survey is about old and new results about the modular representation\\ntheory of finite reductive groups with a strong emphasis on local methods. This\\nincludes subpairs, Brauer\\'s Main Theorems, fusion, Rickard equivalences. In the\\ndefining characteristic we describe the relation between $p$-local subgroups\\nand parabolic subgroups, then give classical consequences on simple modules and\\nblocks, including the Alperin weight conjecture in that case. In the\\nnon-defining characteristics, we sketch a picture of the local methods\\npioneered by Fong-Srinivasan in the determination of blocks and their ordinary\\ncharacters. This includes the relationship with Lusztig\\'s twisted induction and\\nthe determination of defect groups. We conclude with a survey of the results\\nand methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks\\nthat preserve defect groups and the local structures.\\nThe text grew out of the course and talks given by the author in July and\\nSeptember 2016 during the program \"Local representation theory and simple\\ngroups\" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume\\npublished by EMS.\\n',\n",
       " '  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is\\npresented in this paper where the motion planning to realize \"human-like\"\\nautonomous navigation and manipulation tasks is studied. Firstly, an improved\\nMaxiMin NSGA-II algorithm, which optimizes five objective functions to solve\\nthe problems of singularity, redundancy, and coupling between mobile base and\\nmanipulator simultaneously, is proposed to design the optimal pose to\\nmanipulate the target object. Then, in order to link the initial pose and that\\noptimal pose, an off-line motion planning algorithm is designed. In detail, an\\nefficient direct-connect bidirectional RRT and gradient descent algorithm is\\nproposed to reduce the sampled nodes largely, and a geometric optimization\\nmethod is proposed for path pruning. Besides, head forward behaviors are\\nrealized by calculating the reasonable orientations and assigning them to the\\nmobile base to improve the quality of human-robot interaction. Thirdly, the\\nextension to on-line planning is done by introducing real-time sensing,\\ncollision-test and control cycles to update robotic motion in dynamic\\nenvironments. Fourthly, an EEs\\' via-point-based multi-objective genetic\\nalgorithm is proposed to design the \"human-like\" via-poses by optimizing four\\nobjective functions. Finally, numerous simulations are presented to validate\\nthe effectiveness of proposed algorithms.\\n',\n",
       " '  This article discusses a framework to support the design and end-to-end\\nplanning of fixed millimeter-wave networks. Compared to traditional techniques,\\nthe framework allows an organization to quickly plan a deployment in a\\ncost-effective way. We start by using LiDAR data---basically, a 3D point cloud\\ncaptured from a city---to estimate potential sites to deploy antennas and\\nwhether there is line-of-sight between them. With that data on hand, we use\\ncombinatorial optimization techniques to determine the optimal set of locations\\nand how they should communicate with each other, to satisfy engineering (e.g.,\\nlatency, polarity), design (e.g., reliability) and financial (e.g., total cost\\nof operation) constraints. The primary goal is to connect as many people as\\npossible to the network. Our methodology can be used for strategic planning\\nwhen an organization is in the process of deciding whether to adopt a\\nmillimeter-wave technology or choosing between locations, or for operational\\nplanning when conducting a detailed design of the actual network to be deployed\\nin a selected location.\\n',\n",
       " \"  The amount of ultraviolet irradiation and ablation experienced by a planet\\ndepends strongly on the temperature of its host star. Of the thousands of\\nextra-solar planets now known, only four giant planets have been found that\\ntransit hot, A-type stars (temperatures of 7300-10,000K), and none are known to\\ntransit even hotter B-type stars. WASP-33 is an A-type star with a temperature\\nof ~7430K, which hosts the hottest known transiting planet; the planet is\\nitself as hot as a red dwarf star of type M. The planet displays a large heat\\ndifferential between its day-side and night-side, and is highly inflated,\\ntraits that have been linked to high insolation. However, even at the\\ntemperature of WASP-33b's day-side, its atmosphere likely resembles the\\nmolecule-dominated atmospheres of other planets, and at the level of\\nultraviolet irradiation it experiences, its atmosphere is unlikely to be\\nsignificantly ablated over the lifetime of its star. Here we report\\nobservations of the bright star HD 195689, which reveal a close-in (orbital\\nperiod ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star\\nis at the dividing line between stars of type A and B, and we measure the\\nKELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar\\ntype K4. The molecules in K stars are entirely dissociated, and thus the\\nprimary sources of opacity in the day-side atmosphere of KELT-9b are likely\\natomic metals. Furthermore, KELT-9b receives ~700 times more extreme\\nultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,\\nleading to a predicted range of mass-loss rates that could leave the planet\\nlargely stripped of its envelope during the main-sequence lifetime of the host\\nstar.\\n\",\n",
       " \"  Online video services, messaging systems, games and social media services are\\ntremendously popular among young people and children in many countries. Most of\\nthe digital services offered on the internet are advertising funded, which\\nmakes advertising ubiquitous in children's everyday life. To understand the\\nimpact of advertising-based digital services on children, we study the\\ncollective behavior of users of YouTube for kids channels and present the\\ndemographics of a large number of users. We collected data from 12,848 videos\\nfrom 17 channels in US and UK and 24 channels in Brazil. The channels in\\nEnglish have been viewed more than 37 billion times. We also collected more\\nthan 14 million comments made by users. Based on a combination of text-analysis\\nand face recognition tools, we show the presence of racial and gender biases in\\nour large sample of users. We also identify children actively using YouTube,\\nalthough the minimum age for using the service is 13 years in most countries.\\nWe provide comparisons of user behavior among the three countries, which\\nrepresent large user populations in the global North and the global South.\\n\",\n",
       " '  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks\\nsatisfies cdh-descent. We apply this descent result to prove that if X is a\\nNoetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.\\nK_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).\\nOur descent and vanishing results apply more generally to certain Artin stacks\\nwhose stabilizers are extensions of finite group schemes by group schemes of\\nmultiplicative type.\\n',\n",
       " '  Screened modified gravity (SMG) is a kind of scalar-tensor theory with\\nscreening mechanisms, which can suppress the fifth force in dense regions and\\nallow theories to evade the solar system and laboratory tests. In this paper,\\nwe investigate how the screening mechanisms in SMG affect the gravitational\\nradiation damping effects, calculate in detail the rate of the energy loss due\\nto the emission of tensor and scalar gravitational radiations, and derive their\\ncontributions to the change in the orbital period of the binary system. We find\\nthat the scalar radiation depends on the screened parameters and the\\npropagation speed of scalar waves, and the scalar dipole radiation dominates\\nthe orbital decay of the binary system. For strongly self-gravitating bodies,\\nall effects of scalar sector are strongly suppressed by the screening\\nmechanisms in SMG. By comparing our results to observations of binary system\\nPSR J1738+0333, we place the stringent constraints on the screening mechanisms\\nin SMG. As an application of these results, we focus on three specific models\\nof SMG (chameleon, symmetron, and dilaton), and derive the constraints on the\\nmodel parameters, respectively.\\n',\n",
       " '  Selective weed treatment is a critical step in autonomous crop management as\\nrelated to crop health and yield. However, a key challenge is reliable, and\\naccurate weed detection to minimize damage to surrounding plants. In this\\npaper, we present an approach for dense semantic weed classification with\\nmultispectral images collected by a micro aerial vehicle (MAV). We use the\\nrecently developed encoder-decoder cascaded Convolutional Neural Network (CNN),\\nSegnet, that infers dense semantic classes while allowing any number of input\\nimage channels and class balancing with our sugar beet and weed datasets. To\\nobtain training datasets, we established an experimental field with varying\\nherbicide levels resulting in field plots containing only either crop or weed,\\nenabling us to use the Normalized Difference Vegetation Index (NDVI) as a\\ndistinguishable feature for automatic ground truth generation. We train 6\\nmodels with different numbers of input channels and condition (fine-tune) it to\\nachieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification\\nmetrics. For model deployment, an embedded GPU system (Jetson TX2) is tested\\nfor MAV integration. Dataset used in this paper is released to support the\\ncommunity and future work.\\n',\n",
       " '  Let $G:=\\\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\\n$SL_2$ and $\\\\bar{\\\\mathcal{X}}$ the associated affine Grassmannian. We determine\\nan inductive formula for the Schubert basis structure constants in the\\ntorus-equivariant Grothendieck group of $\\\\bar{\\\\mathcal{X}}$. In the case of\\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\\nstructure constants. We also determine an inductive formula for the structure\\nconstants in the torus-equivariant cohomology ring, and use this formula to\\nfind closed forms for some of the structure constants.\\n',\n",
       " '  The existing measurement theory interprets the variance as the dispersion of\\nmeasured value, which is actually contrary to a general mathematical knowledge\\nthat the variance of a constant is 0. This paper will fully demonstrate that\\nthe variance in measurement theory is actually the evaluation of probability\\ninterval of an error instead of the dispersion of a measured value, point out\\nthe key point of mistake in the existing interpretation, and fully interpret a\\nseries of changes in conceptual logic and processing method brought about by\\nthis new concept.\\n',\n",
       " '  Researchers are often interested in analyzing conditional treatment effects.\\nOne variant of this is \"causal moderation,\" which implies that intervention\\nupon a third (moderator) variable would alter the treatment effect. This study\\npresents a generalized, non-parametric framework for estimating causal\\nmoderation effects given randomized treatments and non-randomized moderators\\nthat achieves a number of goals. First, it highlights how conventional\\napproaches do not constitute unbiased or consistent estimators of causal\\nmoderation effects. Second, it offers researchers a simple, transparent\\napproach for estimating causal moderation effects and lays out the assumptions\\nunder which this can be performed consistently and/or without bias. Third, as\\npart of the estimation process, it allows researchers to implement their\\npreferred method of covariate adjustment, including parametric and\\nnon-parametric methods, or alternative identification strategies of their\\nchoosing. Fourth, it provides a set-up whereby sensitivity analysis designed\\nfor the average-treatment-effect context can be extended to the moderation\\ncontext. An original application is also presented.\\n',\n",
       " '  This paper deals with some simple results about spherical functions of type\\n$\\\\delta$, namely new integral formulas, new results about behavior at infinity\\nand some facts about the related $C_\\\\sigma$ functions.\\n',\n",
       " '  In this study, we determine all modular curves $X_0(N)$ that admit infinitely\\nmany cubic points.\\n',\n",
       " \"  In the framework of multi-body dynamics, successive encounters with a third\\nbody, even if well outside of its sphere of influence, can noticeably alter the\\ntrajectory of a spacecraft. Examples of these effects have already been\\nexploited by past missions such as SMART-1, as well as are proposed to benefit\\nfuture missions to Jupiter, Saturn or Neptune, and disposal strategies from\\nEarth's High Eccentric or Libration Point Orbits. This paper revises three\\ntotally different descriptions of the effects of the third body gravitational\\nperturbation. These are the averaged dynamics of the classical third body\\nperturbing function, the Opik's close encounter theory and the Keplerian map\\napproach. The first two techniques have respectively been applied to the cases\\nof a spacecraft either always remaining very far or occasionally experiencing\\nextremely close approaches to the third body. However, the paper also seeks\\nsolutions for trajectories that undergo one or more close approaches at\\ndistances in the order of the sphere of influence of the third body. The paper\\nattempts to gain insight into the accuracy of these different perturbative\\ntechniques into each of these scenarios, as compared with the motion in the\\nCircular Restricted Three Body Problem.\\n\",\n",
       " '  Capsule Networks envision an innovative point of view about the\\nrepresentation of the objects in the brain and preserve the hierarchical\\nspatial relationships between them. This type of networks exhibits a huge\\npotential for several Machine Learning tasks like image classification, while\\noutperforming Convolutional Neural Networks (CNNs). A large body of work has\\nexplored adversarial examples for CNNs, but their efficacy to Capsule Networks\\nis not well explored. In our work, we study the vulnerabilities in Capsule\\nNetworks to adversarial attacks. These perturbations, added to the test inputs,\\nare small and imperceptible to humans, but fool the network to mis-predict. We\\npropose a greedy algorithm to automatically generate targeted imperceptible\\nadversarial examples in a black-box attack scenario. We show that this kind of\\nattacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),\\nmislead Capsule Networks. Moreover, we apply the same kind of adversarial\\nattacks to a 9-layer CNN and analyze the outcome, compared to the Capsule\\nNetworks to study their differences / commonalities.\\n',\n",
       " '  The effects of MHD boundary layer flow of non-linear thermal radiation with\\nconvective heat transfer and non-uniform heat source/sink in presence of\\nthermophortic velocity and chemical reaction investigated in this study.\\nSuitable similarity transformation are used to solve the partial ordinary\\ndifferential equation of considered governing flow. Runge-Kutta fourth fifth\\norder Fehlberg method with shooting techniques are used to solved\\nnon-dimensional governing equations. The variation of different parameters such\\nas thermophoretic parameter, chemical reaction parameter, non- uniform heat\\nsource/sink parameters are studied on velocity, temperature and concentration\\nprofiles, and are described by suitable graphs and tables. The obtained results\\nare in very well agreement with previous results.\\n',\n",
       " '  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily\\npractice and academic research stay discreet compared to resting-state BOLD.\\nHowever, by giving direct access to cerebral blood flow maps, rs-fASL leads to\\nsignificant clinical subject scaled application as CBF can be considered as a\\nbiomarker in common neuropathology. Our work here focuses on the link between\\noverall quality of rs-fASL and duration of acquisition. To this end, we\\nconsider subject self-Default Mode Network (DMN), and assess DMN quality\\ndepletion compared to a gold standard DMN depending on the duration of\\nacquisition.\\n',\n",
       " \"  We propose a novel end-to-end neural network architecture that, once trained,\\ndirectly outputs a probabilistic clustering of a batch of input examples in one\\npass. It estimates a distribution over the number of clusters $k$, and for each\\n$1 \\\\leq k \\\\leq k_\\\\mathrm{max}$, a distribution over the individual cluster\\nassignment for each data point. The network is trained in advance in a\\nsupervised fashion on separate data to learn grouping by any perceptual\\nsimilarity criterion based on pairwise labels (same/different group). It can\\nthen be applied to different data containing different groups. We demonstrate\\npromising performance on high-dimensional data like images (COIL-100) and\\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\\ndifference to deep metric learning, semi-supervise clustering and other related\\napproaches while having the advantage of performing learnable clustering fully\\nend-to-end.\\n\",\n",
       " '  The collective magnetic excitations in the spin-orbit Mott insulator\\n(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\\\\,0.01,\\\\,0.04,\\\\, 0.1$) were investigated by\\nmeans of resonant inelastic x-ray scattering. We report significant magnon\\nenergy gaps at both the crystallographic and antiferromagnetic zone centers at\\nall doping levels, along with a remarkably pronounced momentum-dependent\\nlifetime broadening. The spin-wave gap is accounted for by a significant\\nanisotropy in the interactions between $J_\\\\text{eff}=1/2$ isospins, thus\\nmarking the departure of Sr$_2$IrO$_4$ from the essentially isotropic\\nHeisenberg model appropriate for the superconducting cuprates.\\n',\n",
       " '  We report the proximity induced anomalous transport behavior in a Nb\\nBi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of\\nBi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm\\nthick Niobium superconductor using DC magnetron sputtering by shadow masking\\ntechnique. The magnetotransport (MR) measurements have been performed\\nsimultaneously on the TI sample with and without Nb top layer in the\\ntemperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI\\nregion shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous\\nlinear change in resistance is observed in the field range of negative 4T to\\npositive 4T at which Nb is superconducting. At 0 T field, the temperature\\ndependence of resistance on the Nb covered region revealed a superconducting\\ntransition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of\\nzero resistance states due to the additional resistance from superconductor\\n(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed\\nan enhancement in resistance for positive field and prominent fall in\\nresistance for negative field direction. This indicates the directional\\ndependent scattering of the Cooper pairs on the surface of the TI due to the\\nsuperposition of spin singlet and triplet states in the superconductor and TI\\nrespectively.\\n',\n",
       " '  Networked control systems (NCS) have attracted considerable attention in\\nrecent years. While the stabilizability and optimal control of NCS for a given\\ncommunication system has already been studied extensively, the design of the\\ncommunication system for NCS has recently seen an increase in more thorough\\ninvestigation. In this paper, we address an optimal scheduling problem for a\\nset of NCS sharing a dedicated communication channel, providing performance\\nbounds and asymptotic stability. We derive a suboptimal scheduling policy with\\ndynamic state-based priorities calculated at the sensors, which are then used\\nfor stateless priority queuing in the network, making it both scalable and\\nefficient to implement on routers or multi-layer switches. These properties are\\nbeneficial towards leveraging existing IP networks for control, which will be a\\ncrucial factor for the proliferation of wide-area NCS applications. By allowing\\nfor an arbitrary number of concurrent transmissions, we are able to investigate\\nthe relationship between available bandwidth, transmission rate, and delay. To\\ndemonstrate the feasibility of our approach, we provide a proof-of-concept\\nimplementation of the priority scheduler using real networking hardware.\\n',\n",
       " \"  Given a property of representations satisfying a basic stability condition,\\nRamakrishna developed a variant of Mazur's Galois deformation theory for\\nrepresentations with that property. We introduce an axiomatic definition of\\npseudorepresentations with such a property. Among other things, we show that\\npseudorepresentations with a property enjoy a good deformation theory,\\ngeneralizing Ramakrishna's theory to pseudorepresentations.\\n\",\n",
       " '  A novel adaptive local surface refinement technique based on Locally Refined\\nNon-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model\\ncomplex geometries exactly and are the rational extension of LR B-splines. The\\nlocal representation of the parameter space overcomes the drawback of\\nnon-existent local refinement in standard NURBS-based isogeometric analysis.\\nFor a convenient embedding into general finite element code, the Bézier\\nextraction operator for LR NURBS is formulated. An automatic remeshing\\ntechnique is presented that allows adaptive local refinement and coarsening of\\nLR NURBS. In this work, LR NURBS are applied to contact computations of 3D\\nsolids and membranes. For solids, LR NURBS-enriched finite elements are used to\\ndiscretize the contact surfaces with LR NURBS finite elements, while the rest\\nof the body is discretized by linear Lagrange finite elements. For membranes,\\nthe entire surface is discretized by LR NURBS. Various numerical examples are\\nshown, and they demonstrate the benefit of using LR NURBS: Compared to uniform\\nrefinement, LR NURBS can achieve high accuracy at lower computational cost.\\n',\n",
       " '  This paper introduces a general method to approximate the convolution of an\\narbitrary program with a Gaussian kernel. This process has the effect of\\nsmoothing out a program. Our compiler framework models intermediate values in\\nthe program as random variables, by using mean and variance statistics. Our\\napproach breaks the input program into parts and relates the statistics of the\\ndifferent parts, under the smoothing process. We give several approximations\\nthat can be used for the different parts of the program. These include the\\napproximation of Dorn et al., a novel adaptive Gaussian approximation, Monte\\nCarlo sampling, and compactly supported kernels. Our adaptive Gaussian\\napproximation is accurate up to the second order in the standard deviation of\\nthe smoothing kernel, and mathematically smooth. We show how to construct a\\ncompiler that applies chosen approximations to given parts of the input\\nprogram. Because each expression can have multiple approximation choices, we\\nuse a genetic search to automatically select the best approximations. We apply\\nthis framework to the problem of automatically bandlimiting procedural shader\\nprograms. We evaluate our method on a variety of complex shaders, including\\nshaders with parallax mapping, animation, and spatially varying statistics. The\\nresulting smoothed shader programs outperform previous approaches both\\nnumerically, and aesthetically, due to the smoothing properties of our\\napproximations.\\n',\n",
       " '  Large-scale computational experiments, often running over weeks and over\\nlarge datasets, are used extensively in fields such as epidemiology,\\nmeteorology, computational biology, and healthcare to understand phenomena, and\\ndesign high-stakes policies affecting everyday health and economy. For\\ninstance, the OpenMalaria framework is a computationally-intensive simulation\\nused by various non-governmental and governmental agencies to understand\\nmalarial disease spread and effectiveness of intervention strategies, and\\nsubsequently design healthcare policies. Given that such shared results form\\nthe basis of inferences drawn, technological solutions designed, and day-to-day\\npolicies drafted, it is essential that the computations are validated and\\ntrusted. In particular, in a multi-agent environment involving several\\nindependent computing agents, a notion of trust in results generated by peers\\nis critical in facilitating transparency, accountability, and collaboration.\\nUsing a novel combination of distributed validation of atomic computation\\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\\nuniversal framework for distributed trust in computations. In particular we\\naddress the scalaibility problem by reducing the storage and communication\\ncosts using a lossy compression scheme. This framework guarantees not only\\nverifiability of final results, but also the validity of local computations,\\nand its cost-benefit tradeoffs are studied using a synthetic example of\\ntraining a neural network.\\n',\n",
       " '  We provide an overview of several non-linear activation functions in a neural\\nnetwork architecture that have proven successful in many machine learning\\napplications. We conduct an empirical analysis on the effectiveness of using\\nthese function on the MNIST classification task, with the aim of clarifying\\nwhich functions produce the best results overall. Based on this first set of\\nresults, we examine the effects of building deeper architectures with an\\nincreasing number of hidden layers. We also survey the impact of using, on the\\nsame task, different initialisation schemes for the weights of our neural\\nnetwork. Using these sets of experiments as a base, we conclude by providing a\\noptimal neural network architecture that yields impressive results in accuracy\\non the MNIST classification task.\\n',\n",
       " '  Advancements in deep learning over the years have attracted research into how\\ndeep artificial neural networks can be used in robotic systems. This research\\nsurvey will present a summarization of the current research with a specific\\nfocus on the gains and obstacles for deep learning to be applied to mobile\\nrobotics.\\n',\n",
       " \"  Our desire and fascination with intelligent machines dates back to the\\nantiquity's mythical automaton Talos, Aristotle's mode of mechanical thought\\n(syllogism) and Heron of Alexandria's mechanical machines and automata.\\nHowever, the quest for Artificial General Intelligence (AGI) is troubled with\\nrepeated failures of strategies and approaches throughout the history. This\\ndecade has seen a shift in interest towards bio-inspired software and hardware,\\nwith the assumption that such mimicry entails intelligence. Though these steps\\nare fruitful in certain directions and have advanced automation, their singular\\ndesign focus renders them highly inefficient in achieving AGI. Which set of\\nrequirements have to be met in the design of AGI? What are the limits in the\\ndesign of the artificial? Here, a careful examination of computation in\\nbiological systems hints that evolutionary tinkering of contextual processing\\nof information enabled by a hierarchical architecture is the key to build AGI.\\n\",\n",
       " '  Conventional crystalline magnets are characterized by symmetry breaking and\\nnormal modes of excitation called magnons with quantized angular momentum\\n$\\\\hbar$. Neutron scattering correspondingly features extra magnetic Bragg\\ndiffraction at low temperatures and dispersive inelastic scattering associated\\nwith single magnon creation and annihilation. Exceptions are anticipated in\\nso-called quantum spin liquids as exemplified by the one-dimensional spin-1/2\\nchain which has no magnetic order and where magnons accordingly fractionalize\\ninto spinons with angular momentum $\\\\hbar/2$. This is spectacularly revealed by\\na continuum of inelastic neutron scattering associated with two-spinon\\nprocesses and the absence of magnetic Bragg diffraction. Here, we report\\nevidence for these same key features of a quantum spin liquid in the\\nthree-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific\\nheat and neutron scattering measurements, Monte Carlo simulations, and analytic\\napproximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is\\nan almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on\\na pyrochlore lattice with weak connectivity and frustrated interactions.\\nMagnetic Bragg diffraction is absent and 90\\\\% of the spectral weight forms a\\ncontinuum of magnetic scattering not dissimilar to that of the spin-1/2 chain\\nbut with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb\\nphase. The residual entropy and diffuse elastic scattering points to an exotic\\nstate of matter driven by frustration, quantum fluctuations and weak exchange\\ndisorder.\\n',\n",
       " '  We introduce the new version of SimProp, a Monte Carlo code for simulating\\nthe propagation of ultra-high energy cosmic rays in intergalactic space. This\\nversion, SimProp v2r4, together with an overall improvement of the code\\ncapabilities with a substantial reduction in the computation time, also\\ncomputes secondary cosmogenic particles such as electron-positron pairs and\\ngamma rays produced during the propagation of ultra-high energy cosmic rays. As\\nrecently pointed out by several authors, the flux of this secondary radiation\\nand its products, within reach of the current observatories, provides useful\\ninformation about models of ultra-high energy cosmic ray sources which would be\\nhard to discriminate otherwise.\\n',\n",
       " \"  Given a collection of data points, non-negative matrix factorization (NMF)\\nsuggests to express them as convex combinations of a small set of `archetypes'\\nwith non-negative entries. This decomposition is unique only if the true\\narchetypes are non-negative and sufficiently sparse (or the weights are\\nsufficiently sparse), a regime that is captured by the separability condition\\nand its generalizations.\\nIn this paper, we study an approach to NMF that can be traced back to the\\nwork of Cutler and Breiman (1994) and does not require the data to be\\nseparable, while providing a generally unique decomposition. We optimize the\\ntrade-off between two objectives: we minimize the distance of the data points\\nfrom the convex envelope of the archetypes (which can be interpreted as an\\nempirical risk), while minimizing the distance of the archetypes from the\\nconvex envelope of the data (which can be interpreted as a data-dependent\\nregularization). The archetypal analysis method of (Cutler, Breiman, 1994) is\\nrecovered as the limiting case in which the last term is given infinite weight.\\nWe introduce a `uniqueness condition' on the data which is necessary for\\nexactly recovering the archetypes from noiseless data. We prove that, under\\nuniqueness (plus additional regularity conditions on the geometry of the\\narchetypes), our estimator is robust. While our approach requires solving a\\nnon-convex optimization problem, we find that standard optimization methods\\nsucceed in finding good solutions both for real and synthetic data.\\n\",\n",
       " \"  Muon reconstruction in the Daya Bay water pools would serve to verify the\\nsimulated muon fluxes and offer the possibility of studying cosmic muons in\\ngeneral. This reconstruction is, however, complicated by many optical obstacles\\nand the small coverage of photomultiplier tubes (PMTs) as compared to other\\nlarge water Cherenkov detectors. The PMTs' timing information is useful only in\\nthe case of direct, unreflected Cherenkov light. This requires PMTs to be added\\nand removed as an hypothesized muon trajectory is iteratively improved, to\\naccount for the changing effects of obstacles and direction of light.\\nTherefore, muon reconstruction in the Daya Bay water pools does not lend itself\\nto a general fitting procedure employing smoothly varying functions with\\ncontinuous derivatives. Here, an algorithm is described which overcomes these\\ncomplications. It employs the method of Least Mean Squares to determine an\\nhypothesized trajectory from the PMTs' charge-weighted positions. This\\ninitially hypothesized trajectory is then iteratively refined using the PMTs'\\ntiming information. Reconstructions with simulated data reproduce the simulated\\ntrajectory to within about 5 degrees in direction and about 45 cm in position\\nat the pool surface, with a bias that tends to pull tracks away from the\\nvertical by about 3 degrees.\\n\",\n",
       " '  We present a method to improve the accuracy of a foot-mounted,\\nzero-velocity-aided inertial navigation system (INS) by varying estimator\\nparameters based on a real-time classification of motion type. We train a\\nsupport vector machine (SVM) classifier using inertial data recorded by a\\nsingle foot-mounted sensor to differentiate between six motion types (walking,\\njogging, running, sprinting, crouch-walking, and ladder-climbing) and report\\nmean test classification accuracy of over 90% on a dataset with five different\\nsubjects. From these motion types, we select two of the most common (walking\\nand running), and describe a method to compute optimal zero-velocity detection\\nparameters tailored to both a specific user and motion type by maximizing the\\ndetector F-score. By combining the motion classifier with a set of optimal\\ndetection parameters, we show how we can reduce INS position error during mixed\\nwalking and running motion. We evaluate our adaptive system on a total of 5.9\\nkm of indoor pedestrian navigation performed by five different subjects moving\\nalong a 130 m path with surveyed ground truth markers.\\n',\n",
       " '  The primary function of memory allocators is to allocate and deallocate\\nchunks of memory primarily through the malloc API. Many memory allocators also\\nimplement other API extensions, such as deriving the size of an allocated\\nobject from the object\\'s pointer, or calculating the base address of an\\nallocation from an interior pointer. In this paper, we propose a general\\npurpose extended allocator API built around these common extensions. We argue\\nthat such extended APIs have many applications and demonstrate several use\\ncases, such as (manual) memory error detection, meta data storage, typed\\npointers and compact data-structures. Because most existing allocators were not\\ndesigned for the extended API, traditional implementations are expensive or not\\npossible.\\nRecently, the LowFat allocator for heap and stack objects has been developed.\\nThe LowFat allocator is an implementation of the idea of low-fat pointers,\\nwhere object bounds information (size and base) are encoded into the native\\nmachine pointer representation itself. The \"killer app\" for low-fat pointers is\\nautomated bounds check instrumentation for program hardening and bug detection.\\nHowever, the LowFat allocator can also be used to implement highly optimized\\nversion of the extended allocator API, which makes the new applications (listed\\nabove) possible. In this paper, we implement and evaluate several applications\\nbased efficient memory allocator API extensions using low-fat pointers. We also\\nextend the LowFat allocator to cover global objects for the first time.\\n',\n",
       " '  We extend a data-based model-free multifractal method of exoplanet detection\\nto probe exoplanetary atmospheres. Whereas the transmission spectrum is studied\\nduring the primary eclipse, we analyze the emission spectrum during the\\nsecondary eclipse, thereby probing the atmospheric limb. In addition to the\\nspectral structure of exoplanet atmospheres, the approach provides information\\nto study phenomena such as atmospheric flows, tidal-locking behavior, and the\\ndayside-nightside redistribution of energy. The approach is demonstrated using\\nSpitzer data for exoplanet HD189733b. The central advantage of the method is\\nthe lack of model assumptions in the detection and observational schemes.\\n',\n",
       " '  We formulate and analyze a novel hypothesis testing problem for inferring the\\nedge structure of an infection graph. In our model, a disease spreads over a\\nnetwork via contagion or random infection, where the random variables governing\\nthe rates of contracting the disease from neighbors or random infection are\\nindependent exponential random variables with unknown rate parameters. A subset\\nof nodes is also censored uniformly at random. Given the statuses of nodes in\\nthe network, the goal is to determine the underlying graph. We present a\\nprocedure based on permutation testing, and we derive sufficient conditions for\\nthe validity of our test in terms of automorphism groups of the graphs\\ncorresponding to the null and alternative hypotheses. Further, the test is\\nvalid more generally for infection processes satisfying a basic symmetry\\ncondition. Our test is easy to compute and does not involve estimating unknown\\nparameters governing the process. We also derive risk bounds for our\\npermutation test in a variety of settings, and motivate our test statistic in\\nterms of approximate equivalence to likelihood ratio testing and maximin tests.\\nWe conclude with an application to real data from an HIV infection network.\\n',\n",
       " '  This work is motivated by a particular problem of a modern paper\\nmanufacturing industry, in which maximum efficiency of the fiber-filler\\nrecovery process is desired. A lot of unwanted materials along with valuable\\nfibers and fillers come out as a by-product of the paper manufacturing process\\nand mostly goes as waste. The job of an efficient Krofta supracell is to\\nseparate the unwanted materials from the valuable ones so that fibers and\\nfillers can be collected from the waste materials and reused in the\\nmanufacturing process. The efficiency of Krofta depends on several crucial\\nprocess parameters and monitoring them is a difficult proposition. To solve\\nthis problem, we propose a novel hybridization of regression trees (RT) and\\nartificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of\\nlow recovery percentage of the supracell. This model is used to achieve the\\ngoal of improving supracell efficiency, viz., gain in percentage recovery. In\\naddition, theoretical results for the universal consistency of the proposed\\nmodel are given with the optimal value of a vital model parameter. Experimental\\nfindings show that the proposed hybrid RT-ANN model achieves higher accuracy in\\npredicting Krofta recovery percentage than other conventional regression models\\nfor solving the Krofta efficiency problem. This work will help the paper\\nmanufacturing company to become environmentally friendly with minimal\\necological damage and improved waste recovery.\\n',\n",
       " '  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)\\ndetect images of the atmospheric showers generated by gamma rays and cosmic\\nrays as they are absorbed by the atmosphere. The much more frequent cosmic-ray\\nevents form the main background when looking for gamma-ray sources, and\\ntherefore IACT sensitivity is significantly driven by the capability to\\ndistinguish between these two types of events. Supervised learning algorithms,\\nlike random forests and boosted decision trees, have been shown to effectively\\nclassify IACT events. In this contribution we present results from exploratory\\nwork using deep learning as an event classification method for the Cherenkov\\nTelescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an\\ninternational project for a next-generation ground-based gamma-ray observatory,\\naiming to improve on the sensitivity of current-generation experiments by an\\norder of magnitude and provide energy coverage from 20 GeV to more than 300\\nTeV.\\n',\n",
       " '  Transition metal dichalcogenides (TMDs) are emerging as promising\\ntwo-dimensional (2d) semiconductors for optoelectronic and flexible devices.\\nHowever, a microscopic explanation of their photophysics -- of pivotal\\nimportance for the understanding and optimization of device operation -- is\\nstill lacking. Here we use femtosecond transient absorption spectroscopy, with\\npump pulse tunability and broadband probing, to monitor the relaxation dynamics\\nof single-layer MoS2 over the entire visible range, upon photoexcitation of\\ndifferent excitonic transitions. We find that, irrespective of excitation\\nphoton energy, the transient absorption spectrum shows the simultaneous\\nbleaching of all excitonic transitions and corresponding red-shifted\\nphotoinduced absorption bands. First-principle modeling of the ultrafast\\noptical response reveals that a transient bandgap renormalization, caused by\\nthe presence of photo-excited carriers, is primarily responsible for the\\nobserved features. Our results demonstrate the strong impact of many-body\\neffects in the transient optical response of TMDs even in the\\nlow-excitation-density regime.\\n',\n",
       " '  In a classical regression model, it is usually assumed that the explanatory\\nvariables are independent of each other and error terms are normally\\ndistributed. But when these assumptions are not met, situations like the error\\nterms are not independent or they are not identically distributed or both of\\nthese, LSE will not be robust. Hence, quantile regression has been used to\\ncomplement this deficiency of classical regression analysis and to improve the\\nleast square estimation (LSE). In this study, we consider preliminary test and\\nshrinkage estimation strategies for quantile regression models with\\nindependently and non-identically distributed (i.ni.d.) errors. A Monte Carlo\\nsimulation study is conducted to assess the relative performance of the\\nestimators. Also, we numerically compare their performance with Ridge, Lasso,\\nElastic Net penalty estimation strategies. A real data example is presented to\\nillustrate the usefulness of the suggested methods. Finally, we obtain the\\nasymptotic results of suggested estimators\\n',\n",
       " '  We present a clustering-based language model using word embeddings for text\\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\\nholds true for word embeddings whose training is done by observing word\\nco-occurrences. We argue that clustering with word embeddings in the metric\\nspace should yield feature representations in a higher semantic space\\nappropriate for text regression. Also, by representing features in terms of\\nhistograms, our approach can naturally address documents of varying lengths. An\\nempirical evaluation using the Common Core Standards corpus reveals that the\\nfeatures formed on our clustering-based language model significantly improve\\nthe previously known results for the same corpus in readability prediction. We\\nalso evaluate the task of sentence matching based on semantic relatedness using\\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\\nperformance.\\n',\n",
       " \"  We study special circle bundles over two elementary moduli spaces of\\nmeromorphic quadratic differentials with real periods denoted by $\\\\mathcal\\nQ_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$. The space\\n$\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ is the moduli space of meromorphic quadratic\\ndifferentials on the Riemann sphere with one pole of order 7 with real periods;\\nit appears naturally in the study of a neighbourhood of the Witten's cycle\\n$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic\\ndifferentials of $\\\\mathcal M_{g,n}$. The space $\\\\mathcal Q^{\\\\mathbb\\nR}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the\\nRiemann sphere with two poles of order at most 3 with real periods; it appears\\nin description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the\\ncombinatorial model. The application of the formalism of the Bergman\\ntau-function to the combinatorial model (with the goal of computing\\nanalytically Poincare dual cycles to certain combinations of tautological\\nclasses) requires the study of special sections of circle bundles over\\n$\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$; in the\\ncase of the space $\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ a section of this circle\\nbundle is given by the argument of the modular discriminant. We study the\\nspaces $\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$,\\nalso called the spaces of Boutroux curves, in detail, together with\\ncorresponding circle bundles.\\n\",\n",
       " \"  We report a method to control the positions of ellipsoidal magnets in flowing\\nchannels of rectangular or circular cross section at low Reynolds number.A\\nstatic uniform magnetic field is used to pin the particle orientation, and the\\nparticles move with translational drift velocities resulting from hydrodynamic\\ninteractions with the channel walls which can be described using Blake's image\\ntensor.Building on his insights, we are able to present a far-field theory\\npredicting the particle motion in rectangular channels, and validate the\\naccuracy of the theory by comparing to numerical solutions using the boundary\\nelement method.We find that, by changing the direction of the applied magnetic\\nfield, the motion can be controlled so that particles move either to a curved\\nfocusing region or to the channel walls.We also use simulations to show that\\nthe particles are focused to a single line in a circular channel.Our results\\nsuggest ways to focus and segregate magnetic particles in lab-on-a-chip\\ndevices.\\n\",\n",
       " \"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\\\\chi$ be a\\nDirichlet character of conductor coprime with $N,$ and let\\n$\\\\mathfrak{L}(f\\\\otimes \\\\chi, s)$ denote either $\\\\log L(f\\\\otimes \\\\chi, s)$ or\\n$(L'/L)(f\\\\otimes \\\\chi, s).$ In this article we study the distribution of the\\nvalues of $\\\\mathfrak{L}$ when either $\\\\chi$ or $f$ vary. First, for a\\nquasi-character $\\\\psi\\\\colon \\\\mathbb{C} \\\\to \\\\mathbb{C}^\\\\times$ we find the limit\\nfor the average $\\\\mathrm{Avg}\\\\_\\\\chi \\\\psi(L(f\\\\otimes\\\\chi, s)),$ when $f$ is\\nfixed and $\\\\chi$ varies through the set of characters with prime conductor that\\ntends to infinity. Second, we prove an equidistribution result for the values\\nof $\\\\mathfrak{L}(f\\\\otimes \\\\chi,s)$ by establishing analytic properties of the\\nabove limit function. Third, we study the limit of the harmonic average\\n$\\\\mathrm{Avg}^h\\\\_f \\\\psi(L(f, s)),$ when $f$ runs through the set of primitive\\ncusp forms of given weight $k$ and level $N\\\\to \\\\infty.$ Most of the results are\\nobtained conditionally on the Generalized Riemann Hypothesis for\\n$L(f\\\\otimes\\\\chi, s).$\\n\",\n",
       " '  Hierarchical graph clustering is a common technique to reveal the multi-scale\\nstructure of complex networks. We propose a novel metric for assessing the\\nquality of a hierarchical clustering. This metric reflects the ability to\\nreconstruct the graph from the dendrogram, which encodes the hierarchy. The\\noptimal representation of the graph defines a class of reducible linkages\\nleading to regular dendrograms by greedy agglomerative clustering.\\n',\n",
       " '  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced\\nlong-ranged dynamical heterogeneities as dynamical arrest is approached. Here\\nwe focus on the most direct approach to study these heterogeneities: we\\nidentify clusters of slow particles and determine their size, $N_c$, and their\\nradius of gyration, $R_G$. We show that $N_c\\\\propto R_G^{d_f}$, providing\\ndirect evidence that the most immobile particles arrange in fractal objects\\nwith a fractal dimension, $d_f$, that is observed to increase with packing\\nfraction $\\\\phi$. The cluster size distribution obeys scaling, approaching an\\nalgebraic decay in the limit of structural arrest, i.e., $\\\\phi\\\\to\\\\phi_c$.\\nAlternatively, dynamical heterogeneities are analyzed via the four-point\\nstructure factor $S_4(q,t)$ and the dynamical susceptibility $\\\\chi_4(t)$.\\n$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,\\n$0.6\\\\leq\\\\phi\\\\leq 0.805$, and to become increasingly long-ranged as\\n$\\\\phi\\\\to\\\\phi_c$. Finite size scaling of $\\\\chi_4(t)$ provides a consistency\\ncheck for the previously analyzed divergences of $\\\\chi_4(t)\\\\propto\\n(\\\\phi-\\\\phi_c)^{-\\\\gamma_{\\\\chi}}$ and the correlation length $\\\\xi\\\\propto\\n(\\\\phi-\\\\phi_c)^{-\\\\gamma_{\\\\xi}}$. We check the robustness of our results with\\nrespect to our definition of mobility. The divergences and the scaling for\\n$\\\\phi\\\\to\\\\phi_c$ suggest a non-equilibrium glass transition which seems\\nqualitatively independent of the coefficient of restitution.\\n',\n",
       " '  We study the \\\\emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm\\nintroduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured\\nconvex-concave saddle point problems consisting of the sum of a smooth convex\\nfunction, a finite collection of nonsmooth convex functions and bilinear terms.\\nWe introduce the notion of pointwise quadratic supportability, which is a\\nrelaxation of a standard strong convexity assumption and allows us to show that\\nthe primal sequence is R-linearly convergent to an optimal solution and the\\nprimal-dual sequence is globally Q-linearly convergent. We illustrate the\\nproposed method on total variation denoising problems and on locally adaptive\\nestimation in signal/image deconvolution and denoising with multiresolution\\nstatistical constraints.\\n',\n",
       " '  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their\\nstability against dissociating agents. In this paper, a comparative study of\\nstability between native CMs and CMs cross-linked with genipin (CMs-GP) as a\\nfunction of pH is described. Stability to temperature and ethanol were\\ninvestigated in the pH range 2.0-7.0. The size and the charge\\n($\\\\zeta$-potential) of the particles were determined by dynamic light\\nscattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH\\n3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.\\nThe isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability\\nagainst heat and ethanol was observed for CMs-GP at pH 2, where visible\\ncoagulation was determined only after 800 s at 140 $^\\\\circ$C or 87.5% (v/v) of\\nethanol. These results confirmed the hypothesis that cross-linking by GP\\nincreased the stability of CMs.\\n',\n",
       " '  We investigate a construction of an integral residuated lattice starting from\\nan integral residuated lattice and two sets with an injective mapping from one\\nset into the second one. The resulting algebra has a shape of a Chinese cascade\\nkite, therefore, we call this algebra simply a kite. We describe subdirectly\\nirreducible kites and we classify them. We show that the variety of integral\\nresiduated lattices generated by kites is generated by all finite-dimensional\\nkites. In particular, we describe some homomorphisms among kites.\\n',\n",
       " '  Nowadays, online video platforms mostly recommend related videos by analyzing\\nuser-driven data such as viewing patterns, rather than the content of the\\nvideos. However, content is more important than any other element when videos\\naim to deliver knowledge. Therefore, we have developed a web application which\\nrecommends related TED lecture videos to the users, considering the content of\\nthe videos from the transcripts. TED Talk Recommender constructs a network for\\nrecommending videos that are similar content-wise and providing a user\\ninterface.\\n',\n",
       " '  Exploration of asteroids and small-bodies can provide valuable insight into\\nthe origins of the solar system, into the origins of Earth and the origins of\\nthe building blocks of life. However, the low-gravity and unknown surface\\nconditions of asteroids presents a daunting challenge for surface exploration,\\nmanipulation and for resource processing. This has resulted in the loss of\\nseveral landers or shortened missions. Fundamental studies are required to\\nobtain better readings of the material surface properties and physical models\\nof these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat\\ncentrifuge laboratory that spins at up to 4 rpm to simulate the milligravity\\nconditions of sub 1 km asteroids. Such a laboratory will help to de-risk\\ndevelopment and testing of landing and resource processing technology for\\nasteroids. Inside the laboratory are crushed meteorites, the remains of\\nasteroids. The laboratory is equipped with cameras and actuators to perform a\\nseries of science experiments to better understand material properties and\\nasteroid surface physics. These results will help to improve our physics models\\nof asteroids. The CubeSat has been designed to be low-cost and contains 3-axis\\nmagnetorquers and a single reaction-wheel to induce spin. In our work, we first\\nanalyze how the attitude control system will de-tumble the spacecraft after\\ndeployment. Further analysis has been conducted to analyze the impact and\\nstability of the attitude control system to shifting mass (crushed meteorites)\\ninside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be\\nthe first in a series of low-cost CubeSat centrifuges that will be launched\\nsetting the stage for a larger, permanent, on-orbit centrifuge laboratory for\\nexperiments in planetary science, life sciences and manufacturing.\\n',\n",
       " '  In automatic speech processing systems, speaker diarization is a crucial\\nfront-end component to separate segments from different speakers. Inspired by\\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\\ntriplet loss-based architectures have been successfully used for this problem.\\nHowever, existing work utilizes conventional i-vectors as the input\\nrepresentation and builds simple fully connected networks for metric learning,\\nthus not fully leveraging the modeling power of DNN architectures. This paper\\ninvestigates the importance of learning effective representations from the\\nsequences directly in metric learning pipelines for speaker diarization. More\\nspecifically, we propose to employ attention models to learn embeddings and the\\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\\nCALLHOME conversational speech corpus. The diarization results demonstrate\\nthat, besides providing a unified model, the proposed approach achieves\\nimproved performance when compared against existing approaches.\\n',\n",
       " '  Predicting when rupture occurs or cracks progress is a major challenge in\\nnumerous elds of industrial, societal and geophysical importance. It remains\\nlargely unsolved: Stress enhancement at cracks and defects, indeed, makes the\\nmacroscale dynamics extremely sensitive to the microscale material disorder.\\nThis results in giant statistical uctuations and non-trivial behaviors upon\\nupscaling dicult to assess via the continuum approaches of engineering. These\\nissues are examined here. We will see: How linear elastic fracture mechanics\\nsidetracks the diculty by reducing the problem to that of the propagation of a\\nsingle crack in an eective material free of defects, How slow cracks sometimes\\ndisplay jerky dynamics, with sudden violent events incompatible with the\\nprevious approach, and how some paradigms of statistical physics can explain\\nit, How abnormally fast cracks sometimes emerge due to the formation of\\nmicrocracks at very small scales.\\n',\n",
       " '  This paper investigates the multiplicative spread spectrum watermarking\\nmethod for the image. The information bit is spreaded into middle-frequency\\nDiscrete Cosine Transform (DCT) coefficients of each block of an image using a\\ngenerated pseudo-random sequence. Unlike the conventional signal modeling, we\\nsuppose that both signal and noise are distributed with Laplacian distribution\\nbecause the sample loss of digital media can be better modeled with this\\ndistribution than the Gaussian one. We derive the optimum decoder for the\\nproposed embedding method thanks to the maximum likelihood decoding scheme. We\\nalso analyze our watermarking system in the presence of noise and provide\\nanalytical evaluations and several simulations. The results show that it has\\nthe suitable performance and transparency required for watermarking\\napplications.\\n',\n",
       " '  We present a representation learning algorithm that learns a low-dimensional\\nlatent dynamical system from high-dimensional \\\\textit{sequential} raw data,\\ne.g., video. The framework builds upon recent advances in amortized inference\\nmethods that use both an inference network and a refinement procedure to output\\nsamples from a variational distribution given an observation sequence, and\\ntakes advantage of the duality between control and inference to approximately\\nsolve the intractable inference problem using the path integral control\\napproach. The learned dynamical model can be used to predict and plan the\\nfuture states; we also present the efficient planning method that exploits the\\nlearned low-dimensional latent dynamics. Numerical experiments show that the\\nproposed path-integral control based variational inference method leads to\\ntighter lower bounds in statistical model learning of sequential data. The\\nsupplementary video: this https URL\\n',\n",
       " \"  Consider a social network where only a few nodes (agents) have meaningful\\ninteractions in the sense that the conditional dependency graph over node\\nattribute variables (behaviors) is sparse. A company that can only observe the\\ninteractions between its own customers will generally not be able to accurately\\nestimate its customers' dependency subgraph: it is blinded to any external\\ninteractions of its customers and this blindness creates false edges in its\\nsubgraph. In this paper we address the semiblind scenario where the company has\\naccess to a noisy summary of the complementary subgraph connecting external\\nagents, e.g., provided by a consolidator. The proposed framework applies to\\nother applications as well, including field estimation from a network of awake\\nand sleeping sensors and privacy-constrained information sharing over social\\nsubnetworks. We propose a penalized likelihood approach in the context of a\\ngraph signal obeying a Gaussian graphical models (GGM). We use a convex-concave\\niterative optimization algorithm to maximize the penalized likelihood.\\n\",\n",
       " '  In this paper, we present a novel structure, Semi-AutoEncoder, based on\\nAutoEncoder. We generalize it into a hybrid collaborative filtering model for\\nrating prediction as well as personalized top-n recommendations. Experimental\\nresults on two real-world datasets demonstrate its state-of-the-art\\nperformances.\\n',\n",
       " '  We show how a characteristic length scale imprinted in the galaxy two-point\\ncorrelation function, dubbed the \"linear point\", can serve as a comoving\\ncosmological standard ruler. In contrast to the Baryon Acoustic Oscillation\\npeak location, this scale is constant in redshift and is unaffected by\\nnon-linear effects to within $0.5$ percent precision. We measure the location\\nof the linear point in the galaxy correlation function of the LOWZ and CMASS\\nsamples from the Twelfth Data Release (DR12) of the Baryon Oscillation\\nSpectroscopic Survey (BOSS) collaboration. We combine our linear-point\\nmeasurement with cosmic-microwave-background constraints from the Planck\\nsatellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying\\non a model-template or reconstruction method. We find $D_V(0.32)=1264\\\\pm 28$\\nMpc and $D_V(0.57)=2056\\\\pm 22$ Mpc respectively, consistent with the quoted\\nvalues from the BOSS collaboration. This remarkable result suggests that all\\nthe distance information contained in the baryon acoustic oscillations can be\\nconveniently compressed into the single length associated with the linear\\npoint.\\n',\n",
       " '  Starting from isentropic compressible Navier-Stokes equations with growth\\nterm in the continuity equation, we rigorously justify that performing an\\nincompressible limit one arrives to the two-phase free boundary fluid system.\\n',\n",
       " \"  We study a stochastic primal-dual method for constrained optimization over\\nRiemannian manifolds with bounded sectional curvature. We prove non-asymptotic\\nconvergence to the optimal objective value. More precisely, for the class of\\nhyperbolic manifolds, we establish a convergence rate that is related to the\\nsectional curvature lower bound. To prove a convergence rate in terms of\\nsectional curvature for the elliptic manifolds, we leverage Toponogov's\\ncomparison theorem. In addition, we provide convergence analysis for the\\nasymptotically elliptic manifolds, where the sectional curvature at each given\\npoint on manifold is locally bounded from below by the distance function. We\\ndemonstrate the performance of the primal-dual algorithm on the sphere for the\\nnon-negative principle component analysis (PCA). In particular, under the\\nnon-negativity constraint on the principle component and for the symmetric\\nspiked covariance model, we empirically show that the primal-dual approach\\noutperforms the spectral method. We also examine the performance of the\\nprimal-dual method for the anchored synchronization from partial noisy\\nmeasurements of relative rotations on the Lie group SO(3). Lastly, we show that\\nthe primal-dual algorithm can be applied to the weighted MAX-CUT problem under\\nconstraints on the admissible cut. Specifically, we propose different\\napproximation algorithms for the weighted MAX-CUT problem based on optimizing a\\nfunction on the manifold of direct products of the unit spheres as well as the\\nmanifold of direct products of the rotation groups.\\n\",\n",
       " '  We investigate a new sampling scheme aimed at improving the performance of\\nparticle filters whenever (a) there is a significant mismatch between the\\nassumed model dynamics and the actual system, or (b) the posterior probability\\ntends to concentrate in relatively small regions of the state space. The\\nproposed scheme pushes some particles towards specific regions where the\\nlikelihood is expected to be high, an operation known as nudging in the\\ngeophysics literature. We re-interpret nudging in a form applicable to any\\nparticle filtering scheme, as it does not involve any changes in the rest of\\nthe algorithm. Since the particles are modified, but the importance weights do\\nnot account for this modification, the use of nudging leads to additional bias\\nin the resulting estimators. However, we prove analytically that nudged\\nparticle filters can still attain asymptotic convergence with the same error\\nrates as conventional particle methods. Simple analysis also yields an\\nalternative interpretation of the nudging operation that explains its\\nrobustness to model errors. Finally, we show numerical results that illustrate\\nthe improvements that can be attained using the proposed scheme. In particular,\\nwe present nonlinear tracking examples with synthetic data and a model\\ninference example using real-world financial data.\\n',\n",
       " '  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the\\nnoncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet\\nFermi surface are identified, including a small pocket (frequency 40 T) which\\nis three dimensional and anisotropic. From the temperature dependence of the\\namplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\\\\pm$\\n0.1) $m_e$. Further analysis showed a non-trivial $\\\\pi$-Berry phase is\\nassociated with the 40 T pocket, which strongly supports the presence of\\ntopological states in bulk BiPd and may result in topological superconductivity\\ndue to the proximity coupling to other bands.\\n',\n",
       " '  Statistical inference can be computationally prohibitive in\\nultrahigh-dimensional linear models. Correlation-based variable screening, in\\nwhich one leverages marginal correlations for removal of irrelevant variables\\nfrom the model prior to statistical inference, can be used to overcome this\\nchallenge. Prior works on correlation-based variable screening either impose\\nstrong statistical priors on the linear model or assume specific post-screening\\ninference methods. This paper first extends the analysis of correlation-based\\nvariable screening to arbitrary linear models and post-screening inference\\ntechniques. In particular, ($i$) it shows that a condition---termed the\\nscreening condition---is sufficient for successful correlation-based screening\\nof linear models, and ($ii$) it provides insights into the dependence of\\nmarginal correlation-based screening on different problem parameters. Numerical\\nexperiments confirm that these insights are not mere artifacts of analysis;\\nrather, they are reflective of the challenges associated with marginal\\ncorrelation-based variable screening. Second, the paper explicitly derives the\\nscreening condition for two families of linear models, namely, sub-Gaussian\\nlinear models and arbitrary (random or deterministic) linear models. In the\\nprocess, it establishes that---under appropriate conditions---it is possible to\\nreduce the dimension of an ultrahigh-dimensional, arbitrary linear model to\\nalmost the sample size even when the number of active variables scales almost\\nlinearly with the sample size.\\n',\n",
       " \"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy\\ncurrency of life. Chemiosmosis, a proton centric mechanism, advocates that\\nComplex V harnesses a transmembrane potential (TMP) for ATP synthesis. This\\nperception of cellular respiration requires oxygen to stay tethered at Complex\\nIV (an association inhibited by cyanide) and diffusible reactive oxygen species\\n(DROS) are considered wasteful and toxic products. With new mechanistic\\ninsights on heme and flavin enzymes, an oxygen or DROS centric explanation\\n(called murburn concept) was recently proposed for mOxPhos. In the new\\nmechanism, TMP is not directly harnessed, protons are a rate limiting reactant\\nand DROS within matrix serve as the chemical coupling agents that directly link\\nNADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites\\nand solvent accessible DROS channels in respiratory proteins, which validate\\nthe oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.\\nSince cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is\\nlethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study\\nalso provides comprehensive arguments against Mitchell's and Boyer's\\nexplanations and extensive support for murburn concept based holistic\\nperspectives for mOxPhos.\\n\",\n",
       " '  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling\\nStormer for special JB-algebras, we prove that a synaptic algebra is norm\\ncomplete (i.e., Banach) if and only if it is isomorphic to the self-adjoint\\npart of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic\\nalgebra that are equivalent to the condition that it is isomorphic to the\\nself-adjoint part of an AW*-algebra. Moreover, we study some relationships\\nbetween synaptic algebras and so-called generalized Hermitian algebras.\\n',\n",
       " \"  High-pressure neutron powder diffraction, muon-spin rotation and\\nmagnetization studies of the structural, magnetic and the superconducting\\nproperties of the Ce-underdoped superconducting (SC) electron-doped cuprate\\nsystem T'-Pr_1.3-xLa_0.7Ce_xCuO_4 with x = 0.1 are reported. A strong reduction\\nof the lattice constants a and c is observed under pressure. However, no\\nindication of any pressure induced phase transition from T' to T structure is\\nobserved up to the maximum applied pressure of p = 11 GPa. Large and non-linear\\nincrease of the short-range magnetic order temperature T_so in\\nT'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1) was observed under pressure.\\nSimultaneously pressure causes a non-linear decrease of the SC transition\\ntemperature T_c. All these experiments establish the short-range magnetic order\\nas an intrinsic and a new competing phase in SC T'-Pr_1.2La_0.7Ce_0.1CuO_4. The\\nobserved pressure effects may be interpreted in terms of the improved nesting\\nconditions through the reduction of the in-plane and out-of-plane lattice\\nconstants upon hydrostatic pressure.\\n\",\n",
       " '  This paper presents a fixturing strategy for regrasping that does not require\\na physical fixture. To regrasp an object in a gripper, a robot pushes the\\nobject against external contact/s in the environment such that the external\\ncontact keeps the object stationary while the fingers slide over the object. We\\ncall this manipulation technique fixtureless fixturing. Exploiting the\\nmechanics of pushing, we characterize a convex polyhedral set of pushes that\\nresults in fixtureless fixturing. These pushes are robust against uncertainty\\nin the object inertia, grasping force, and the friction at the contacts. We\\npropose a sampling-based planner that uses the sets of robust pushes to rapidly\\nbuild a tree of reachable grasps. A path in this tree is a pushing strategy,\\npossibly involving pushes from different sides, to regrasp the object. We\\ndemonstrate the experimental validity and robustness of the proposed\\nmanipulation technique with different regrasp examples on a manipulation\\nplatform. Such a fast and flexible regrasp planner facilitates versatile and\\nflexible automation solutions.\\n',\n",
       " '  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard\\nunsupervised Latent Dirichlet Allocation (LDA) algorithm, to address\\nmulti-label learning tasks. Previous work has shown it to perform in par with\\nother state-of-the-art multi-label methods. Nonetheless, with increasing label\\nsets sizes LLDA encounters scalability issues. In this work, we introduce\\nSubset LLDA, a simple variant of the standard LLDA algorithm, that not only can\\neffectively scale up to problems with hundreds of thousands of labels but also\\nimproves over the LLDA state-of-the-art. We conduct extensive experiments on\\neight data sets, with label sets sizes ranging from hundreds to hundreds of\\nthousands, comparing our proposed algorithm with the previously proposed LLDA\\nalgorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme\\nmulti-label classification. The results show a steady advantage of our method\\nover the other LLDA algorithms and competitive results compared to the extreme\\nmulti-label classification algorithms.\\n',\n",
       " '  Multimedia Forensics allows to determine whether videos or images have been\\ncaptured with the same device, and thus, eventually, by the same person.\\nCurrently, the most promising technology to achieve this task, exploits the\\nunique traces left by the camera sensor into the visual content. Anyway, image\\nand video source identification are still treated separately from one another.\\nThis approach is limited and anachronistic if we consider that most of the\\nvisual media are today acquired using smartphones, that capture both images and\\nvideos. In this paper we overcome this limitation by exploring a new approach\\nthat allows to synergistically exploit images and videos to study the device\\nfrom which they both come. Indeed, we prove it is possible to identify the\\nsource of a digital video by exploiting a reference sensor pattern noise\\ngenerated from still images taken by the same device of the query video. The\\nproposed method provides comparable or even better performance, when compared\\nto the current video identification strategies, where a reference pattern is\\nestimated from video frames. We also show how this strategy can be effective\\neven in case of in-camera digitally stabilized videos, where a non-stabilized\\nreference is not available, by solving some state-of-the-art limitations. We\\nexplore a possible direct application of this result, that is social media\\nprofile linking, i.e. discovering relationships between two or more social\\nmedia profiles by comparing the visual contents - images or videos - shared\\ntherein.\\n',\n",
       " '  We present an informal review of recent work on the asymptotics of\\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\\nhave more data? The results we review show that ABC can perform well in terms\\nof point estimation, but standard implementations will over-estimate the\\nuncertainty about the parameters. If we use the regression correction of\\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\\ntheoretical results also have practical implications for how to implement ABC.\\n',\n",
       " '  We consider the problem of learning sparse polymatrix games from observations\\nof strategic interactions. We show that a polynomial time method based on\\n$\\\\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash\\nequilibria are the $\\\\epsilon$-Nash equilibria of the game from which the data\\nwas generated (true game), in $\\\\mathcal{O}(m^4 d^4 \\\\log (pd))$ samples of\\nstrategy profiles --- where $m$ is the maximum number of pure strategies of a\\nplayer, $p$ is the number of players, and $d$ is the maximum degree of the game\\ngraph. Under slightly more stringent separability conditions on the payoff\\nmatrices of the true game, we show that our method learns a game with the exact\\nsame Nash equilibria as the true game. We also show that $\\\\Omega(d \\\\log (pm))$\\nsamples are necessary for any method to consistently recover a game, with the\\nsame Nash-equilibria as the true game, from observations of strategic\\ninteractions. We verify our theoretical results through simulation experiments.\\n',\n",
       " '  The KdV equation can be derived in the shallow water limit of the Euler\\nequations. Over the last few decades, this equation has been extended to\\ninclude higher order effects. Although this equation has only one conservation\\nlaw, exact periodic and solitonic solutions exist. Khare and Saxena\\n\\\\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact\\nsolutions by combining known ones for several fundamental equations (e.g.,\\nKorteweg - de Vries, Nonlinear Schrödinger). Here we find that this\\nconstruction can be repeated for higher order, non-integrable extensions of\\nthese equations. Contrary to many statements in the literature, there seems to\\nbe no correlation between integrability and the number of nonlinear one\\nvariable wave solutions.\\n',\n",
       " '  This paper proposes a new actor-critic-style algorithm called Dual\\nActor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian\\ndual form of the Bellman optimality equation, which can be viewed as a\\ntwo-player game between the actor and a critic-like function, which is named as\\ndual critic. Compared to its actor-critic relatives, Dual-AC has the desired\\nproperty that the actor and dual critic are updated cooperatively to optimize\\nthe same objective function, providing a more transparent way for learning the\\ncritic that is directly related to the objective function of the actor. We then\\nprovide a concrete algorithm that can effectively solve the minimax\\noptimization problem, using techniques of multi-step bootstrapping, path\\nregularization, and stochastic dual ascent algorithm. We demonstrate that the\\nproposed algorithm achieves the state-of-the-art performances across several\\nbenchmarks.\\n',\n",
       " '  Counting dominating sets in a graph $G$ is closely related to the\\nneighborhood complex of $G$. We exploit this relation to prove that the number\\nof dominating sets $d(G)$ of a graph is determined by the number of complete\\nbipartite subgraphs of its complement. More precisely, we state the following.\\nLet $G$ be a simple graph of order $n$ such that its complement has exactly\\n$a(G)$ subgraphs isomorphic to $K_{2p,2q}$ and exactly $b(G)$ subgraphs\\nisomorphic to $K_{2p+1,2q+1}$. Then $d(G) = 2^n -1 + 2[a(G)-b(G)]$. We also\\nshow some new relations between the domination polynomial and the neighborhood\\npolynomial of a graph.\\n',\n",
       " '  High signal to noise ratio (SNR) consistency of model selection criteria in\\nlinear regression models has attracted a lot of attention recently. However,\\nmost of the existing literature on high SNR consistency deals with model order\\nselection. Further, the limited literature available on the high SNR\\nconsistency of subset selection procedures (SSPs) is applicable to linear\\nregression with full rank measurement matrices only. Hence, the performance of\\nSSPs used in underdetermined linear models (a.k.a compressive sensing (CS)\\nalgorithms) at high SNR is largely unknown. This paper fills this gap by\\nderiving necessary and sufficient conditions for the high SNR consistency of\\npopular CS algorithms like $l_0$-minimization, basis pursuit de-noising or\\nLASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions\\nanalytically establish the high SNR inconsistency of CS algorithms when used\\nwith the tuning parameters discussed in literature. Novel tuning parameters\\nwith SNR adaptations are developed using the sufficient conditions and the\\nchoice of SNR adaptations are discussed analytically using convergence rate\\nanalysis. CS algorithms with the proposed tuning parameters are numerically\\nshown to be high SNR consistent and outperform existing tuning parameters in\\nthe moderate to high SNR regime.\\n',\n",
       " \"  Reduction of communication and efficient partitioning are key issues for\\nachieving scalability in hierarchical $N$-Body algorithms like FMM. In the\\npresent work, we propose four independent strategies to improve partitioning\\nand reduce communication. First of all, we show that the conventional wisdom of\\nusing space-filling curve partitioning may not work well for boundary integral\\nproblems, which constitute about 50% of FMM's application user base. We propose\\nan alternative method which modifies orthogonal recursive bisection to solve\\nthe cell-partition misalignment that has kept it from scaling previously.\\nSecondly, we optimize the granularity of communication to find the optimal\\nbalance between a bulk-synchronous collective communication of the local\\nessential tree and an RDMA per task per cell. Finally, we take the dynamic\\nsparse data exchange proposed by Hoefler et al. and extend it to a hierarchical\\nsparse data exchange, which is demonstrated at scale to be faster than the MPI\\nlibrary's MPI_Alltoallv that is commonly used.\\n\",\n",
       " '  In this paper, we focus on fully automatic traffic surveillance camera\\ncalibration, which we use for speed measurement of passing vehicles. We improve\\nover a recent state-of-the-art camera calibration method for traffic\\nsurveillance based on two detected vanishing points. More importantly, we\\npropose a novel automatic scene scale inference method. The method is based on\\nmatching bounding boxes of rendered 3D models of vehicles with detected\\nbounding boxes in the image. The proposed method can be used from arbitrary\\nviewpoints, since it has no constraints on camera placement. We evaluate our\\nmethod on the recent comprehensive dataset for speed measurement BrnoCompSpeed.\\nExperiments show that our automatic camera calibration method by detection of\\ntwo vanishing points reduces error by 50% (mean distance ratio error reduced\\nfrom 0.18 to 0.09) compared to the previous state-of-the-art method. We also\\nshow that our scene scale inference method is more precise, outperforming both\\nstate-of-the-art automatic calibration method for speed measurement (error\\nreduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error\\nreduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results\\nof the proposed automatic camera calibration method on video sequences obtained\\nfrom real surveillance cameras in various places, and under different lighting\\nconditions (night, dawn, day).\\n',\n",
       " '  The success of autonomous systems will depend upon their ability to safely\\nnavigate human-centric environments. This motivates the need for a real-time,\\nprobabilistic forecasting algorithm for pedestrians, cyclists, and other agents\\nsince these predictions will form a necessary step in assessing the risk of any\\naction. This paper presents a novel approach to probabilistic forecasting for\\npedestrians based on weighted sums of ordinary differential equations that are\\nlearned from historical trajectory information within a fixed scene. The\\nresulting algorithm is embarrassingly parallel and is able to work at real-time\\nspeeds using a naive Python implementation. The quality of predicted locations\\nof agents generated by the proposed algorithm is validated on a variety of\\nexamples and considerably higher than existing state of the art approaches over\\nlong time horizons.\\n',\n",
       " '  This paper presents a distance-based discriminative framework for learning\\nwith probability distributions. Instead of using kernel mean embeddings or\\ngeneralized radial basis kernels, we introduce embeddings based on\\ndissimilarity of distributions to some reference distributions denoted as\\ntemplates. Our framework extends the theory of similarity of Balcan et al.\\n(2008) to the population distribution case and we show that, for some learning\\nproblems, some dissimilarity on distribution achieves low-error linear decision\\nfunctions with high probability. Our key result is to prove that the theory\\nalso holds for empirical distributions. Algorithmically, the proposed approach\\nconsists in computing a mapping based on pairwise dissimilarity where learning\\na linear decision function is amenable. Our experimental results show that the\\nWasserstein distance embedding performs better than kernel mean embeddings and\\ncomputing Wasserstein distance is far more tractable than estimating pairwise\\nKullback-Leibler divergence of empirical distributions.\\n',\n",
       " '  Molecular reflections on usual wall surfaces can be statistically described\\nby the Maxwell diffuse reflection model, which has been successfully applied in\\nthe DSBGK simulations. We develop the DSBGK algorithm to implement the\\nCercignani-Lampis-Lord (CLL) reflection model, which is widely applied to\\npolished surfaces and used particularly in modeling space shuttles to predict\\nthe heat and force loads exerted by the high-speed flows around the surfaces.\\nWe also extend the DSBGK method to simulate gas mixtures and high contrast of\\nnumber densities of different components can be handled at a cost of memory\\nusage much lower than that needed by the DSMC simulations because the average\\nnumbers of simulated molecules of different components per cell can be equal in\\nthe DSBGK simulations.\\n',\n",
       " '  Let $f(a,b,c,d)=\\\\sqrt{a^2+b^2}+\\\\sqrt{c^2+d^2}-\\\\sqrt{(a+c)^2+(b+d)^2}$, let\\n$(a,b,c,d)$ stand for $a,b,c,d\\\\in\\\\mathbb Z_{\\\\geq 0}$ such that $ad-bc=1$.\\nDefine \\\\begin{equation} \\\\label{eq_main} F(s) = \\\\sum_{(a,b,c,d)} f(a,b,c,d)^s.\\n\\\\end{equation} In other words, we consider the sum of the powers of the\\ntriangle inequality defects for the lattice parallelograms (in the first\\nquadrant) of area one.\\nWe prove that $F(s)$ converges when $s>1/2$ and diverges at $s=1/2$. We also\\nprove $$\\\\sum\\\\limits_{\\\\substack{(a,b,c,d),\\\\\\\\ 1\\\\leq a\\\\leq b, 1\\\\leq c\\\\leq d}}\\n\\\\frac{1}{(a+b)^2(c+d)^2(a+b+c+d)^2} = 1/24,$$ and show a general method to\\nobtain such formulae. The method comes from the consideration of the tropical\\nanalogue of the caustic curves, whose moduli give a complete set of continuous\\ninvariants on the space of convex domains.\\n',\n",
       " '  We consider the task of generating draws from a Markov jump process (MJP)\\nbetween two time points at which the process is known. Resulting draws are\\ntypically termed bridges and the generation of such bridges plays a key role in\\nsimulation-based inference algorithms for MJPs. The problem is challenging due\\nto the intractability of the conditioned process, necessitating the use of\\ncomputationally intensive methods such as weighted resampling or Markov chain\\nMonte Carlo. An efficient implementation of such schemes requires an\\napproximation of the intractable conditioned hazard/propensity function that is\\nboth cheap and accurate. In this paper, we review some existing approaches to\\nthis problem before outlining our novel contribution. Essentially, we leverage\\nthe tractability of a Gaussian approximation of the MJP and suggest a\\ncomputationally efficient implementation of the resulting conditioned hazard\\napproximation. We compare and contrast our approach with existing methods using\\nthree examples.\\n',\n",
       " '  Using holography, we model experiments in which a 2+1D strange metal is\\npumped by a laser pulse into a highly excited state, after which the time\\nevolution of the optical conductivity is probed. We consider a finite-density\\nstate with mildly broken translation invariance and excite it by oscillating\\nelectric field pulses. At zero density, the optical conductivity would assume\\nits thermalized value immediately after the pumping has ended. At finite\\ndensity, pulses with significant DC components give rise to slow exponential\\nrelaxation, governed by a vector quasinormal mode. In contrast, for\\nhigh-frequency pulses the amplitude of the quasinormal mode is strongly\\nsuppressed, so that the optical conductivity assumes its thermalized value\\neffectively instantaneously. This surprising prediction may provide a stimulus\\nfor taking up the challenge to realize these experiments in the laboratory.\\nSuch experiments would test a crucial open question faced by applied\\nholography: Are its predictions artefacts of the large $N$ limit or do they\\nenjoy sufficient UV independence to hold at least qualitatively in real-world\\nsystems?\\n',\n",
       " '  In this paper, we study random subsampling of Gaussian process regression,\\none of the simplest approximation baselines, from a theoretical perspective.\\nAlthough subsampling discards a large part of training data, we show provable\\nguarantees on the accuracy of the predictive mean/variance and its\\ngeneralization ability. For analysis, we consider embedding kernel matrices\\ninto graphons, which encapsulate the difference of the sample size and enables\\nus to evaluate the approximation and generalization errors in a unified manner.\\nThe experimental results show that the subsampling approximation achieves a\\nbetter trade-off regarding accuracy and runtime than the Nyström and random\\nFourier expansion methods.\\n',\n",
       " '  Both hybrid automata and action languages are formalisms for describing the\\nevolution of dynamic systems. This paper establishes a formal relationship\\nbetween them. We show how to succinctly represent hybrid automata in an action\\nlanguage which in turn is defined as a high-level notation for answer set\\nprogramming modulo theories (ASPMT) --- an extension of answer set programs to\\nthe first-order level similar to the way satisfiability modulo theories (SMT)\\nextends propositional satisfiability (SAT). We first show how to represent\\nlinear hybrid automata with convex invariants by an action language modulo\\ntheories. A further translation into SMT allows for computing them using SMT\\nsolvers that support arithmetic over reals. Next, we extend the representation\\nto the general class of non-linear hybrid automata allowing even non-convex\\ninvariants. We represent them by an action language modulo ODE (Ordinary\\nDifferential Equations), which can be compiled into satisfiability modulo ODE.\\nWe developed a prototype system cplus2aspmt based on these translations, which\\nallows for a succinct representation of hybrid transition systems that can be\\ncomputed effectively by the state-of-the-art SMT solver dReal.\\n',\n",
       " '  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice\\nBoltzmann (LB) method is developed for solid-liquid phase change heat transfer\\nin metal foams under local thermal non-equilibrium (LTNE) condition. The\\nenthalpy-based MRT-LB method consists of three different MRT-LB models: one for\\nflow field based on the generalized non-Darcy model, and the other two for\\nphase change material (PCM) and metal foam temperature fields described by the\\nLTNE model. The moving solid-liquid phase interface is implicitly tracked\\nthrough the liquid fraction, which is simultaneously obtained when the energy\\nequations of PCM and metal foam are solved. The present method has several\\ndistinctive features. First, as compared with previous studies, the present\\nmethod avoids the iteration procedure, thus it retains the inherent merits of\\nthe standard LB method and is superior over the iteration method in terms of\\naccuracy and computational efficiency. Second, a volumetric LB scheme instead\\nof the bounce-back scheme is employed to realize the no-slip velocity condition\\nin the interface and solid phase regions, which is consistent with the actual\\nsituation. Last but not least, the MRT collision model is employed, and with\\nadditional degrees of freedom, it has the ability to reduce the numerical\\ndiffusion across phase interface induced by solid-liquid phase change.\\nNumerical tests demonstrate that the present method can be served as an\\naccurate and efficient numerical tool for studying metal foam enhanced\\nsolid-liquid phase change heat transfer in latent heat storage. Finally,\\ncomparisons and discussions are made to offer useful information for practical\\napplications of the present method.\\n',\n",
       " '  Barchan dunes are crescentic shape dunes with horns pointing downstream. The\\npresent paper reports the formation of subaqueous barchan dunes from initially\\nconical heaps in a rectangular channel. Because the most unique feature of a\\nbarchan dune is its horns, we associate the timescale for the appearance of\\nhorns to the formation of a barchan dune. A granular heap initially conical was\\nplaced on the bottom wall of a closed conduit and it was entrained by a water\\nflow in turbulent regime. After a certain time, horns appear and grow, until an\\nequilibrium length is reached. Our results show the existence of the timescales\\n$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,\\nrespectively, where $t_c$ is a characteristic time that scales with the grains\\ndiameter, gravity acceleration, densities of the fluid and grains, and shear\\nand threshold velocities.\\n',\n",
       " '  Isotonic regression is a standard problem in shape-constrained estimation\\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\\nfrom independent pairs $(x_i, y_i)$ where $\\\\mathbb{E}[y_i]=f(x_i), i=1, \\\\ldots\\nn$. While this problem is well understood both statistically and\\ncomputationally, much less is known about its uncoupled counterpart where one\\nis given only the unordered sets $\\\\{x_1, \\\\ldots, x_n\\\\}$ and $\\\\{y_1, \\\\ldots,\\ny_n\\\\}$. In this work, we leverage tools from optimal transport theory to derive\\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\\nalgorithm achieving optimal rates. Both upper and lower bounds employ\\nmoment-matching arguments that are also pertinent to learning mixtures of\\ndistributions and deconvolution.\\n',\n",
       " '  This paper considers a time-inconsistent stopping problem in which the\\ninconsistency arises from non-constant time preference rates. We show that the\\nsmooth pasting principle, the main approach that has been used to construct\\nexplicit solutions for conventional time-consistent optimal stopping problems,\\nmay fail under time-inconsistency. Specifically, we prove that the smooth\\npasting principle solves a time-inconsistent problem within the intra-personal\\ngame theoretic framework if and only if a certain inequality on the model\\nprimitives is satisfied. We show that the violation of this inequality can\\nhappen even for very simple non-exponential discount functions. Moreover, we\\ndemonstrate that the stopping problem does not admit any intra-personal\\nequilibrium whenever the smooth pasting principle fails. The \"negative\" results\\nin this paper caution blindly extending the classical approaches for\\ntime-consistent stopping problems to their time-inconsistent counterparts.\\n',\n",
       " '  The Internet of Things (IoT) demands authentication systems which can provide\\nboth security and usability. Recent research utilizes the rich sensing\\ncapabilities of smart devices to build security schemes operating without human\\ninteraction, such as zero-interaction pairing (ZIP) and zero-interaction\\nauthentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and\\nreported promising results. However, those schemes were often evaluated under\\nconditions which do not reflect realistic IoT scenarios. In addition, drawing\\nany comparison among the existing schemes is impossible due to the lack of a\\ncommon public dataset and unavailability of scheme implementations.\\nIn this paper, we address these challenges by conducting the first\\nlarge-scale comparative study of ZIP and ZIA schemes, carried out under\\nrealistic conditions. We collect and release the most comprehensive dataset in\\nthe domain to date, containing over 4250 hours of audio recordings and 1\\nbillion sensor readings from three different scenarios, and evaluate five\\nstate-of-the-art schemes based on these data. Our study reveals that the\\neffectiveness of the existing proposals is highly dependent on the scenario\\nthey are used in. In particular, we show that these schemes are subject to\\nerror rates between 0.6% and 52.8%.\\n',\n",
       " '  The increasing number of protein-based metamaterials demands reliable and\\nefficient methods to study the physicochemical properties they may display. In\\nthis regard, we develop a simulation strategy based on Molecular Dynamics (MD)\\nthat addresses the geometric degrees of freedom of an auxetic two-dimensional\\nprotein crystal. This model consists of a network of impenetrable rigid squares\\nlinked through massless rigid rods, thus featuring a large number of both\\nholonomic and nonholonomic constraints. Our MD methodology is optimized to\\nstudy highly constrained systems and allows for the simulation of long-time\\ndynamics with reasonably large timesteps. The data extracted from the\\nsimulations shows a persistent motional interdependence among the protein\\nsubunits in the crystal. We characterize the dynamical correlations featured by\\nthese subunits and identify two regimes characterized by their locality or\\nnonlocality, depending on the geometric parameters of the crystal. From the\\nsame data, we also calculate the Poisson\\\\rq{}s (longitudinal to axial strain)\\nratio of the crystal, and learn that, due to holonomic constraints (rigidness\\nof the rod links), the crystal remains auxetic even after significant changes\\nin the original geometry. The nonholonomic ones (collisions between subunits)\\nincrease the number of inhomogeneous deformations of the crystal, thus driving\\nit away from an isotropic response. Our work provides the first simulation of\\nthe dynamics of protein crystals and offers insights into promising mechanical\\nproperties afforded by these materials.\\n',\n",
       " '  Recent advances in the field of network representation learning are mostly\\nattributed to the application of the skip-gram model in the context of graphs.\\nState-of-the-art analogues of skip-gram model in graphs define a notion of\\nneighbourhood and aim to find the vector representation for a node, which\\nmaximizes the likelihood of preserving this neighborhood.\\nIn this paper, we take a drastic departure from the existing notion of\\nneighbourhood of a node by utilizing the idea of coreness. More specifically,\\nwe utilize the well-established idea that nodes with similar core numbers play\\nequivalent roles in the network and hence induce a novel and an organic notion\\nof neighbourhood. Based on this idea, we propose core2vec, a new algorithmic\\nframework for learning low dimensional continuous feature mapping for a node.\\nConsequently, the nodes having similar core numbers are relatively closer in\\nthe vector space that we learn.\\nWe further demonstrate the effectiveness of core2vec by comparing word\\nsimilarity scores obtained by our method where the node representations are\\ndrawn from standard word association graphs against scores computed by other\\nstate-of-the-art network representation techniques like node2vec, DeepWalk and\\nLINE. Our results always outperform these existing methods\\n',\n",
       " '  Numerous studies have been carried out to measure wind pressures around\\ncircular cylinders since the early 20th century due to its engineering\\nsignificance. Consequently, a large amount of wind pressure data sets have\\naccumulated, which presents an excellent opportunity for using machine learning\\n(ML) techniques to train models to predict wind pressures around circular\\ncylinders. Wind pressures around smooth circular cylinders are a function of\\nmainly the Reynolds number (Re), turbulence intensity (Ti) of the incident\\nwind, and circumferential angle of the cylinder. Considering these three\\nparameters as the inputs, this study trained two ML models to predict mean and\\nfluctuating pressures respectively. Three machine learning algorithms including\\ndecision tree regressor, random forest, and gradient boosting regression trees\\n(GBRT) were tested. The GBRT models exhibited the best performance for\\npredicting both mean and fluctuating pressures, and they are capable of making\\naccurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to\\n15%. It is believed that the GBRT models provide very efficient and economical\\nalternative to traditional wind tunnel tests and computational fluid dynamic\\nsimulations for determining wind pressures around smooth circular cylinders\\nwithin the studied Re and Ti range.\\n',\n",
       " '  We study the problem of constructing a (near) uniform random proper\\n$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum\\ndegree $\\\\Delta$. (Proper in that no edge is mono-colored and simple in that two\\nedges have maximum intersection of size one). We show that if $q\\\\geq\\n\\\\max\\\\{C_k\\\\log n,500k^3\\\\Delta^{1/(k-1)}\\\\}$ then the Glauber Dynamics will become\\nclose to uniform in $O(n\\\\log n)$ time, given a random (improper) start. This\\nimproves on the results in Frieze and Melsted [5].\\n',\n",
       " '  We begin by introducing the main ideas of the paper under discussion. We\\ndiscuss some interesting issues regarding adaptive component-wise credible\\nintervals. We then briefly touch upon the concepts of self-similarity and\\nexcessive bias restriction. This is then followed by some comments on the\\nextensive simulation study carried out in the paper.\\n',\n",
       " '  Time series shapelets are discriminative sub-sequences and their similarity\\nto time series can be used for time series classification. Initial shapelet\\nextraction algorithms searched shapelets by complete enumeration of all\\npossible data sub-sequences. Research on shapelets for univariate time series\\nproposed a mechanism called shapelet learning which parameterizes the shapelets\\nand learns them jointly with a prediction model in an optimization procedure.\\nTrivial extension of this method to multivariate time series does not yield\\nvery good results due to the presence of noisy channels which lead to\\noverfitting. In this paper we propose a shapelet learning scheme for\\nmultivariate time series in which we introduce channel masks to discount noisy\\nchannels and serve as an implicit regularization.\\n',\n",
       " '  In this work, we present an experimental study of spin mediated enhanced\\nnegative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The\\nresistance measurement shows a reduction of ~2.5% for the bilayer specimen as\\ncompared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane\\napplied magnetic field of 3T. In the Ni80Fe20-only film, the negative\\nmagnetoresistance behavior is attributed to anisotropic magnetoresistance. We\\npropose that spin polarization due to spin-Hall effect is the underlying cause\\nof the enhanced negative magnetoresistance observed in the bilayer. Silicon has\\nweak spin orbit coupling so spin Hall magnetoresistance measurement is not\\nfeasible. We use V2{\\\\omega} and V3{\\\\omega} measurement as a function of\\nmagnetic field and angular rotation of magnetic field in direction normal to\\nelectric current to elucidate the spin-Hall effect. The angular rotation of\\nmagnetic field shows a sinusoidal behavior for both V2{\\\\omega} and V3{\\\\omega},\\nwhich is attributed to the spin phonon interactions resulting from the\\nspin-Hall effect mediated spin polarization. We propose that the spin\\npolarization leads to a decrease in hole-phonon scattering resulting in\\nenhanced negative magnetoresistance.\\n',\n",
       " '  Recent work on the representation of functions on sets has considered the use\\nof summation in a latent space to enforce permutation invariance. In\\nparticular, it has been conjectured that the dimension of this latent space may\\nremain fixed as the cardinality of the sets under consideration increases.\\nHowever, we demonstrate that the analysis leading to this conjecture requires\\nmappings which are highly discontinuous and argue that this is only of limited\\npractical use. Motivated by this observation, we prove that an implementation\\nof this model via continuous mappings (as provided by e.g. neural networks or\\nGaussian processes) actually imposes a constraint on the dimensionality of the\\nlatent space. Practical universal function representation for set inputs can\\nonly be achieved with a latent dimension at least the size of the maximum\\nnumber of input elements.\\n',\n",
       " '  Measurement error in observational datasets can lead to systematic bias in\\ninferences based on these datasets. As studies based on observational data are\\nincreasingly used to inform decisions with real-world impact, it is critical\\nthat we develop a robust set of techniques for analyzing and adjusting for\\nthese biases. In this paper we present a method for estimating the distribution\\nof an outcome given a binary exposure that is subject to underreporting. Our\\nmethod is based on a missing data view of the measurement error problem, where\\nthe true exposure is treated as a latent variable that is marginalized out of a\\njoint model. We prove three different conditions under which the outcome\\ndistribution can still be identified from data containing only error-prone\\nobservations of the exposure. We demonstrate this method on synthetic data and\\nanalyze its sensitivity to near violations of the identifiability conditions.\\nFinally, we use this method to estimate the effects of maternal smoking and\\nopioid use during pregnancy on childhood obesity, two import problems from\\npublic health. Using the proposed method, we estimate these effects using only\\nsubject-reported drug use data and substantially refine the range of estimates\\ngenerated by a sensitivity analysis-based approach. Further, the estimates\\nproduced by our method are consistent with existing literature on both the\\neffects of maternal smoking and the rate at which subjects underreport smoking.\\n',\n",
       " \"  An extremely simple, description of Karmarkar's algorithm with very few\\ntechnical terms is given.\\n\",\n",
       " '  We consider a wireless sensor network that uses inductive near-field coupling\\nfor wireless powering or communication, or for both. The severely limited range\\nof an inductively coupled source-destination pair can be improved using\\nresonant relay devices, which are purely passive in nature. Utilization of such\\nmagneto-inductive relays has only been studied for regular network topologies,\\nallowing simplified assumptions on the mutual antenna couplings. In this work\\nwe present an analysis of magneto-inductive passive relaying in arbitrarily\\narranged networks. We find that the resulting channel has characteristics\\nsimilar to multipath fading: the channel power gain is governed by a\\nnon-coherent sum of phasors, resulting in increased frequency selectivity. We\\npropose and study two strategies to increase the channel power gain of random\\nrelay networks: i) deactivation of individual relays by open-circuit switching\\nand ii) frequency tuning. The presented results show that both methods improve\\nthe utilization of available passive relays, leading to reliable and\\nsignificant performance gains.\\n',\n",
       " '  Transfer operators such as the Perron--Frobenius or Koopman operator play an\\nimportant role in the global analysis of complex dynamical systems. The\\neigenfunctions of these operators can be used to detect metastable sets, to\\nproject the dynamics onto the dominant slow processes, or to separate\\nsuperimposed signals. We extend transfer operator theory to reproducing kernel\\nHilbert spaces and show that these operators are related to Hilbert space\\nrepresentations of conditional distributions, known as conditional mean\\nembeddings in the machine learning community. Moreover, numerical methods to\\ncompute empirical estimates of these embeddings are akin to data-driven methods\\nfor the approximation of transfer operators such as extended dynamic mode\\ndecomposition and its variants. One main benefit of the presented kernel-based\\napproaches is that these methods can be applied to any domain where a\\nsimilarity measure given by a kernel is available. We illustrate the results\\nwith the aid of guiding examples and highlight potential applications in\\nmolecular dynamics as well as video and text data analysis.\\n',\n",
       " '  In this paper we consider the three-dimensional Schrödinger operator with\\na $\\\\delta$-interaction of strength $\\\\alpha > 0$ supported on an unbounded\\nsurface parametrized by the mapping $\\\\mathbb{R}^2\\\\ni x\\\\mapsto (x,\\\\beta f(x))$,\\nwhere $\\\\beta \\\\in [0,\\\\infty)$ and $f\\\\colon \\\\mathbb{R}^2\\\\rightarrow\\\\mathbb{R}$,\\n$f\\\\not\\\\equiv 0$, is a $C^2$-smooth, compactly supported function. The surface\\nsupporting the interaction can be viewed as a local deformation of the plane.\\nIt is known that the essential spectrum of this Schrödinger operator\\ncoincides with $[-\\\\frac14\\\\alpha^2,+\\\\infty)$. We prove that for all sufficiently\\nsmall $\\\\beta > 0$ its discrete spectrum is non-empty and consists of a unique\\nsimple eigenvalue. Moreover, we obtain an asymptotic expansion of this\\neigenvalue in the limit $\\\\beta \\\\rightarrow 0+$. In particular, this eigenvalue\\ntends to $-\\\\frac14\\\\alpha^2$ exponentially fast as $\\\\beta\\\\rightarrow 0+$.\\n',\n",
       " '  In this paper, a comparative study was conducted between complex networks\\nrepresenting origin and destination survey data. Similarities were found\\nbetween the characteristics of the networks of Brazilian cities with networks\\nof foreign cities. Power laws were found in the distributions of edge weights\\nand this scale - free behavior can occur due to the economic characteristics of\\nthe cities.\\n',\n",
       " \"  Tension-network (`tensegrity') robots encounter many control challenges as\\narticulated soft robots, due to the structures' high-dimensional nonlinear\\ndynamics. Control approaches have been developed which use the inverse\\nkinematics of tensegrity structures, either for open-loop control or as\\nequilibrium inputs for closed-loop controllers. However, current formulations\\nof the tensegrity inverse kinematics problem are limited in robotics\\napplications: first, they can lead to higher than needed cable tensions, and\\nsecond, may lack solutions when applied to robots with high node-to-cable\\nratios. This work provides progress in both directions. To address the first\\nlimitation, the objective function for the inverse kinematics optimization\\nproblem is modified to produce cable tensions as low or lower than before, thus\\nreducing the load on the robots' motors. For the second, a reformulation of the\\nstatic equilibrium constraint is proposed, which produces solutions independent\\nof the number of nodes within each rigid body. Simulation results using the\\nsecond reformulation on a specific tensegrity spine robot show reasonable\\nopen-loop control results, whereas the previous formulation could not produce\\nany solution.\\n\",\n",
       " \"  The statistical behaviour of the smallest eigenvalue has important\\nimplications for systems which can be modeled using a Wishart-Laguerre\\nensemble, the regular one or the fixed trace one. For example, the density of\\nthe smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role\\nin characterizing multiple channel telecommunication systems. Similarly, in the\\nquantum entanglement problem, the smallest eigenvalue of the fixed trace\\nensemble carries information regarding the nature of entanglement.\\nFor real Wishart-Laguerre matrices, there exists an elegant recurrence scheme\\nsuggested by Edelman to directly obtain the exact expression for the smallest\\neigenvalue density. In the case of complex Wishart-Laguerre matrices, for\\nfinding exact and explicit expressions for the smallest eigenvalue density,\\nexisting results based on determinants become impractical when the determinants\\ninvolve large-size matrices. In this work, we derive a recurrence scheme for\\nthe complex case which is analogous to that of Edelman's for the real case.\\nThis is used to obtain exact results for the smallest eigenvalue density for\\nboth the regular, and the fixed trace complex Wishart-Laguerre ensembles. We\\nvalidate our analytical results using Monte Carlo simulations. We also study\\nscaled Wishart-Laguerre ensemble and investigate its efficacy in approximating\\nthe fixed-trace ensemble. Eventually, we apply our result for the fixed-trace\\nensemble to investigate the behaviour of the smallest eigenvalue in the\\nparadigmatic system of coupled kicked tops.\\n\",\n",
       " '  Dam breach models are commonly used to predict outflow hydrographs of\\npotentially failing dams and are key ingredients for evaluating flood risk. In\\nthis paper a new dam breach modeling framework is introduced that shall improve\\nthe reliability of hydrograph predictions of homogeneous earthen embankment\\ndams. Striving for a small number of parameters, the simplified physics-based\\nmodel describes the processes of failing embankment dams by breach enlargement,\\ndriven by progressive surface erosion. Therein the erosion rate of dam material\\nis modeled by empirical sediment transport formulations. Embedding the model\\ninto a Bayesian multilevel framework allows for quantitative analysis of\\ndifferent categories of uncertainties. To this end, data available in\\nliterature of observed peak discharge and final breach width of historical dam\\nfailures was used to perform model inversion by applying Markov Chain Monte\\nCarlo simulation. Prior knowledge is mainly based on non-informative\\ndistribution functions. The resulting posterior distribution shows that the\\nmain source of uncertainty is a correlated subset of parameters, consisting of\\nthe residual error term and the epistemic term quantifying the breach erosion\\nrate. The prediction intervals of peak discharge and final breach width are\\ncongruent with values known from literature. To finally predict the outflow\\nhydrograph for real case applications, an alternative residual model was\\nformulated that assumes perfect data and a perfect model. The fully\\nprobabilistic fashion of hydrograph prediction has the potential to improve the\\nadequate risk management of downstream flooding.\\n',\n",
       " '  We study the near-infrared properties of 690 Mira candidates in the central\\nregion of the Large Magellanic Cloud, based on time-series observations at\\nJHKs. We use densely-sampled I-band observations from the OGLE project to\\ngenerate template light curves in the near infrared and derive robust mean\\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\\nstudy the Period-Luminosity-Color relations and the color excesses of\\nCarbon-rich Miras, which show evidence for a substantially different reddening\\nlaw.\\n',\n",
       " '  There is an inherent need for autonomous cars, drones, and other robots to\\nhave a notion of how their environment behaves and to anticipate changes in the\\nnear future. In this work, we focus on anticipating future appearance given the\\ncurrent frame of a video. Existing work focuses on either predicting the future\\nappearance as the next frame of a video, or predicting future motion as optical\\nflow or motion trajectories starting from a single video frame. This work\\nstretches the ability of CNNs (Convolutional Neural Networks) to predict an\\nanticipation of appearance at an arbitrarily given future time, not necessarily\\nthe next video frame. We condition our predicted future appearance on a\\ncontinuous time variable that allows us to anticipate future frames at a given\\ntemporal distance, directly from the input video frame. We show that CNNs can\\nlearn an intrinsic representation of typical appearance changes over time and\\nsuccessfully generate realistic predictions at a deliberate time difference in\\nthe near future.\\n',\n",
       " '  We consider the problem of dynamic spectrum access for network utility\\nmaximization in multichannel wireless networks. The shared bandwidth is divided\\ninto K orthogonal channels. In the beginning of each time slot, each user\\nselects a channel and transmits a packet with a certain transmission\\nprobability. After each time slot, each user that has transmitted a packet\\nreceives a local observation indicating whether its packet was successfully\\ndelivered or not (i.e., ACK signal). The objective is a multi-user strategy for\\naccessing the spectrum that maximizes a certain network utility in a\\ndistributed manner without online coordination or message exchanges between\\nusers. Obtaining an optimal solution for the spectrum access problem is\\ncomputationally expensive in general due to the large state space and partial\\nobservability of the states. To tackle this problem, we develop a novel\\ndistributed dynamic spectrum access algorithm based on deep multi-user\\nreinforcement leaning. Specifically, at each time slot, each user maps its\\ncurrent state to spectrum access actions based on a trained deep-Q network used\\nto maximize the objective function. Game theoretic analysis of the system\\ndynamics is developed for establishing design principles for the implementation\\nof the algorithm. Experimental results demonstrate strong performance of the\\nalgorithm.\\n',\n",
       " '  We design a new myopic strategy for a wide class of sequential design of\\nexperiment (DOE) problems, where the goal is to collect data in order to to\\nfulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling\\n(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for\\nmulti-armed bandits and leverages the flexibility of probabilistic programming\\nand approximate Bayesian inference to address a broad set of problems.\\nEmpirically, this general-purpose strategy is competitive with more specialised\\nmethods in a wide array of DOE tasks, and more importantly, enables addressing\\ncomplex DOE goals where no existing method seems applicable. On the theoretical\\nside, we leverage ideas from adaptive submodularity and reinforcement learning\\nto derive conditions under which MPS achieves sublinear regret against natural\\nbenchmark policies.\\n',\n",
       " '  Deep convolutional neural networks have liberated its extraordinary power on\\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\\nmodels into real-world applications due to their high computational complexity.\\nHow can we design a compact and effective network without massive experiments\\nand expert knowledge? In this paper, we propose a simple and effective\\nframework to learn and prune deep models in an end-to-end manner. In our\\nframework, a new type of parameter -- scaling factor is first introduced to\\nscale the outputs of specific structures, such as neurons, groups or residual\\nblocks. Then we add sparsity regularizations on these factors, and solve this\\noptimization problem by a modified stochastic Accelerated Proximal Gradient\\n(APG) method. By forcing some of the factors to zero, we can safely remove the\\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\\nwith other structure selection methods that may need thousands of trials or\\niterative fine-tuning, our method is trained fully end-to-end in one training\\npass without bells and whistles. We evaluate our method, Sparse Structure\\nSelection with several state-of-the-art CNNs, and demonstrate very promising\\nresults with adaptive depth and width selection.\\n',\n",
       " '  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both\\nwith and without a significant mean field are obtained. Exact bounds are\\nderived for the total energy which conform with the Kolmogorov phenomenology of\\nturbulence. Best fits to numerical data show the same functional dependences as\\nthe inequalities obtained from optimum theory.\\n',\n",
       " '  Using a projection-based decoupling of the Fokker-Planck equation, control\\nstrategies that allow to speed up the convergence to the stationary\\ndistribution are investigated. By means of an operator theoretic framework for\\na bilinear control system, two different feedback control laws are proposed.\\nProjected Riccati and Lyapunov equations are derived and properties of the\\nassociated solutions are given. The well-posedness of the closed loop systems\\nis shown and local and global stabilization results, respectively, are\\nobtained. An essential tool in the construction of the controls is the choice\\nof appropriate control shape functions. Results for a two dimensional double\\nwell potential illustrate the theoretical findings in a numerical setup.\\n',\n",
       " '  We offer a generalization of a formula of Popov involving the Von Mangoldt\\nfunction. Some commentary on its relation to other results in analytic number\\ntheory is mentioned as well as an analogue involving the m$\\\\ddot{o}$bius\\nfunction.\\n',\n",
       " '  In this paper, we show that any compact manifold that carries a\\nSL(n;R)-foliation is fibered on the circle S^1.\\n',\n",
       " '  Given the importance of crystal symmetry for the emergence of topological\\nquantum states, we have studied, as exemplified in NbNiTe2, the interplay of\\ncrystal symmetry, atomic displacements (lattice vibration), band degeneracy,\\nand band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an\\ninversion center arising from two glide planes and one mirror plane with a\\n2-fold rotation and screw axis - a full gap opening exists between two band\\nmanifolds near the Fermi energy. Upon atomic displacements by optical phonons,\\nthe symmetry lowers to space group 28 (Pma2), eliminating one glide plane along\\nc, the associated rotation and screw axis, and the inversion center. As a\\nresult, twenty Weyl points emerge, including four type-II Weyl points in the\\nG-X direction at the boundary between a pair of adjacent electron and hole\\nbands. Thus, optical phonons may offer control of the transition to a Weyl\\nfermion state.\\n',\n",
       " \"  Several social, medical, engineering and biological challenges rely on\\ndiscovering the functionality of networks from their structure and node\\nmetadata, when it is available. For example, in chemoinformatics one might want\\nto detect whether a molecule is toxic based on structure and atomic types, or\\ndiscover the research field of a scientific collaboration network. Existing\\ntechniques rely on counting or measuring structural patterns that are known to\\nshow large variations from network to network, such as the number of triangles,\\nor the assortativity of node metadata. We introduce the concept of multi-hop\\nassortativity, that captures the similarity of the nodes situated at the\\nextremities of a randomly selected path of a given length. We show that\\nmulti-hop assortativity unifies various existing concepts and offers a\\nversatile family of 'fingerprints' to characterize networks. These fingerprints\\nallow in turn to recover the functionalities of a network, with the help of the\\nmachine learning toolbox. Our method is evaluated empirically on established\\nsocial and chemoinformatic network benchmarks. Results reveal that our\\nassortativity based features are competitive providing highly accurate results\\noften outperforming state of the art methods for the network classification\\ntask.\\n\",\n",
       " '  This paper is concerned with the online estimation of a nonlinear dynamic\\nsystem from a series of noisy measurements. The focus is on cases wherein\\noutliers are present in-between normal noises. We assume that the outliers\\nfollow an unknown generating mechanism which deviates from that of normal\\nnoises, and then model the outliers using a Bayesian nonparametric model called\\nDirichlet process mixture (DPM). A sequential particle-based algorithm is\\nderived for posterior inference for the outlier model as well as the state of\\nthe system to be estimated. The resulting algorithm is termed DPM based robust\\nPF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to\\n\"speak for itself\" to determine the complexity and structure of the outlier\\nmodel. Simulation results show that it performs remarkably better than two\\nstate-of-the-art methods especially when outliers appear frequently along time.\\n',\n",
       " '  Computed tomography (CT) examinations are commonly used to predict lung\\nnodule malignancy in patients, which are shown to improve noninvasive early\\ndiagnosis of lung cancer. It remains challenging for computational approaches\\nto achieve performance comparable to experienced radiologists. Here we present\\nNoduleX, a systematic approach to predict lung nodule malignancy from CT data,\\nbased on deep learning convolutional neural networks (CNN). For training and\\nvalidation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.\\nAll nodules were identified and classified by four experienced thoracic\\nradiologists who participated in the LIDC project. NoduleX achieves high\\naccuracy for nodule malignancy classification, with an AUC of ~0.99. This is\\ncommensurate with the analysis of the dataset by experienced radiologists. Our\\napproach, NoduleX, provides an effective framework for highly accurate nodule\\nmalignancy prediction with the model trained on a large patient population. Our\\nresults are replicable with software available at\\nthis http URL.\\n',\n",
       " '  The turbulent Rayleigh--Taylor system in a rotating reference frame is\\ninvestigated by direct numerical simulations within the Oberbeck-Boussinesq\\napproximation. On the basis of theoretical arguments, supported by our\\nsimulations, we show that the Rossby number decreases in time, and therefore\\nthe Coriolis force becomes more important as the system evolves and produces\\nmany effects on Rayleigh--Taylor turbulence. We find that rotation reduces the\\nintensity of turbulent velocity fluctuations and therefore the growth rate of\\nthe temperature mixing layer. Moreover, in presence of rotation the conversion\\nof potential energy into turbulent kinetic energy is found to be less effective\\nand the efficiency of the heat transfer is reduced. Finally, during the\\nevolution of the mixing layer we observe the development of a\\ncyclone-anticyclone asymmetry.\\n',\n",
       " '  In the animal world, the competition between individuals belonging to\\ndifferent species for a resource often requires the cooperation of several\\nindividuals in groups. This paper proposes a generalization of the Hawk-Dove\\nGame for an arbitrary number of agents: the N-person Hawk-Dove Game. In this\\nmodel, doves exemplify the cooperative behavior without intraspecies conflict,\\nwhile hawks represent the aggressive behavior. In the absence of hawks, doves\\nshare the resource equally and avoid conflict, but having hawks around lead to\\ndoves escaping without fighting. Conversely, hawks fight for the resource at\\nthe cost of getting injured. Nevertheless, if doves are present in sufficient\\nnumber to expel the hawks, they can aggregate to protect the resource, and thus\\navoid being plundered by hawks. We derive and numerically solve an exact\\nequation for the evolution of the system in both finite and infinite well-mixed\\npopulations, finding the conditions for stable coexistence between both\\nspecies. Furthermore, by varying the different parameters, we found a scenario\\nof bifurcations that leads the system from dominating hawks and coexistence to\\nbi-stability, multiple interior equilibria and dominating doves.\\n',\n",
       " \"  With approximately half of the world's population at risk of contracting\\ndengue, this mosquito-borne disease is of global concern. International\\ntravellers significantly contribute to dengue's rapid and large-scale spread by\\nimporting the disease from endemic into non-endemic countries. To prevent\\nfuture outbreaks and dengue from establishing in non-endemic countries,\\nknowledge about the arrival time and location of infected travellers is\\ncrucial. We propose a network model that predicts the monthly number of dengue\\ninfected air passengers arriving at any given airport. We consider\\ninternational air travel volumes, monthly dengue incidence rates and temporal\\ninfection dynamics. Our findings shed light onto dengue importation routes and\\nreveal country-specific reporting rates that have been until now largely\\nunknown.\\n\",\n",
       " '  The best summary of a long video differs among different people due to its\\nhighly subjective nature. Even for the same person, the best summary may change\\nwith time or mood. In this paper, we introduce the task of generating\\ncustomized video summaries through simple text. First, we train a deep\\narchitecture to effectively learn semantic embeddings of video frames by\\nleveraging the abundance of image-caption data via a progressive and residual\\nmanner. Given a user-specific text description, our algorithm is able to select\\nsemantically relevant video segments and produce a temporally aligned video\\nsummary. In order to evaluate our textually customized video summaries, we\\nconduct experimental comparison with baseline methods that utilize ground-truth\\ninformation. Despite the challenging baselines, our method still manages to\\nshow comparable or even exceeding performance. We also show that our method is\\nable to generate semantically diverse video summaries by only utilizing the\\nlearned visual embeddings.\\n',\n",
       " '  Recently, heavily doped semiconductors are emerging as an alternate for low\\nloss plasmonic materials. InN, belonging to the group III nitrides, possesses\\nthe unique property of surface electron accumulation (SEA) which provides two\\ndimensional electron gas (2DEG) system. In this report, we demonstrated the\\nsurface plasmon properties of InN nanoparticles originating from SEA using the\\nreal space mapping of the surface plasmon fields for the first time. The SEA is\\nconfirmed by Raman studies which are further corroborated by photoluminescence\\nand photoemission spectroscopic studies. The frequency of 2DEG corresponding to\\nSEA is found to be in the THz region. The periodic fringes are observed in the\\nnear-field scanning optical microscopic images of InN nanostructures. The\\nobserved fringes are attributed to the interference of propagated and back\\nreflected surface plasmon polaritons (SPPs). The observation of SPPs is solely\\nattributed to the 2DEG corresponding to the SEA of InN. In addition, resonance\\nkind of behavior with the enhancement of the near-field intensity is observed\\nin the near-field images of InN nanostructures. Observation of SPPs indicates\\nthat InN with SEA can be a promising THz plasmonic material for the light\\nconfinement.\\n',\n",
       " '  It is shown that using beam splitters with non-equal wave vectors results in\\na new recoil diagram which is qualitatively different from the well-known\\ndiagram associated with the Mach-Zehnder atom interferometer. We predict a new\\nasymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses\\na Raman beam splitter. The main feature is that the phase of AMZAI contains a\\nquantum part proportional to the recoil frequency. A response sensitive only to\\nthe quantum phase was found. A new technique to measure the recoil frequency\\nand fine structure constant is proposed and studied outside of the Raman-Nath\\napproximation.\\n',\n",
       " '  In this paper, we consider a partial information two-person zero-sum\\nstochastic differential game problem where the system is governed by a backward\\nstochastic differential equation driven by Teugels martingales associated with\\na Lévy process and an independent Brownian motion. One sufficient (a\\nverification theorem) and one necessary conditions for the existence of optimal\\ncontrols are proved. To illustrate the general results, a linear quadratic\\nstochastic differential game problem is discussed.\\n',\n",
       " \"  In recent years, research has been done on applying Recurrent Neural Networks\\n(RNNs) as recommender systems. Results have been promising, especially in the\\nsession-based setting where RNNs have been shown to outperform state-of-the-art\\nmodels. In many of these experiments, the RNN could potentially improve the\\nrecommendations by utilizing information about the user's past sessions, in\\naddition to its own interactions in the current session. A problem for\\nsession-based recommendation, is how to produce accurate recommendations at the\\nstart of a session, before the system has learned much about the user's current\\ninterests. We propose a novel approach that extends a RNN recommender to be\\nable to process the user's recent sessions, in order to improve\\nrecommendations. This is done by using a second RNN to learn from recent\\nsessions, and predict the user's interest in the current session. By feeding\\nthis information to the original RNN, it is able to improve its\\nrecommendations. Our experiments on two different datasets show that the\\nproposed approach can significantly improve recommendations throughout the\\nsessions, compared to a single RNN working only on the current session. The\\nproposed model especially improves recommendations at the start of sessions,\\nand is therefore able to deal with the cold start problem within sessions.\\n\",\n",
       " '  Shock wave interactions with defects, such as pores, are known to play a key\\nrole in the chemical initiation of energetic materials. The shock response of\\nhexanitrostilbene is studied through a combination of large scale reactive\\nmolecular dynamics and mesoscale hydrodynamic simulations. In order to extend\\nour simulation capability at the mesoscale to include weak shock conditions (<\\n6 GPa), atomistic simulations of pore collapse are used to define a strain rate\\ndependent strength model. Comparing these simulation methods allows us to\\nimpose physically-reasonable constraints on the mesoscale model parameters. In\\ndoing so, we have been able to study shock waves interacting with pores as a\\nfunction of this viscoplastic material response. We find that the pore collapse\\nbehavior of weak shocks is characteristically different to that of strong\\nshocks.\\n',\n",
       " '  We propose factor models for the cross-section of daily cryptoasset returns\\nand provide source code for data downloads, computing risk factors and\\nbacktesting them out-of-sample. In \"cryptoassets\" we include all\\ncryptocurrencies and a host of various other digital assets (coins and tokens)\\nfor which exchange market data is available. Based on our empirical analysis,\\nwe identify the leading factor that appears to strongly contribute into daily\\ncryptoasset returns. Our results suggest that cross-sectional statistical\\narbitrage trading may be possible for cryptoassets subject to efficient\\nexecutions and shorting.\\n',\n",
       " '  Many signals on Cartesian product graphs appear in the real world, such as\\ndigital images, sensor observation time series, and movie ratings on Netflix.\\nThese signals are \"multi-dimensional\" and have directional characteristics\\nalong each factor graph. However, the existing graph Fourier transform does not\\ndistinguish these directions, and assigns 1-D spectra to signals on product\\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\\nmain result is a multi-dimensional graph Fourier transform that solves such\\nproblems associated with the conventional GFT. Using algebraic properties of\\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\\nthe conventional GFT into the multi-dimensional frequency domain, of which each\\ndimension represents a directional frequency along each factor graph. Thus, the\\nmulti-dimensional graph Fourier transform enables directional frequency\\nanalysis, in addition to frequency analysis with the conventional GFT.\\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\\nfilterings and stationarities that utilize dimensional information of graph\\nsignals, which are also discussed in this study. The proposed methods are\\napplicable to a wide variety of data that can be regarded as signals on\\nCartesian product graphs. This study also notes that multivariate graph signals\\ncan be regarded as 2-D univariate graph signals. This correspondence provides\\nnatural definitions of the multivariate graph Fourier transform and the\\nmultivariate stationarity based on their 2-D univariate versions.\\n',\n",
       " '  The double exponential formula was introduced for calculating definite\\nintegrals with singular point oscillation functions and Fourier-integrals. The\\ndouble exponential transformation is not only useful for numerical computations\\nbut it is also used in different methods of Sinc theory. In this paper we use\\ndouble exponential transformation for calculating particular improper\\nintegrals. By improving integral estimates having singular final points. By\\ncomparison between double exponential transformations and single exponential\\ntransformations it is proved that the error margin of double exponential\\ntransformations is smaller. Finally Fourier-integral and double exponential\\ntransformations are discussed.\\n',\n",
       " '  Strain engineering has attracted great attention, particularly for epitaxial\\nfilms grown on a different substrate. Residual strains of SiC have been widely\\nemployed to form ultra-high frequency and high Q factor resonators. However, to\\ndate the highest residual strain of SiC was reported to be limited to\\napproximately 0.6%. Large strains induced into SiC could lead to several\\ninteresting physical phenomena, as well as significant improvement of resonant\\nfrequencies. We report an unprecedented nano strain-amplifier structure with an\\nultra-high residual strain up to 8% utilizing the natural residual stress\\nbetween epitaxial 3C SiC and Si. In addition, the applied strain can be tuned\\nby changing the dimensions of the amplifier structure. The possibility of\\nintroducing such a controllable and ultra-high strain will open the door to\\ninvestigating the physics of SiC in large strain regimes, and the development\\nof ultra sensitive mechanical sensors.\\n',\n",
       " '  A complex system can be represented and analyzed as a network, where nodes\\nrepresent the units of the network and edges represent connections between\\nthose units. For example, a brain network represents neurons as nodes and axons\\nbetween neurons as edges. In many networks, some nodes have a\\ndisproportionately high number of edges. These nodes also have many edges\\nbetween each other, and are referred to as the rich club. In many different\\nnetworks, the nodes of this club are assumed to support global network\\nintegration. However, another set of nodes potentially exhibits a connectivity\\nstructure that is more advantageous to global network integration. Here, in a\\nmyriad of different biological and man-made networks, we discover the diverse\\nclub--a set of nodes that have edges diversely distributed across the network.\\nThe diverse club exhibits, to a greater extent than the rich club, properties\\nconsistent with an integrative network function--these nodes are more highly\\ninterconnected and their edges are more critical for efficient global\\nintegration. Moreover, we present a generative evolutionary network model that\\nproduces networks with a diverse club but not a rich club, thus demonstrating\\nthat these two clubs potentially evolved via distinct selection pressures.\\nGiven the variety of different networks that we analyzed--the c. elegans, the\\nmacaque brain, the human brain, the United States power grid, and global air\\ntraffic--the diverse club appears to be ubiquitous in complex networks. These\\nresults warrant the distinction and analysis of two critical clubs of nodes in\\nall complex systems.\\n',\n",
       " \"  Neural network based generative models with discriminative components are a\\npowerful approach for semi-supervised learning. However, these techniques a)\\ncannot account for model uncertainty in the estimation of the model's\\ndiscriminative component and b) lack flexibility to capture complex stochastic\\npatterns in the label generation process. To avoid these problems, we first\\npropose to use a discriminative component with stochastic inputs for increased\\nnoise flexibility. We show how an efficient Gibbs sampling procedure can\\nmarginalize the stochastic inputs when inferring missing labels in this model.\\nFollowing this, we extend the discriminative component to be fully Bayesian and\\nproduce estimates of uncertainty in its parameter values. This opens the door\\nfor semi-supervised Bayesian active learning.\\n\",\n",
       " '  Detection of interactions between treatment effects and patient descriptors\\nin clinical trials is critical for optimizing the drug development process. The\\nincreasing volume of data accumulated in clinical trials provides a unique\\nopportunity to discover new biomarkers and further the goal of personalized\\nmedicine, but it also requires innovative robust biomarker detection methods\\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\\nof novel univariate statistical tests, based on the theory of random walks,\\nwhich are able to capture non-linear and non-monotonic covariate-treatment\\ninteractions. We also propose a novel combined test, which leverages the power\\nof all of our proposed univariate tests into a single general-case tool. We\\npresent results for both synthetic trials as well as real-world clinical\\ntrials, where we compare our method with state-of-the-art techniques and\\ndemonstrate the utility and robustness of our approach.\\n',\n",
       " '  The limitations in performance of the present RICH system in the LHCb\\nexperiment are given by the natural chromatic dispersion of the gaseous\\nCherenkov radiator, the aberrations of the optical system and the pixel size of\\nthe photon detectors. Moreover, the overall PID performance can be affected by\\nhigh detector occupancy as the pattern recognition becomes more difficult with\\nhigh particle multiplicities. This paper shows a way to improve performance by\\nsystematically addressing each of the previously mentioned limitations. These\\nideas are applied in the present and future upgrade phases of the LHCb\\nexperiment. Although applied to specific circumstances, they are used as a\\nparadigm on what is achievable in the development and realisation of high\\nprecision RICH detectors.\\n',\n",
       " '  Given a klt singularity $x\\\\in (X, D)$, we show that a quasi-monomial\\nvaluation $v$ with a finitely generated associated graded ring is the minimizer\\nof the normalized volume function $\\\\widehat{\\\\rm vol}_{(X,D),x}$, if and only if\\n$v$ induces a degeneration to a K-semistable log Fano cone singularity.\\nMoreover, such a minimizer is unique among all quasi-monomial valuations up to\\nrescaling. As a consequence, we prove that for a klt singularity $x\\\\in X$ on\\nthe Gromov-Hausdorff limit of Kähler-Einstein Fano manifolds, the\\nintermediate K-semistable cone associated to its metric tangent cone is\\nuniquely determined by the algebraic structure of $x\\\\in X$, hence confirming a\\nconjecture by Donaldson-Sun.\\n',\n",
       " '  Condensed-matter analogs of the Higgs boson in particle physics allow\\ninsights into its behavior in different symmetries and dimensionalities.\\nEvidence for the Higgs mode has been reported in a number of different\\nsettings, including ultracold atomic gases, disordered superconductors, and\\ndimerized quantum magnets. However, decay processes of the Higgs mode (which\\nare eminently important in particle physics) have not yet been studied in\\ncondensed matter due to the lack of a suitable material system coupled to a\\ndirect experimental probe. A quantitative understanding of these processes is\\nparticularly important for low-dimensional systems where the Higgs mode decays\\nrapidly and has remained elusive to most experimental probes. Here, we discover\\nand study the Higgs mode in a two-dimensional antiferromagnet using\\nspin-polarized inelastic neutron scattering. Our spin-wave spectra of\\nCa$_2$RuO$_4$ directly reveal a well-defined, dispersive Higgs mode, which\\nquickly decays into transverse Goldstone modes at the antiferromagnetic\\nordering wavevector. Through a complete mapping of the transverse modes in the\\nreciprocal space, we uniquely specify the minimal model Hamiltonian and\\ndescribe the decay process. We thus establish a novel condensed matter platform\\nfor research on the dynamics of the Higgs mode.\\n',\n",
       " '  Well-known for its simplicity and effectiveness in classification, AdaBoost,\\nhowever, suffers from overfitting when class-conditional distributions have\\nsignificant overlap. Moreover, it is very sensitive to noise that appears in\\nthe labels. This article tackles the above limitations simultaneously via\\noptimizing a modified loss function (i.e., the conditional risk). The proposed\\napproach has the following two advantages. (1) It is able to directly take into\\naccount label uncertainty with an associated label confidence. (2) It\\nintroduces a \"trustworthiness\" measure on training samples via the Bayesian\\nrisk rule, and hence the resulting classifier tends to have finite sample\\nperformance that is superior to that of the original AdaBoost when there is a\\nlarge overlap between class conditional distributions. Theoretical properties\\nof the proposed method are investigated. Extensive experimental results using\\nsynthetic data and real-world data sets from UCI machine learning repository\\nare provided. The empirical study shows the high competitiveness of the\\nproposed method in predication accuracy and robustness when compared with the\\noriginal AdaBoost and several existing robust AdaBoost algorithms.\\n',\n",
       " '  We study the problem of sparsity constrained $M$-estimation with arbitrary\\ncorruptions to both {\\\\em explanatory and response} variables in the\\nhigh-dimensional regime, where the number of variables $d$ is larger than the\\nsample size $n$. Our main contribution is a highly efficient gradient-based\\noptimization algorithm that we call Trimmed Hard Thresholding -- a robust\\nvariant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient\\ncomputations. Our algorithm can deal with a wide class of sparsity constrained\\n$M$-estimation problems, and we can tolerate a nearly dimension independent\\nfraction of arbitrarily corrupted samples. More specifically, when the\\ncorrupted fraction satisfies $\\\\epsilon \\\\lesssim {1} /\\\\left({\\\\sqrt{k} \\\\log\\n(nd)}\\\\right)$, where $k$ is the sparsity of the parameter, we obtain accurate\\nestimation and model selection guarantees with optimal sample complexity.\\nFurthermore, we extend our algorithm to sparse Gaussian graphical model\\n(precision matrix) estimation via a neighborhood selection approach. We\\ndemonstrate the effectiveness of robust estimation in sparse linear, logistic\\nregression, and sparse precision matrix estimation on synthetic and real-world\\nUS equities data.\\n',\n",
       " \"  We present a communication- and data-sensitive formulation of ADER-DG for\\nhyperbolic differential equation systems. Sensitive here has multiple flavours:\\nFirst, the formulation reduces the persistent memory footprint. This reduces\\npressure on the memory subsystem. Second, the formulation realises the\\nunderlying predictor-corrector scheme with single-touch semantics, i.e., each\\ndegree of freedom is read on average only once per time step from the main\\nmemory. This reduces communication through the memory controllers. Third, the\\nformulation breaks up the tight coupling of the explicit time stepping's\\nalgorithmic steps to mesh traversals. This averages out data access peaks.\\nDifferent operations and algorithmic steps are ran on different grid entities.\\nFinally, the formulation hides distributed memory data transfer behind the\\ncomputation aligned with the mesh traversal. This reduces pressure on the\\nmachine interconnects. All techniques applied by our formulation are elaborated\\nby means of a rigorous task formalism. They break up ADER-DG's tight causal\\ncoupling of compute steps and can be generalised to other predictor-corrector\\nschemes.\\n\",\n",
       " '  This paper outlines a methodology for Bayesian multimodel uncertainty\\nquantification (UQ) and propagation and presents an investigation into the\\neffect of prior probabilities on the resulting uncertainties. The UQ\\nmethodology is adapted from the information-theoretic method previously\\npresented by the authors (Zhang and Shields, 2018) to a fully Bayesian\\nconstruction that enables greater flexibility in quantifying uncertainty in\\nprobability model form. Being Bayesian in nature and rooted in UQ from small\\ndatasets, prior probabilities in both probability model form and model\\nparameters are shown to have a significant impact on quantified uncertainties\\nand, consequently, on the uncertainties propagated through a physics-based\\nmodel. These effects are specifically investigated for a simplified plate\\nbuckling problem with uncertainties in material properties derived from a small\\nnumber of experiments using noninformative priors and priors derived from past\\nstudies of varying appropriateness. It is illustrated that prior probabilities\\ncan have a significant impact on multimodel UQ for small datasets and\\ninappropriate (but seemingly reasonable) priors may even have lingering effects\\nthat bias probabilities even for large datasets. When applied to uncertainty\\npropagation, this may result in probability bounds on response quantities that\\ndo not include the true probabilities.\\n',\n",
       " '  Failing to distinguish between a sheepdog and a skyscraper should be worse\\nand penalized more than failing to distinguish between a sheepdog and a poodle;\\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\\nclassification/recognition via neural networks seldom view a sheepdog as more\\nsimilar to a poodle than to a skyscraper. We define a metric that, inter alia,\\ncan penalize failure to distinguish between a sheepdog and a skyscraper more\\nthan failure to distinguish between a sheepdog and a poodle. Unlike previously\\nemployed possibilities, this metric is based on an ultrametric tree associated\\nwith any given tree organization into a semantically meaningful hierarchy of a\\nclassifier\\'s classes.\\n',\n",
       " '  Achieving the goals in the title (and others) relies on a cardinality-wise\\nscanning of the ideals of the poset. Specifically, the relevant numbers\\nattached to the k+1 element ideals are inferred from the corresponding numbers\\nof the k-element (order) ideals. Crucial in all of this is a compressed\\nrepresentation (using wildcards) of the ideal lattice. The whole scheme invites\\ndistributed computation.\\n',\n",
       " '  We present a scalable, black box, perception-in-the-loop technique to find\\nadversarial examples for deep neural network classifiers. Black box means that\\nour procedure only has input-output access to the classifier, and not to the\\ninternal structure, parameters, or intermediate confidence values.\\nPerception-in-the-loop means that the notion of proximity between inputs can be\\ndirectly queried from human participants rather than an arbitrarily chosen\\nmetric. Our technique is based on covariance matrix adaptation evolution\\nstrategy (CMA-ES), a black box optimization approach. CMA-ES explores the\\nsearch space iteratively in a black box manner, by generating populations of\\ncandidates according to a distribution, choosing the best candidates according\\nto a cost function, and updating the posterior distribution to favor the best\\ncandidates. We run CMA-ES using human participants to provide the fitness\\nfunction, using the insight that the choice of best candidates in CMA-ES can be\\nnaturally modeled as a perception task: pick the top $k$ inputs perceptually\\nclosest to a fixed input. We empirically demonstrate that finding adversarial\\nexamples is feasible using small populations and few iterations. We compare the\\nperformance of CMA-ES on the MNIST benchmark with other black-box approaches\\nusing $L_p$ norms as a cost function, and show that it performs favorably both\\nin terms of success in finding adversarial examples and in minimizing the\\ndistance between the original and the adversarial input. In experiments on the\\nMNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find\\nperceptually similar adversarial inputs with a small number of iterations and\\nsmall population sizes when using perception-in-the-loop. Finally, we show that\\nnetworks trained specifically to be robust against $L_\\\\infty$ norm can still be\\nsusceptible to perceptually similar adversarial examples.\\n',\n",
       " '  This paper presents a novel generative model to synthesize fluid simulations\\nfrom a set of reduced parameters. A convolutional neural network is trained on\\na collection of discrete, parameterizable fluid simulation velocity fields. Due\\nto the capability of deep learning architectures to learn representative\\nfeatures of the data, our generative model is able to accurately approximate\\nthe training data set, while providing plausible interpolated in-betweens. The\\nproposed generative model is optimized for fluids by a novel loss function that\\nguarantees divergence-free velocity fields at all times. In addition, we\\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\\nadvance simulations in time by integrating in the latent space with a second\\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\\napplications such as fast construction of simulations, interpolation of fluids\\nwith different parameters, time re-sampling, latent space simulations, and\\ncompression of fluid simulation data. Reconstructed velocity fields are\\ngenerated up to 700x faster than traditional CPU solvers, while achieving\\ncompression rates of over 1300x.\\n',\n",
       " '  The two-stage least-squares (2SLS) estimator is known to be biased when its\\nfirst-stage fit is poor. I show that better first-stage prediction can\\nalleviate this bias. In a two-stage linear regression model with Normal noise,\\nI consider shrinkage in the estimation of the first-stage instrumental variable\\ncoefficients. For at least four instrumental variables and a single endogenous\\nregressor, I establish that the standard 2SLS estimator is dominated with\\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\\nin a first-stage high-dimensional Normal-means problem followed by a\\ncontrol-function approach in the second stage. It preserves invariances of the\\nstructural instrumental variable equations.\\n',\n",
       " '  An unsupervised learning classification model is described. It achieves\\nclassification error probability competitive with that of popular supervised\\nlearning classifiers such as SVM or kNN. The model is based on the incremental\\nexecution of small step shift and rotation operations upon selected\\ndiscriminative hyperplanes at the arrival of input samples. When applied, in\\nconjunction with a selected feature extractor, to a subset of the ImageNet\\ndataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by\\nmerely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both\\nusing same feature extractor. This result may also be contrasted with popular\\nunsupervised learning schemes such as k-Means which is shown to be practically\\nuseless on same dataset.\\n',\n",
       " '  We investigate the predictability of several range-based stock volatility\\nestimators, and compare them to the standard close-to-close estimator which is\\nmost commonly acknowledged as the volatility. The patterns of volatility\\nchanges are analyzed using LSTM recurrent neural networks, which are a state of\\nthe art method of sequence learning. We implement the analysis on all current\\nconstituents of the Dow Jones Industrial Average index, and report averaged\\nevaluation results. We find that changes in the values of range-based\\nestimators are more predictable than that of the estimator using daily closing\\nvalues only.\\n',\n",
       " \"  Correlated random walks (CRW) have been used for a long time as a null model\\nfor animal's random search movement in two dimensions (2D). An increasing\\nnumber of studies focus on animals' movement in three dimensions (3D), but the\\nkey properties of CRW, such as the way the mean squared displacement is related\\nto the path length, are well known only in 1D and 2D. In this paper I derive\\nsuch properties for 3D CRW, in a consistent way with the expression of these\\nproperties in 2D. This should allow 3D CRW to act as a null model when\\nanalyzing actual 3D movements similarly to what is done in 2D\\n\",\n",
       " '  This paper presents a novel context-based approach for pedestrian motion\\nprediction in crowded, urban intersections, with the additional flexibility of\\nprediction in similar, but new, environments. Previously, Chen et. al. combined\\nMarkovian-based and clustering-based approaches to learn motion primitives in a\\ngrid-based world and subsequently predict pedestrian trajectories by modeling\\nthe transition between learned primitives as a Gaussian Process (GP). This work\\nextends that prior approach by incorporating semantic features from the\\nenvironment (relative distance to curbside and status of pedestrian traffic\\nlights) in the GP formulation for more accurate predictions of pedestrian\\ntrajectories over the same timescale. We evaluate the new approach on\\nreal-world data collected using one of the vehicles in the MIT Mobility On\\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\\nto quantify the span of predicted set of trajectories, such that a lower AUC\\ncorresponds to a higher level of confidence in the future direction of\\npedestrian motion.\\n',\n",
       " '  We present E NERGY N ET , a new framework for analyzing and building\\nartificial neural network architectures. Our approach adaptively learns the\\nstructure of the networks in an unsupervised manner. The methodology is based\\nupon the theoretical guarantees of the energy function of restricted Boltzmann\\nmachines (RBM) of infinite number of nodes. We present experimental results to\\nshow that the final network adapts to the complexity of a given problem.\\n',\n",
       " '  Finding the dense regions of a graph and relations among them is a\\nfundamental problem in network analysis. Core and truss decompositions reveal\\ndense subgraphs with hierarchical relations. The incremental nature of\\nalgorithms for computing these decompositions and the need for global\\ninformation at each step of the algorithm hinders scalable parallelization and\\napproximations since the densest regions are not revealed until the end. In a\\nprevious work, Lu et al. proposed to iteratively compute the $h$-indices of\\nneighbor vertex degrees to obtain the core numbers and prove that the\\nconvergence is obtained after a finite number of iterations. This work\\ngeneralizes the iterative $h$-index computation for truss decomposition as well\\nas nucleus decomposition which leverages higher-order structures to generalize\\ncore and truss decompositions. In addition, we prove convergence bounds on the\\nnumber of iterations. We present a framework of local algorithms to obtain the\\ncore, truss, and nucleus decompositions. Our algorithms are local, parallel,\\noffer high scalability, and enable approximations to explore time and quality\\ntrade-offs. Our shared-memory implementation verifies the efficiency,\\nscalability, and effectiveness of our local algorithms on real-world networks.\\n',\n",
       " '  We propose a robust gesture-based communication pipeline for divers to\\ninstruct an Autonomous Underwater Vehicle (AUV) to assist them in performing\\nhigh-risk tasks and helping in case of emergency. A gesture communication\\nlanguage (CADDIAN) is developed, based on consolidated and standardized diver\\ngestures, including an alphabet, syntax and semantics, ensuring a logical\\nconsistency. A hierarchical classification approach is introduced for hand\\ngesture recognition based on stereo imagery and multi-descriptor aggregation to\\nspecifically cope with underwater image artifacts, e.g. light backscatter or\\ncolor attenuation. Once the classification task is finished, a syntax check is\\nperformed to filter out invalid command sequences sent by the diver or\\ngenerated by errors in the classifier. Throughout this process, the diver\\nreceives constant feedback from an underwater tablet to acknowledge or abort\\nthe mission at any time. The objective is to prevent the AUV from executing\\nunnecessary, infeasible or potentially harmful motions. Experimental results\\nunder different environmental conditions in archaeological exploration and\\nbridge inspection applications show that the system performs well in the field.\\n',\n",
       " '  Unique among alkali-doped $\\\\textit {A}$$_3$C$_{60}$ fullerene compounds, the\\nA15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying\\nunder hydrostatic pressure with highest transition temperatures at $T_\\\\textrm\\n{C}$$^\\\\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that\\nthese two compounds under pressure represent the optimal materials of the\\n$\\\\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated\\nsuperconductivity is mediated through Coulombic interactions with charges on\\nthe alkalis. A derivation of the interlayer Coulombic pairing model of\\nhigh-$T_\\\\textrm {C}$ superconductivity employing non-planar geometry is\\nintroduced, generalizing the picture of two interacting layers to an\\ninteraction between charge reservoirs located on the C$_{60}$ and alkali ions.\\nThe optimal transition temperature follows the algebraic expression, $T_\\\\textrm\\n{C0}$ = (12.474 nm$^2$ K)/$\\\\ell$${\\\\zeta}$, where $\\\\ell$ relates to the mean\\nspacing between interacting surface charges on the C$_{60}$ and ${\\\\zeta}$ is\\nthe average radial distance between the C$_{60}$ surface and the neighboring Cs\\nions. Values of $T_\\\\textrm {C0}$ for the measured cation stoichiometries of\\nCs$_{3-\\\\textrm{x}}$C$_{60}$ with x $\\\\approx$ 0 are found to be 38.19 and 36.88\\nK for the A15 and fcc forms, respectively, with the dichotomy in transition\\ntemperature reflecting the larger ${\\\\zeta}$ and structural disorder in the fcc\\nform. In the A15 form, modeled interacting charges and Coulomb potential\\ne$^2$/${\\\\zeta}$ are shown to agree quantitatively with findings from\\nnuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,\\nsuppression of $T_\\\\textrm {C}$$^\\\\textrm {meas}$ below $T_\\\\textrm {C0}$ is\\nascribed to native structural disorder. Phononic effects in conjunction with\\nCoulombic pairing are discussed.\\n',\n",
       " '  Improving the performance of superconducting qubits and resonators generally\\nresults from a combination of materials and fabrication process improvements\\nand design modifications that reduce device sensitivity to residual losses. One\\ninstance of this approach is to use trenching into the device substrate in\\ncombination with superconductors and dielectrics with low intrinsic losses to\\nimprove quality factors and coherence times. Here we demonstrate titanium\\nnitride coplanar waveguide resonators with mean quality factors exceeding two\\nmillion and controlled trenching reaching 2.2 $\\\\mu$m into the silicon\\nsubstrate. Additionally, we measure sets of resonators with a range of sizes\\nand trench depths and compare these results with finite-element simulations to\\ndemonstrate quantitative agreement with a model of interface dielectric loss.\\nWe then apply this analysis to determine the extent to which trenching can\\nimprove resonator performance.\\n',\n",
       " '  This paper will detail changes in the operational paradigm of the Fermi\\nNational Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to\\nupgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions\\nfor High Energy Physics (HEP) experiments were extracted at ~18 keV vertically\\ndownward into a 90 degree bending magnet and accelerated through a\\nCockcroft-Walton accelerating column to 750 keV. Following the upgrade in the\\nfall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35\\nkeV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This\\nchange in extraction energy as well as the orientation of the ion source\\nrequired not only a redesign of the ion source, but an updated understanding of\\nits operation at these new values. Discussed in detail are the changes to the\\nion source timing, arc discharge current, hydrogen gas pressure, and cesium\\ndelivery system that were needed to maintain consistent operation at >99%\\nuptime for HEP, with an increased ion source lifetime of over 9 months.\\n',\n",
       " \"  We consider the withdrawal of a ball from a fluid reservoir to understand the\\nlongevity of the connection between that ball and the fluid it breaks away\\nfrom, at intermediate Reynolds numbers. Scaling arguments based on the\\nprocesses observed as the ball interacts with the fluid surface were applied to\\nthe `pinch-off time', when the ball breaks its connection with the fluid from\\nwhich it has been withdrawn, measured experimentally. At the lowest Reynolds\\nnumbers tested, pinch-off occurs in a `surface seal' close to the reservoir\\nsurface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'\\nclose to the ball. Our scaling analysis shows that the connection between ball\\nand fluid is controlled by the fluid film draining from the ball as it\\ncontinues to be winched away from the fluid reservoir. The draining flow itself\\ndepends on the amount of fluid coating the ball on exit from the reservoir. We\\nconsider the possibilities that this coating was created through: a surface\\ntension driven Landau Levitch Derjaguin wetting of the surface; a\\nvisco-inertial quick coating; or alternatively through the inertia of the fluid\\nmoving with the ball through the reservoir. We show that although the pinch-off\\nmechanism is controlled by viscosity, the coating mechanism is governed by a\\ndifferent length and timescale, dictated by the inertial added mass of the ball\\nwhen submersed.\\n\",\n",
       " '  We disentangle all the individual degrees of freedom in the quantum impurity\\nproblem to deconstruct the Kondo singlet, both in real and energy space, by\\nstudying the contribution of each individual free electron eigenstate. This is\\na problem of two spins coupled to a bath, where the bath is formed by the\\nremaining conduction electrons. Being a mixed state, we resort to the\\n\"concurrence\" to quantify entanglement. We identify \"projected natural\\norbitals\" that allow us to individualize a single-particle electronic wave\\nfunction that is responsible of more than $90\\\\%$ of the impurity screening. In\\nthe weak coupling regime, the impurity is entangled to an electron at the Fermi\\nlevel, while in the strong coupling regime, the impurity counterintuitively\\nentangles mostly with the high energy electrons and disentangles completely\\nfrom the low-energy states carving a \"hole\" around the Fermi level. This\\nenables one to use concurrence as a pseudo order parameter to compute the\\ncharacteristic \"size\" of the Kondo cloud, beyond which electrons are are weakly\\ncorrelated to the impurity and are dominated by the physics of the boundary.\\n',\n",
       " '  Motivated by the recently proposed parallel orbital-updating approach in real\\nspace method, we propose a parallel orbital-updating based plane-wave basis\\nmethod for electronic structure calculations, for solving the corresponding\\neigenvalue problems. In addition, we propose two new modified parallel\\norbital-updating methods. Compared to the traditional plane-wave methods, our\\nmethods allow for two-level parallelization, which is particularly interesting\\nfor large scale parallelization. Numerical experiments show that these new\\nmethods are more reliable and efficient for large scale calculations on modern\\nsupercomputers\\n',\n",
       " '  The particular type of four-kink multi-solitons (or quadrons) adiabatic\\ndynamics of the sine-Gordon equation in a model with two identical point\\nattracting impurities has been studied. This model can be used for describing\\nmagnetization localized waves in multilayer ferromagnet. The quadrons structure\\nand properties has been numerically investigated. The cases of both large and\\nsmall distances between impurities has been viewed. The dependence of the\\nlocalized in impurity region nonlinear high-amplitude waves frequencies on the\\ndistance between the impurities has been found. For an analytical description\\nof two bound localized on impurities nonlinear waves dynamics, using\\nperturbation theory, the system of differential equations for harmonic\\noscillators with elastic link has been found. The analytical model\\nqualitatively describes the results of the sine-Gordon equation numerical\\nsimulation.\\n',\n",
       " \"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the\\ncosmic microwave background (CMB) differ at the $\\\\sim$3-$\\\\sigma$ level,\\nindicating a potential issue with the standard $\\\\Lambda$CDM cosmology.\\nInterpreting this tension correctly requires a model comparison calculation\\ndepending on not only the traditional `$n$-$\\\\sigma$' mismatch but also the\\ntails of the likelihoods. Determining the form of the tails of the local $H_0$\\nlikelihood is impossible with the standard Gaussian least-squares\\napproximation, as it requires using non-Gaussian distributions to faithfully\\nrepresent anchor likelihoods and model outliers in the Cepheid and supernova\\n(SN) populations, and simultaneous fitting of the full distance-ladder dataset\\nto correctly propagate uncertainties. We have developed a Bayesian hierarchical\\nmodel that describes the full distance ladder, from nearby geometric anchors\\nthrough Cepheids to Hubble-Flow SNe. This model does not rely on any\\ndistributions being Gaussian, allowing outliers to be modeled and obviating the\\nneed for arbitrary data cuts. Sampling from the $\\\\sim$3000-parameter joint\\nposterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\\\\pm$ 1.67)\\n${\\\\rm km\\\\,s^{-1}\\\\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.\\n(2016) data, and ($73.15 \\\\pm 1.78$) ${\\\\rm km\\\\,s^{-1}\\\\,Mpc^{-1}}$ with SN\\noutliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the\\ndistance-ladder likelihood allows us to apply Bayesian model comparison to\\nassess the evidence for deviation from $\\\\Lambda$CDM. We set up this comparison\\nto yield a lower limit on the odds of the underlying model being $\\\\Lambda$CDM\\ngiven the distance-ladder and Planck XIII (2016) CMB data. The odds against\\n$\\\\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers\\nare cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)\\nlikelihood is used.\\n\",\n",
       " '  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms\\nfor uncoordinated spectrum access. The number of users is assumed to be unknown\\nto each user. A stochastic setting is first considered, where the rewards on a\\nchannel are the same for each user. In contrast to prior work, it is assumed\\nthat the number of users can possibly exceed the number of channels, and that\\nrewards can be non-zero even under collisions. The proposed algorithm consists\\nof an estimation phase and an allocation phase. It is shown that if every user\\nadopts the algorithm, the system wide regret is constant with time with high\\nprobability. The regret guarantees hold for any number of users and channels,\\nin particular, even when the number of users is less than the number of\\nchannels. Next, an adversarial multi-user MAB framework is considered, where\\nthe rewards on the channels are user-dependent. It is assumed that the number\\nof users is less than the number of channels, and that the users receive zero\\nreward on collision. The proposed algorithm combines the Exp3.P algorithm\\ndeveloped in prior work for single user adversarial bandits with a collision\\nresolution mechanism to achieve sub-linear regret. It is shown that if every\\nuser employs the proposed algorithm, the system wide regret is of the order\\n$O(T^\\\\frac{3}{4})$ over a horizon of time $T$. The algorithms in both\\nstochastic and adversarial scenarios are extended to the dynamic case where the\\nnumber of users in the system evolves over time and are shown to lead to\\nsub-linear regret.\\n',\n",
       " '  In this paper, we analyze the effects of contact models on contact-implicit\\ntrajectory optimization for manipulation. We consider three different\\napproaches: (1) a contact model that is based on complementarity constraints,\\n(2) a smooth contact model, and our proposed method (3) a variable smooth\\ncontact model. We compare these models in simulation in terms of physical\\naccuracy, quality of motions, and computation time. In each case, the\\noptimization process is initialized by setting all torque variables to zero,\\nnamely, without a meaningful initial guess. For simulations, we consider a\\npushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our\\nresults demonstrate that the optimization based on the proposed variable smooth\\ncontact model provides a good trade-off between the physical fidelity and\\nquality of motions at the cost of increased computation time.\\n',\n",
       " '  The challenge of assigning importance to individual neurons in a network is\\nof interest when interpreting deep learning models. In recent work, Dhamdhere\\net al. proposed Total Conductance, a \"natural refinement of Integrated\\nGradients\" for attributing importance to internal neurons. Unfortunately, the\\nauthors found that calculating conductance in tensorflow required the addition\\nof several custom gradient operators and did not scale well. In this work, we\\nshow that the formula for Total Conductance is mathematically equivalent to\\nPath Integrated Gradients computed on a hidden layer in the network. We provide\\na scalable implementation of Total Conductance using standard tensorflow\\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\\napproach that is applicable to calculating internal neuron importance. We find\\nthat DeepLIFT produces strong empirical results and is faster to compute, but\\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\\nmay not always be preferred in practice. Colab notebook reproducing results:\\nthis http URL\\n',\n",
       " \"  An elementary rheory of concatenation is introduced and used to establish\\nmutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory,\\nthe quantifier-free part of Kirby's finitary set theory, and Adjunctive Set\\nTheory, with or without extensionality.\\n\",\n",
       " '  JavaBIP allows the coordination of software components by clearly separating\\nthe functional and coordination aspects of the system behavior. JavaBIP\\nimplements the principles of the BIP component framework rooted in rigorous\\noperational semantics. Recent work both on BIP and JavaBIP allows the\\ncoordination of static components defined prior to system deployment, i.e., the\\narchitecture of the coordinated system is fixed in terms of its component\\ninstances. Nevertheless, modern systems, often make use of components that can\\nregister and deregister dynamically during system execution. In this paper, we\\npresent an extension of JavaBIP that can handle this type of dynamicity. We use\\nfirst-order interaction logic to define synchronization constraints based on\\ncomponent types. Additionally, we use directed graphs with edge coloring to\\nmodel dependencies among components that determine the validity of an online\\nsystem. We present the software architecture of our implementation, provide and\\ndiscuss performance evaluation results.\\n',\n",
       " '  In rapid release development processes, patches that fix critical issues, or\\nimplement high-value features are often promoted directly from the development\\nchannel to a stabilization channel, potentially skipping one or more\\nstabilization channels. This practice is called patch uplift. Patch uplift is\\nrisky, because patches that are rushed through the stabilization phase can end\\nup introducing regressions in the code. This paper examines patch uplift\\noperations at Mozilla, with the aim to identify the characteristics of uplifted\\npatches that introduce regressions. Through statistical and manual analyses, we\\nquantitatively and qualitatively investigate the reasons behind patch uplift\\ndecisions and the characteristics of uplifted patches that introduced\\nregressions. Additionally, we interviewed three Mozilla release managers to\\nunderstand organizational factors that affect patch uplift decisions and\\noutcomes. Results show that most patches are uplifted because of a wrong\\nfunctionality or a crash. Uplifted patches that lead to faults tend to have\\nlarger patch size, and most of the faults are due to semantic or memory errors\\nin the patches. Also, release managers are more inclined to accept patch uplift\\nrequests that concern certain specific components, and-or that are submitted by\\ncertain specific developers.\\n',\n",
       " '  Examining games from a fresh perspective we present the idea of game-inspired\\nand game-based algorithms, dubbed \"gamorithms\".\\n',\n",
       " '  We establish the convergence rates and asymptotic distributions of the common\\nbreak change-point estimators, obtained by least squares and maximum likelihood\\nin panel data models and compare their asymptotic variances. Our model\\nassumptions accommodate a variety of commonly encountered probability\\ndistributions and, in particular, models of particular interest in econometrics\\nbeyond the commonly analyzed Gaussian model, including the zero-inflated\\nPoisson model for count data, and the probit and tobit models. We also provide\\nnovel results for time dependent data in the signal-plus-noise model, with\\nemphasis on a wide array of noise processes, including Gaussian process,\\nMA$(\\\\infty)$ and $m$-dependent processes. The obtained results show that\\nmaximum likelihood estimation requires a stronger signal-to-noise model\\nidentifiability condition compared to its least squares counterpart. Finally,\\nsince there are three different asymptotic regimes that depend on the behavior\\nof the norm difference of the model parameters before and after the change\\npoint, which cannot be realistically assumed to be known, we develop a novel\\ndata driven adaptive procedure that provides valid confidence intervals for the\\ncommon break, without requiring a priori knowledge of the asymptotic regime the\\nproblem falls in.\\n',\n",
       " '  The study of relays with the scope of energy-harvesting (EH) looks\\ninteresting as a means of enabling sustainable, wireless communication without\\nthe need to recharge or replace the battery driving the relays. However,\\nreliability of such communication systems becomes an important design challenge\\nwhen such relays scavenge energy from the information bearing RF signals\\nreceived from the source, using the technique of simultaneous wireless\\ninformation and power transfer (SWIPT). To this aim, this work studies\\nbidirectional communication in a decode-and-forward (DF) relay assisted\\ncooperative wireless network in presence of co-channel interference (CCI). In\\norder to quantify the reliability of the bidirectional communication systems, a\\nclosed form expression for the outage probability of the system is derived for\\nboth power splitting (PS) and time switching (TS) mode of operation of the\\nrelay. Simulation results are used to validate the accuracy of our analytical\\nresults and illustrate the dependence of the outage probability on various\\nsystem parameters, like PS factor, TS factor, and distance of the relay from\\nboth the users. Results of performance comparison between PS relaying (PSR) and\\nTS relaying (TSR) schemes are also presented. Besides, simulation results are\\nalso used to illustrate the spectral-efficiency and the energy-efficiency of\\nthe proposed system. The results show that, both in terms of spectral\\nefficiency and the energy-efficiency, the two-way communication system in\\npresence of moderate CCI power, performs better than the similar system without\\nCCI. Additionally, it is also found that PSR is superior to TSR protocol in\\nterms of peak energy-efficiency.\\n',\n",
       " '  Generative Adversarial Networks (GANs) were intuitively and attractively\\nexplained under the perspective of game theory, wherein two involving parties\\nare a discriminator and a generator. In this game, the task of the\\ndiscriminator is to discriminate the real and generated (i.e., fake) data,\\nwhilst the task of the generator is to generate the fake data that maximally\\nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs,\\nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows\\na connection between the general loss of a classification problem regarding a\\nconvex loss function and a f-divergence between the true and fake data\\ndistributions. Mathematically, we proposed a setting for the classification\\nproblem of the true and fake data, wherein we can prove that the general loss\\nof this classification problem is exactly the negative f-divergence for a\\ncertain convex function f. This allows us to interpret the problem of learning\\nthe generator for dismissing the f-divergence between the true and fake data\\ndistributions as that of maximizing the general loss which is equivalent to the\\nmin-max problem in GAN if the Logistic loss is used in the classification\\nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows\\nus to employ any convex loss function for the discriminator. Second, it\\nsuggests that rather than limiting ourselves in NN-based discriminators, we can\\nalternatively utilize other powerful families. Bearing this viewpoint, we then\\npropose using the kernel-based family for discriminators. This family has two\\nappealing features: i) a powerful capacity in classifying non-linear nature\\ndata and ii) being convex in the feature space. Using the convexity of this\\nfamily, we can further develop Fenchel duality to equivalently transform the\\nmax-min problem to the max-max dual problem.\\n',\n",
       " '  This paper is concerned with the computation of representation matrices for\\nthe action of Frobenius to the cohomology groups of algebraic varieties.\\nSpecifically we shall give an algorithm to compute the matrices for arbitrary\\nalgebraic varieties with defining equations over perfect fields of positive\\ncharacteristic, and estimate its complexity. Moreover, we propose a specific\\nefficient method, which works for complete intersections.\\n',\n",
       " '  We present possible explanations of pulsations in early B-type main sequence\\nstars which arise purely from the excitation of gravity modes. There are three\\nstars with this type of oscillations detected from the BRITE light curves:\\n$\\\\kappa$ Cen, a Car, $\\\\kappa$ Vel. We show that by changing metallicity or the\\nopacity profile it is possible in some models to dump pressure modes keeping\\ngravity modes unstable. Other possible scenario involves pulsations of a lower\\nmass companion.\\n',\n",
       " '  Recently introduced composition operator for credal sets is an analogy of\\nsuch operators in probability, possibility, evidence and valuation-based\\nsystems theories. It was designed to construct multidimensional models (in the\\nframework of credal sets) from a system of low- dimensional credal sets. In\\nthis paper we study its potential from the computational point of view\\nutilizing methods of polyhedral geometry.\\n',\n",
       " '  Internet-of-Things end-nodes demand low power processing platforms\\ncharacterized by heterogeneous dedicated units, controlled by a processor core\\nrunning concurrent control threads. Such architecture scheme fits one of the\\nmain target application domain of the RISC-V instruction set. We present an\\nopen-source processing core compliant with RISC-V on the software side and with\\nthe popular Pulpino processor platform on the hardware side, while supporting\\ninterleaved multi-threading for IoT applications. The latter feature is a novel\\ncontribution in this application domain. We report details about the\\nmicroarchitecture design along with performance data.\\n',\n",
       " '  During the last two decades, Genetic Programming (GP) has been largely used\\nto tackle optimization, classification, and automatic features selection\\nrelated tasks. The widespread use of GP is mainly due to its flexible and\\ncomprehensible tree-type structure. Similarly, research is also gaining\\nmomentum in the field of Image Processing (IP) because of its promising results\\nover wide areas of applications ranging from medical IP to multispectral\\nimaging. IP is mainly involved in applications such as computer vision, pattern\\nrecognition, image compression, storage and transmission, and medical\\ndiagnostics. This prevailing nature of images and their associated algorithm\\ni.e complexities gave an impetus to the exploration of GP. GP has thus been\\nused in different ways for IP since its inception. Many interesting GP\\ntechniques have been developed and employed in the field of IP. To give the\\nresearch community an extensive view of these techniques, this paper presents\\nthe diverse applications of GP in IP and provides useful resources for further\\nresearch. Also, comparison of different parameters used in ten different\\napplications of IP are summarized in tabular form. Moreover, analysis of\\ndifferent parameters used in IP related tasks is carried-out to save the time\\nneeded in future for evaluating the parameters of GP. As more advancement is\\nmade in GP methodologies, its success in solving complex tasks not only related\\nto IP but also in other fields will increase. Additionally, guidelines are\\nprovided for applying GP in IP related tasks, pros and cons of GP techniques\\nare discussed, and some future directions are also set.\\n',\n",
       " '  The control of dynamical, networked systems continues to receive much\\nattention across the engineering and scientific research fields. Of particular\\ninterest is the proper way to determine which nodes of the network should\\nreceive external control inputs in order to effectively and efficiently control\\nportions of the network. Published methods to accomplish this task either find\\na minimal set of driver nodes to guarantee controllability or a larger set of\\ndriver nodes which optimizes some control metric. Here, we investigate the\\ncontrol of lattice systems which provides analytical insight into the\\nrelationship between network structure and controllability. First we derive a\\nclosed form expression for the individual elements of the controllability\\nGramian of infinite lattice systems. Second, we focus on nearest neighbor\\nlattices for which the distance between nodes appears in the expression for the\\ncontrollability Gramian. We show that common control energy metrics scale\\nexponentially with respect to the maximum distance between a driver node and a\\ntarget node.\\n',\n",
       " '  We construct constant mean curvature surfaces in euclidean space with genus\\nzero and n ends asymptotic to Delaunay surfaces using the DPW method.\\n',\n",
       " '  We discuss various universality aspects of numerical computations using\\nstandard algorithms. These aspects include empirical observations and rigorous\\nresults. We also make various speculations about computation in a broader\\nsense.\\n',\n",
       " '  The relativistic jets created by some active galactic nuclei are important\\nagents of AGN feedback. In spite of this, our understanding of what produces\\nthese jets is still incomplete. X-ray observations, which can probe the\\nprocesses operating in the central regions in immediate vicinity of the\\nsupermassive black hole, the presumed jet launching point, are potentially\\nparticularly valuable in illuminating the jet formation process. Here, we\\npresent the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in\\na joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our\\nspectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and\\nconfirms the presence of ionized reflection in the source. From the average\\nspectrum we detect that the accretion disk is mildly recessed with an inner\\nradius of $R_\\\\mathrm{in}=4-180\\\\,R_\\\\mathrm{g}$. However, no significant\\nevolution of the inner radius is seen during the three months covered by our\\nNuSTAR campaign. This lack of variation could mean that the jet formation in\\nthis radio-loud quasar differs from what is observed in broad-line radio\\ngalaxies.\\n',\n",
       " '  A new synthesis scheme is proposed to effectively generate a random vector\\nwith prescribed joint density that induces a (latent) Gaussian tree structure.\\nThe quality of synthesis is measured by total variation distance between the\\nsynthesized and desired statistics. The proposed layered and successive\\nencoding scheme relies on the learned structure of tree to use minimal number\\nof common random variables to synthesize the desired density. We characterize\\nthe achievable rate region for the rate tuples of multi-layer latent Gaussian\\ntree, through which the number of bits needed to simulate such Gaussian joint\\ndensity are determined. The random sources used in our algorithm are the latent\\nvariables at the top layer of tree, the additive independent Gaussian noises,\\nand the Bernoulli sign inputs that capture the ambiguity of correlation signs\\nbetween the variables.\\n',\n",
       " '  We present a machine learning based information retrieval system for\\nastronomical observatories that tries to address user defined queries related\\nto an instrument. In the modern instrumentation scenario where heterogeneous\\nsystems and talents are simultaneously at work, the ability to supply with the\\nright information helps speeding up the detector maintenance operations.\\nEnhancing the detector uptime leads to increased coincidence observation and\\nimproves the likelihood for the detection of astrophysical signals. Besides,\\nsuch efforts will efficiently disseminate technical knowledge to a wider\\naudience and will help the ongoing efforts to build upcoming detectors like the\\nLIGO-India etc even at the design phase to foresee possible challenges. The\\nproposed method analyses existing documented efforts at the site to\\nintelligently group together related information to a query and to present it\\non-line to the user. The user in response can further go into interesting links\\nand find already developed solutions or probable ways to address the present\\nsituation optimally. A web application that incorporates the above idea has\\nbeen implemented and tested for LIGO Livingston, LIGO Hanford and Virgo\\nobservatories.\\n',\n",
       " \"  In today's databases, previous query answers rarely benefit answering future\\nqueries. For the first time, to the best of our knowledge, we change this\\nparadigm in an approximate query processing (AQP) context. We make the\\nfollowing observation: the answer to each query reveals some degree of\\nknowledge about the answer to another query because their answers stem from the\\nsame underlying distribution that has produced the entire dataset. Exploiting\\nand refining this knowledge should allow us to answer queries more\\nanalytically, rather than by reading enormous amounts of raw data. Also,\\nprocessing more queries should continuously enhance our knowledge of the\\nunderlying distribution, and hence lead to increasingly faster response times\\nfor future queries.\\nWe call this novel idea---learning from past query answers---Database\\nLearning. We exploit the principle of maximum entropy to produce answers, which\\nare in expectation guaranteed to be more accurate than existing sample-based\\napproximations. Empowered by this idea, we build a query engine on top of Spark\\nSQL, called Verdict. We conduct extensive experiments on real-world query\\ntraces from a large customer of a major database vendor. Our results\\ndemonstrate that Verdict supports 73.7% of these queries, speeding them up by\\nup to 23.0x for the same accuracy level compared to existing AQP systems.\\n\",\n",
       " '  Beam search is a desirable choice of test-time decoding algorithm for neural\\nsequence models because it potentially avoids search errors made by simpler\\ngreedy methods. However, typical cross entropy training procedures for these\\nmodels do not directly consider the behaviour of the final decoding method. As\\na result, for cross-entropy trained models, beam decoding can sometimes yield\\nreduced test performance when compared with greedy decoding. In order to train\\nmodels that can more effectively make use of beam search, we propose a new\\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\\napproach, we form a sub-differentiable surrogate objective by introducing a\\nnovel continuous approximation of the beam search decoding procedure. In\\nexperiments, we show that optimizing this new training objective yields\\nsubstantially better results on two sequence tasks (Named Entity Recognition\\nand CCG Supertagging) when compared with both cross entropy trained greedy\\ndecoding and cross entropy trained beam decoding baselines.\\n',\n",
       " '  In 1997 B. Weiss introduced the notion of measurably entire functions and\\nproved that they exist on every arbitrary free C- action defined on standard\\nprobability space. In the same paper he asked about the minimal possible growth\\nof measurably entire functions. In this work we show that for every arbitrary\\nfree C- action defined on a standard probability space there exists a\\nmeasurably entire function whose growth does not exceed exp (exp[log^p |z|])\\nfor any p > 3. This complements a recent result by Buhovski, Glücksam,\\nLogunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot\\ngrow slower than exp (exp[log^p |z|]) for any p < 2.\\n',\n",
       " '  In this paper, we show how to construct graph theoretical models of\\nn-dimensional continuous objects and manifolds. These models retain topological\\nproperties of their continuous counterparts. An LCL collection of n-cells in\\nEuclidean space is introduced and investigated. If an LCL collection of n-cells\\nis a cover of a continuous n-dimensional manifold then the intersection graph\\nof this cover is a digital closed n-dimensional manifold with the same topology\\nas its continuous counterpart. As an example, we prove that the digital model\\nof a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2\\npoints, the digital model of a continuous projective plane is a digital\\nprojective plane with at least eleven points, the digital model of a continuous\\nKlein bottle is the digital Klein bottle with at least sixteen points, the\\ndigital model of a continuous torus is the digital torus with at least sixteen\\npoints and the digital model of a continuous Moebius band is the digital\\nMoebius band with at least twelve points.\\n',\n",
       " '  Let $f(x,y)=ax^2+bxy+cy^2$ be a binary quadratic form with integer\\ncoefficients. For a prime $p$ not dividing the discriminant of $f$, we say $f$\\nis completely $p$-primitive if for any non-zero integer $N$, the diophantine\\nequation $f(x,y)=N$ has always an integer solution $(x,y)=(m,n)$ with\\n$(m,n,p)=1$ whenever it has an integer solution. In this article, we study\\nvarious properties of completely $p$-primitive binary quadratic forms. In\\nparticular, we give a necessary and sufficient condition for a definite binary\\nquadratic form $f$ to be completely $p$-primitive.\\n',\n",
       " '  The numerical availability of statistical inference methods for a modern and\\nrobust analysis of longitudinal- and multivariate data in factorial experiments\\nis an essential element in research and education. While existing approaches\\nthat rely on specific distributional assumptions of the data (multivariate\\nnormality and/or characteristic covariance matrices) are implemented in\\nstatistical software packages, there is a need for user-friendly software that\\ncan be used for the analysis of data that do not fulfill the aforementioned\\nassumptions and provide accurate p-value and confidence interval estimates.\\nTherefore, newly developed statistical methods for the analysis of repeated\\nmeasures designs and multivariate data that neither assume multivariate\\nnormality nor specific covariance matrices have been implemented in the freely\\navailable R-package MANOVA.RM. The package is equipped with a graphical user\\ninterface for plausible applications in academia and other educational purpose.\\nSeveral motivating examples illustrate the application of the methods.\\n',\n",
       " '  Phaseless super-resolution is the problem of recovering an unknown signal\\nfrom measurements of the magnitudes of the low frequency Fourier transform of\\nthe signal. This problem arises in applications where measuring the phase, and\\nmaking high-frequency measurements, are either too costly or altogether\\ninfeasible. The problem is especially challenging because it combines the\\ndifficult problems of phase retrieval and classical super-resolution\\n',\n",
       " \"  This paper is the first chapter of three of the author's undergraduate\\nthesis. We study the random matrix ensemble of covariance matrices arising from\\nrandom $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and\\n$N$ white vertices, for $d_b \\\\gg \\\\log^4 N$. We simultaneously prove that the\\nGreen's functions of these covariance matrices and the adjacency matrices of\\nthe underlying graphs agree with the corresponding limiting law (e.g.\\nMarchenko-Pastur law for covariance matrices) down to the optimal scale. This\\nis an improvement from the previously known mesoscopic results. We obtain\\neigenvector delocalization for the covariance matrix ensemble as consequence,\\nas well as a weak rigidity estimate.\\n\",\n",
       " '  Photoelectron yields of extruded scintillation counters with titanium dioxide\\ncoating and embedded wavelength shifting fibers read out by silicon\\nphotomultipliers have been measured at the Fermilab Test Beam Facility using\\n120\\\\,GeV protons. The yields were measured as a function of transverse,\\nlongitudinal, and angular positions for a variety of scintillator compositions\\nand reflective coating mixtures, fiber diameters, and photosensor sizes. Timing\\nperformance was also studied. These studies were carried out by the Cosmic Ray\\nVeto Group of the Mu2e collaboration as part of their R\\\\&D program.\\n',\n",
       " '  We report a precise measurement of hyperfine structure in the $ \\\\rm\\n{3\\\\,S_{1/2}} $ state of the odd isotope of Li, namely $ \\\\rm {^7Li} $. The state\\nis excited from the ground $ \\\\rm {2\\\\,S_{1/2}} $ state (which has the same\\nparity) using two single-photon transitions via the intermediate $ \\\\rm\\n{2\\\\,P_{3/2}} $ state. The value of the hyperfine constant we measure is $ A =\\n93.095(52)$ MHz, which resolves two discrepant values reported in the\\nliterature measured using other techniques. Our value is also consistent with\\ntheoretical calculations.\\n',\n",
       " '  We study a dynamical system induced by the Artin reciprocity map for a global\\nfield. We translate the conjugacy of such dynamical systems into various\\narithmetical properties that are equivalent to field isomorphism, relating it\\nto anabelian geometry.\\n',\n",
       " '  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that\\nallows to distinguish smooth real algebraic surfaces up to birational\\ndiffeomorphism. As an application, we construct infinite families of smooth\\nrational real algebraic surfaces with trivial homology groups, whose real loci\\nare diffeomorphic to $\\\\mathbb{R}^2$, but which are pairwise not birationally\\ndiffeomorphic. There are thus infinitely many non-trivial models of the\\neuclidean plane, contrary to the compact case.\\n',\n",
       " \"  Effective communication is required for teams of robots to solve\\nsophisticated collaborative tasks. In practice it is typical for both the\\nencoding and semantics of communication to be manually defined by an expert;\\nthis is true regardless of whether the behaviors themselves are bespoke,\\noptimization based, or learned. We present an agent architecture and training\\nmethodology using neural networks to learn task-oriented communication\\nsemantics based on the example of a communication-unaware expert policy. A\\nperimeter defense game illustrates the system's ability to handle dynamically\\nchanging numbers of agents and its graceful degradation in performance as\\ncommunication constraints are tightened or the expert's observability\\nassumptions are broken.\\n\",\n",
       " \"  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a\\nplanetesimal born in another planetary system. This interloper exhibits a\\nvariable colour within a range that is broadly consistent with local small\\nbodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited\\nKuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an\\naxial ratio exceeding 5:1. Rotation period estimates are inconsistent and\\nvaried, with reported values between 6.9 and 8.3 hours. Here we analyse all\\navailable optical photometry reported to date. No single rotation period can\\nexplain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be\\nin an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or\\ntumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135\\nand 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the\\navailable data are insufficient to uniquely constrain the true frequencies and\\nshape. Assuming a body that responds to NPA rotation in a similar manner to\\nSolar System asteroids and comets, the timescale to damp 1I/'Oumuamua's\\ntumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling\\nwithin its parent planetary system, and will remain tumbling well after it has\\nleft ours.\\n\",\n",
       " '  In the context of orientable circuits and subcomplexes of these as\\nrepresenting certain singular spaces, we consider characteristic class formulas\\ngeneralizing those classical results as seen for the Riemann-Hurwitz formula\\nfor regulating the topology of branched covering maps and that for monoidal\\ntransformations which include the standard blowing-up process. Here the results\\nare presented as cap product pairings, which will be elements of a suitable\\nhomology theory, rather than characteristic numbers as would be the case when\\ntaking Kronecker products once Poincaré duality is defined. We further\\nconsider possible applications and examples including branched covering maps,\\nsingular varieties involving virtual tangent bundles, the\\nChern-Schwartz-MacPherson class, the homology L-class, generalized signature,\\nand the cohomology signature class.\\n',\n",
       " '  Purpose: Basic surgical skills of suturing and knot tying are an essential\\npart of medical training. Having an automated system for surgical skills\\nassessment could help save experts time and improve training efficiency. There\\nhave been some recent attempts at automated surgical skills assessment using\\neither video analysis or acceleration data. In this paper, we present a novel\\napproach for automated assessment of OSATS based surgical skills and provide an\\nanalysis of different features on multi-modal data (video and accelerometer\\ndata). Methods: We conduct the largest study, to the best of our knowledge, for\\nbasic surgical skills assessment on a dataset that contained video and\\naccelerometer data for suturing and knot-tying tasks. We introduce \"entropy\\nbased\" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy\\n(XApEn), which quantify the amount of predictability and regularity of\\nfluctuations in time-series data. The proposed features are compared to\\nexisting methods of Sequential Motion Texture (SMT), Discrete Cosine Transform\\n(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.\\nResults: We report average performance of different features across all\\napplicable OSATS criteria for suturing and knot tying tasks. Our analysis shows\\nthat the proposed entropy based features out-perform previous state-of-the-art\\nmethods using video data. For accelerometer data, our method performs better\\nfor suturing only. We also show that fusion of video and acceleration features\\ncan improve overall performance with the proposed entropy features achieving\\nhighest accuracy. Conclusions: Automated surgical skills assessment can be\\nachieved with high accuracy using the proposed entropy features. Such a system\\ncan significantly improve the efficiency of surgical training in medical\\nschools and teaching hospitals.\\n',\n",
       " '  Many complex systems share two characteristics: 1) they are stochastic in\\nnature, and 2) they are characterized by a large number of factors. At the same\\ntime, various natural complex systems appear to have two types of intertwined\\nconstituents that exhibit counteracting effects on their equilibrium. In this\\nstudy, we employ these few characteristics to lay the groundwork for analyzing\\nsuch complex systems. The equilibrium point of these systems is generally\\nstudied either through the kinetic notion of equilibrium or its energetic\\nnotion, but not both. We postulate that these systems attempt to regulate the\\nstate vector of their constituents such that both the kinetic and the energetic\\nnotions of equilibrium are met. Based on this postulate, we prove: 1) the\\nexistence of a point such that the kinetic notion of equilibrium is met for the\\nless abundant constituents and, at the same time, the state vector of more\\nabundant entities is regulated to minimize the energetic notion of equilibrium;\\n2) the effect of unboundedly increasing less (more) abundant constituents\\nstabilizes (destabilizes) the system; and 3) the (unrestricted) equilibrium of\\nthe system is the point at which the number of stabilizing and destabilizing\\nentities increase unboundedly with the same rate.\\n',\n",
       " '  Even- and odd-frequency superconductivity coexist due to broken time-reversal\\nsymmetry under magnetic field. In order to describe this mixing, we extend the\\nlinearized Eliashberg equation for the spin and charge fluctuation mechanism in\\nstrongly correlated electron systems. We apply this extended Eliashberg\\nequation to the odd-frequency superconductivity on a quasi-one-dimensional\\nisosceles triangular lattice under in-plane magnetic field and examine the\\neffect of the even-frequency component.\\n',\n",
       " '  Let X be an irreducible smooth projective curve, of genus at least two, over\\nan algebraically closed field k. Let $\\\\mathcal{M}^d_G$ denote the moduli stack\\nof principal G-bundles over X of fixed topological type $d \\\\in \\\\pi_1(G)$, where\\nG is any almost simple affine algebraic group over k. We prove that the\\nuniversal bundle over $X \\\\times \\\\mathcal{M}^d_G$ is stable with respect to any\\npolarization on $X \\\\times \\\\mathcal{M}^d_G$. A similar result is proved for the\\nPoincaré adjoint bundle over $X \\\\times M_G^{d, rs}$, where $M_G^{d, rs}$ is\\nthe coarse moduli space of regularly stable principal G-bundles over X of fixed\\ntopological type d.\\n',\n",
       " '  With $\\\\Fq$ the finite field of $q$ elements, we investigate the following\\nquestion. If $\\\\gamma$ generates $\\\\Fqn$ over $\\\\Fq$ and $\\\\beta$ is a non-zero\\nelement of $\\\\Fqn$, is there always an $a \\\\in \\\\Fq$ such that $\\\\beta(\\\\gamma + a)$\\nis a primitive element? We resolve this case when $n=3$, thereby proving a\\nconjecture by Cohen. We also improve substantially on what is known when $n=4$.\\n',\n",
       " '  In this paper we exhibit Morse geodesics, often called \"hyperbolic\\ndirections\", in infinite unbounded torsion groups. The groups studied are\\nlacunary hyperbolic groups and constructed using graded small cancellation\\nconditions. In all previously known examples, Morse geodesics were found in\\ngroups which also contained Morse elements, infinite order elements whose\\ncyclic subgroup gives a Morse quasi-geodesic. Our result presents the first\\nexample of a group which contains Morse geodesics but no Morse elements. In\\nfact, we show that there is an isometrically embedded $7$-regular tree inside\\nsuch groups where every infinite, simple path is a Morse geodesic.\\n',\n",
       " '  We study the ultimate bounds on the estimation of temperature for an\\ninteracting quantum system. We consider two coupled bosonic modes that are\\nassumed to be thermal and using quantum estimation theory establish the role\\nthe Hamiltonian parameters play in thermometry. We show that in the case of a\\nconserved particle number the interaction between the modes leads to a decrease\\nin the overall sensitivity to temperature, while interestingly, if particle\\nexchange is allowed with the thermal bath the converse is true. We explain this\\ndichotomy by examining the energy spectra. Finally, we devise experimentally\\nimplementable thermometry schemes that rely only on locally accessible\\ninformation from the total system, showing that almost Heisenberg limited\\nprecision can still be achieved, and we address the (im)possibility for\\nmultiparameter estimation in the system.\\n',\n",
       " \"  We show that publishing results using the statistical significance\\nfilter---publishing only when the p-value is less than 0.05---leads to a\\nvicious cycle of overoptimistic expectation of the replicability of results.\\nFirst, we show analytically that when true statistical power is relatively low,\\ncomputing power based on statistically significant results will lead to\\noverestimates of power. Then, we present a case study using 10 experimental\\ncomparisons drawn from a recently published meta-analysis in psycholinguistics\\n(Jäger et al., 2017). We show that the statistically significant results\\nyield an illusion of replicability. This illusion holds even if the researcher\\ndoesn't conduct any formal power analysis but just uses statistical\\nsignificance to informally assess robustness (i.e., replicability) of results.\\n\",\n",
       " '  The center-of-mass motion of a single optically levitated nanoparticle\\nresembles three uncoupled harmonic oscillators. We show how a suitable\\nmodulation of the optical trapping potential can give rise to a coupling\\nbetween two of these oscillators, such that their dynamics are governed by a\\nclassical equation of motion that resembles the Schrödinger equation for a\\ntwo-level system. Based on experimental data, we illustrate the dynamics of\\nthis parametrically coupled system both in the frequency and in the time\\ndomain. We discuss the limitations and differences of the mechanical analogue\\nin comparison to a true quantum mechanical system.\\n',\n",
       " '  We introduce new techniques to the analysis of neural spatiotemporal dynamics\\nvia applying $\\\\epsilon$-machine reconstruction to electroencephalography (EEG)\\nmicrostate sequences. Microstates are short duration quasi-stable states of the\\ndynamically changing electrical field topographies recorded via an array of\\nelectrodes from the human scalp, and cluster into four canonical classes. The\\nsequence of microstates observed under particular conditions can be considered\\nan information source with unknown underlying structure. $\\\\epsilon$-machines\\nare discrete dynamical system automata with state-dependent probabilities on\\ndifferent future observations (in this case the next measured EEG microstate).\\nThey artificially reproduce underlying structure in an optimally predictive\\nmanner as generative models exhibiting dynamics emulating the behaviour of the\\nsource. Here we present experiments using both simulations and empirical data\\nsupporting the value of associating these discrete dynamical systems with\\nmental states (e.g. mind-wandering, focused attention, etc.) and with clinical\\npopulations. The neurodynamics of mental states and clinical populations can\\nthen be further characterized by properties of these dynamical systems,\\nincluding: i) statistical complexity (determined by the number of states of the\\ncorresponding $\\\\epsilon$-automaton); ii) entropy rate; iii) characteristic\\nsequence patterning (syntax, probabilistic grammars); iv) duration, persistence\\nand stability of dynamical patterns; and v) algebraic measures such as\\nKrohn-Rhodes complexity or holonomy length of the decompositions of these. The\\npotential applications include the characterization of mental states in\\nneurodynamic terms for mental health diagnostics, well-being interventions,\\nhuman-machine interface, and others on both subject-specific and\\ngroup/population-level.\\n',\n",
       " '  We describe a 20-year survey carried out by the Lick-Carnegie Exoplanet\\nSurvey Team (LCES), using precision radial velocities from HIRES on the Keck-I\\ntelescope to find and characterize extrasolar planetary systems orbiting nearby\\nF, G, K, and M dwarf stars. We provide here 60,949 precision radial velocities\\nfor 1,624 stars contained in that survey. We tabulate a list of 357 significant\\nperiodic signals that are of constant period and phase, and not coincident in\\nperiod and/or phase with stellar activity indices. These signals are thus\\nstrongly suggestive of barycentric reflex motion of the star induced by one or\\nmore candidate exoplanets in Keplerian motion about the host star. Of these\\nsignals, 225 have already been published as planet claims, 60 are classified as\\nsignificant unpublished planet candidates that await photometric follow-up to\\nrule out activity-related causes, and 54 are also unpublished, but are\\nclassified as \"significant\" signals that require confirmation by additional\\ndata before rising to classification as planet candidates. Of particular\\ninterest is our detection of a candidate planet with a minimum mass of 3.9\\nEarth masses and an orbital period of 9.9 days orbiting Lalande 21185, the\\nfourth-closest main sequence star to the Sun. For each of our exoplanetary\\ncandidate signals, we provide the period and semi-amplitude of the Keplerian\\norbital fit, and a likelihood ratio estimate of its statistical significance.\\nWe also tabulate 18 Keplerian-like signals that we classify as likely arising\\nfrom stellar activity.\\n',\n",
       " '  Artificial intelligence methods have often been applied to perform specific\\nfunctions or tasks in the cyber-defense realm. However, as adversary methods\\nbecome more complex and difficult to divine, piecemeal efforts to understand\\ncyber-attacks, and malware-based attacks in particular, are not providing\\nsufficient means for malware analysts to understand the past, present and\\nfuture characteristics of malware.\\nIn this paper, we present the Malware Analysis and Attributed using Genetic\\nInformation (MAAGI) system. The underlying idea behind the MAAGI system is that\\nthere are strong similarities between malware behavior and biological organism\\nbehavior, and applying biologically inspired methods to corpora of malware can\\nhelp analysts better understand the ecosystem of malware attacks. Due to the\\nsophistication of the malware and the analysis, the MAAGI system relies heavily\\non artificial intelligence techniques to provide this capability. It has\\nalready yielded promising results over its development life, and will hopefully\\ninspire more integration between the artificial intelligence and cyber--defense\\ncommunities.\\n',\n",
       " '  Let a and b be algebraic numbers such that exactly one of a and b is an\\nalgebraic integer, and let f_t(z):=z^2+t be a family of polynomials\\nparametrized by t. We prove that the set of all algebraic numbers t for which\\nthere exist positive integers m and n such that f_t^m(a)=f_t^n(b) has bounded\\nWeil height. This is a special case of a more general result supporting a new\\nbounded height conjecture in dynamics. Our results fit into the general setting\\nof the principle of unlikely intersections in arithmetic dynamics.\\n',\n",
       " '  This note establishes the input-to-state stability (ISS) property for a\\nclamped-free damped string with respect to distributed and boundary\\ndisturbances. While efficient methods for establishing ISS properties for\\ndistributed parameter systems with respect to distributed disturbances have\\nbeen developed during the last decades, establishing ISS properties with\\nrespect to boundary disturbances remains challenging. One of the well-known\\nmethods for well-posedness analysis of systems with boundary inputs is to use\\nan adequate lifting operator, which transfers the boundary disturbance to a\\ndistributed one. However, the resulting distributed disturbance involves time\\nderivatives of the boundary perturbation. Thus, the subsequent ISS estimate\\ndepends on its amplitude, and may not be expressed in the strict form of ISS\\nproperties. To solve this problem, we show for a clamped-free damped string\\nequation that the projection of the original system trajectories in an adequate\\nRiesz basis can be used to establish the desired ISS property.\\n',\n",
       " \"  The problem of reliable communication over the multiple-access channel (MAC)\\nwith states is investigated. We propose a new coding scheme for this problem\\nwhich uses quasi-group codes (QGC). We derive a new computable single-letter\\ncharacterization of the achievable rate region. As an example, we investigate\\nthe problem of doubly-dirty MAC with modulo-$4$ addition. It is shown that the\\nsum-rate $R_1+R_2=1$ bits per channel use is achievable using the new scheme.\\nWhereas, the natural extension of the Gel'fand-Pinsker scheme, sum-rates\\ngreater than $0.32$ are not achievable.\\n\",\n",
       " '  We present a new paradigm for understanding optical absorption and hot\\nelectron dynamics experiments in graphene. Our analysis pivots on assigning\\nproper importance to phonon assisted indirect processes and bleaching of direct\\nprocesses. We show indirect processes figure in the excess absorption in the UV\\nregion. Experiments which were thought to indicate ultrafast relaxation of\\nelectrons and holes, reaching a thermal distribution from an extremely\\nnon-thermal one in under 5-10 fs, instead are explained by the nascent electron\\nand hole distributions produced by indirect transitions. These need no\\nrelaxation or ad-hoc energy removal to agree with the observed emission spectra\\nand fast pulsed absorption spectra. The fast emission following pulsed\\nabsorption is dominated by phonon assisted processes, which vastly outnumber\\ndirect ones and are always available, connecting any electron with any hole any\\ntime. Calculations are given, including explicitly calculating the magnitude of\\nindirect processes, supporting these views.\\n',\n",
       " '  We propose a probabilistic model for interpreting gene expression levels that\\nare observed through single-cell RNA sequencing. In the model, each cell has a\\nlow-dimensional latent representation. Additional latent variables account for\\ntechnical effects that may erroneously set some observations of gene expression\\nlevels to zero. Conditional distributions are specified by neural networks,\\ngiving the proposed model enough flexibility to fit the data well. We use\\nvariational inference and stochastic optimization to approximate the posterior\\ndistribution. The inference procedure scales to over one million cells, whereas\\ncompeting algorithms do not. Even for smaller datasets, for several tasks, the\\nproposed procedure outperforms state-of-the-art methods like ZIFA and\\nZINB-WaVE. We also extend our framework to take into account batch effects and\\nother confounding factors and propose a natural Bayesian hypothesis framework\\nfor differential expression that outperforms tradition DESeq2.\\n',\n",
       " \"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered\\nby radioactive beta decay of hundreds of $r$-process nuclides. We derive, using\\nFermi's theory of beta decay, an analytic estimate of the nuclear heating rate.\\nWe show that the heating rate evolves as a power law ranging between $t^{-6/5}$\\nto $t^{-4/3}$. The overall magnitude of the heating rate is determined by the\\nmean values of nuclear quantities, e.g., the nuclear matrix elements of beta\\ndecay. These values are specified by using nuclear experimental data. We\\ndiscuss the role of higher order beta transitions and the robustness of the\\npower law. The robust and simple form of the heating rate suggests that\\nobservations of the late-time bolometric light curve $\\\\propto t^{-\\\\frac{4}{3}}$\\nwould be a direct evidence of a $r$-process driven macronova. Such observations\\ncould also enable us to estimate the total amount of $r$-process nuclei\\nproduced in the merger.\\n\",\n",
       " '  The calculation of caloric properties such as heat capacity, Joule-Thomson\\ncoefficients and the speed of sound by classical force-field-based molecular\\nsimulation methodology has received scant attention in the literature,\\nparticularly for systems composed of complex molecules whose force fields (FFs)\\nare characterized by a combination of intramolecular and intermolecular terms\\n(referred to herein as \"flexible FFs\"). The calculation of a thermodynamic\\nproperty for a system whose molecules are described by such a FF involves the\\ncalculation of the residual property prior to its addition to the corresponding\\nideal-gas (IG) property, the latter of which is separately calculated, either\\nusing thermochemical compilations or nowadays accurate quantum mechanical\\ncalculations. Although the simulation of a volumetric residual property\\nproceeds by simply replacing the intermolecular FF in the rigid molecule case\\nby the total (intramolecular plus intermolecular) FF, this is not the case for\\na caloric property. We discuss the methodology required in performing such\\ncalculations, and focus on the example of the molar heat capacity at constant\\npressure, $c_P$, one of the most important caloric properties. We also consider\\nthree approximations for the calculation procedure, and illustrate their\\nconsequences for the examples of the relatively simple molecule 2-propanol,\\n${\\\\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\\\\rm HO(CH_2)_2NH_2}$, an\\nimportant fluid used in carbon capture.\\n',\n",
       " '  We study well-posedness of a velocity-vorticity formulation of the\\nNavier--Stokes equations, supplemented with no-slip velocity boundary\\nconditions, a no-penetration vorticity boundary condition, along with a natural\\nvorticity boundary condition depending on a pressure functional. In the\\nstationary case we prove existence and uniqueness of a suitable weak solution\\nto the system under a small data condition. The topic of the paper is driven by\\nrecent developments of vorticity based numerical methods for the Navier--Stokes\\nequations.\\n',\n",
       " '  Generative Adversarial Networks (GANs) represent a promising class of\\ngenerative networks that combine neural networks with game theory. From\\ngenerating realistic images and videos to assisting musical creation, GANs are\\ntransforming many fields of arts and sciences. However, their application to\\nhealthcare has not been fully realized, more specifically in generating\\nelectronic health records (EHR) data. In this paper, we propose a framework for\\nexploring the value of GANs in the context of continuous laboratory time series\\ndata. We devise an unsupervised evaluation method that measures the predictive\\npower of synthetic laboratory test time series. Further, we show that when it\\ncomes to predicting the impact of drug exposure on laboratory test data,\\nincorporating representation learning of the training cohorts prior to training\\nGAN models is beneficial.\\n',\n",
       " '  A database of minima and transition states corresponds to a network where the\\nminima represent nodes and the transition states correspond to edges between\\nthe pairs of minima they connect via steepest-descent paths. Here we construct\\nnetworks for small clusters bound by the Morse potential for a selection of\\nphysically relevant parameters, in two and three dimensions. The properties of\\nthese unweighted and undirected networks are analysed to examine two features:\\nwhether they are small-world, where the shortest path between nodes involves\\nonly a small number or edges; and whether they are scale-free, having a degree\\ndistribution that follows a power law. Small-world character is present, but\\nstatistical tests show that a power law is not a good fit, so the networks are\\nnot scale-free. These results for clusters are compared with the corresponding\\nproperties for the molecular and atomic structural glass formers\\northo-terphenyl and binary Lennard-Jones. These glassy systems do not show\\nsmall-world properties, suggesting that such behaviour is linked to the\\nstructure-seeking landscapes of the Morse clusters.\\n',\n",
       " '  Asynchronous distributed machine learning solutions have proven very\\neffective so far, but always assuming perfectly functioning workers. In\\npractice, some of the workers can however exhibit Byzantine behavior, caused by\\nhardware failures, software bugs, corrupt data, or even malicious attacks. We\\nintroduce \\\\emph{Kardam}, the first distributed asynchronous stochastic gradient\\ndescent (SGD) algorithm that copes with Byzantine workers. Kardam consists of\\ntwo complementary components: a filtering and a dampening component. The first\\nis scalar-based and ensures resilience against $\\\\frac{1}{3}$ Byzantine workers.\\nEssentially, this filter leverages the Lipschitzness of cost functions and acts\\nas a self-stabilizer against Byzantine workers that would attempt to corrupt\\nthe progress of SGD. The dampening component bounds the convergence rate by\\nadjusting to stale information through a generic gradient weighting scheme. We\\nprove that Kardam guarantees almost sure convergence in the presence of\\nasynchrony and Byzantine behavior, and we derive its convergence rate. We\\nevaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead\\nwith respect to non Byzantine-resilient solutions. We empirically show that\\nKardam does not introduce additional noise to the learning procedure but does\\ninduce a slowdown (the cost of Byzantine resilience) that we both theoretically\\nand empirically show to be less than $f/n$, where $f$ is the number of\\nByzantine failures tolerated and $n$ the total number of workers.\\nInterestingly, we also empirically observe that the dampening component is\\ninteresting in its own right for it enables to build an SGD algorithm that\\noutperforms alternative staleness-aware asynchronous competitors in\\nenvironments with honest workers.\\n',\n",
       " '  This paper describes an English audio and textual dataset of debating\\nspeeches, a unique resource for the growing research field of computational\\nargumentation and debating technologies. We detail the process of speech\\nrecording by professional debaters, the transcription of the speeches with an\\nAutomatic Speech Recognition (ASR) system, their consequent automatic\\nprocessing to produce a text that is more \"NLP-friendly\", and in parallel --\\nthe manual transcription of the speeches in order to produce gold-standard\\n\"reference\" transcripts. We release 60 speeches on various controversial\\ntopics, each in five formats corresponding to the different stages in the\\nproduction of the data. The intention is to allow utilizing this resource for\\nmultiple research purposes, be it the addition of in-domain training data for a\\ndebate-specific ASR system, or applying argumentation mining on either noisy or\\nclean debate transcripts. We intend to make further releases of this data in\\nthe future.\\n',\n",
       " '  Deep learning has been demonstrated to achieve excellent results for image\\nclassification and object detection. However, the impact of deep learning on\\nvideo analysis (e.g. action detection and recognition) has been limited due to\\ncomplexity of video data and lack of annotations. Previous convolutional neural\\nnetworks (CNN) based video action detection approaches usually consist of two\\nmajor steps: frame-level action proposal detection and association of proposals\\nacross frames. Also, these methods employ two-stream CNN framework to handle\\nspatial and temporal feature separately. In this paper, we propose an\\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\\naction detection in videos. The proposed architecture is a unified network that\\nis able to recognize and localize action based on 3D convolution features. A\\nvideo is first divided into equal length clips and for each clip a set of tube\\nproposals are generated next based on 3D Convolutional Network (ConvNet)\\nfeatures. Finally, the tube proposals of different clips are linked together\\nemploying network flow and spatio-temporal action detection is performed using\\nthese linked video proposals. Extensive experiments on several video datasets\\ndemonstrate the superior performance of T-CNN for classifying and localizing\\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.\\n',\n",
       " '  We develop the theoretical foundations of a network distance that has\\nrecently been applied to various subfields of topological data analysis, namely\\npersistent homology and hierarchical clustering. While this network distance\\nhas previously appeared in the context of finite networks, we extend the\\nsetting to that of compact networks. The main challenge in this new setting is\\nthe lack of an easy notion of sampling from compact networks; we solve this\\nproblem in the process of obtaining our results. The generality of our setting\\nmeans that we automatically establish results for exotic objects such as\\ndirected metric spaces and Finsler manifolds. We identify readily computable\\nnetwork invariants and establish their quantitative stability under this\\nnetwork distance. We also discuss the computational complexity involved in\\nprecisely computing this distance, and develop easily-computable lower bounds\\nby using the identified invariants. By constructing a wide range of explicit\\nexamples, we show that these lower bounds are effective in distinguishing\\nbetween networks. Finally, we provide a simple algorithm that computes a lower\\nbound on the distance between two networks in polynomial time and illustrate\\nour metric and invariant constructions on a database of random networks and a\\ndatabase of simulated hippocampal networks.\\n',\n",
       " '  Using the Panama Papers, we show that the beginning of media reporting on\\nexpropriations and property confiscations in a country increases the\\nprobability that offshore entities are incorporated by agents from the same\\ncountry in the same month. This result is robust to the use of country-year\\nfixed effects and the exclusion of tax havens. Further analysis shows that the\\neffect is driven by countries with non-corrupt and effective governments, which\\nsupports the notion that offshore entities are incorporated when reasonably\\nwell-intended and well-functioning governments become more serious about\\nfighting organized crime by confiscating proceeds of crime.\\n',\n",
       " '  Mammography screening for early detection of breast lesions currently suffers\\nfrom high amounts of false positive findings, which result in unnecessary\\ninvasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many\\nof these false-positive findings prior to biopsy. Current approaches estimate\\ntissue properties by means of quantitative parameters taken from generative,\\nbiophysical models fit to the q-space encoded signal under certain assumptions\\nregarding noise and spatial homogeneity. This process is prone to fitting\\ninstability and partial information loss due to model simplicity. We reveal\\nunexplored potentials of the signal by integrating all data processing\\ncomponents into a convolutional neural network (CNN) architecture that is\\ndesigned to propagate clinical target information down to the raw input images.\\nThis approach enables simultaneous and target-specific optimization of image\\nnormalization, signal exploitation, global representation learning and\\nclassification. Using a multicentric data set of 222 patients, we demonstrate\\nthat our approach significantly improves clinical decision making with respect\\nto the current state of the art.\\n',\n",
       " '  Small depth networks arise in a variety of network related applications,\\noften in the form of maximum flow and maximum weighted matching. Recent works\\nhave generalized such methods to include costs arising from concave functions.\\nIn this paper we give an algorithm that takes a depth $D$ network and strictly\\nincreasing concave weight functions of flows on the edges and computes a $(1 -\\n\\\\epsilon)$-approximation to the maximum weight flow in time $mD \\\\epsilon^{-1}$\\ntimes an overhead that is logarithmic in the various numerical parameters\\nrelated to the magnitudes of gradients and capacities.\\nOur approach is based on extending the scaling algorithm for approximate\\nmaximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small\\ndepth networks, and then generalizing it to concave functions. In this more\\nrestricted setting of linear weights in the range $[w_{\\\\min}, w_{\\\\max}]$, it\\nproduces a $(1 - \\\\epsilon)$-approximation in time $O(mD \\\\epsilon^{-1} \\\\log(\\nw_{\\\\max} /w_{\\\\min}))$. The algorithm combines a variety of tools and provides a\\nunified approach towards several problems involving small depth networks.\\n',\n",
       " '  Knowledge-intensive companies that adopt Agile Software Development (ASD)\\nrelay on efficient implementation of Knowledge Management (KM) strategies to\\npromotes different Knowledge Processes (KPs) to gain competitive advantage.\\nThis study aims to explore how companies that adopt ASD implement KM strategies\\nutilizing practices that promote the KPs in the different organizational\\nlayers. Through a systematic literature review, we analyzed 32 primary studies,\\nselected by automated search and snowballing in the extant literature. To\\nanalyze the data, we applied narrative synthesis. Most of the identified KM\\npractices implement personalization strategies (81 %), supported by\\ncodification (19 %). Our review shows that the primary studies do not report KM\\npractices in the strategic layer and two of them in the product portfolio\\nlayer; on the other hand, in the project layer, the studies report 33 practices\\nthat implement personalization strategy, and seven practices that implement\\ncodification. KM strategies in ASD promote mainly the knowledge transfer\\nprocess with practices that stimulate social interaction to share tacit\\nknowledge in the project layer. As a result of using informal communication, a\\nsignificant amount of knowledge can be lost or not properly transferred to\\nother individuals and, instead of propagating the knowledge, it remains inside\\na few individuals minds.\\n',\n",
       " '  Recent 60Fe results have suggested that the estimated distances of supernovae\\nin the last few million years should be reduced from 100 pc to 50 pc. Two\\nevents or series of events are suggested, one about 2.7 million years to 1.7\\nmillion years ago, and another may at 6.5 to 8.7 million years ago. We ask what\\neffects such supernovae are expected to have on the terrestrial atmosphere and\\nbiota. Assuming that the Local Bubble was formed before the event being\\nconsidered, and that the supernova and the Earth were both inside a weak,\\ndisordered magnetic field at that time, TeV-PeV cosmic rays at Earth will\\nincrease by a factor of a few hundred. Tropospheric ionization will increase\\nproportionately, and the overall muon radiation load on terrestrial organisms\\nwill increase by a factor of 150. All return to pre-burst levels within 10kyr.\\nIn the case of an ordered magnetic field, effects depend strongly on the field\\norientation. The upper bound in this case is with a largely coherent field\\naligned along the line of sight to the supernova, in which case TeV-PeV cosmic\\nray flux increases are 10^4; in the case of a transverse field they are below\\ncurrent levels. We suggest a substantial increase in the extended effects of\\nsupernovae on Earth and in the lethal distance estimate; more work is\\nneeded.This paper is an explicit followup to Thomas et al. (2016). We also here\\nprovide more detail on the computational procedures used in both works.\\n',\n",
       " '  The surface tension of flowing soap films is measured with respect to the\\nfilm thickness and the concentration of soap solution. We perform this\\nmeasurement by measuring the curvature of the nylon wires that bound the soap\\nfilm channel and use the measured curvature to parametrize the relation between\\nthe surface tension and the tension of the wire. We find the surface tension of\\nour soap films increases when the film is relatively thin or made of soap\\nsolution of low concentration, otherwise it approaches an asymptotic value 30\\nmN/m. A simple adsorption model with only two parameters describes our\\nobservations reasonably well. With our measurements, we are also able to\\nmeasure Gibbs elasticity for our soap film.\\n',\n",
       " '  Indoor localization based on Visible Light Communication (VLC) has been in\\nfavor with both the academia and industry for years. In this paper, we present\\na prototyping photodiode-based VLC system towards large-scale localization.\\nSpecially, we give in-depth analysis of the design constraints and\\nconsiderations for large-scale indoor localization research. After that we\\nidentify the key enablers for such systems: 1) distributed architecture, 2)\\none-way communication, and 3) random multiple access. Accordingly, we propose\\nPlugo -- a photodiode-based VLC system conforming to the aforementioned\\ncriteria. We present a compact design of the VLC-compatible LED bulbs featuring\\nplug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii\\nArea (ALOHA) is exploited to achieve random multiple access over the shared\\noptical medium. We show its effectiveness in beacon broadcasting by\\nexperiments, and further demonstrate its scalability to large-scale scenarios\\nthrough simulations. Finally, preliminary localization experiments are\\nconducted using fingerprinting-based methods in a customized testbed, achieving\\nan average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.\\n',\n",
       " '  We show that the output of a (residual) convolutional neural network (CNN)\\nwith an appropriate prior over the weights and biases is a Gaussian process\\n(GP) in the limit of infinitely many convolutional filters, extending similar\\nresults for dense networks. For a CNN, the equivalent kernel can be computed\\nexactly and, unlike \"deep kernels\", has very few parameters: only the\\nhyperparameters of the original CNN. Further, we show that this kernel has two\\nproperties that allow it to be computed efficiently; the cost of evaluating the\\nkernel for a pair of images is similar to a single forward pass through the\\noriginal CNN with only one filter per layer. The kernel equivalent to a\\n32-layer ResNet obtains 0.84% classification error on MNIST, a new record for\\nGPs with a comparable number of parameters.\\n',\n",
       " '  Detecting attacks in control systems is an important aspect of designing\\nsecure and resilient control systems. Recently, a dynamic watermarking approach\\nwas proposed for detecting malicious sensor attacks for SISO LTI systems with\\npartial state observations and MIMO LTI systems with a full rank input matrix\\nand full state observations; however, these previous approaches cannot be\\napplied to general LTI systems that are MIMO and have partial state\\nobservations. This paper designs a dynamic watermarking approach for detecting\\nmalicious sensor attacks for general LTI systems, and we provide a new set of\\nasymptotic and statistical tests. We prove these tests can detect attacks that\\nfollow a specified attack model (more general than replay attacks), and we also\\nshow that these tests simplify to existing tests when the system is SISO or has\\nfull rank input matrix and full state observations. The benefit of our approach\\nis demonstrated with a simulation analysis of detecting sensor attacks in\\nautonomous vehicles. Our approach can distinguish between sensor attacks and\\nwind disturbance (through an internal model principle framework), whereas\\nimproperly designed tests cannot distinguish between sensor attacks and wind\\ndisturbance.\\n',\n",
       " '  For people with visual impairments, tactile graphics are an important means\\nto learn and explore information. However, raised line tactile graphics created\\nwith traditional materials such as embossing are static. While available\\nrefreshable displays can dynamically change the content, they are still too\\nexpensive for many users, and are limited in size. These factors limit\\nwide-spread adoption and the representation of large graphics or data sets. In\\nthis paper, we present FluxMaker, an inexpensive scalable system that renders\\ndynamic information on top of static tactile graphics with movable tactile\\nmarkers. These dynamic tactile markers can be easily reconfigured and used to\\nannotate static raised line tactile graphics, including maps, graphs, and\\ndiagrams. We developed a hardware prototype that actuates magnetic tactile\\nmarkers driven by low-cost and scalable electromagnetic coil arrays, which can\\nbe fabricated with standard printed circuit board manufacturing. We evaluate\\nour prototype with six participants with visual impairments and found positive\\nresults across four application areas: location finding or navigating on\\ntactile maps, data analysis, and physicalization, feature identification for\\ntactile graphics, and drawing support. The user study confirms advantages in\\napplication domains such as education and data exploration.\\n',\n",
       " '  Very often features come with their own vectorial descriptions which provide\\ndetailed information about their properties. We refer to these vectorial\\ndescriptions as feature side-information. In the standard learning scenario,\\ninput is represented as a vector of features and the feature side-information\\nis most often ignored or used only for feature selection prior to model\\nfitting. We believe that feature side-information which carries information\\nabout features intrinsic property will help improve model prediction if used in\\na proper way during learning process. In this paper, we propose a framework\\nthat allows for the incorporation of the feature side-information during the\\nlearning of very general model families to improve the prediction performance.\\nWe control the structures of the learned models so that they reflect features\\nsimilarities as these are defined on the basis of the side-information. We\\nperform experiments on a number of benchmark datasets which show significant\\npredictive performance gains, over a number of baselines, as a result of the\\nexploitation of the side-information.\\n',\n",
       " '  We give a short proof of the $L^{1}$ criterion for Beurling generalized\\nintegers to have a positive asymptotic density. We actually prove the existence\\nof density under a weaker hypothesis. We also discuss related sufficient\\nconditions for the estimate $m(x)=\\\\sum_{n_{k}\\\\leq x} \\\\mu(n_k)/n_k=o(1)$, with\\n$\\\\mu$ the Beurling analog of the Moebius function.\\n',\n",
       " '  Social media users often make explicit predictions about upcoming events.\\nSuch statements vary in the degree of certainty the author expresses toward the\\noutcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\"\\nor \"No way Leonardo wins!\". Can popular beliefs on social media predict who\\nwill win? To answer this question, we build a corpus of tweets annotated for\\nveridicality on which we train a log-linear classifier that detects positive\\nveridicality with high precision. We then forecast uncertain outcomes using the\\nwisdom of crowds, by aggregating users\\' explicit predictions. Our method for\\nforecasting winners is fully automated, relying only on a set of contenders as\\ninput. It requires no training data of past outcomes and outperforms sentiment\\nand tweet volume baselines on a broad range of contest prediction tasks. We\\nfurther demonstrate how our approach can be used to measure the reliability of\\nindividual accounts\\' predictions and retrospectively identify surprise\\noutcomes.\\n',\n",
       " '  Applications involving autonomous navigation and planning of mobile agents\\ncan benefit greatly by employing online Simultaneous Localization and Mapping\\n(SLAM) techniques, however, their proper implementation still warrants an\\nefficient amalgamation with any offline path planning method that may be used\\nfor the particular application. In this paper, such a case of amalgamation is\\nconsidered for a LiDAR-based indoor mapping system which presents itself as a\\n2D coverage path planning problem implemented along with online SLAM. This\\npaper shows how classic offline Coverage Path Planning (CPP) can be altered for\\nuse with online SLAM by proposing two modifications: (i) performing convex\\ndecomposition of the polygonal coverage area to allow for an arbitrary choice\\nof an initial point while still tracing the shortest coverage path and (ii)\\nusing a new approach to stitch together the different cells within the\\npolygonal area to form a continuous coverage path. Furthermore, an alteration\\nto the SLAM operation to suit the coverage path planning strategy is also made\\nthat evaluates navigation errors in terms of an area coverage cost function.\\nThe implementation results show how the combination of the two modified offline\\nand online planning strategies allow for an improvement in the total area\\ncoverage by the mapping system - the modification thus presents an approach for\\nmodifying offline and online navigation strategies for robust operation.\\n',\n",
       " '  We apply a method that combines the tight-binding approximation and the\\nLowdin down-folding procedure to evaluate the electronic band structure of the\\nnewly discovered pressure-induced superconductor CrAs. By integrating out all\\nlow-lying arsenic degrees of freedom, we derive an effective Hamiltonian model\\ndescribing the Cr d bands near the Fermi level. We calculate and make\\npredictions for the energy spectra, the Fermi surface, the density of states\\nand transport and magnetic properties of this compound. Our results are\\nconsistent with local-density approximation calculations as well as they show\\ngood agreement with available experimental data for resistivity and Cr magnetic\\nmoment.\\n',\n",
       " '  In this lecture note, we describe high dynamic range (HDR) imaging systems;\\nsuch systems are able to represent luminances of much larger brightness and,\\ntypically, also a larger range of colors than conventional standard dynamic\\nrange (SDR) imaging systems. The larger luminance range greatly improve the\\noverall quality of visual content, making it appears much more realistic and\\nappealing to observers. HDR is one of the key technologies of the future\\nimaging pipeline, which will change the way the digital visual content is\\nrepresented and manipulated today.\\n',\n",
       " '  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use\\nthis classification to build a pro-$p$ analogue of the curve complex and\\nestablish its basic properties. We conclude with some statements concerning\\nseparability properties of the mapping class group.\\n',\n",
       " '  Sports data analysis is becoming increasingly large-scale, diversified, and\\nshared, but difficulty persists in rapidly accessing the most crucial\\ninformation. Previous surveys have focused on the methodologies of sports video\\nanalysis from the spatiotemporal viewpoint instead of a content-based\\nviewpoint, and few of these studies have considered semantics. This study\\ndevelops a deeper interpretation of content-aware sports video analysis by\\nexamining the insight offered by research into the structure of content under\\ndifferent scenarios. On the basis of this insight, we provide an overview of\\nthe themes particularly relevant to the research on content-aware systems for\\nbroadcast sports. Specifically, we focus on the video content analysis\\ntechniques applied in sportscasts over the past decade from the perspectives of\\nfundamentals and general review, a content hierarchical model, and trends and\\nchallenges. Content-aware analysis methods are discussed with respect to\\nobject-, event-, and context-oriented groups. In each group, the gap between\\nsensation and content excitement must be bridged using proper strategies. In\\nthis regard, a content-aware approach is required to determine user demands.\\nFinally, the paper summarizes the future trends and challenges for sports video\\nanalysis. We believe that our findings can advance the field of research on\\ncontent-aware video analysis for broadcast sports.\\n',\n",
       " '  In kernel methods, temporal information on the data is commonly included by\\nusing time-delayed embeddings as inputs. Recently, an alternative formulation\\nwas proposed by defining a gamma-filter explicitly in a reproducing kernel\\nHilbert space, giving rise to a complex model where multiple kernels operate on\\ndifferent temporal combinations of the input signal. In the original\\nformulation, the kernels are then simply combined to obtain a single kernel\\nmatrix (for instance by averaging), which provides computational benefits but\\ndiscards important information on the temporal structure of the signal.\\nInspired by works on multiple kernel learning, we overcome this drawback by\\nconsidering the different kernels separately. We propose an efficient strategy\\nto adaptively combine and select these kernels during the training phase. The\\nresulting batch and online algorithms automatically learn to process highly\\nnonlinear temporal information extracted from the input signal, which is\\nimplicitly encoded in the kernel values. We evaluate our proposal on several\\nartificial and real tasks, showing that it can outperform classical approaches\\nboth in batch and online settings.\\n',\n",
       " '  We consider the problem of sequential learning from categorical observations\\nbounded in [0,1]. We establish an ordering between the Dirichlet posterior over\\ncategorical outcomes and a Gaussian posterior under observations with N(0,1)\\nnoise. We establish that, conditioned upon identical data with at least two\\nobservations, the posterior mean of the categorical distribution will always\\nsecond-order stochastically dominate the posterior mean of the Gaussian\\ndistribution. These results provide a useful tool for the analysis of\\nsequential learning under categorical outcomes.\\n',\n",
       " '  We develop a new approach to learn the parameters of regression models with\\nhidden variables. In a nutshell, we estimate the gradient of the regression\\nfunction at a set of random points, and cluster the estimated gradients. The\\ncenters of the clusters are used as estimates for the parameters of hidden\\nunits. We justify this approach by studying a toy model, whereby the regression\\nfunction is a linear combination of sigmoids. We prove that indeed the\\nestimated gradients concentrate around the parameter vectors of the hidden\\nunits, and provide non-asymptotic bounds on the number of required samples. To\\nthe best of our knowledge, no comparable guarantees have been proven for linear\\ncombinations of sigmoids.\\n',\n",
       " '  The intricate interplay between optically dark and bright excitons governs\\nthe light-matter interaction in transition metal dichalcogenide monolayers. We\\nhave performed a detailed investigation of the \"spin-forbidden\" dark excitons\\nin WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field\\nBz. In agreement with the theoretical predictions deduced from group theory\\nanalysis, magneto-photoluminescence experiments reveal a zero field splitting\\n$\\\\delta=0.6 \\\\pm 0.1$ meV between two dark exciton states. The low energy state\\nbeing strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state\\nis partially coupled to light with z polarization (\"grey\" exciton). The first\\ndetermination of the dark neutral exciton lifetime $\\\\tau_D$ in a transition\\nmetal dichalcogenide monolayer is obtained by time-resolved photoluminescence.\\nWe measure $\\\\tau_D \\\\sim 110 \\\\pm 10$ ps for the grey exciton state, i.e. two\\norders of magnitude longer than the radiative lifetime of the bright neutral\\nexciton at T=12 K.\\n',\n",
       " '  In this article, we develop a notion of Quillen bifibration which combines\\nthe two notions of Grothendieck bifibration and of Quillen model structure. In\\nparticular, given a bifibration $p:\\\\mathcal E\\\\to\\\\mathcal B$, we describe when a\\nfamily of model structures on the fibers $\\\\mathcal E_A$ and on the basis\\ncategory $\\\\mathcal B$ combines into a model structure on the total category\\n$\\\\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and\\nweak equivalences. Using this Grothendieck construction for model structures,\\nwe revisit the traditional definition of Reedy model structures, and possible\\ngeneralizations, and exhibit their bifibrational nature.\\n',\n",
       " '  In this paper we define canonical sine and cosine transform, convolution\\noperations, prove convolution theorems in space of integrable functions on real\\nspace. Further, obtain some results require to construct the spaces of\\nintegrable Boehmians then extend this canonical sine and canonical cosine\\ntransforms to space of integrable Boehmians and obtain their properties.\\n',\n",
       " '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\\nsparse (or compressible) signals from an incomplete number of measurements,\\nusing of a sensing mechanism implemented by an appropriate projection matrix.\\nThe CS theory is based on random Gaussian projection matrices, which satisfy\\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\\nprojections are more suitable for hardware implementation. In this paper, we\\npresent a deep learning approach to obtain very sparse ternary projections for\\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\\nexperimental results on real images demonstrate the effectiveness of the\\nproposed approach compared to state-of-the-art methods, with significant\\nadvantage in terms of complexity.\\n',\n",
       " '  Predictive models for music are studied by researchers of algorithmic\\ncomposition, the cognitive sciences and machine learning. They serve as base\\nmodels for composition, can simulate human prediction and provide a\\nmultidisciplinary application domain for learning algorithms. A particularly\\nwell established and constantly advanced subtask is the prediction of\\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\\nthe recent feature discovery and learning method PULSE to the realm of symbolic\\nmusic modeling. PULSE is comprised of a feature generating operation and\\nL1-regularized optimization. These are used to iteratively expand and cull the\\nfeature set, effectively exploring feature spaces that are too large for common\\nfeature selection approaches. I design a general Python framework for PULSE,\\npropose task-optimized feature generating operations and various\\nmusic-theoretically motivated features that are evaluated on a standard corpus\\nof monophonic folk and chorale melodies. The proposed method significantly\\noutperforms comparable state-of-the-art models. I further discuss the free\\nparameters of the learning algorithm and analyze the feature composition of the\\nlearned models. The models learned by PULSE afford an easy inspection and are\\nmusicologically interpreted for the first time.\\n',\n",
       " '  Many real-world data sets, especially in biology, are produced by highly\\nmultivariate and nonlinear complex dynamical systems. In this paper, we focus\\non brain imaging data, including both calcium imaging and functional MRI data.\\nStandard vector-autoregressive models are limited by their linearity\\nassumptions, while nonlinear general-purpose, large-scale temporal models, such\\nas LSTM networks, typically require large amounts of training data, not always\\nreadily available in biological applications; furthermore, such models have\\nlimited interpretability. We introduce here a novel approach for learning a\\nnonlinear differential equation model aimed at capturing brain dynamics.\\nSpecifically, we propose a variable-projection optimization approach to\\nestimate the parameters of the multivariate (coupled) van der Pol oscillator,\\nand demonstrate that such a model can accurately represent nonlinear dynamics\\nof the brain data. Furthermore, in order to improve the predictive accuracy\\nwhen forecasting future brain-activity time series, we use this analytical\\nmodel as an unlimited source of simulated data for pretraining LSTM; such\\nmodel-specific data augmentation approach consistently improves LSTM\\nperformance on both calcium and fMRI imaging data.\\n',\n",
       " '  In this research, we propose a deep learning based approach for speeding up\\nthe topology optimization methods. The problem we seek to solve is the layout\\nproblem. The main novelty of this work is to state the problem as an image\\nsegmentation task. We leverage the power of deep learning methods as the\\nefficient pixel-wise image labeling technique to perform the topology\\noptimization. We introduce convolutional encoder-decoder architecture and the\\noverall approach of solving the above-described problem with high performance.\\nThe conducted experiments demonstrate the significant acceleration of the\\noptimization process. The proposed approach has excellent generalization\\nproperties. We demonstrate the ability of the application of the proposed model\\nto other problems. The successful results, as well as the drawbacks of the\\ncurrent method, are discussed.\\n',\n",
       " '  We present and evaluate a technique for computing path-sensitive interference\\nconditions during abstract interpretation of concurrent programs. In lieu of\\nfixed point computation, we use prime event structures to compactly represent\\ncausal dependence and interference between sequences of transformers. Our main\\ncontribution is an unfolding algorithm that uses a new notion of independence\\nto avoid redundant transformer application, thread-local fixed points to reduce\\nthe size of the unfolding, and a novel cutoff criterion based on subsumption to\\nguarantee termination of the analysis. Our experiments show that the abstract\\nunfolding produces an order of magnitude fewer false alarms than a mature\\nabstract interpreter, while being several orders of magnitude faster than\\nsolver-based tools that have the same precision.\\n',\n",
       " '  The next generation of cosmological surveys will operate over unprecedented\\nscales, and will therefore provide exciting new opportunities for testing\\ngeneral relativity. The standard method for modelling the structures that these\\nsurveys will observe is to use cosmological perturbation theory for linear\\nstructures on horizon-sized scales, and Newtonian gravity for non-linear\\nstructures on much smaller scales. We propose a two-parameter formalism that\\ngeneralizes this approach, thereby allowing interactions between large and\\nsmall scales to be studied in a self-consistent and well-defined way. This uses\\nboth post-Newtonian gravity and cosmological perturbation theory, and can be\\nused to model realistic cosmological scenarios including matter, radiation and\\na cosmological constant. We find that the resulting field equations can be\\nwritten as a hierarchical set of perturbation equations. At leading-order,\\nthese equations allow us to recover a standard set of Friedmann equations, as\\nwell as a Newton-Poisson equation for the inhomogeneous part of the Newtonian\\nenergy density in an expanding background. For the perturbations in the\\nlarge-scale cosmology, however, we find that the field equations are sourced by\\nboth non-linear and mode-mixing terms, due to the existence of small-scale\\nstructures. These extra terms should be expected to give rise to new\\ngravitational effects, through the mixing of gravitational modes on small and\\nlarge scales - effects that are beyond the scope of standard linear\\ncosmological perturbation theory. We expect our formalism to be useful for\\naccurately modelling gravitational physics in universes that contain non-linear\\nstructures, and for investigating the effects of non-linear gravity in the era\\nof ultra-large-scale surveys.\\n',\n",
       " '  Playing the game of heads or tails in zero gravity demonstrates that there\\nexists a contextual \"measurement\" in classical mechanics. When the coin is\\nflipped, its orientation is a continuous variable. However, the \"measurement\"\\nthat occurs when the coin is caught by clapping two hands together gives a\\ndiscrete value (heads or tails) that depends on the context (orientation of the\\nhands). It is then shown that there is a strong analogy with the spin\\nmeasurement of the Stern-Gerlach experiment, and in particular with Stern and\\nGerlach\\'s sequential measurements. Finally, we clarify the analogy by recalling\\nhow the de Broglie-Bohm interpretation simply explains the spin \"measurement\".\\n',\n",
       " '  The variational autoencoder (VAE) is a popular model for density estimation\\nand representation learning. Canonically, the variational principle suggests to\\nprefer an expressive inference model so that the variational approximation is\\naccurate. However, it is often overlooked that an overly-expressive inference\\nmodel can be detrimental to the test set performance of both the amortized\\nposterior approximator and, more importantly, the generative density estimator.\\nIn this paper, we leverage the fact that VAEs rely on amortized inference and\\npropose techniques for amortized inference regularization (AIR) that control\\nthe smoothness of the inference model. We demonstrate that, by applying AIR, it\\nis possible to improve VAE generalization on both inference and generative\\nperformance. Our paper challenges the belief that amortized inference is simply\\na mechanism for approximating maximum likelihood training and illustrates that\\nregularization of the amortization family provides a new direction for\\nunderstanding and improving generalization in VAEs.\\n',\n",
       " \"  Synchronization on multiplex networks have attracted increasing attention in\\nthe past few years. We investigate collective behaviors of Kuramoto oscillators\\non single layer and duplex spacial networks with total cost restriction, which\\nwas introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,\\nStanley H. E. and Jr A. J., {\\\\it Phys. Rev. Lett.} 104, 018701 (2010)] and\\ntermed as the Li network afterwards. In the Li network model, with the increase\\nof its spacial exponent, the network's structure will vary from the random type\\nto the small-world one, and finally to the regular lattice.We first explore how\\nthe spacial exponent influences the synchronizability of Kuramoto oscillators\\non single layer Li networks and find that the closer the Li network is to a\\nregular lattice, the more difficult for it to evolve into synchronization. Then\\nwe investigate synchronizability of duplex Li networks and find that the\\nexistence of inter-layer interaction can greatly enhance inter-layer and global\\nsynchronizability. When the inter-layer coupling strength is larger than a\\ncertain critical value, whatever the intra-layer coupling strength is, the\\ninter-layer synchronization will always occur. Furthermore, on single layer Li\\nnetworks, nodes with larger degrees more easily reach global synchronization,\\nwhile on duplex Li networks, this phenomenon becomes much less obvious.\\nFinally, we study the impact of inter-link density on global synchronization\\nand obtain that sparse inter-links can lead to the emergence of global\\nsynchronization for duplex Li networks just as dense inter-links do. In a word,\\ninter-layer interaction plays a vital role in determining synchronizability for\\nduplex spacial networks with total cost constraint.\\n\",\n",
       " '  We continue to investigate binary sequence $(f_u)$ over $\\\\{0,1\\\\}$ defined by\\n$(-1)^{f_u}=\\\\left(\\\\frac{(u^w-u^{wp})/p}{p}\\\\right)$ for integers $u\\\\ge 0$, where\\n$\\\\left(\\\\frac{\\\\cdot}{p}\\\\right)$ is the Legendre symbol and we restrict\\n$\\\\left(\\\\frac{0}{p}\\\\right)=1$. In an earlier work, the linear complexity of\\n$(f_u)$ was determined for $w=p-1$ under the assumption of $2^{p-1}\\\\not\\\\equiv 1\\n\\\\pmod {p^2}$. In this work, we give possible values on the linear complexity of\\n$(f_u)$ for all $1\\\\le w<p-1$ under the same conditions. We also state that the\\ncase of larger $w(\\\\geq p)$ can be reduced to that of $0\\\\leq w\\\\leq p-1$.\\n',\n",
       " '  A central question in science of science concerns how time affects citations.\\nDespite the long-standing interests and its broad impact, we lack systematic\\nanswers to this simple yet fundamental question. By reviewing and classifying\\nprior studies for the past 50 years, we find a significant lack of consensus in\\nthe literature, primarily due to the coexistence of retrospective and\\nprospective approaches to measuring citation age distributions. These two\\napproaches have been pursued in parallel, lacking any known connections between\\nthe two. Here we developed a new theoretical framework that not only allows us\\nto connect the two approaches through precise mathematical relationships, it\\nalso helps us reconcile the interplay between temporal decay of citations and\\nthe growth of science, helping us uncover new functional forms characterizing\\ncitation age distributions. We find retrospective distribution follows a\\nlognormal distribution with exponential cutoff, while prospective distribution\\nis governed by the interplay between a lognormal distribution and the growth in\\nthe number of references. Most interestingly, the two approaches can be\\nconnected once rescaled by the growth of publications and citations. We further\\nvalidate our framework using both large-scale citation datasets and analytical\\nmodels capturing citation dynamics. Together this paper presents a\\ncomprehensive analysis of the time dimension of science, representing a new\\nempirical and theoretical basis for all future studies in this area.\\n',\n",
       " '  Excited states of a single donor in bulk silicon have previously been studied\\nextensively based on effective mass theory. However, a proper theoretical\\ndescription of the excited states of a donor cluster is still scarce. Here we\\nstudy the excitations of lines of defects within a single-valley spherical band\\napproximation, thus mapping the problem to a scaled hydrogen atom array. A\\nseries of detailed full configuration-interaction and time-dependent hybrid\\ndensity-functional theory calculations have been performed to understand linear\\nclusters of up to 10 donors. Our studies illustrate the generic features of\\ntheir excited states, addressing the competition between formation of\\ninter-donor ionic states and intra-donor atomic excited states. At short\\ninter-donor distances, excited states of donor molecules are dominant, at\\nintermediate distances ionic states play an important role, and at long\\ndistances the intra-donor excitations are predominant as expected. The\\ncalculations presented here emphasise the importance of correlations between\\ndonor electrons, and are thus complementary to other recent approaches that\\ninclude effective mass anisotropy and multi-valley effects. The exchange\\nsplittings between relevant excited states have also been estimated for a donor\\npair and for a three-donor arrays; the splittings are much larger than those in\\nthe ground state in the range of donor separations between 10 and 20 nm. This\\nestablishes a solid theoretical basis for the use of excited-state exchange\\ninteractions for controllable quantum gate operations in silicon.\\n',\n",
       " \"  We present a statistical study on the [C I]($^{3} \\\\rm P_{1} \\\\rightarrow {\\\\rm\\n^3 P}_{0}$), [C I] ($^{3} \\\\rm P_{2} \\\\rightarrow {\\\\rm ^3 P}_{1}$) lines\\n(hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0)\\nline for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore\\nthe correlations between the luminosities of CO (1$-$0) and [C I] lines, and\\nfind that $L'_\\\\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_\\n\\\\mathrm{[CI](1-0)}$ and $L'_\\\\mathrm{[CI](2-1)}$, suggesting that [C I] lines\\ncan trace total molecular gas mass at least for (U)LIRGs. We also investigate\\nthe dependence of $L'_\\\\mathrm{[CI](1-0)}$/$L'_\\\\mathrm{CO(1-0)}$,\\n$L'_\\\\mathrm{[CI](2-1)}$/$L'_\\\\mathrm{CO(1-0)}$ and\\n$L'_\\\\mathrm{[CI](2-1)}$/$L'_\\\\mathrm{[CI](1-0)}$ on the far-infrared color of\\n60-to-100 $\\\\mu$m, and find non-correlation, a weak correlation and a modest\\ncorrelation, respectively. Under the assumption that these two carbon\\ntransitions are optically thin, we further calculate the [C I] line excitation\\ntemperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass\\nconversion factors for our sample. The resulting $\\\\mathrm{H_2}$ masses using\\nthese [C I]-based conversion factors roughly agree with those derived from\\n$L'_\\\\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.\\n\",\n",
       " '  We study large-scale kernel methods for acoustic modeling in speech\\nrecognition and compare their performance to deep neural networks (DNNs). We\\nperform experiments on four speech recognition datasets, including the TIMIT\\nand Broadcast News benchmark tasks, and compare these two types of models on\\nframe-level performance metrics (accuracy, cross-entropy), as well as on\\nrecognition metrics (word/character error rate). In order to scale kernel\\nmethods to these large datasets, we use the random Fourier feature method of\\nRahimi and Recht (2007). We propose two novel techniques for improving the\\nperformance of kernel acoustic models. First, in order to reduce the number of\\nrandom features required by kernel models, we propose a simple but effective\\nmethod for feature selection. The method is able to explore a large number of\\nnon-linear features while maintaining a compact model more efficiently than\\nexisting approaches. Second, we present a number of frame-level metrics which\\ncorrelate very strongly with recognition performance when computed on the\\nheldout set; we take advantage of these correlations by monitoring these\\nmetrics during training in order to decide when to stop learning. This\\ntechnique can noticeably improve the recognition performance of both DNN and\\nkernel models, while narrowing the gap between them. Additionally, we show that\\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\\nof our kernel models significantly, in addition to speeding up training and\\nmaking the models more compact. Together, these three methods dramatically\\nimprove the performance of kernel acoustic models, making their performance\\ncomparable to DNNs on the tasks we explored.\\n',\n",
       " '  We determine the composition factors of the tensor product $S(E)\\\\otimes S(E)$\\nof two copies of the symmetric algebra of the natural module $E$ of a general\\nlinear group over an algebraically closed field of positive characteristic. Our\\nmain result may be regarded as a substantial generalisation of the tensor\\nproduct theorem of Krop and Sullivan, on composition factors of $S(E)$. We\\nearlier answered the question of which polynomially injective modules are\\ninfinitesimally injective in terms of the \"divisibility index\". We are now able\\nto give an explicit description of the divisibility index for polynomial\\nmodules for general linear groups of degree at most $3$.\\n',\n",
       " '  Current understanding of how contractility emerges in disordered actomyosin\\nnetworks of non-muscle cells is still largely based on the intuition derived\\nfrom earlier works on muscle contractility. This view, however, largely\\noverlooks the free energy gain following passive cross-linker binding, which,\\neven in the absence of active fluctuations, provides a thermodynamic drive\\ntowards highly overlapping filamentous states. In this work, we shed light on\\nthis phenomenon, showing that passive cross-linkers, when considered in the\\ncontext of two anti-parallel filaments, generate noticeable contractile forces.\\nHowever, as binding free energy of cross-linkers is increased, a sharp onset of\\nkinetic arrest follows, greatly diminishing effectiveness of this contractility\\nmechanism, allowing the network to contract only with weakly resisting tensions\\nat its boundary. We have carried out stochastic simulations elucidating this\\nmechanism, followed by a mean-field treatment that predicts how contractile\\nforces asymptotically scale at small and large binding energies, respectively.\\nFurthermore, when considering an active contractile filament pair, based on\\nnon-muscle myosin II, we found that the non-processive nature of these motors\\nleads to highly inefficient force generation, due to recoil slippage of the\\noverlap during periods when the motor is dissociated. However, we discovered\\nthat passive cross-linkers can serve as a structural ratchet during these\\nunbound motor time spans, resulting in vast force amplification. Our results\\nshed light on the non-equilibrium effects of transiently binding proteins in\\nbiological active matter, as observed in the non-muscle actin cytoskeleton,\\nshowing that highly efficient contractile force dipoles result from synergy of\\npassive cross-linker and active motor dynamics, via a ratcheting mechanism on a\\nfunneled energy landscape.\\n',\n",
       " '  This work encompasses Rate-Splitting (RS), providing significant benefits in\\nmulti-user settings in the context of huge degrees of freedom promised by\\nmassive Multiple-Input Multiple-Output (MIMO). However, the requirement of\\nmassive MIMO for cost-efficient implementation makes them more prone to\\nhardware imperfections such as phase noise (PN). As a result, we focus on a\\nrealistic broadcast channel with a large number of antennas and hampered by the\\nunavoidable PN. Moreover, we employ the RS transmission strategy, and we show\\nits robustness against PN, since the sum-rate does not saturate at high\\nsignal-to-noise ratio (SNR). Although, the analytical results are obtained by\\nmeans of the deterministic equivalent analysis, they coincide with simulation\\nresults even for finite system dimensions.\\n',\n",
       " '  We prove a general width duality theorem for combinatorial structures with\\nwell-defined notions of cohesion and separation. These might be graphs and\\nmatroids, but can be much more general or quite different. The theorem asserts\\na duality between the existence of high cohesiveness somewhere local and a\\nglobal overall tree structure.\\nWe describe cohesive substructures in a unified way in the format of tangles:\\nas orientations of low-order separations satisfying certain consistency axioms.\\nThese axioms can be expressed without reference to the underlying structure,\\nsuch as a graph or matroid, but just in terms of the poset of the separations\\nthemselves. This makes it possible to identify tangles, and apply our\\ntangle-tree duality theorem, in very diverse settings.\\nOur result implies all the classical duality theorems for width parameters in\\ngraph minor theory, such as path-width, tree-width, branch-width or rank-width.\\nIt yields new, tangle-type, duality theorems for tree-width and path-width. It\\nimplies the existence of width parameters dual to cohesive substructures such\\nas $k$-blocks, edge-tangles, or given subsets of tangles, for which no width\\nduality theorems were previously known.\\nAbstract separation systems can be found also in structures quite unlike\\ngraphs and matroids. For example, our theorem can be applied to image analysis\\nby capturing the regions of an image as tangles of separations defined as\\nnatural partitions of its set of pixels. It can be applied in big data contexts\\nby capturing clusters as tangles. It can be applied in the social sciences,\\ne.g. by capturing as tangles the few typical mindsets of individuals found by a\\nsurvey. It could also be applied in pure mathematics, e.g. to separations of\\ncompact manifolds.\\n',\n",
       " '  Network pruning is aimed at imposing sparsity in a neural network\\narchitecture by increasing the portion of zero-valued weights for reducing its\\nsize regarding energy-efficiency consideration and increasing evaluation speed.\\nIn most of the conducted research efforts, the sparsity is enforced for network\\npruning without any attention to the internal network characteristics such as\\nunbalanced outputs of the neurons or more specifically the distribution of the\\nweights and outputs of the neurons. That may cause severe accuracy drop due to\\nuncontrolled sparsity. In this work, we propose an attention mechanism that\\nsimultaneously controls the sparsity intensity and supervised network pruning\\nby keeping important information bottlenecks of the network to be active. On\\nCIFAR-10, the proposed method outperforms the best baseline method by 6% and\\nreduced the accuracy drop by 2.6x at the same level of sparsity.\\n',\n",
       " '  Graph models are widely used to analyse diffusion processes embedded in\\nsocial contacts and to develop applications. A range of graph models are\\navailable to replicate the underlying social structures and dynamics\\nrealistically. However, most of the current graph models can only consider\\nconcurrent interactions among individuals in the co-located interaction\\nnetworks. However, they do not account for indirect interactions that can\\ntransmit spreading items to individuals who visit the same locations at\\ndifferent times but within a certain time limit. The diffusion phenomena\\noccurring through direct and indirect interactions is called same place\\ndifferent time (SPDT) diffusion. This paper introduces a model to synthesize\\nco-located interaction graphs capturing both direct interactions, where\\nindividuals meet at a location, and indirect interactions, where individuals\\nvisit the same location at different times within a set timeframe. We analyze\\n60 million location updates made by 2 million users from a social networking\\napplication to characterize the graph properties, including the space-time\\ncorrelations and its time evolving characteristics, such as bursty or ongoing\\nbehaviors. The generated synthetic graph reproduces diffusion dynamics of a\\nrealistic contact graph, and reduces the prediction error by up to 82% when\\ncompare to other contact graph models demonstrating its potential for\\nforecasting epidemic spread.\\n',\n",
       " '  Training neural networks involves finding minima of a high-dimensional\\nnon-convex loss function. Knowledge of the structure of this energy landscape\\nis sparse. Relaxing from linear interpolations, we construct continuous paths\\nbetween minima of recent neural network architectures on CIFAR10 and CIFAR100.\\nSurprisingly, the paths are essentially flat in both the training and test\\nlandscapes. This implies that neural networks have enough capacity for\\nstructural changes, or that these changes are small between minima. Also, each\\nminimum has at least one vanishing Hessian eigenvalue in addition to those\\nresulting from trivial invariance.\\n',\n",
       " '  We study the homogenization process for families of strongly nonlinear\\nelliptic systems with the homogeneous Dirichlet boundary conditions. The growth\\nand the coercivity of the elliptic operator is assumed to be indicated by a\\ngeneral inhomogeneous anisotropic $N-$function, which may be possibly also\\ndependent on the spatial variable, i.e., the homogenization process will change\\nthe characteristic function spaces at each step. Such a problem is well known\\nand there exists many positive results for the function satisfying $\\\\Delta_2$\\nand $\\\\nabla_2$ conditions an being in addition Hölder continuous with\\nrespect to the spatial variable. We shall show that cases these conditions can\\nbe neglected and will deal with a rather general problem in general function\\nspace setting.\\n',\n",
       " \"  A polynomial $p\\\\in\\\\mathbb{R}[z_1,\\\\dots,z_n]$ is real stable if it has no\\nroots in the upper-half complex plane. Gurvits's permanent inequality gives a\\nlower bound on the coefficient of the $z_1z_2\\\\dots z_n$ monomial of a real\\nstable polynomial $p$ with nonnegative coefficients. This fundamental\\ninequality has been used to attack several counting and optimization problems.\\nHere, we study a more general question: Given a stable multilinear polynomial\\n$p$ with nonnegative coefficients and a set of monomials $S$, we show that if\\nthe polynomial obtained by summing up all monomials in $S$ is real stable, then\\nwe can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.\\nWe also prove generalizations of this theorem to (real stable) polynomials that\\nare not multilinear. We use our theorem to give a new proof of Schrijver's\\ninequality on the number of perfect matchings of a regular bipartite graph,\\ngeneralize a recent result of Nikolov and Singh, and give deterministic\\npolynomial time approximation algorithms for several counting problems.\\n\",\n",
       " '  This volume contains the proceedings of the Fifth International Workshop on\\nVerification and Program Transformation (VPT 2017). The workshop took place in\\nUppsala, Sweden, on April 29th, 2017, affiliated with the European Joint\\nConferences on Theory and Practice of Software (ETAPS). The aim of the VPT\\nworkshop series is to provide a forum where people from the areas of program\\ntransformation and program verification can fruitfully exchange ideas and gain\\na deeper understanding of the interactions between those two fields. Seven\\npapers were presented at the workshop. Additionally, three invited talks were\\ngiven by Javier Esparza (Technische Universität München, Germany), Manuel\\nHermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov\\n(Linux Verification Center, ISPRAS, Moscow, Russia).\\n',\n",
       " '  This chapter presents an H-infinity filtering framework for cloud-aided\\nsemiactive suspension system with time-varying delays. In this system, road\\nprofile information is downloaded from a cloud database to facilitate onboard\\nestimation of suspension states. Time-varying data transmission delays are\\nconsidered and assumed to be bounded. A quarter-car linear suspension model is\\nused and an H-infinity filter is designed with both onboard sensor measurements\\nand delayed road profile information from the cloud. The filter design\\nprocedure is designed based on linear matrix inequalities (LMIs). Numerical\\nsimulation results are reported that illustrates the fusion of cloud-based and\\non-board information that can be achieved in Vehicleto- Cloud-to-Vehicle\\n(V2C2V) implementation.\\n',\n",
       " '  In this work we introduce a time- and memory-efficient method for structured\\nprediction that couples neuron decisions across both space at time. We show\\nthat we are able to perform exact and efficient inference on a densely\\nconnected spatio-temporal graph by capitalizing on recent advances on deep\\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\\nalongside contemporary deep networks for video understanding. We experiment\\nwith multiple connectivity patterns in the temporal domain, and present\\nempirical improvements over strong baselines on the tasks of both semantic and\\ninstance segmentation of videos.\\n',\n",
       " \"  This paper introduces a new approach to Large-Eddy Simulation (LES) where\\nsubgrid-scale (SGS) dissipation is applied proportionally to the degree of\\nlocal spectral broadening, hence mitigated or deactivated in regions dominated\\nby large-scale and/or laminar vortical motion. The proposed Coherent vorticity\\npreserving (CvP) LES methodology is based on the evaluation of the ratio of the\\ntest-filtered to resolved (or grid-filtered) enstrophy $\\\\sigma$. Values of\\n$\\\\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying\\nlocal deactivation of the SGS dissipation. The intensity of the SGS dissipation\\nis progressively increased for $\\\\sigma < 1$ which corresponds to a small-scale\\nspectral broadening. The SGS dissipation is then fully activated in developed\\nturbulence characterized by $\\\\sigma \\\\le \\\\sigma_{eq}$, where the value\\n$\\\\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach\\ncan be applied to any eddy-viscosity model, is algorithmically simple and\\ncomputationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates\\nthat the CvP methodology improves the performance of traditional, non-dynamic\\ndissipative SGS models, capturing the peak of total turbulent kinetic energy\\ndissipation during transition. Similar accuracy is obtained by adopting\\nGermano's dynamic procedure albeit at more than twice the computational\\noverhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to\\npredict accurately the experimentally observed growth rate using coarse\\nresolutions. The ability of the CvP methodology to dynamically sort the\\ncoherent, large-scale motion from the smaller, broadband scales during\\ntransition is demonstrated via flow visualizations. LES of compressible channel\\nare carried out and show a good match with a reference DNS.\\n\",\n",
       " '  We consider the problem of inference in a causal generative model where the\\nset of available observations differs between data instances. We show how\\ncombining samples drawn from the graphical model with an appropriate masking\\nfunction makes it possible to train a single neural network to approximate all\\nthe corresponding conditional marginal distributions and thus amortize the cost\\nof inference. We further demonstrate that the efficiency of importance sampling\\nmay be improved by basing proposals on the output of the neural network. We\\nalso outline how the same network can be used to generate samples from an\\napproximate joint posterior via a chain decomposition of the graph.\\n',\n",
       " '  The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard\\nproblem with numerous applications arising in artificial intelligence. As an\\nefficient tool for heuristic design, the backbone has been applied to\\nheuristics design for many NP-hard problems. In this paper, we investigated the\\ncomputational complexity for retrieving the backbone in weighted MAX-SAT and\\ndeveloped a new algorithm for solving this problem. We showed that it is\\nintractable to retrieve the full backbone under the assumption that . Moreover,\\nit is intractable to retrieve a fixed fraction of the backbone as well. And\\nthen we presented a backbone guided local search (BGLS) with Walksat operator\\nfor weighted MAX-SAT. BGLS consists of two phases: the first phase samples the\\nbackbone information from local optima and the backbone phase conducts local\\nsearch under the guideline of backbone. Extensive experimental results on the\\nbenchmark showed that BGLS outperforms the existing heuristics in both solution\\nquality and runtime.\\n',\n",
       " '  We show that in the presence of magnetic field, two superconducting phases\\nwith the center-of-mass momentum of Cooper pair parallel to the magnetic field\\nare induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,\\nat small magnetic field, the center-of-mass momentum is induced due to the\\nenergy-spectrum distortion and no unpairing region with vanishing singlet\\ncorrelation appears. We refer to this superconducting state as the drift-BCS\\nstate. By further increasing the magnetic field, the superconducting state\\nfalls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the\\nunpairing regions. The observed abrupt enhancement of the center-of-mass\\nmomenta and suppression on the order parameters during the crossover indicate\\nthe first-order phase transition. Enhanced Pauli limit and hence enlarged\\nmagnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the\\nspin-flip terms of the spin-orbit coupling, are revealed. We also address the\\ntriplet correlations induced by the spin-orbit coupling, and show that the\\nCooper-pair spin polarizations, generated by the magnetic field and\\ncenter-of-mass momentum with the triplet correlations, exhibit totally\\ndifferent magnetic-field dependences between the drift-BCS and\\nFulde-Ferrell-Larkin-Ovchinnikov states.\\n',\n",
       " \"  We have recently established some integral inequalities for convex functions\\nvia the Hermite-Hadamard's inequalities. In continuation here, we also\\nestablish some interesting new integral inequalities for convex functions via\\nthe Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful\\napplications involving special means are also included.\\n\",\n",
       " '  The support vector machine (SVM) is a powerful and widely used classification\\nalgorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide\\nrigorous mathematical proof for new insights into the behavior of SVM. These\\ninsights provide perhaps unexpected relationships between SVM and two other\\nlinear classifiers: the mean difference and the maximal data piling direction.\\nFor example, we show that in many cases SVM can be viewed as a cropped version\\nof these classifiers. By carefully exploring these connections we show how SVM\\ntuning behavior is affected by characteristics including: balanced vs.\\nunbalanced classes, low vs. high dimension, separable vs. non-separable data.\\nThese results provide further insights into tuning SVM via cross-validation by\\nexplaining observed pathological behavior and motivating improved\\ncross-validation methodology. Finally, we also provide new results on the\\ngeometry of complete data piling directions in high dimensional space.\\n',\n",
       " '  Data analytics and data science play a significant role in nowadays society.\\nIn the context of Smart Grids (SG), the collection of vast amounts of data has\\nseen the emergence of a plethora of data analysis approaches. In this paper, we\\nconduct a Systematic Mapping Study (SMS) aimed at getting insights about\\ndifferent facets of SG data analysis: application sub-domains (e.g., power load\\ncontrol), aspects covered (e.g., forecasting), used techniques (e.g.,\\nclustering), tool-support, research methods (e.g., experiments/simulations),\\nreplicability/reproducibility of research. The final goal is to provide a view\\nof the current status of research. Overall, we found that each sub-domain has\\nits peculiarities in terms of techniques, approaches and research methodologies\\napplied. Simulations and experiments play a crucial role in many areas. The\\nreplicability of studies is limited concerning the provided implemented\\nalgorithms, and to a lower extent due to the usage of private datasets.\\n',\n",
       " \"  In this paper, we present a new task that investigates how people interact\\nwith and make judgments about towers of blocks. In Experiment~1, participants\\nin the lab solved a series of problems in which they had to re-configure three\\nblocks from an initial to a final configuration. We recorded whether they used\\none hand or two hands to do so. In Experiment~2, we asked participants online\\nto judge whether they think the person in the lab used one or two hands. The\\nresults revealed a close correspondence between participants' actions in the\\nlab, and the mental simulations of participants online. To explain\\nparticipants' actions and mental simulations, we develop a model that plans\\nover a symbolic representation of the situation, executes the plan using a\\ngeometric solver, and checks the plan's feasibility by taking into account the\\nphysical constraints of the scene. Our model explains participants' actions and\\njudgments to a high degree of quantitative accuracy.\\n\",\n",
       " '  Recent progress in applying complex network theory to problems faced in\\nquantum information and computation has resulted in a beneficial crossover\\nbetween two fields. Complex network methods have successfully been used to\\ncharacterize quantum walk and transport models, entangled communication\\nnetworks, graph-theoretic models of emergent space-time and in detecting\\nmesoscale structure in quantum systems. Information physics is setting the\\nstage for a theory of complex and networked systems with quantum\\ninformation-inspired methods appearing in complex network science, including\\ninformation-theoretic distance and correlation measures for network\\ncharacterization. Novel quantum induced effects have been predicted in random\\ngraphs---where edges represent entangled links---and quantum computer\\nalgorithms have recently been proposed to offer super-polynomial enhancement\\nfor several network and graph theoretic problems. Here we review the results at\\nthe cutting edge, pinpointing the similarities and reconciling the differences\\nfound in the series of results at the intersection of these two fields.\\n',\n",
       " '  Under suitable conditions, a substitution tiling gives rise to a Smale space,\\nfrom which three equivalence relations can be constructed, namely the stable,\\nunstable, and asymptotic equivalence relations. We denote with $S$, $U$, and\\n$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article\\nwe show that the $K$-theories of $S$ and $U$ can be computed from the\\ncohomology and homology of a single cochain complex with connecting maps for\\ntilings of the line and of the plane. Moreover, we provide formulas to compute\\nthe $K$-theory for these three $C^*$-algebras. Furthermore, we show that the\\n$K$-theory groups for tilings of dimension 1 are always torsion free. For\\ntilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.\\n',\n",
       " \"  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain\\nrandom planar maps converge to $\\\\gamma$-Liouville quantum gravity\\n($\\\\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized\\nmatings of correlated continuum random trees, and $\\\\gamma$ ranges from $0$ to\\n$2$ as one varies the correlation parameter. We also show that the associated\\nspace-filling path on the embedded map converges to space-filling\\nSLE$_{\\\\kappa}$ for $\\\\kappa =16/\\\\gamma^2$ (in the annealed sense) and that\\nsimple random walk on the embedded map converges to Brownian motion (in the\\nquenched sense). Our arguments also yield analogous statements for the Smith\\n(square tiling) embedding of the mated-CRT map.\\nThis work constitutes the first proof that a discrete conformal embedding of\\na random planar map converges to LQG. Many more such statements have been\\nconjectured. Since the mated-CRT map can be viewed as a coarse-grained\\napproximation to other random planar maps (the UIPT, tree-weighted maps,\\nbipolar-oriented maps, etc.), our results indicate a potential approach for\\nproving that embeddings of these maps converge to LQG as well.\\nTo prove the main result, we establish several (independently interesting)\\ntheorems about LQG surfaces decorated by space-filling SLE. There is a natural\\nway to use the SLE curve to divide the plane into `cells' corresponding to\\nvertices of the mated-CRT map. We study the law of the shape of the\\norigin-containing cell, in particular proving moments for the ratio of its\\nsquared diameter to its area. We also give bounds on the degree of the\\norigin-containing cell and establish a form of ergodicity for the entire\\nconfiguration. Ultimately, we use these properties to show (using a general\\ntheorem proved in a separate paper) that random walk on these cells converges\\nto a time change of Brownian motion, which in turn leads to the Tutte embedding\\nresult.\\n\",\n",
       " '  Muroga [M52] showed how to express the Shannon channel capacity of a discrete\\nchannel with noise [S49] as an explicit function of the transition\\nprobabilities. His method accommodates channels with any finite number of input\\nsymbols, any finite number of output symbols and any transition probability\\nmatrix. Silverman [S55] carried out Muroga\\'s method in the special case of a\\nbinary channel (and went on to analyse \"cascades\" of several such binary\\nchannels).\\nThis article is a note on the resulting formula for the capacity C(a, c) of a\\nsingle binary channel. We aim to clarify some of the arguments and correct a\\nsmall error. In service of this aim, we first formulate several of Shannon\\'s\\ndefinitions and proofs in terms of discrete measure-theoretic probability\\ntheory. We provide an alternate proof to Silverman\\'s, of the feasibility of the\\noptimal input distribution for a binary channel. For convenience, we also\\nexpress C(a, c) in a single expression explicitly dependent on a and c only,\\nwhich Silverman stopped short of doing.\\n',\n",
       " '  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,\\nwhich is finitely generated on both sides. We study Gorenstein homological\\nproperties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$\\nis Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein\\nprojective $T_R(M)$-modules in terms of $R$-modules.\\n',\n",
       " '  Regression based methods are not performing as well as detection based\\nmethods for human pose estimation. A central problem is that the structural\\ninformation in the pose is not well exploited in the previous regression\\nmethods. In this work, we propose a structure-aware regression approach. It\\nadopts a reparameterized pose representation using bones instead of joints. It\\nexploits the joint connection structure to define a compositional loss function\\nthat encodes the long range interactions in the pose. It is simple, effective,\\nand general for both 2D and 3D pose estimation in a unified setting.\\nComprehensive evaluation validates the effectiveness of our approach. It\\nsignificantly advances the state-of-the-art on Human3.6M and is competitive\\nwith state-of-the-art results on MPII.\\n',\n",
       " '  We consider systems with memory represented by stochastic functional\\ndifferential equations. Substantially, these are stochastic differential\\nequations with coefficients depending on the past history of the process\\nitself. Such coefficients are hence defined on a functional space. Models with\\nmemory appear in many applications ranging from biology to finance. Here we\\nconsider the results of some evaluations based on these models (e.g. the prices\\nof some financial products) and the risks connected to the choice of these\\nmodels. In particular we focus on the impact of the initial condition on the\\nevaluations. This problem is known as the analysis of sensitivity to the\\ninitial condition and, in the terminology of finance, it is referred to as the\\nDelta. In this work the initial condition is represented by the relevant past\\nhistory of the stochastic functional differential equation. This naturally\\nleads to the redesign of the definition of Delta. We suggest to define it as a\\nfunctional directional derivative, this is a natural choice. For this we study\\na representation formula which allows for its computation without requiring\\nthat the evaluation functional is differentiable. This feature is particularly\\nrelevant for applications. Our formula is achieved by studying an appropriate\\nrelationship between Malliavin derivative and functional directional\\nderivative. For this we introduce the technique of {\\\\it randomisation of the\\ninitial condition}.\\n',\n",
       " '  We discuss the concept of inner function in reproducing kernel Hilbert spaces\\nwith an orthogonal basis of monomials and examine connections between inner\\nfunctions and optimal polynomial approximants to $1/f$, where $f$ is a function\\nin the space. We revisit some classical examples from this perspective, and\\nshow how a construction of Shapiro and Shields can be modified to produce inner\\nfunctions.\\n',\n",
       " '  Data center networks are an important infrastructure in various applications\\nof modern information technologies. Note that each data center always has a\\nfinite lifetime, thus once a data center fails, then it will lose all its\\nstorage files and useful information. For this, it is necessary to replicate\\nand copy each important file into other data centers such that this file can\\nincrease its lifetime of staying in a data center network. In this paper, we\\ndescribe a large-scale data center network with a file d-threshold policy,\\nwhich is to replicate each important file into at most d-1 other data centers\\nsuch that this file can maintain in the data center network under a given level\\nof data security in the long-term. To this end, we develop three relevant\\nMarkov processes to propose two effective methods for assessing the file\\nlifetime and data security. By using the RG-factorizations, we show that the\\ntwo methods are used to be able to more effectively evaluate the file lifetime\\nof large-scale data center networks. We hope the methodology and results given\\nin this paper are applicable in the file lifetime study of more general data\\ncenter networks with replication mechanism.\\n',\n",
       " '  In the setting of high-dimensional linear regression models, we propose two\\nframeworks for constructing pointwise and group confidence sets for penalized\\nestimators which incorporate prior knowledge about the organization of the\\nnon-zero coefficients. This is done by desparsifying the estimator as in van de\\nGeer et al. [18] and van de Geer and Stucky [17], then using an appropriate\\nestimator for the precision matrix $\\\\Theta$. In order to estimate the precision\\nmatrix a corresponding structured matrix norm penalty has to be introduced.\\nAfter normalization the result is an asymptotic pivot.\\nThe asymptotic behavior is studied and simulations are added to study the\\ndifferences between the two schemes.\\n',\n",
       " '  Finding patterns in data and being able to retrieve information from those\\npatterns is an important task in Information retrieval. Complex search\\nrequirements which are not fulfilled by simple string matching and require\\nexploring certain patterns in data demand a better query engine that can\\nsupport searching via structured queries. In this article, we built a\\nstructured query engine which supports searching data through structured\\nqueries on the lines of ElasticSearch. We will show how we achieved real time\\nindexing and retrieving of data through a RESTful API and how complex queries\\ncan be created and processed using efficient data structures we created for\\nstoring the data in structured way. Finally, we will conclude with an example\\nof movie recommendation system built on top of this query engine.\\n',\n",
       " '  As a natural extension of compressive sensing and the requirement of some\\npractical problems, Phaseless Compressed Sensing (PCS) has been introduced and\\nstudied recently. Many theoretical results have been obtained for PCS with the\\naid of its convex relaxation. Motivated by successful applications of nonconvex\\nrelaxed methods for solving compressive sensing, in this paper, we try to\\ninvestigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the\\nreal context by the corresponding $\\\\ell_p$-minimization with $p\\\\in (0,1)$. We\\nshow that there exists a constant $p^\\\\ast\\\\in (0,1]$ such that for any fixed\\n$p\\\\in(0, p^\\\\ast)$, every optimal solution to the $\\\\ell_p$-minimization also\\nsolves the concerned problem; and derive an expression of such a constant\\n$p^\\\\ast$ by making use of the known data and the sparsity level of the\\nconcerned problem. These provide a theoretical basis for solving this class of\\nproblems via the corresponding $\\\\ell_p$-minimization.\\n',\n",
       " '  We present an exhaustive census of Lyman alpha (Ly$\\\\alpha$) emission in the\\ngeneral galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber\\nSystem (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of\\n625 galaxies homogeneously distributed in the range\\n$7.6<\\\\log{\\\\mbox{M$_*$/M$_{\\\\odot}$}}<10.6$. Our sample is selected from the\\n3D-HST/CANDELS survey, which provides the complementary data to estimate\\nLy$\\\\alpha$ equivalent widths ($W_{Ly\\\\alpha}$) and escape fractions ($f_{esc}$)\\nfor our galaxies. We find both quantities to anti-correlate with M$_*$,\\nstar-formation rate (SFR), UV luminosity, and UV slope ($\\\\beta$). We then model\\nthe $W_{Ly\\\\alpha}$ distribution as a function of M$_{UV}$ and $\\\\beta$ using a\\nBayesian approach. Based on our model and matching the properties of typical\\nLyman break galaxy (LBG) selections, we conclude that the $W_{Ly\\\\alpha}$\\ndistribution in such samples is heavily dependent on the limiting M$_{UV}$ of\\nthe survey. Regarding narrowband surveys, we find their $W_{Ly\\\\alpha}$\\nselections to bias samples toward low M$_*$, while their line-flux limitations\\npreferentially leave out low-SFR galaxies. We can also use our model to predict\\nthe fraction of Ly$\\\\alpha$-emitting LBGs at $4\\\\leqslant z\\\\leqslant 7$. We show\\nthat reported drops in the Ly$\\\\alpha$ fraction at $z\\\\geqslant6$, usually\\nattributed to the rapidly increasing neutral gas fraction of the universe, can\\nalso be explained by survey M$_{UV}$ incompleteness. This result does not\\ndismiss reionization occurring at $z\\\\sim7$, but highlights that current data is\\nnot inconsistent with this process taking place at $z>7$.\\n',\n",
       " '  Building on insights of Jovanovic (1982) and subsequent authors, we develop a\\ncomprehensive theory of optimal timing of decisions based around continuation\\nvalue functions and operators that act on them. Optimality results are provided\\nunder general settings, with bounded or unbounded reward functions. This\\napproach has several intrinsic advantages that we exploit in developing the\\ntheory. One is that continuation value functions are smoother than value\\nfunctions, allowing for sharper analysis of optimal policies and more efficient\\ncomputation. Another is that, for a range of problems, the continuation value\\nfunction exists in a lower dimensional space than the value function,\\nmitigating the curse of dimensionality. In one typical experiment, this reduces\\nthe computation time from over a week to less than three minutes.\\n',\n",
       " '  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This\\narticle describes how pristine was defined based on expectations of Bennu and\\non a realistic understanding of what is achievable with a constrained schedule\\nand budget, and how that definition flowed to requirements and implementation.\\nTo return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was\\nmaintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the\\nsampler head through precision cleaning, control of materials, and vigilance.\\nContamination is further characterized via witness material exposed to the\\nspacecraft assembly and testing environment as well as in space. This\\ncharacterization provided knowledge of the expected background and will be used\\nin conjunction with archived spacecraft components for comparison with the\\nsamples when they are delivered to Earth for analysis. Most of all, the\\ncleanliness of the OSIRIS-REx spacecraft was achieved through communication\\namong scientists, engineers, managers, and technicians.\\n',\n",
       " \"  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)\\nwith a constant step-size and the so called Polyak-Ruppert (PR) averaging of\\niterates. LSAs are widely applied in machine learning and reinforcement\\nlearning (RL), where the aim is to compute an appropriate $\\\\theta_{*} \\\\in\\n\\\\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$\\nupdates per iteration. In this paper, we are motivated by the problem (in RL)\\nof policy evaluation from experience replay using the \\\\emph{temporal\\ndifference} (TD) class of learning algorithms that are also LSAs. For LSAs with\\na constant step-size, and PR averaging, we provide bounds for the mean squared\\nerror (MSE) after $t$ iterations. We assume that data is \\\\iid with finite\\nvariance (underlying distribution being $P$) and that the expected dynamics is\\nHurwitz. For a given LSA with PR averaging, and data distribution $P$\\nsatisfying the said assumptions, we show that there exists a range of constant\\nstep-sizes such that its MSE decays as $O(\\\\frac{1}{t})$.\\nWe examine the conditions under which a constant step-size can be chosen\\nuniformly for a class of data distributions $\\\\mathcal{P}$, and show that not\\nall data distributions `admit' such a uniform constant step-size. We also\\nsuggest a heuristic step-size tuning algorithm to choose a constant step-size\\nof a given LSA for a given data distribution $P$. We compare our results with\\nrelated work and also discuss the implication of our results in the context of\\nTD algorithms that are LSAs.\\n\",\n",
       " '  We study the fundamental tradeoffs between statistical accuracy and\\ncomputational tractability in the analysis of high dimensional heterogeneous\\ndata. As examples, we study sparse Gaussian mixture model, mixture of sparse\\nlinear regressions, and sparse phase retrieval model. For these models, we\\nexploit an oracle-based computational model to establish conjecture-free\\ncomputationally feasible minimax lower bounds, which quantify the minimum\\nsignal strength required for the existence of any algorithm that is both\\ncomputationally tractable and statistically accurate. Our analysis shows that\\nthere exist significant gaps between computationally feasible minimax risks and\\nclassical ones. These gaps quantify the statistical price we must pay to\\nachieve computational tractability in the presence of data heterogeneity. Our\\nresults cover the problems of detection, estimation, support recovery, and\\nclustering, and moreover, resolve several conjectures of Azizyan et al. (2013,\\n2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our\\nresults reveal a new but counter-intuitive phenomenon in heterogeneous data\\nanalysis that more data might lead to less computation complexity.\\n',\n",
       " '  Audio-visual speech recognition (AVSR) system is thought to be one of the\\nmost promising solutions for robust speech recognition, especially in noisy\\nenvironment. In this paper, we propose a novel multimodal attention based\\nmethod for audio-visual speech recognition which could automatically learn the\\nfused representation from both modalities based on their importance. Our method\\nis realized using state-of-the-art sequence-to-sequence (Seq2seq)\\narchitectures. Experimental results show that relative improvements from 2% up\\nto 36% over the auditory modality alone are obtained depending on the different\\nsignal-to-noise-ratio (SNR). Compared to the traditional feature concatenation\\nmethods, our proposed approach can achieve better recognition performance under\\nboth clean and noisy conditions. We believe modality attention based end-to-end\\nmethod can be easily generalized to other multimodal tasks with correlated\\ninformation.\\n',\n",
       " '  The goal of this survey article is to explain and elucidate the affine\\nstructure of recent models appearing in the rough volatility literature, and\\nshow how it leads to exponential-affine transform formulas.\\n',\n",
       " '  White dwarf stars have been used as flux standards for decades, thanks to\\ntheir staid simplicity. We have empirically tested their photometric stability\\nby analyzing the light curves of 398 high-probability candidates and\\nspectroscopically confirmed white dwarfs observed during the original Kepler\\nmission and later with K2 Campaigns 0-8. We find that the vast majority (>97\\nper cent) of non-pulsating and apparently isolated white dwarfs are stable to\\nbetter than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,\\nconfirming that these stellar remnants are useful flux standards. From the\\ncases that do exhibit significant variability, we caution that binarity,\\nmagnetism, and pulsations are three important attributes to rule out when\\nestablishing white dwarfs as flux standards, especially those hotter than\\n30,000 K.\\n',\n",
       " '  Most existing approaches address multi-view subspace clustering problem by\\nconstructing the affinity matrix on each view separately and afterwards propose\\nhow to extend spectral clustering algorithm to handle multi-view data. This\\npaper presents an approach to multi-view subspace clustering that learns a\\njoint subspace representation by constructing affinity matrix shared among all\\nviews. Relying on the importance of both low-rank and sparsity constraints in\\nthe construction of the affinity matrix, we introduce the objective that\\nbalances between the agreement across different views, while at the same time\\nencourages sparsity and low-rankness of the solution. Related low-rank and\\nsparsity constrained optimization problem is for each view solved using the\\nalternating direction method of multipliers. Furthermore, we extend our\\napproach to cluster data drawn from nonlinear subspaces by solving the\\ncorresponding problem in a reproducing kernel Hilbert space. The proposed\\nalgorithm outperforms state-of-the-art multi-view subspace clustering\\nalgorithms on one synthetic and four real-world datasets.\\n',\n",
       " \"  The P300 event-related potential (ERP), evoked in scalp-recorded\\nelectroencephalography (EEG) by external stimuli, has proven to be a reliable\\nresponse for controlling a BCI. The P300 component of an event related\\npotential is thus widely used in brain-computer interfaces to translate the\\nsubjects' intent by mere thoughts into commands to control artificial devices.\\nThe main challenge in the classification of P300 trials in\\nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of\\nthe P300 response. To overcome the low SNR of individual trials, it is common\\npractice to average together many consecutive trials, which effectively\\ndiminishes the random noise. Unfortunately, when more repeated trials are\\nrequired for applications such as the P300 speller, the communication rate is\\ngreatly reduced. This has resulted in a need for better methods to improve\\nsingle-trial classification accuracy of P300 response. In this work, we use\\nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear\\nDiscriminant Analysis (LDA)and neural networks for classification. The results\\nshow that a combination of PCA with these methods provided as high as 13\\\\%\\naccuracy gain for single-trial classification while using only 3 to 4 principal\\ncomponents.\\n\",\n",
       " '  The purpose of this paper is to study stable representations of partially\\nordered sets (posets) and compare it to the well known theory for quivers. In\\nparticular, we prove that every indecomposable representation of a poset of\\nfinite type is stable with respect to some weight and construct that weight\\nexplicitly in terms of the dimension vector. We show that if a poset is\\nprimitive then Coxeter transformations preserve stable representations. When\\nthe base field is the field of complex numbers we establish the connection\\nbetween the polystable representations and the unitary $\\\\chi$-representations\\nof posets. This connection explains the similarity of the results obtained in\\nthe series of papers.\\n',\n",
       " \"  Segmentation in dynamic outdoor environments can be difficult when the\\nillumination levels and other aspects of the scene cannot be controlled.\\nSpecifically in orchard and vineyard automation contexts, a background material\\nis often used to shield a camera's field of view from other rows of crops. In\\nthis paper, we describe a method that uses superpixels to determine low texture\\nregions of the image that correspond to the background material, and then show\\nhow this information can be integrated with the color distribution of the image\\nto compute optimal segmentation parameters to segment objects of interest.\\nQuantitative and qualitative experiments demonstrate the suitability of this\\napproach for dynamic outdoor environments, specifically for tree reconstruction\\nand apple flower detection applications.\\n\",\n",
       " '  Stochastic bandit algorithms can be used for challenging non-convex\\noptimization problems. Hyperparameter tuning of neural networks is particularly\\nchallenging, necessitating new approaches. To this end, we present a method\\nthat adaptively partitions the combined space of hyperparameters, context, and\\ntraining resources (e.g., total number of training iterations). By adaptively\\npartitioning the space, the algorithm is able to focus on the portions of the\\nhyperparameter search space that are most relevant in a practical way. By\\nincluding the resources in the combined space, the method tends to use fewer\\ntraining resources overall. Our experiments show that this method can surpass\\nstate-of-the-art methods in tuning neural networks on benchmark datasets. In\\nsome cases, our implementations can achieve the same levels of accuracy on\\nbenchmark datasets as existing state-of-the-art approaches while saving over\\n50% of our computational resources (e.g. time, training iterations).\\n',\n",
       " '  Dynamic security analysis is an important problem of power systems on\\nensuring safe operation and stable power supply even when certain faults occur.\\nNo matter such faults are caused by vulnerabilities of system components,\\nphysical attacks, or cyber-attacks that are more related to cyber-security,\\nthey eventually affect the physical stability of a power system. Examples of\\nthe loss of physical stability include the Northeast blackout of 2003 in North\\nAmerica and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid\\nnature, that is, nonlinear continuous dynamics integrated with discrete\\nswitching, and the high degree of freedom property of power system dynamics\\nmake it challenging to conduct the dynamic security analysis. In this paper, we\\nuse the hybrid automaton model to describe the dynamics of a power system and\\nmainly deal with the index-1 differential-algebraic equation models regarding\\nthe continuous dynamics in different discrete states. The analysis problem is\\nformulated as a reachability problem of the associated hybrid model. A\\nsampling-based algorithm is then proposed by integrating modeling and\\nrandomized simulation of the hybrid dynamics to search for a feasible execution\\nconnecting an initial state of the post-fault system and a target set in the\\ndesired operation mode. The proposed method enables the use of existing power\\nsystem simulators for the synthesis of discrete switching and control\\nstrategies through randomized simulation. The effectiveness and performance of\\nthe proposed approach are demonstrated with an application to the dynamic\\nsecurity analysis of the New England 39-bus benchmark power system exhibiting\\nhybrid dynamics. In addition to evaluating the dynamic security, the proposed\\nmethod searches for a feasible strategy to ensure the dynamic security of the\\nsystem in face of disruptions.\\n',\n",
       " '  Second order conic programming (SOCP) has been used to model various\\napplications in power systems, such as operation and expansion planning. In\\nthis paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model\\nfor the distribution system expansion planning problem that considers\\nuncertainty and also captures the nonlinear AC power flow. To avoid costly\\ninvestment plans due to some extreme scenarios, we further present a\\nchance-constrained variant that could lead to cost-effective solutions. To\\naddress the computational challenge, we extend the basic Benders decomposition\\nmethod and develop a bilinear variant to compute stochastic and\\nchance-constrained MISOCP formulations. A set of numerical experiments is\\nperformed to illustrate the performance of our models and computational\\nmethods. In particular, results show that our Benders decomposition algorithms\\ndrastically outperform a professional MISOCP solver in handling stochastic\\nscenarios by orders of magnitude.\\n',\n",
       " '  We present the multi-hop extensions of the recently proposed energy-efficient\\ntime synchronization scheme for wireless sensor networks, which is based on the\\nasynchronous source clock frequency recovery and reversed two-way message\\nexchanges. We consider two hierarchical extensions based on packet relaying and\\ntime-translating gateways, respectively, and analyze their performance with\\nrespect to the number of layers and the delay variations through simulations.\\nThe simulation results demonstrate that the time synchronization performance of\\nthe packet relaying, which has lower complexity, is close to that of\\ntime-translating gateways.\\n',\n",
       " '  Instructional labs are widely seen as a unique, albeit expensive, way to\\nteach scientific content. We measured the effectiveness of introductory lab\\ncourses at achieving this educational goal across nine different lab courses at\\nthree very different institutions. These institutions and courses encompassed a\\nbroad range of student populations and instructional styles. The nine courses\\nstudied had two key things in common: the labs aimed to reinforce the content\\npresented in lectures, and the labs were optional. By comparing the performance\\nof students who did and did not take the labs (with careful normalization for\\nselection effects), we found universally and precisely no added value to\\nlearning from taking the labs as measured by course exam performance. This work\\nshould motivate institutions and departments to reexamine the goals and conduct\\nof their lab courses, given their resource-intensive nature. We show why these\\nresults make sense when looking at the comparative mental processes of students\\ninvolved in research and instructional labs, and offer alternative goals and\\ninstructional approaches that would make lab courses more educationally\\nvaluable.\\n',\n",
       " '  The formation of pattern in biological systems may be modeled by a set of\\nreaction-diffusion equations. A diffusion-type coupling operator biologically\\nsignificant in neuroscience is a difference of Gaussian functions (Mexican Hat\\noperator) used as a spatial-convolution kernel. We are interested in the\\ndifference among behaviors of \\\\emph{stochastic} neural field equations, namely\\nspace-time stochastic differential-integral equations, and similar\\ndeterministic ones. We explore, quantitatively, how the parameters of our model\\nthat measure the shape of the coupling kernel, coupling strength, and aspects\\nof the spatially-smoothed space-time noise, control the pattern in the\\nresulting evolving random field. We find that a spatial pattern that is damped\\nin time in a deterministic system may be sustained and amplified by\\nstochasticity, most strikingly at an optimal spatio-temporal noise level. In\\naddition, we find that spatially-smoothed noise alone causes pattern formation\\neven without spatial coupling.\\n',\n",
       " \"  Named-entity recognition (NER) aims at identifying entities of interest in a\\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\\nexisting NER systems. However, ANNs remain challenging to use for non-expert\\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\\nrecognition tool based on ANNs. Users can annotate entities using a graphical\\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\\n\",\n",
       " \"  Blocking objects (blockages) between a transmitter and receiver cause\\nwireless communication links to transition from line-of-sight (LOS) to\\nnon-line-of-sight (NLOS) propagation, which can greatly reduce the received\\npower, particularly at higher frequencies such as millimeter wave (mmWave). We\\nconsider a cellular network in which a mobile user attempts to connect to two\\nor more base stations (BSs) simultaneously, to increase the probability of at\\nleast one LOS link, which is a form of macrodiversity. We develop a framework\\nfor determining the LOS probability as a function of the number of BSs, when\\ntaking into account the correlation between blockages: for example, a single\\nblockage close to the device -- including the user's own body -- could block\\nmultiple BSs. We consider the impact of the size of blocking objects on the\\nsystem reliability probability and show that macrodiversity gains are higher\\nwhen the blocking objects are small. We also show that the BS density must\\nscale as the square of the blockage density to maintain a given level of\\nreliability.\\n\",\n",
       " '  We present an introduction to a novel model of an individual and group\\nopinion dynamics, taking into account different ways in which different sources\\nof information are filtered due to cognitive biases. The agent based model,\\nusing Bayesian updating of the individual belief distribution, is based on the\\nrecent psychology work by Dan Kahan. Open nature of the model allows to study\\nthe effects of both static and time-dependent biases and information processing\\nfilters. In particular, the paper compares the effects of two important\\npsychological mechanisms: the confirmation bias and the politically motivated\\nreasoning. Depending on the effectiveness of the information filtering (agent\\nbias), the agents confronted with an objective information source may either\\nreach a consensus based on the truth, or remain divided despite the evidence.\\nIn general, the model might provide an understanding into the increasingly\\npolarized modern societies, especially as it allows mixing of different types\\nof filters: psychological, social, and algorithmic.\\n',\n",
       " '  Convolutional Neural Networks (CNNs) are commonly thought to recognise\\nobjects by learning increasingly complex representations of object shapes. Some\\nrecent studies suggest a more important role of image textures. We here put\\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\\nhuman observers on images with a texture-shape cue conflict. We show that\\nImageNet-trained CNNs are strongly biased towards recognising textures rather\\nthan shapes, which is in stark contrast to human behavioural evidence and\\nreveals fundamentally different classification strategies. We then demonstrate\\nthat the same standard architecture (ResNet-50) that learns a texture-based\\nrepresentation on ImageNet is able to learn a shape-based representation\\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\\nThis provides a much better fit for human behavioural performance in our\\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\\npsychophysical trials across 97 observers) and comes with a number of\\nunexpected emergent benefits such as improved object detection performance and\\npreviously unseen robustness towards a wide range of image distortions,\\nhighlighting advantages of a shape-based representation.\\n',\n",
       " '  In this paper, we obtain some formulae for harmonic sums, alternating\\nharmonic sums and Stirling number sums by using the method of integral\\nrepresentations of series. As applications of these formulae, we give explicit\\nformula of several quadratic and cubic Euler sums through zeta values and\\nlinear sums. Furthermore, some relationships between harmonic numbers and\\nStirling numbers of the first kind are established.\\n',\n",
       " '  In this paper we develop cyclic proof systems for the problem of inclusion\\nbetween the least sets of models of mutually recursive predicates, when the\\nground constraints in the inductive definitions belong to the quantifier-free\\nfragments of (i) First Order Logic with the canonical Herbrand interpretation\\nand (ii) Separation Logic, respectively. Inspired by classical\\nautomata-theoretic techniques of proving language inclusion between tree\\nautomata, we give a small set of inference rules, that are proved to be sound\\nand complete, under certain semantic restrictions, involving the set of\\nconstraints in the inductive system. Moreover, we investigate the decidability\\nand computational complexity of these restrictions for all the logical\\nfragments considered and provide a proof search semi-algorithm that becomes a\\ndecision procedure for the entailment problem, for those systems that fulfill\\nthe restrictions.\\n',\n",
       " '  In this paper, we introduce a new model for leveraging unlabeled data to\\nimprove generalization performances of image classifiers: a two-branch\\nencoder-decoder architecture called HybridNet. The first branch receives\\nsupervision signal and is dedicated to the extraction of invariant\\nclass-related representations. The second branch is fully unsupervised and\\ndedicated to model information discarded by the first branch to reconstruct\\ninput data. To further support the expected behavior of our model, we propose\\nan original training objective. It favors stability in the discriminative\\nbranch and complementarity between the learned representations in the two\\nbranches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,\\nSVHN and STL-10 in various semi-supervised settings. In addition,\\nvisualizations and ablation studies validate our contributions and the behavior\\nof the model on both CIFAR-10 and STL-10 datasets.\\n',\n",
       " '  We describe a neural network model that jointly learns distributed\\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\\nKB, we train our proposed model to predict entities that are relevant to the\\ntext. Our model is designed to be generic with the ability to address various\\nNLP tasks with ease. We train the model using a large corpus of texts and their\\nentity annotations extracted from Wikipedia. We evaluated the model on three\\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\\nfactoid question answering) involving both unsupervised and supervised\\nsettings. As a result, we achieved state-of-the-art results on all three of\\nthese tasks. Our code and trained models are publicly available for further\\nacademic research.\\n',\n",
       " '  For $n\\\\ge5$, it is well known that the moduli space $\\\\mathfrak{M_{0,\\\\:n}}$ of\\nunordered $n$ points on the Riemann sphere is a quotient space of the Zariski\\nopen set $K_n$ of $\\\\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this\\n$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the\\ngroups fixing the sets of $n$ points on the Riemann sphere. Let $\\\\alpha$ be a\\nsubset of $n$ distinct points on the Riemann sphere. We call the group of all\\nlinear fractional transformations leaving $\\\\alpha$ invariant the stabilizer of\\n$\\\\alpha$, which is finite by observation. For each non-trivial finite subgroup\\n$G$ of the group ${\\\\rm PSL}(2,{\\\\Bbb C})$ of linear fractional transformations,\\nwe give the necessary and sufficient condition for finite subsets of the\\nRiemann sphere under which the stabilizers of them are conjugate to $G$. We\\nalso prove that there does exist some finite subset of the Riemann sphere whose\\nstabilizer coincides with $G$. Next we obtain the irreducible decompositions of\\nthe representations of the stabilizers on the tangent spaces at the\\nsingularities of $\\\\mathfrak{M_{0,\\\\:n}}$. At last, on $\\\\mathfrak{M_{0,\\\\:5}}$ and\\n$\\\\mathfrak{M_{0,\\\\:6}}$, we work out explicitly the singularities and the\\nrepresentations of their stabilizers on the tangent spaces at them.\\n',\n",
       " \"  The radio interferometric positioning system (RIPS) is an accurate node\\nlocalization method featuring a novel phase-based ranging process. Multipath is\\nthe limiting error source for RIPS in ground-deployed scenarios or indoor\\napplications. There are four distinct channels involved in the ranging process\\nfor RIPS. Multipath reflections affect both the phase and amplitude of the\\nranging signal for each channel. By exploiting untapped amplitude information,\\nwe put forward a scheme to estimate each channel's multipath profile, which is\\nthen subsequently used to correct corresponding errors in phase measurements.\\nSimulations show that such a scheme is very effective in reducing multipath\\nphase errors, which are essentially brought down to the level of receiver noise\\nunder moderate multipath conditions. It is further demonstrated that ranging\\nerrors in RIPS are also greatly reduced via the proposed scheme.\\n\",\n",
       " '  We present an affine analog of the evaluation map for quantum groups. Namely\\nwe introduce a surjective homomorphism from the quantum toroidal gl(n) algebra\\nto the quantum affine gl(n) algebra completed with respect to the homogeneous\\ngrading. We give a brief discussion of evaluation modules.\\n',\n",
       " '  In this paper we present a framework for risk-sensitive model predictive\\ncontrol (MPC) of linear systems affected by stochastic multiplicative\\nuncertainty. Our key innovation is to consider a time-consistent, dynamic risk\\nevaluation of the cumulative cost as the objective function to be minimized.\\nThis framework is axiomatically justified in terms of time-consistency of risk\\nassessments, is amenable to dynamic optimization, and is unifying in the sense\\nthat it captures a full range of risk preferences from risk-neutral (i.e.,\\nexpectation) to worst case. Within this framework, we propose and analyze an\\nonline risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,\\nby exploiting the dual representation of time-consistent, dynamic risk\\nmeasures, we cast the computation of the MPC control law as a convex\\noptimization problem amenable to real-time implementation. Simulation results\\nare presented and discussed.\\n',\n",
       " \"  Ground-based astronomical observations may be limited by telluric water vapor\\nabsorption, which is highly variable in time and significantly complicates both\\nspectroscopy and photometry in the near-infrared (NIR). To achieve the\\nsensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous\\nmonitoring of precipitable water vapor (PWV) becomes necessary to mitigate the\\nimpact of variable telluric lines on radial velocity measurements and transit\\nlight curves. To address this issue, we present the Camera for the Automatic\\nMonitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch\\naperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple\\nObservatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to\\ntrace the amount of atmospheric water vapor affecting simultaneous observations\\nwith the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red\\ntelescopes. Here we present the current design of CAMAL, discuss our data\\nanalysis methods, and show results from 11 nights of PWV measurements taken\\nwith CAMAL. For seven nights of data, we have independent PWV measurements\\nextracted from high-resolution stellar spectra taken with the Tillinghast\\nReflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the\\nTRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between\\nCAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm\\nover a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates\\nto PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)\\nconditions. We also find that CAMAL-derived PWVs are highly correlated with\\nthose from a GPS-based water vapor monitor located approximately 90 km away at\\nKitt Peak National Observatory, with a root mean square PWV difference of 0.8\\nmm.\\n\",\n",
       " '  Gossip protocols aim at arriving, by means of point-to-point or group\\ncommunications, at a situation in which all the agents know each other secrets.\\nRecently a number of authors studied distributed epistemic gossip protocols.\\nThese protocols use as guards formulas from a simple epistemic logic, which\\nmakes their analysis and verification substantially easier.\\nWe study here common knowledge in the context of such a logic. First, we\\nanalyze when it can be reduced to iterated knowledge. Then we show that the\\nsemantics and truth for formulas without nested common knowledge operator are\\ndecidable. This implies that implementability, partial correctness and\\ntermination of distributed epistemic gossip protocols that use non-nested\\ncommon knowledge operator is decidable, as well. Given that common knowledge is\\nequivalent to an infinite conjunction of nested knowledge, these results are\\nnon-trivial generalizations of the corresponding decidability results for the\\noriginal epistemic logic, established in (Apt & Wojtczak, 2016).\\nK. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In\\nProc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.\\n',\n",
       " '  We examine discrete vortex dynamics in two-dimensional flow through a\\nnetwork-theoretic approach. The interaction of the vortices is represented with\\na graph, which allows the use of network-theoretic approaches to identify key\\nvortex-to-vortex interactions. We employ sparsification techniques on these\\ngraph representations based on spectral theory for constructing sparsified\\nmodels and evaluating the dynamics of vortices in the sparsified setup.\\nIdentification of vortex structures based on graph sparsification and sparse\\nvortex dynamics are illustrated through an example of point-vortex clusters\\ninteracting amongst themselves. We also evaluate the performance of\\nsparsification with increasing number of point vortices. The\\nsparsified-dynamics model developed with spectral graph theory requires reduced\\nnumber of vortex-to-vortex interactions but agrees well with the full nonlinear\\ndynamics. Furthermore, the sparsified model derived from the sparse graphs\\nconserves the invariants of discrete vortex dynamics. We highlight the\\nsimilarities and differences between the present sparsified-dynamics model and\\nthe reduced-order models.\\n',\n",
       " '  In this paper we study a non-linear partial differential equation (PDE),\\nproposed by N. Kudryashov [arXiv:1611.06813v1[nlin.SI]], using continuum limit\\napproximation of mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models. This\\ngeneralized semi-discrete equation can be considered as a model for the\\ndescription of non-linear dislocation waves in crystal lattice and the\\ncorresponding continuous system can be called mixed generalized potential KdV\\nand sine-Gordon equation. We obtain the Bäcklund transformation of this\\nequation in Riccati form in inverse method. We further study the\\nquasi-integrable deformation of this model.\\n',\n",
       " \"  An important, yet largely unstudied, problem in student data analysis is to\\ndetect misconceptions from students' responses to open-response questions.\\nMisconception detection enables instructors to deliver more targeted feedback\\non the misconceptions exhibited by many students in their class, thus improving\\nthe quality of instruction. In this paper, we propose a new natural language\\nprocessing-based framework to detect the common misconceptions among students'\\ntextual responses to short-answer questions. We propose a probabilistic model\\nfor students' textual responses involving misconceptions and experimentally\\nvalidate it on a real-world student-response dataset. Experimental results show\\nthat our proposed framework excels at classifying whether a response exhibits\\none or more misconceptions. More importantly, it can also automatically detect\\nthe common misconceptions exhibited across responses from multiple students to\\nmultiple questions; this property is especially important at large scale, since\\ninstructors will no longer need to manually specify all possible misconceptions\\nthat students might exhibit.\\n\",\n",
       " '  A Schottky structure on a handlebody $M$ of genus $g$ is provided by a\\nSchottky group of rank $g$. A symmetry (an orientation-reversing involution) of\\n$M$ is known to have at most $(g+1)$ connected components of fixed points. Each\\nof these components is either a point or a compact bordered surface (either\\norientable or not) whose boundary is contained in the border of $M$. In this\\npaper, we derive sharp upper bounds for the total number of connected\\ncomponents of the sets of fixed points of given two or three symmetries of $M$.\\nIn order to obtain such an upper bound, we obtain a geometrical structure\\ndescription of those extended Kleinian groups $K$ containing a Schottky group\\n$\\\\Gamma$ as finite index normal subgroup so that $K/\\\\Gamma$ is a dihedral group\\n(called dihedral Schottky groups). Our upper bounds turn out to be different to\\nthe corresponding ones at the level of closed Riemann surfaces. In contrast to\\nthe case of Riemann surfaces, we observe that $M$ cannot have two different\\nmaximal symmetries.\\n',\n",
       " '  In this paper we propose a new method of speaker diarization that employs a\\ndeep learning architecture to learn speaker embeddings. In contrast to the\\ntraditional approaches that build their speaker embeddings using manually\\nhand-crafted spectral features, we propose to train for this purpose a\\nrecurrent convolutional neural network applied directly on magnitude\\nspectrograms. To compare our approach with the state of the art, we collect and\\nrelease for the public an additional dataset of over 6 hours of fully annotated\\nbroadcast material. The results of our evaluation on the new dataset and three\\nother benchmark datasets show that our proposed method significantly\\noutperforms the competitors and reduces diarization error rate by a large\\nmargin of over 30% with respect to the baseline.\\n',\n",
       " '  This paper describes an implementation of the L-BFGS method designed to deal\\nwith two adversarial situations. The first occurs in distributed computing\\nenvironments where some of the computational nodes devoted to the evaluation of\\nthe function and gradient are unable to return results on time. A similar\\nchallenge occurs in a multi-batch approach in which the data points used to\\ncompute function and gradients are purposely changed at each iteration to\\naccelerate the learning process. Difficulties arise because L-BFGS employs\\ngradient differences to update the Hessian approximations, and when these\\ngradients are computed using different data points the updating process can be\\nunstable. This paper shows how to perform stable quasi-Newton updating in the\\nmulti-batch setting, studies the convergence properties for both convex and\\nnonconvex functions, and illustrates the behavior of the algorithm in a\\ndistributed computing platform on binary classification logistic regression and\\nneural network training problems that arise in machine learning.\\n',\n",
       " '  Networks of vertically c-oriented prism shaped InN nanowalls, are grown on\\nc-GaN/sapphire templates using a CVD technique, where pure indium and ammonia\\nare used as metal and nitrogen precursors. A systematic study of the growth,\\nstructural and electronic properties of these samples shows a preferential\\ngrowth of the islands along [11-20] and [0001] directions leading to the\\nformation of such a network structure, where the vertically [0001] oriented\\ntapered walls are laterally align along one of the three [11-20] directions.\\nInclined facets of these walls are identified as r-planes [(1-102)-planes] of\\nwurtzite InN. Onset of absorption for these samples is observed to be higher\\nthan the band gap of InN suggesting a high background carrier concentration in\\nthis material. Study of the valence band edge through XPS indicates the\\nformation of positive depletion regions below the r-plane side facets of the\\nwalls. This is in contrast with the observation for c-plane InN epilayers,\\nwhere electron accumulation is often reported below the top surface.\\n',\n",
       " '  Machine Learning focuses on the construction and study of systems that can\\nlearn from data. This is connected with the classification problem, which\\nusually is what Machine Learning algorithms are designed to solve. When a\\nmachine learning method is used by people with no special expertise in machine\\nlearning, it is important that the method be robust in classification, in the\\nsense that reasonable performance is obtained with minimal tuning of the\\nproblem at hand. Algorithms are evaluated based on how robust they can classify\\nthe given data. In this paper, we propose a quantifiable measure of robustness,\\nand describe a particular learning method that is robust according to this\\nmeasure in the context of classification problem. We proposed Adaptive Boosting\\n(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold\\n(P) and number of iterations (I) for boosting algorithm. To benchmark the\\nperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump as\\nbase learner without tuning parameters. By tuning parameters and using J48 as\\nbase learner, we are able to reduce the overall average error rate ratio\\n(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2\\nfor evaluation sets of data.\\n',\n",
       " '  This paper presents a simple agent-based model of an economic system,\\npopulated by agents playing different games according to their different view\\nabout social cohesion and tax payment. After a first set of simulations,\\ncorrectly replicating results of existing literature, a wider analysis is\\npresented in order to study the effects of a dynamic-adaptation rule, in which\\ncitizens may possibly decide to modify their individual tax compliance\\naccording to individual criteria, such as, the strength of their ethical\\ncommitment, the satisfaction gained by consumption of the public good and the\\nperceived opinion of neighbors. Results show the presence of thresholds levels\\nin the composition of society - between taxpayers and evaders - which explain\\nthe extent of damages deriving from tax evasion.\\n',\n",
       " \"  The variability response function (VRF) is generalized to statically\\ndeterminate Euler Bernoulli beams with arbitrary stress-strain laws following\\nCauchy elastic behavior. The VRF is a Green's function that maps the spectral\\ndensity function (SDF) of a statistically homogeneous random field describing\\nthe correlation structure of input uncertainty to the variance of a response\\nquantity. The appeal of such Green's functions is that the variance can be\\ndetermined for any correlation structure by a trivial computation of a\\nconvolution integral. The method introduced in this work derives VRFs in closed\\nform for arbitrary nonlinear Cauchy-elastic constitutive laws and is\\ndemonstrated through three examples. It is shown why and how higher order\\nspectra of the random field affect the response variance for nonlinear\\nconstitutive laws. In the general sense, the VRF for a statically determinate\\nbeam is found to be a matrix kernel whose inner product by a matrix of higher\\norder SDFs and statistical moments is integrated to give the response variance.\\nThe resulting VRF matrix is unique regardless of the random field's marginal\\nprobability density function (PDF) and SDFs.\\n\",\n",
       " '  We show that training a deep network using batch normalization is equivalent\\nto approximate inference in Bayesian models. We further demonstrate that this\\nfinding allows us to make meaningful estimates of the model uncertainty using\\nconventional architectures, without modifications to the network or the\\ntraining procedure. Our approach is thoroughly validated by measuring the\\nquality of uncertainty in a series of empirical experiments on different tasks.\\nIt outperforms baselines with strong statistical significance, and displays\\ncompetitive performance with recent Bayesian approaches.\\n',\n",
       " '  In this paper we consider a single-cell downlink scenario where a\\nmultiple-antenna base station delivers contents to multiple cache-enabled user\\nterminals. Based on the multicasting opportunities provided by the so-called\\nCoded Caching technique, we investigate three delivery approaches. Our baseline\\nscheme employs the coded caching technique on top of max-min fair multicasting.\\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\\ncaching, where the coded chunks are formed in the signal domain (complex\\nfield). The third scheme is similar to the second one with the difference that\\nthe coded chunks are formed in the data domain (finite field). We derive\\nclosed-form rate expressions where our results suggest that the latter two\\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\\nthe intermediate SNR regime forming coded chunks in the signal domain results\\nin power loss, and will deteriorate throughput of the second scheme. The main\\nmessage of our paper is that the schemes performing well in terms of DoF may\\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\\nshould be employed.\\n',\n",
       " '  The block bootstrap approximates sampling distributions from dependent data\\nby resampling data blocks. A fundamental problem is establishing its\\nconsistency for the distribution of a sample mean, as a prototypical statistic.\\nWe use a structural relationship with subsampling to characterize the bootstrap\\nin a new and general manner. While subsampling and block bootstrap differ, the\\nblock bootstrap distribution of a sample mean equals that of a $k$-fold\\nself-convolution of a subsampling distribution. Motivated by this, we provide\\nsimple necessary and sufficient conditions for a convolved subsampling\\nestimator to produce a normal limit that matches the target of bootstrap\\nestimation. These conditions may be linked to consistency properties of an\\noriginal subsampling distribution, which are often obtainable under minimal\\nassumptions. Through several examples, the results are shown to validate the\\nblock bootstrap for means under significantly weakened assumptions in many\\nexisting (and some new) dependence settings, which also addresses a standing\\nconjecture of Politis, Romano and Wolf(1999). Beyond sample means, the\\nconvolved subsampling estimator may not match the block bootstrap, but instead\\nprovides a hybrid-resampling estimator of interest in its own right. For\\ngeneral statistics with normal limits, results also establish the consistency\\nof convolved subsampling under minimal dependence conditions, including\\nnon-stationarity.\\n',\n",
       " '  Given a polynomial system f associated with a simple multiple zero x of\\nmultiplicity {\\\\mu}, we give a computable lower bound on the minimal distance\\nbetween the simple multiple zero x and other zeros of f. If x is only given\\nwith limited accuracy, we propose a numerical criterion that f is certified to\\nhave {\\\\mu} zeros (counting multiplicities) in a small ball around x.\\nFurthermore, for simple double zeros and simple triple zeros whose Jacobian is\\nof normalized form, we define modified Newton iterations and prove the\\nquantified quadratic convergence when the starting point is close to the exact\\nsimple multiple zero. For simple multiple zeros of arbitrary multiplicity whose\\nJacobian matrix may not have a normalized form, we perform unitary\\ntransformations and modified Newton iterations, and prove its non-quantified\\nquadratic convergence and its quantified convergence for simple triple zeros.\\n',\n",
       " '  It can be difficult to tell whether a trained generative model has learned to\\ngenerate novel examples or has simply memorized a specific set of outputs. In\\npublished work, it is common to attempt to address this visually, for example\\nby displaying a generated example and its nearest neighbor(s) in the training\\nset (in, for example, the L2 metric). As any generative model induces a\\nprobability density on its output domain, we propose studying this density\\ndirectly. We first study the geometry of the latent representation and\\ngenerator, relate this to the output density, and then develop techniques to\\ncompute and inspect the output density. As an application, we demonstrate that\\n\"memorization\" tends to a density made of delta functions concentrated on the\\nmemorized examples. We note that without first understanding the geometry, the\\nmeasurement would be essentially impossible to make.\\n',\n",
       " '  Refraction represents one of the most fundamental operations that may be\\nperformed by a metasurface. However, simple phasegradient metasurface designs\\nsuffer from restricted angular deflection due to spurious diffraction orders.\\nIt has been recently shown, using a circuit-based approach, that refraction\\nwithout spurious diffraction, or diffraction-free, can fortunately be achieved\\nby a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,\\nwe rederive these conditions using a medium-based - and hence more insightfull\\n- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface\\nsusceptibility tensors, and experimentally demonstrate two diffraction-free\\nrefractive metasurfaces that are essentially lossless, passive, bianisotropic\\nand reciprocal.\\n',\n",
       " '  We reconsider the classic problem of estimating accurately a 2D\\ntransformation from point matches between images containing outliers. RANSAC\\ndiscriminates outliers by randomly generating minimalistic sampled hypotheses\\nand verifying their consensus over the input data. Its response is based on the\\nsingle hypothesis that obtained the largest inlier support. In this article we\\nshow that the resulting accuracy can be improved by aggregating all generated\\nhypotheses. This yields RANSAAC, a framework that improves systematically over\\nRANSAC and its state-of-the-art variants by statistically aggregating\\nhypotheses. To this end, we introduce a simple strategy that allows to rapidly\\naverage 2D transformations, leading to an almost negligible extra computational\\ncost. We give practical applications on projective transforms and\\nhomography+distortion models and demonstrate a significant performance gain in\\nboth cases.\\n',\n",
       " '  The Landau collision integral is an accurate model for the small-angle\\ndominated Coulomb collisions in fusion plasmas. We investigate a high order\\naccurate, fully conservative, finite element discretization of the nonlinear\\nmulti-species Landau integral with adaptive mesh refinement using the PETSc\\nlibrary (www.mcs.anl.gov/petsc). We develop algorithms and techniques to\\nefficiently utilize emerging architectures with an approach that minimizes\\nmemory usage and movement and is suitable for vector processing. The Landau\\ncollision integral is vectorized with Intel AVX-512 intrinsics and the solver\\nsustains as much as 22% of the theoretical peak flop rate of the Second\\nGeneration Intel Xeon Phi, Knights Landing, processor.\\n',\n",
       " '  In this paper we combine a survey of the most important topological\\nproperties of kinematic maps that appear in robotics, with the exposition of\\nsome basic results regarding the topological complexity of a map. In\\nparticular, we discuss mechanical devices that consist of rigid parts connected\\nby joints and show how the geometry of the joints determines the forward\\nkinematic map that relates the configuration of joints with the pose of the\\nend-effector of the device. We explain how to compute the dimension of the\\njoint space and describe topological obstructions for a kinematic map to be a\\nfibration or to admit a continuous section. In the second part of the paper we\\ndefine the complexity of a continuous map and show how the concept can be\\nviewed as a measure of the difficulty to find a robust manipulation plan for a\\ngiven mechanical device. We also derive some basic estimates for the complexity\\nand relate it to the degree of instability of a manipulation plan.\\n',\n",
       " '  The Butler-Portugal algorithm for obtaining the canonical form of a tensor\\nexpression with respect to slot symmetries and dummy-index renaming suffers, in\\ncertain cases with a high degree of symmetry, from $O(n!)$ explosion in both\\ncomputation time and memory. We present a modified algorithm which alleviates\\nthis problem in the most common cases---tensor expressions with subsets of\\nindices which are totally symmetric or totally antisymmetric---in polynomial\\ntime. We also present an implementation of the label-renaming mechanism which\\nimproves upon that of the original Butler-Portugal algorithm, thus providing a\\nsignificant speed increase for the average case as well as the highly-symmetric\\nspecial case. The worst-case behavior remains $O(n!)$, although it occurs in\\nmore limited situations unlikely to appear in actual computations. We comment\\non possible strategies to take if the nature of a computation should make these\\nsituations more likely.\\n',\n",
       " '  The mass-preconditioning (MP) technique has become a standard tool to enhance\\nthe efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with\\ndynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as\\nits extension to the case of one-flavor by taking the square-root of the\\nfermion determinant of 2-flavors with degenerate masses. However, for lattice\\nQCD with domain-wall fermion, the fermion determinant of any single fermion\\nflavor can be expressed as a functional integral with an exact pseudofermion\\naction $ \\\\phi^\\\\dagger H^{-1} \\\\phi $, where $ H^{-1} $ is a positive-definite\\nHermitian operator without taking square-root, and with the chiral structure\\n\\\\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact\\none-flavor action (EOFA) does not necessarily follow the conventional (old) MP\\npattern. In this paper, we present a new mass-preconditioning for the EOFA,\\nwhich is more efficient than the old MP which we have used in Refs.\\n\\\\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD\\nwith $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one\\nmass-preconditioner applied to one of the exact one-flavor actions, and we find\\nthat the efficiency of the new MP is more than 20\\\\% higher than that of the old\\nMP.\\n',\n",
       " \"  A model in which a three-dimensional elastic medium is represented by a\\nnetwork of identical masses connected by springs of random strengths and\\nallowed to vibrate only along a selected axis of the reference frame, exhibits\\nan Anderson localization transition. To study this transition, we assume that\\nthe dynamical matrix of the network is given by a product of a sparse random\\nmatrix with real, independent, Gaussian-distributed non-zero entries and its\\ntranspose. A finite-time scaling analysis of system's response to an initial\\nexcitation allows us to estimate the critical parameters of the localization\\ntransition. The critical exponent is found to be $\\\\nu = 1.57 \\\\pm 0.02$ in\\nagreement with previous studies of Anderson transition belonging to the\\nthree-dimensional orthogonal universality class.\\n\",\n",
       " \"  We analyze the response of a type II superconducting wire to an external\\nmagnetic field parallel to it in the framework of Ginzburg-Landau theory. We\\nfocus on the surface superconductivity regime of applied field between the\\nsecond and third critical values, where the superconducting state survives only\\nclose to the sample's boundary. Our first finding is that, in first\\napproximation, the shape of the boundary plays no role in determining the\\ndensity of superconducting electrons. A second order term is however isolated,\\ndirectly proportional to the mean curvature of the boundary. This demonstrates\\nthat points of higher boundary curvature (counted inwards) attract\\nsuperconducting electrons.\\n\",\n",
       " '  Classical principal component analysis (PCA) is not robust to the presence of\\nsparse outliers in the data. The use of the $\\\\ell_1$ norm in the Robust PCA\\n(RPCA) method successfully eliminates the weakness of PCA in separating the\\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\\nnorm, we propose a weighted low rank (WLR) method to avoid the often\\ncomputationally expensive algorithms relying on the $\\\\ell_1$ norm. As a proof\\nof concept, a background estimation model has been presented and compared with\\ntwo $\\\\ell_1$ norm minimization algorithms. We illustrate that as long as a\\nsimple weight matrix is inferred from the data, one can use the weighted\\nFrobenius norm and achieve the same or better performance.\\n',\n",
       " '  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the\\ndegeneracy of left- and right-circularly polarized spin waves. This\\nrelativistic coupling increases the efficiency of spin-wave-induced domain wall\\nmotion and leads to higher drift velocities. We show that in biaxial\\nantiferromagnets, the spin-wave helicity controls both the direction and\\nmagnitude of the magnonic force on chiral domain walls. By contrast, in\\nuniaxial antiferromagnets, the magnonic force is propulsive with a helicity\\ndependent strength.\\n',\n",
       " '  In disordered elastic systems, driven by displacing a parabolic confining\\npotential adiabatically slowly, all advance of the system is in bursts, termed\\navalanches. Avalanches have a finite extension in time, which is much smaller\\nthan the waiting-time between them. Avalanches also have a finite extension\\n$\\\\ell$ in space, i.e. only a part of the interface of size $\\\\ell$ moves during\\nan avalanche. Here we study their spatial shape $\\\\left< S(x)\\\\right>_{\\\\ell}$\\ngiven $\\\\ell$, as well as its fluctuations encoded in the second cumulant\\n$\\\\left< S^{2}(x)\\\\right>_{\\\\ell}^{\\\\rm c}$. We establish scaling relations\\ngoverning the behavior close to the boundary. We then give analytic results for\\nthe Brownian force model, in which the microscopic disorder for each degree of\\nfreedom is a random walk. Finally, we confirm these results with numerical\\nsimulations. To do this properly we elucidate the influence of discretization\\neffects, which also confirms the assumptions entering into the scaling ansatz.\\nThis allows us to reach the scaling limit already for avalanches of moderate\\nsize. We find excellent agreement for the universal shape, its fluctuations,\\nincluding all amplitudes.\\n',\n",
       " '  The search for a superconductor with non-s-wave pairing is important not only\\nfor understanding unconventional mechanisms of superconductivity but also for\\nfinding new types of quasiparticles such as Majorana bound states. Materials\\nwith both topological band structure and superconductivity are promising\\ncandidates as $p+ip$ superconducting states can be generated through pairing\\nthe spin-polarized topological surface states. In this work, the electronic and\\nphonon properties of the superconductor molybdenum carbide (MoC) are studied\\nwith first-principles methods. Our calculations show that nontrivial band\\ntopology and superconductivity coexist in both structural phases of MoC,\\nnamely, the cubic $\\\\alpha$ and hexagonal $\\\\gamma$ phases. The $\\\\alpha$ phase is\\na strong topological insulator and the $\\\\gamma$ phase is a topological nodal\\nline semimetal with drumhead surface states. In addition, hole doping can\\nstabilize the crystal structure of the $\\\\alpha$ phase and elevate the\\ntransition temperature in the $\\\\gamma$ phase. Therefore, MoC in different\\nstructural forms can be a practical material platform for studying topological\\nsuperconductivity and elusive Majorana fermions.\\n',\n",
       " '  The extension complexity $\\\\mathsf{xc}(P)$ of a polytope $P$ is the minimum\\nnumber of facets of a polytope that affinely projects to $P$. Let $G$ be a\\nbipartite graph with $n$ vertices, $m$ edges, and no isolated vertices. Let\\n$\\\\mathsf{STAB}(G)$ be the convex hull of the stable sets of $G$. It is easy to\\nsee that $n \\\\leqslant \\\\mathsf{xc} (\\\\mathsf{STAB}(G)) \\\\leqslant n+m$. We improve\\nboth of these bounds. For the upper bound, we show that $\\\\mathsf{xc}\\n(\\\\mathsf{STAB}(G))$ is $O(\\\\frac{n^2}{\\\\log n})$, which is an improvement when\\n$G$ has quadratically many edges. For the lower bound, we prove that\\n$\\\\mathsf{xc} (\\\\mathsf{STAB}(G))$ is $\\\\Omega(n \\\\log n)$ when $G$ is the\\nincidence graph of a finite projective plane. We also provide examples of\\n$3$-regular bipartite graphs $G$ such that the edge vs stable set matrix of $G$\\nhas a fooling set of size $|E(G)|$.\\n',\n",
       " '  This work provides a comprehensive scaling law based performance analysis for\\nmulti-cell multi-user massive multiple-input-multiple-output (MIMO) downlink\\nsystems. Imperfect channel state information (CSI), pilot contamination, and\\nchannel spatial correlation are all considered. First, a sum- rate lower bound\\nis derived by exploiting the asymptotically deterministic property of the\\nreceived signal power, while keeping the random nature of other components in\\nthe signal-to-interference-plus-noise-ratio (SINR) intact. Via a general\\nscaling model on important network parameters, including the number of users,\\nthe channel training energy and the data transmission power, with respect to\\nthe number of base station antennas, the asymptotic scaling law of the\\neffective SINR is obtained, which reveals quantitatively the tradeoff of the\\nnetwork parameters. More importantly, pilot contamination and pilot\\ncontamination elimination (PCE) are considered in the analytical framework. In\\naddition, the applicability of the derived asymptotic scaling law in practical\\nsystems with large but finite antenna numbers are discussed. Finally,\\nsufficient conditions on the parameter scalings for the SINR to be\\nasymptotically deterministic in the sense of mean square convergence are\\nprovided, which covers existing results on such analysis as special cases and\\nshows the effect of PCE explicitly.\\n',\n",
       " \"  In view of a resurgence of concern about the measurement problem, it is\\npointed out that the Relativistic Transactional Interpretation (RTI) remedies\\nissues previously considered as drawbacks or refutations of the original TI.\\nSpecifically, once one takes into account relativistic processes that are not\\nrepresentable at the non-relativistic level (such as particle creation and\\nannihilation, and virtual propagation), absorption is quantitatively defined in\\nunambiguous physical terms. In addition, specifics of the relativistic\\ntransactional model demonstrate that the Maudlin `contingent absorber'\\nchallenge to the original TI cannot even be mounted: basic features of\\nestablished relativistic field theories (in particular, the asymmetry between\\nfield sources and the bosonic fields, and the fact that slow-moving bound\\nstates, such as atoms, are not offer waves) dictate that the `slow-moving offer\\nwave' required for the challenge scenario cannot exist. It is concluded that\\nissues previously considered obstacles for TI are no longer legitimately viewed\\nas such, and that reconsideration of the transactional picture is warranted in\\nconnection with solving the measurement problem.\\n\",\n",
       " '  We explore the response of Ir $5d$ orbitals to pressure in\\n$\\\\beta$-$\\\\mathrm{Li_2IrO_3}$, a hyperhoneycomb iridate in proximity to a Kitaev\\nquantum spin liquid (QSL) ground state. X-ray absorption spectroscopy reveals a\\nreconstruction of the electronic ground state below 2 GPa, the same pressure\\nrange where x-ray magnetic circular dichroism shows an apparent collapse of\\nmagnetic order. The electronic reconstruction, which manifests a reduction in\\nthe effective spin-orbit (SO) interaction in $5d$ orbitals, pushes\\n$\\\\beta$-$\\\\mathrm{Li_2IrO_3}$ further away from the pure $J_{\\\\rm eff}=1/2$\\nlimit. Although lattice symmetry is preserved across the electronic transition,\\nx-ray diffraction shows a highly anisotropic compression of the hyperhoneycomb\\nlattice which affects the balance of bond-directional Ir-Ir exchange\\ninteractions driven by spin-orbit coupling at Ir sites. An enhancement of\\nsymmetric anisotropic exchange over Kitaev and Heisenberg exchange interactions\\nseen in theoretical calculations that use precisely this anisotropic Ir-Ir bond\\ncompression provides one possible route to realization of a QSL state in this\\nhyperhoneycomb iridate at high pressures.\\n',\n",
       " '  An infinite convergent sum of independent and identically distributed random\\nvariables discounted by a multiplicative random walk is called perpetuity,\\nbecause of a possible actuarial application. We give three disjoint groups of\\nsufficient conditions which ensure that the distribution right tail of a\\nperpetuity $\\\\mathbb{P}\\\\{X>x\\\\}$ is asymptotic to $ax^ce^{-bx}$ as $x\\\\to\\\\infty$\\nfor some $a,b>0$ and $c\\\\in\\\\mathbb{R}$. Our results complement those of Denisov\\nand Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we\\nprovide criteria for the finiteness of the one-sided exponential moments of\\nperpetuities. Several examples are given in which the distributions of\\nperpetuities are explicitly identified.\\n',\n",
       " '  We consider the problem of estimating from sample paths the absolute spectral\\ngap $\\\\gamma_*$ of a reversible, irreducible and aperiodic Markov chain\\n$(X_t)_{t \\\\in \\\\mathbb{N}}$ over a finite state $\\\\Omega$. We propose the ${\\\\tt\\nUCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a\\nlow-complexity algorithm which estimates the spectral gap in time ${\\\\cal O}(n)$\\nand memory space ${\\\\cal O}((\\\\ln n)^2)$ given $n$ samples. This is in stark\\ncontrast with most known methods which require at least memory space ${\\\\cal\\nO}(|\\\\Omega|)$, so that they cannot be applied to large state spaces.\\nFurthermore, ${\\\\tt UCPI}$ is amenable to parallel implementation.\\n',\n",
       " '  The state-of-the-art (SOTA) for mixed precision training is dominated by\\nvariants of low precision floating point operations, and in particular, FP16\\naccumulating into FP32 Micikevicius et al. (2017). On the other hand, while a\\nlot of research has also happened in the domain of low and mixed-precision\\nInteger training, these works either present results for non-SOTA networks (for\\ninstance only AlexNet for ImageNet-1K), or relatively small datasets (like\\nCIFAR-10). In this work, we train state-of-the-art visual understanding neural\\nnetworks on the ImageNet-1K dataset, with Integer operations on General Purpose\\n(GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate\\n(FMA) operations which take two pairs of INT16 operands and accumulate results\\ninto an INT32 output.We propose a shared exponent representation of tensors and\\ndevelop a Dynamic Fixed Point (DFP) scheme suitable for common neural network\\noperations. The nuances of developing an efficient integer convolution kernel\\nis examined, including methods to handle overflow of the INT32 accumulator. We\\nimplement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and\\nthese networks achieve or exceed SOTA accuracy within the same number of\\niterations as their FP32 counterparts without any change in hyper-parameters\\nand with a 1.8X improvement in end-to-end training throughput. To the best of\\nour knowledge these results represent the first INT16 training results on GP\\nhardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported\\naccuracy using half-precision\\n',\n",
       " '  The foreseen implementations of the Small Size Telescopes (SST) in CTA will\\nprovide unique insights into the highest energy gamma rays offering fundamental\\nmeans to discover and under- stand the sources populating the Galaxy and our\\nlocal neighborhood. Aiming at such a goal, the SST-1M is one of the three\\ndifferent implementations that are being prototyped and tested for CTA. SST-1M\\nis a Davies-Cotton single mirror telescope equipped with a unique camera\\ntechnology based on SiPMs with demonstrated advantages over classical\\nphotomultipliers in terms of duty-cycle. In this contribution, we describe the\\ntelescope components, the camera, and the trigger and readout system. The\\nresults of the commissioning of the camera using a dedicated test setup are\\nthen presented. The performances of the camera first prototype in terms of\\nexpected trigger rates and trigger efficiencies for different night-sky\\nbackground conditions are presented, and the camera response is compared to\\nend-to-end simulations.\\n',\n",
       " '  We obtain bounded for all $t$ solutions of ordinary differential equations as\\nlimits of the solutions of the corresponding Dirichlet problems on $(-L,L)$,\\nwith $L \\\\rightarrow \\\\infty$. We derive a priori estimates for the Dirichlet\\nproblems, allowing passage to the limit, via a diagonal sequence. This approach\\ncarries over to the PDE case.\\n',\n",
       " '  Waveforms of gravitational waves provide information about a variety of\\nparameters for the binary system merging. However, standard calculations have\\nbeen performed assuming a FLRW universe with no perturbations. In reality this\\nassumption should be dropped: we show that the inclusion of cosmological\\nperturbations translates into corrections to the estimate of astrophysical\\nparameters derived for the merging binary systems. We compute corrections to\\nthe estimate of the luminosity distance due to velocity, volume, lensing and\\ngravitational potential effects. Our results show that the amplitude of the\\ncorrections will be negligible for current instruments, mildly important for\\nexperiments like the planned DECIGO, and very important for future ones such as\\nthe Big Bang Observer.\\n',\n",
       " '  A simple DNA-based data storage scheme is demonstrated in which information\\nis written using \"addressing\" oligonucleotides. In contrast to other methods\\nthat allow arbitrary code to be stored, the resulting DNA is suitable for\\ndownstream enzymatic and biological processing. This capability is crucial for\\nDNA computers, and may allow for a diverse array of computational operations to\\nbe carried out using this DNA. Although here we use gel-based methods for\\ninformation readout, we also propose more advanced methods involving\\nprotein/DNA complexes and atomic force microscopy/nano-pore schemes for data\\nreadout.\\n',\n",
       " '  This paper presents VEC-NBT, a variation on the unsupervised graph clustering\\ntechnique VEC, which improves upon the performance of the original algorithm\\nsignificantly for sparse graphs. VEC employs a novel application of the\\nstate-of-the-art word2vec model to embed a graph in Euclidean space via random\\nwalks on the nodes of the graph. In VEC-NBT, we modify the original algorithm\\nto use a non-backtracking random walk instead of the normal backtracking random\\nwalk used in VEC. We introduce a modification to a non-backtracking random\\nwalk, which we call a begrudgingly-backtracking random walk, and show\\nempirically that using this model of random walks for VEC-NBT requires shorter\\nwalks on the graph to obtain results with comparable or greater accuracy than\\nVEC, especially for sparser graphs.\\n',\n",
       " '  We have used soft x-ray photoemission electron microscopy to image the\\nmagnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands\\narranged in geometrically frustrated configurations such as square ice and\\nkagome ice geometries. Upon thermal randomization, ensembles of nano-islands\\nwith strong inter-island magnetic coupling relax towards low-energy\\nconfigurations. Statistical analysis shows that the likelihood of ensembles\\nfalling into low-energy configurations depends strongly on the annealing\\ntemperature. Annealing to just below the Curie temperature of the ferromagnetic\\nfilm (T$_{C}$ = 338 K) allows for a much greater probability of achieving low\\nenergy configurations as compared to annealing above the Curie temperature. At\\nthis thermally active temperature of 325 K, the ensemble of ferromagnetic\\nnano-islands explore their energy landscape over time and eventually transition\\nto lower energy states as compared to the frozen-in configurations obtained\\nupon cooling from above the Curie temperature. Thus, this materials system\\nallows for a facile method to systematically study thermal evolution of\\nartificial spin ice arrays of nano-islands at temperatures modestly above room\\ntemperature.\\n',\n",
       " '  A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \\\\geq f(X\\n\\\\cup Y) + f(X \\\\cap Y)$ for any pair $X, Y \\\\subseteq V$. The symmetric\\ndifference transformation (SD-transformation) of $f$ by a canonical set $S\\n\\\\subseteq V$ is a set function $g$ given by $g(X) = f(X \\\\vartriangle S)$ for $X\\n\\\\subseteq V$,where $X \\\\vartriangle S = (X \\\\setminus S) \\\\cup (S \\\\setminus X)$\\ndenotes the symmetric difference between $X$ and $S$. Submodularity and\\nSD-transformations are regarded as the counterparts of convexity and affine\\ntransformations in a discrete space, respectively. However, submodularity is\\nnot preserved under SD-transformations, in contrast to the fact that convexity\\nis invariant under affine transformations. This paper presents a\\ncharacterization of SD-stransformations preserving submodularity. Then, we are\\nconcerned with the problem of discovering a canonical set $S$, given the\\nSD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$\\nis given by a function value oracle. A submodular function $f$ on $V$ is said\\nto be strict if $f(X) + f(Y) > f(X \\\\cup Y) + f(X \\\\cap Y)$ holds whenever both\\n$X \\\\setminus Y$ and $Y \\\\setminus X$ are nonempty. We show that the problem is\\nsolved by using ${\\\\rm O}(|V|)$ oracle calls when $f$ is strictly submodular,\\nalthough it requires exponentially many oracle calls in general.\\n',\n",
       " '  Quick Shift is a popular mode-seeking and clustering algorithm. We present\\nfinite sample statistical consistency guarantees for Quick Shift on mode and\\ncluster recovery under mild distributional assumptions. We then apply our\\nresults to construct a consistent modal regression algorithm.\\n',\n",
       " '  In recent years Deep Neural Networks (DNNs) have been rapidly developed in\\nvarious applications, together with increasingly complex architectures. The\\nperformance gain of these DNNs generally comes with high computational costs\\nand large memory consumption, which may not be affordable for mobile platforms.\\nDeep model quantization can be used for reducing the computation and memory\\ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we\\npropose an optimization framework for deep model quantization. First, we\\npropose a measurement to estimate the effect of parameter quantization errors\\nin individual layers on the overall model prediction accuracy. Then, we propose\\nan optimization process based on this measurement for finding optimal\\nquantization bit-width for each layer. This is the first work that\\ntheoretically analyse the relationship between parameter quantization errors of\\nindividual layers and model accuracy. Our new quantization algorithm\\noutperforms previous quantization optimization methods, and achieves 20-40%\\nhigher compression rate compared to equal bit-width quantization at the same\\nmodel prediction accuracy.\\n',\n",
       " '  Convolutional Neural Networks (CNNs) have been recently introduced in the\\ndomain of session-based next item recommendation. An ordered collection of past\\nitems the user has interacted with in a session (or sequence) are embedded into\\na 2-dimensional latent matrix, and treated as an image. The convolution and\\npooling operations are then applied to the mapped item embeddings. In this\\npaper, we first examine the typical session-based CNN recommender and show that\\nboth the generative model and network architecture are suboptimal when modeling\\nlong-range dependencies in the item sequence. To address the issues, we\\nintroduce a simple, but very effective generative model that is capable of\\nlearning high-level representation from both short- and long-range item\\ndependencies. The network architecture of the proposed model is formed of a\\nstack of \\\\emph{holed} convolutional layers, which can efficiently increase the\\nreceptive fields without relying on the pooling operation. Another contribution\\nis the effective use of residual block structure in recommender systems, which\\ncan ease the optimization for much deeper networks. The proposed generative\\nmodel attains state-of-the-art accuracy with less training time in the next\\nitem recommendation task. It accordingly can be used as a powerful\\nrecommendation baseline to beat in future, especially when there are long\\nsequences of user feedback.\\n',\n",
       " '  We present a terahertz spectroscopic study of polar ferrimagnet\\nFeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode\\neffect, or nonreciprocal directional dichroism, where the transmitted light\\nintensity in one direction is over 100 times lower than intensity transmitted\\nin the opposite direction. The effect takes place in the paramagnetic phase\\nwith no long-range magnetic order in the crystal, which contrasts sharply with\\nall existing reports of the terahertz optical diode effect in other\\nmagnetoelectric materials, where the long-range magnetic ordering is a\\nnecessary prerequisite. In \\\\fzmo, the effect occurs resonantly with a strong\\nmagnetic dipole active transition centered at 1.27 THz and assigned as electron\\nspin resonance between the eigenstates of the single-ion anisotropy\\nHamiltonian. We propose that the optical diode effect in paramagnetic\\nFeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.\\n',\n",
       " \"  We compare the social character networks of biographical, legendary and\\nfictional texts, in search for marks of genre differentiation. We examine the\\ndegree distribution of character appearance and find a power law that does not\\ndepend on the literary genre or historical content. We also analyze local and\\nglobal complex networks measures, in particular, correlation plots between the\\nrecently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and\\nCloseness centralities. Assortativity plots, which previous literature claims\\nto separate fictional from real social networks, were also studied. We've found\\nno relevant differences in the books for these network measures and we give a\\nplausible explanation why the previous assortativity result is not correct.\\n\",\n",
       " '  Motivated by the recent experimental realization of the Haldane model by\\nultracold fermions in an optical lattice, we investigate phase diagrams of the\\nhard-core Bose-Hubbard model on a honeycomb lattice. This model is closely\\nrelated with a spin-1/2 antiferromagnetic (AF) quantum spin model.\\nNearest-neighbor (NN) hopping amplitude is positive and it prefers an AF\\nconfigurations of phases of Bose-Einstein condensates. On the other hand, an\\namplitude of the next-NN hopping depends on an angle variable as in the Haldane\\nmodel. Phase diagrams are obtained by means of an extended path-integral\\nMonte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there\\nappear other phases including a Bose metal in which no long-range orders exist.\\n',\n",
       " \"  Exoplanet host star activity, in the form of unocculted star spots or\\nfaculae, alters the observed transmission and emission spectra of the\\nexoplanet. This effect can be exacerbated when combining data from different\\nepochs if the stellar photosphere varies between observations due to activity.\\nredHere we present a method to characterize and correct for relative changes\\ndue to stellar activity by exploiting multi-epoch ($\\\\ge$2 visits/transits)\\nobservations to place them in a consistent reference frame. Using measurements\\nfrom portions of the planet's orbit where negligible planet transmission or\\nemission can be assumed, we determine changes to the stellar spectral\\namplitude. With the analytical methods described here, we predict the impact of\\nstellar variability on transit observations. Supplementing these forecasts with\\nKepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and\\npredicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude\\nthat stellar activity does not impact infrared transiting exoplanet\\nobservations of most presently-known or predicted TESS targets by current or\\nnear-future platforms, such as JWST.\\n\",\n",
       " '  Developers of Molecular Dynamics (MD) codes face significant challenges when\\nadapting existing simulation packages to new hardware. In a continuously\\ndiversifying hardware landscape it becomes increasingly difficult for\\nscientists to be experts both in their own domain (physics/chemistry/biology)\\nand specialists in the low level parallelisation and optimisation of their\\ncodes. To address this challenge, we describe a \"Separation of Concerns\"\\napproach for the development of parallel and optimised MD codes: the science\\nspecialist writes code at a high abstraction level in a domain specific\\nlanguage (DSL), which is then translated into efficient computer code by a\\nscientific programmer. In a related context, an abstraction for the solution of\\npartial differential equations with grid based methods has recently been\\nimplemented in the (Py)OP2 library. Inspired by this approach, we develop a\\nPython code generation system for molecular dynamics simulations on different\\nparallel architectures, including massively parallel distributed memory systems\\nand GPUs. We demonstrate the efficiency of the auto-generated code by studying\\nits performance and scalability on different hardware and compare it to other\\nstate-of-the-art simulation packages. With growing data volumes the extraction\\nof physically meaningful information from the simulation becomes increasingly\\nchallenging and requires equally efficient implementations. A particular\\nadvantage of our approach is the easy expression of such analysis algorithms.\\nWe consider two popular methods for deducing the crystalline structure of a\\nmaterial from the local environment of each atom, show how they can be\\nexpressed in our abstraction and implement them in the code generation\\nframework.\\n',\n",
       " '  We obtain the non-linear generalization of the Sachs-Wolfe + integrated\\nSachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our\\nformula is valid at all orders in perturbation theory, is also valid in all\\ngauges and includes scalar, vector and tensor modes. A direct consequence of\\nour results is that the maps of the logarithmic temperature anisotropies are\\nmuch cleaner than the usual CMB maps, because they automatically remove many\\nsecondary anisotropies. This can for instance, facilitate the search for\\nprimordial non-Gaussianity in future works. It also disentangles the non-linear\\nISW from other effects. Finally, we provide a method which can iteratively be\\nused to obtain the lensing solution at the desired order.\\n',\n",
       " '  We prove sharp decoupling inequalities for a class of two dimensional\\nnon-degenerate surfaces in R^5, introduced by Prendiville. As a consequence, we\\nobtain sharp bounds on the number of integer solutions of the Diophantine\\nsystems associated with these surfaces.\\n',\n",
       " '  In this paper, we introduce a simple, yet powerful pipeline for medical image\\nsegmentation that combines Fully Convolutional Networks (FCNs) with Fully\\nConvolutional Residual Networks (FC-ResNets). We propose and examine a design\\nthat takes particular advantage of recent advances in the understanding of both\\nConvolutional Neural Networks as well as ResNets. Our approach focuses upon the\\nimportance of a trainable pre-processing when using FC-ResNets and we show that\\na low-capacity FCN model can serve as a pre-processor to normalize medical\\ninput data. In our image segmentation pipeline, we use FCNs to obtain\\nnormalized images, which are then iteratively refined by means of a FC-ResNet\\nto generate a segmentation prediction. As in other fully convolutional\\napproaches, our pipeline can be used off-the-shelf on different image\\nmodalities. We show that using this pipeline, we exhibit state-of-the-art\\nperformance on the challenging Electron Microscopy benchmark, when compared to\\nother 2D methods. We improve segmentation results on CT images of liver\\nlesions, when contrasting with standard FCN methods. Moreover, when applying\\nour 2D pipeline on a challenging 3D MRI prostate segmentation challenge we\\nreach results that are competitive even when compared to 3D methods. The\\nobtained results illustrate the strong potential and versatility of the\\npipeline by achieving highly accurate results on multi-modality images from\\ndifferent anatomical regions and organs.\\n',\n",
       " \"  Our purpose is to focus attention on a new criterion for quantum schemes by\\nbringing together the notions of quantum game and game isomorphism. A quantum\\ngame scheme is required to generate the classical game as a special case. Now,\\ngiven a quantum game scheme and two isomorphic classical games, we additionally\\nrequire the resulting quantum games to be isomorphic as well. We show how this\\nisomorphism condition influences the players' strategy sets. We are concerned\\nwith the Marinatto-Weber type quantum game scheme and the strong isomorphism\\nbetween games in strategic form.\\n\",\n",
       " '  Stochastic variance reduction algorithms have recently become popular for\\nminimizing the average of a large but finite number of loss functions. In this\\npaper, we propose a novel Riemannian extension of the Euclidean stochastic\\nvariance reduced gradient algorithm (R-SVRG) to a manifold search space. The\\nkey challenges of averaging, adding, and subtracting multiple gradients are\\naddressed with retraction and vector transport. We present a global convergence\\nanalysis of the proposed algorithm with a decay step size and a local\\nconvergence rate analysis under a fixed step size under some natural\\nassumptions. The proposed algorithm is applied to problems on the Grassmann\\nmanifold, such as principal component analysis, low-rank matrix completion, and\\ncomputation of the Karcher mean of subspaces, and outperforms the standard\\nRiemannian stochastic gradient descent algorithm in each case.\\n',\n",
       " '  The algorithmic Markov condition states that the most likely causal direction\\nbetween two random variables X and Y can be identified as that direction with\\nthe lowest Kolmogorov complexity. Due to the halting problem, however, this\\nnotion is not computable.\\nWe hence propose to do causal inference by stochastic complexity. That is, we\\npropose to approximate Kolmogorov complexity via the Minimum Description Length\\n(MDL) principle, using a score that is mini-max optimal with regard to the\\nmodel class under consideration. This means that even in an adversarial\\nsetting, such as when the true distribution is not in this class, we still\\nobtain the optimal encoding for the data relative to the class.\\nWe instantiate this framework, which we call CISC, for pairs of univariate\\ndiscrete variables, using the class of multinomial distributions. Experiments\\nshow that CISC is highly accurate on synthetic, benchmark, as well as\\nreal-world data, outperforming the state of the art by a margin, and scales\\nextremely well with regard to sample and domain sizes.\\n',\n",
       " '  The recently proposed Temporal Ensembling has achieved state-of-the-art\\nresults in several semi-supervised learning benchmarks. It maintains an\\nexponential moving average of label predictions on each training example, and\\npenalizes predictions that are inconsistent with this target. However, because\\nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy\\nwhen learning large datasets. To overcome this problem, we propose Mean\\nTeacher, a method that averages model weights instead of label predictions. As\\nan additional benefit, Mean Teacher improves test accuracy and enables training\\nwith fewer labels than Temporal Ensembling. Without changing the network\\narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250\\nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also\\nshow that a good network architecture is crucial to performance. Combining Mean\\nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with\\n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels\\nfrom 35.24% to 9.11%.\\n',\n",
       " '  We present a new code for astrophysical magneto-hydrodynamics specifically\\ndesigned and optimized for high performance and scaling on modern and future\\nsupercomputers. We describe a novel hybrid OpenMP/MPI programming model that\\nemerged from a collaboration between Cray, Inc. and the University of\\nMinnesota. This design utilizes MPI-RMA optimized for thread scaling, which\\nallows the code to run extremely efficiently at very high thread counts ideal\\nfor the latest generation of the multi-core and many-core architectures. Such\\nperformance characteristics are needed in the era of \"exascale\" computing. We\\ndescribe and demonstrate our high-performance design in detail with the intent\\nthat it may be used as a model for other, future astrophysical codes intended\\nfor applications demanding exceptional performance.\\n',\n",
       " '  Virtual Network Functions as a Service (VNFaaS) is currently under attentive\\nstudy by telecommunications and cloud stakeholders as a promising business and\\ntechnical direction consisting of providing network functions as a service on a\\ncloud (NFV Infrastructure), instead of delivering standalone network\\nappliances, in order to provide higher scalability and reduce maintenance\\ncosts. However, the functioning of such NFVI hosting the VNFs is fundamental\\nfor all the services and applications running on top of it, forcing to\\nguarantee a high availability level. Indeed the availability of an VNFaaS\\nrelies on the failure rate of its single components, namely the servers, the\\nvirtualization software, and the communication network. The proper assignment\\nof the virtual machines implementing network functions to NFVI servers and\\ntheir protection is essential to guarantee high availability. We model the High\\nAvailability Virtual Network Function Placement (HA-VNFP) as the problem of\\nfinding the best assignment of virtual machines to servers guaranteeing\\nprotection by replication. We propose a probabilistic approach to measure the\\nreal availability of a system and design both efficient and effective\\nalgorithms that can be used by stakeholders for both online and offline\\nplanning.\\n',\n",
       " \"  The Zika virus has been found in individual cases but has not been confirmed\\nas the cause of in the large number of cases of microcephaly in Brazil in\\n2015-6. Indeed, disparities between the incidence of Zika and microcephaly\\nacross geographic locations has led to questions about the virus's role. Here\\nwe consider whether the insecticide pyriproxyfen used in Brazilian drinking\\nwater might be the primary cause or a cofactor. Pyriproxifen is a juvenile\\nhormone analog which has been shown to correspond in mammals to a number of fat\\nsoluble regulatory molecules including retinoic acid, a metabolite of vitamin\\nA, with which it has cross-reactivity and whose application during development\\nhas been shown to cause microcephaly. Methoprene, another juvenile hormone\\nanalog approved as an insecticide in the 1970s has been shown to cause\\ndevelopmental disorders in mammals. Isotretinoin is another retinoid causing\\nmicrocephaly via activation of the retinoid X receptor in developing fetuses.\\nWe review tests of pyriproxyfen by the manufacturer Sumitomo, which actually\\nfound some evidence for this effect, including low brain mass and\\narhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is\\nunprecedented, never having been applied to a water supply on a large scale.\\nClaims that its geographical pattern of use rule it out as a cause have not\\nbeen documented or confirmed. On the other hand, the very few microcephaly\\ncases reported in Colombia and the wide discrepancies of incidence in different\\nstates across Brazil despite large numbers of Zika cases undermine the claim\\nthat Zika is the cause. Given this combination of potential molecular\\nmechanism, toxicological and epidemiological evidence we strongly recommend\\nthat the use of pyriproxyfen in Brazil be suspended until the potential causal\\nlink to microcephaly is investigated further.\\n\",\n",
       " '  This paper considers the problem of decentralized optimization with a\\ncomposite objective containing smooth and non-smooth terms. To solve the\\nproblem, a proximal-gradient scheme is studied. Specifically, the smooth and\\nnonsmooth terms are dealt with by gradient update and proximal update,\\nrespectively. The studied algorithm is closely related to a previous\\ndecentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.\\nFirst of all, in our new scheme, agents use uncoordinated step-sizes and the\\nstable upper bounds on step-sizes are independent from network topology. The\\nstep-sizes depend on local objective functions, and they can be as large as\\nthat of the gradient descent. Secondly, for the special case without non-smooth\\nterms, linear convergence can be achieved under the strong convexity\\nassumption. The dependence of the convergence rate on the objective functions\\nand the network are separated, and the convergence rate of our new scheme is as\\ngood as one of the two convergence rates that match the typical rates for the\\ngeneral gradient descent and the consensus averaging. We also provide some\\nnumerical experiments to demonstrate the efficacy of the introduced algorithms\\nand validate our theoretical discoveries.\\n',\n",
       " \"  Recent studies have demonstrated that near-data processing (NDP) is an\\neffective technique for improving performance and energy efficiency of\\ndata-intensive workloads. However, leveraging NDP in realistic systems with\\nmultiple memory modules introduces a new challenge. In today's systems, where\\nno computation occurs in memory modules, the physical address space is\\ninterleaved at a fine granularity among all memory modules to help improve the\\nutilization of processor-memory interfaces by distributing the memory traffic.\\nHowever, this is at odds with efficient use of NDP, which requires careful\\nplacement of data in memory modules such that near-data computations and their\\nexclusively used data can be localized in individual memory modules, while\\ndistributing shared data among memory modules to reduce hotspots. In order to\\naddress this new challenge, we propose a set of techniques that (1) enable\\ncollections of OS pages to either be fine-grain interleaved among memory\\nmodules (as is done today) or to be placed contiguously on individual memory\\nmodules (as is desirable for NDP private data), and (2) decide whether to\\nlocalize or distribute each memory object based on its anticipated access\\npattern and steer computations to the memory where the data they access is\\nlocated. Our evaluations across a wide range of workloads show that the\\nproposed mechanism improves performance by 31% and reduces 38% remote data\\naccesses over a baseline system that cannot exploit computate-data affinity\\ncharacteristics.\\n\",\n",
       " \"  Historically, machine learning in computer security has prioritized defense:\\nthink intrusion detection systems, malware classification, and botnet traffic\\nidentification. Offense can benefit from data just as well. Social networks,\\nwith their access to extensive personal data, bot-friendly APIs, colloquial\\nsyntax, and prevalence of shortened links, are the perfect venues for spreading\\nmachine-generated malicious content. We aim to discover what capabilities an\\nadversary might utilize in such a domain. We present a long short-term memory\\n(LSTM) neural network that learns to socially engineer specific users into\\nclicking on deceptive URLs. The model is trained with word vector\\nrepresentations of social media posts, and in order to make a click-through\\nmore likely, it is dynamically seeded with topics extracted from the target's\\ntimeline. We augment the model with clustering to triage high value targets\\nbased on their level of social engagement, and measure success of the LSTM's\\nphishing expedition using click-rates of IP-tracked links. We achieve state of\\nthe art success rates, tripling those of historic email attack campaigns, and\\noutperform humans manually performing the same task.\\n\",\n",
       " '  Deep learning has demonstrated tremendous potential for Automatic Text\\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\\nenhances vanilla neural network models with auxiliary neural coherence\\nfeatures. Our new method proposes a new \\\\textsc{SkipFlow} mechanism that models\\nrelationships between snapshots of the hidden representations of a long\\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\\nrelationships between multiple snapshots are used as auxiliary features for\\nprediction. This has two main benefits. Firstly, essays are typically long\\nsequences and therefore the memorization capability of the LSTM network may be\\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\\nby acting as a protection against vanishing gradients. The parameters of the\\n\\\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\\nmodeling relationships between multiple positions allows our model to learn\\nfeatures that represent and approximate textual coherence. In our model, we\\ncall this \\\\textit{neural coherence} features. Overall, we present a unified\\ndeep learning architecture that generates neural coherence features as it reads\\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\\nperformance on the benchmark ASAP dataset, outperforming not only feature\\nengineering baselines but also other deep learning models.\\n',\n",
       " '  This paper introduces a method, based on deep reinforcement learning, for\\nautomatically generating a general purpose decision making function. A Deep\\nQ-Network agent was trained in a simulated environment to handle speed and lane\\nchange decisions for a truck-trailer combination. In a highway driving case, it\\nis shown that the method produced an agent that matched or surpassed the\\nperformance of a commonly used reference model. To demonstrate the generality\\nof the method, the exact same algorithm was also tested by training it for an\\novertaking case on a road with oncoming traffic. Furthermore, a novel way of\\napplying a convolutional neural network to high level input that represents\\ninterchangeable objects is also introduced.\\n',\n",
       " '  One of the most challenging problems in correlated topological systems is a\\nrealization of the reduction of topological classification, but very few\\nexperimental platforms have been proposed so far. We here demonstrate that\\nultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded\\nin an optical lattice of two-leg ladder geometry can be the first promising\\ntestbed for the reduction $\\\\mathbb{Z}\\\\to\\\\mathbb{Z}_4$, where solid evidence for\\nthe reduction is available thanks to their high controllability. We further\\ngive a detailed account of how to experimentally access this phenomenon; around\\nthe edges, the destruction of one-particle gapless excitations can be observed\\nby the local radio frequency spectroscopy, while that of gapless spin\\nexcitations can be observed by a time-dependent spin expectation value of a\\nsuperposed state of the ground state and the first excited state. We clarify\\nthat even when the reduction occurs, a gapless edge mode is recovered around a\\ndislocation, which can be another piece of evidence for the reduction.\\n',\n",
       " '  Cosmological parameter constraints from observations of time-delay lenses are\\nbecoming increasingly precise. However, there may be significant bias and\\nscatter in these measurements due to, among other things, the so-called\\nmass-sheet degeneracy. To estimate these uncertainties, we analyze strong\\nlenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet\\ntransformation to the radial density profiles of lenses, and by selecting\\nlenses near isothermality, we find that the bias on H0 can be reduced to 5%\\nwith an intrinsic scatter of 10%, confirming previous results performed on a\\ndifferent simulation data set. We further investigate whether combining lensing\\nobservables with kinematic constraints helps to minimize this bias. We do not\\ndetect any significant dependence of the bias on lens model parameters or\\nobservational properties of the galaxy, but depending on the source--lens\\nconfiguration, a bias may still exist. Cross lenses provide an accurate\\nestimate of the Hubble constant, while fold (double) lenses tend to be biased\\nlow (high). With kinematic constraints, double lenses show bias and intrinsic\\nscatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic\\nscatter of 0.5% and 10%, respectively. For lenses with a reduced $\\\\chi^2 > 1$,\\na power-law dependence of the $\\\\chi^2$ on the lens environment (number of\\nnearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two\\ndouble lenses that are significantly biased. We are able to remove the bias,\\nsuggesting that the remaining biases could also be reduced by carefully taking\\ninto account additional sources of systematic uncertainty.\\n',\n",
       " '  We characterize the response of the quiet time (no substorms or storms)\\nlarge-scale ionospheric transient equivalent currents to north-south and\\nsouth-north IMF turnings by using a dynamical network of ground-based\\nmagnetometers. Canonical correlation between all pairs of SuperMAG magnetometer\\nstations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\\\\circ}$)\\nis used to establish the extent of near-simultaneous magnetic response between\\nregions of magnetic local time-MLAT. Parameters and maps that describe\\nspatial-temporal correlation are used to characterize the system and its\\nresponse to the turnings aggregated over several hundred events. We find that\\nregions that experience large increases in correlation post turning coincide\\nwith typical locations of a two-cell convection system and are influenced by\\nthe interplanetary magnetic field $\\\\mathit{B}_{y}$. The time between the\\nturnings reaching the magnetopause and a network response is found to be\\n$\\\\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the\\nnightside.\\n',\n",
       " '  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th\\nsyzygy of a module of finite lengh is greater than or equal to the rank of the\\nd-th syzygy of the residue field, unless the module has finite projective\\ndimension. Assuming that R is Gorenstein, we prove that if the question is\\naffrmative, then R is a hypersurface. If moreover R has dimension two, then we\\nshow that the converse also holds true.\\n',\n",
       " '  Learning to make decisions from observed data in dynamic environments remains\\na problem of fundamental importance in a number of fields, from artificial\\nintelligence and robotics, to medicine and finance. This paper concerns the\\nproblem of learning control policies for unknown linear dynamical systems so as\\nto maximize a quadratic reward function. We present a method to optimize the\\nexpected value of the reward over the posterior distribution of the unknown\\nsystem parameters, given data. The algorithm involves sequential convex\\nprograming, and enjoys reliable local convergence and robust stability\\nguarantees. Numerical simulations and stabilization of a real-world inverted\\npendulum are used to demonstrate the approach, with strong performance and\\nrobustness properties observed in both.\\n',\n",
       " '  Based on the KP hierarchy reduction method, the general bright-dark mixed\\nmulti-soliton solution of the multi-component Maccari system is constructed.\\nThe multi-component Maccari system considered comprised of multiple (say $M$)\\nshort-wave components and one long-wave component with all possible\\ncombinations of nonlinearities including all-focusing, all-defocusing and mixed\\ntypes. We firstly derive the two-bright-one-dark (2-b-1-d) and\\none-bright-two-dark (1-b-2-d) mixed multi-soliton solutions to the\\nthree-component Maccari system in detail. For the interaction between two\\nsolitons, the asymptotic analysis shows that inelastic collision can take place\\nin a $M$-component Maccari system with $M \\\\geq 3$ only if the bright parts of\\nthe mixed solitons appear at least in two short-wave components. The\\nenergy-exchanging inelastic collision characterized by an intensity\\nredistribution among the bright parts of the mixed solitons. While the dark\\nparts of the mixed solitons and the solitons in the long-wave component always\\nundergo elastic collision which just accompanied by a position shift. In the\\nend, we extend the corresponding analysis to the $M$-component Maccari system\\nto obtain its mixed multi-soliton solution. The formula obtained unifies the\\nall-bright, all-dark and mixed multi-soliton solutions.\\n',\n",
       " \"  Deep learning has been successfully applied to various tasks, but its\\nunderlying mechanism remains unclear. Neural networks associate similar inputs\\nin the visible layer to the same state of hidden variables in deep layers. The\\nfraction of inputs that are associated to the same state is a natural measure\\nof similarity and is simply related to the cost in bits required to represent\\nthese inputs. The degeneracy of states with the same information cost provides\\ninstead a natural measure of noise and is simply related the entropy of the\\nfrequency of states, that we call relevance. Representations with minimal\\nnoise, at a given level of similarity (resolution), are those that maximise the\\nrelevance. A signature of such efficient representations is that frequency\\ndistributions follow power laws. We show, in extensive numerical experiments,\\nthat deep neural networks extract a hierarchy of efficient representations from\\ndata, because they i) achieve low levels of noise (i.e. high relevance) and ii)\\nexhibit power law distributions. We also find that the layer that is most\\nefficient to reliably generate patterns of training data is the one for which\\nrelevance and resolution are traded at the same price, which implies that\\nfrequency distribution follows Zipf's law.\\n\",\n",
       " '  The modular Gromov-Hausdorff propinquity is a distance on classes of modules\\nendowed with quantum metric information, in the form of a metric form of a\\nconnection and a left Hilbert module structure. This paper proves that the\\nfamily of Heisenberg modules over quantum two tori, when endowed with their\\ncanonical connections, form a family of metrized quantum vector bundles, as a\\nfirst step in proving that Heisenberg modules form a continuous family for the\\nmodular Gromov-Hausdorff propinquity.\\n',\n",
       " '  We present a prototype of a software tool for exploration of multiple\\ncombinatorial optimisation problems in large real-world and synthetic complex\\nnetworks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial\\nExplorer), provides a unified framework for scalable computation and\\npresentation of high-quality suboptimal solutions and bounds for a number of\\nwidely studied combinatorial optimisation problems. Efficient representation\\nand applicability to large-scale graphs and complex networks are particularly\\nconsidered in its design. The problems currently supported include maximum\\nclique, graph colouring, maximum independent set, minimum vertex clique\\ncovering, minimum dominating set, as well as the longest simple cycle problem.\\nSuboptimal solutions and intervals for optimal objective values are estimated\\nusing scalable heuristics. The tool is designed with extensibility in mind,\\nwith the view of further problems and both new fast and high-performance\\nheuristics to be added in the future. GraphCombEx has already been successfully\\nused as a support tool in a number of recent research studies using\\ncombinatorial optimisation to analyse complex networks, indicating its promise\\nas a research software tool.\\n',\n",
       " '  Word sense disambiguation (WSD) improves many Natural Language Processing\\n(NLP) applications such as Information Retrieval, Machine Translation or\\nLexical Simplification. WSD is the ability of determining a word sense among\\ndifferent ones within a polysemic lexical unit taking into account the context.\\nThe most straightforward approach uses a semantic proximity measure between the\\nword sense candidates of the target word and those of its context. Such a\\nmethod very easily entails a combinatorial explosion. In this paper, we propose\\ntwo methods based on distributional analysis which enable to reduce the\\nexponential complexity without losing the coherence. We present a comparison\\nbetween the selection of distributional neighbors and the linearly nearest\\nneighbors. The figures obtained show that selecting distributional neighbors\\nleads to better results.\\n',\n",
       " '  We develop a theory for non-degenerate parametric resonance in a tunable\\nsuperconducting cavity. We focus on nonlinear effects that are caused by\\nnonlinear Josephson elements connected to the cavity. We analyze parametric\\namplification in a strong nonlinear regime at the parametric instability\\nthreshold, and calculate maximum gain values. Above the threshold, in the\\nparametric oscillator regime the linear cavity response diverges at the\\noscillator frequency at all pump strengths. We show that this divergence is\\nrelated to the continuous degeneracy of the free oscillator state with respect\\nto the phase. Applying on-resonance input lifts the degeneracy and removes the\\ndivergence. We also investigate the quantum noise squeezing. It is shown that\\nin the strong amplification regime the noise undergoes four-mode squeezing, and\\nthat in this regime the output signal to noise ratio can significantly exceed\\nthe input value. We also analyze the intermode frequency conversion and\\nidentify parameters at which full conversion is achieved.\\n',\n",
       " '  Generic generation and manipulation of text is challenging and has limited\\nsuccess compared to recent deep generative modeling in visual domain. This\\npaper aims at generating plausible natural language sentences, whose attributes\\nare dynamically controlled by learning disentangled latent representations with\\ndesignated semantics. We propose a new neural generative model which combines\\nvariational auto-encoders and holistic attribute discriminators for effective\\nimposition of semantic structures. With differentiable approximation to\\ndiscrete text samples, explicit constraints on independent attribute controls,\\nand efficient collaborative learning of generator and discriminators, our model\\nlearns highly interpretable representations from even only word annotations,\\nand produces realistic sentences with desired attributes. Quantitative\\nevaluation validates the accuracy of sentence and attribute generation.\\n',\n",
       " '  Our eyes sample a disproportionately large amount of information at the\\ncentre of gaze with increasingly sparse sampling into the periphery. This\\nsampling scheme is widely believed to be a wiring constraint whereby high\\nresolution at the centre is achieved by sacrificing spatial acuity in the\\nperiphery. Here we propose that this sampling scheme may be optimal for object\\nrecognition because the relevant spatial content is dense near an object and\\nsparse in the surrounding vicinity. We tested this hypothesis by training deep\\nconvolutional neural networks on full-resolution and foveated images. Our main\\nfinding is that networks trained on images with foveated sampling show better\\nobject classification compared to networks trained on full resolution images.\\nImportantly, blurring images according to the human blur function yielded the\\nbest performance compared to images with shallower or steeper blurring. Taken\\ntogether our results suggest that, peripheral blurring in our eyes may have\\nevolved for optimal object recognition, rather than merely to satisfy wiring\\nconstraints.\\n',\n",
       " '  Molecular dynamics simulates the~movements of atoms. Due to its high cost,\\nmany methods have been developed to \"push the~simulation forward\". One of them,\\nmetadynamics, can hasten the~molecular dynamics with the~help of variables\\ndescribing the~simulated process. However, the~evaluation of these variables\\ncan include numerous mean square distance calculations that introduce\\nsubstantial computational demands, thus jeopardize the~benefit of the~approach.\\nRecently, we proposed an~approximative method that significantly reduces\\nthe~number of these distance calculations. Here we evaluate the~performance and\\nthe~scalability on two molecular systems. We assess the~maximal theoretical\\nspeed-up based on the reduction of distance computations and Ahmdal\\'s law and\\ncompare it to the~practical speed-up achieved with our implementation.\\n',\n",
       " '  Developers increasingly rely on text matching tools to analyze the relation\\nbetween natural language words and APIs. However, semantic gaps, namely textual\\nmismatches between words and APIs, negatively affect these tools. Previous\\nstudies have transformed words or APIs into low-dimensional vectors for\\nmatching; however, inaccurate results were obtained due to the failure of\\nmodeling words and APIs simultaneously. To resolve this problem, two main\\nchallenges are to be addressed: the acquisition of massive words and APIs for\\nmining and the alignment of words and APIs for modeling. Therefore, this study\\nproposes Word2API to effectively estimate relatedness of words and APIs.\\nWord2API collects millions of commonly used words and APIs from code\\nrepositories to address the acquisition challenge. Then, a shuffling strategy\\nis used to transform related words and APIs into tuples to address the\\nalignment challenge. Using these tuples, Word2API models words and APIs\\nsimultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness\\nestimation in terms of precision and NDCG. Word2API is also effective on\\nsolving typical software tasks, e.g., query expansion and API documents\\nlinking. A simple system with Word2API-expanded queries recommends up to 21.4%\\nmore related APIs for developers. Meanwhile, Word2API improves comparison\\nalgorithms by 7.9%-17.4% in linking questions in Question&Answer communities to\\nAPI documents.\\n',\n",
       " '  We prove the existence of an optimal feedback controller for a stochastic\\noptimization problem constituted by a variation of the Heston model, where a\\nstochastic input process is added in order to minimize a given performance\\ncriterion. The stochastic feedback controller is searched by solving a\\nnonlinear backward parabolic equation for which one proves the existence of a\\nmartingale solution.\\n',\n",
       " \"  How can we enable novice users to create effective task plans for\\ncollaborative robots? Must there be a tradeoff between generalizability and\\nease of use? To answer these questions, we conducted a user study with the\\nCoSTAR system, which integrates perception and reasoning into a Behavior\\nTree-based task plan editor. In our study, we ask novice users to perform\\nsimple pick-and-place assembly tasks under varying perception and planning\\ncapabilities. Our study shows that users found Behavior Trees to be an\\neffective way of specifying task plans. Furthermore, users were also able to\\nmore quickly, effectively, and generally author task plans with the addition of\\nCoSTAR's planning, perception, and reasoning capabilities. Despite these\\nimprovements, concepts associated with these capabilities were rated by users\\nas less usable, and our results suggest a direction for further refinement.\\n\",\n",
       " \"  The (torsion) complexity of a finite edge-weighted graph is defined to be the\\norder of the torsion subgroup of the abelian group presented by its Laplacian\\nmatrix. When G is d-periodic (i.e., G has a free action of the rank-d free\\nabelian group by graph automorphisms, with finite quotient) the Mahler measure\\nof its Laplacian determinant polynomial is the growth rate of the complexity of\\nfinite quotients of G. Lehmer's question, an open question about the roots of\\nmonic integral polynomials, is equivalent to a question about the complexity\\ngrowth of edge-weighted 1-periodic graphs.\\n\",\n",
       " '  Neutronic performance is investigated for a potential accident tolerant fuel\\n(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison\\nwith current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a\\nlarger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher\\nthermal conductivity and a higher uranium density, which can compensate the\\nreactivity suppressed by FeCrAl. Based on neutronic investigations, a possible\\nU$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental\\nproperties of the suggested fuel-cladding combination are investigated in a\\nfuel assembly.These properties include moderator and fuel temperature\\ncoefficients, control rods worth, radial power distribution (in a fuel rod),\\nand different void reactivity coefficients. The present work proves that the\\nnew combination has less reactivity variation during its service lifetime.\\nAlthough, compared with the current system, it has a little larger deviation on\\npower distribution and a little less negative temperature coefficient and void\\nreactivity coefficient and its control rods worth is less important, variations\\nof these parameters are less important during the service lifetime of fuel.\\nHence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic\\nview.\\n',\n",
       " '  We prove that the only entrywise transforms of rectangular matrices which\\npreserve total positivity or total non-negativity are either constant or\\nlinear. This follows from an extended classification of preservers of these two\\nproperties for matrices of fixed dimension. We also prove that the same\\nassertions hold upon working only with symmetric matrices; for total-positivity\\npreservers our proofs proceed through solving two totally positive completion\\nproblems.\\n',\n",
       " '  The most popular and widely used subtract-with-borrow generator, also known\\nas RANLUX, is reimplemented as a linear congruential generator using large\\ninteger arithmetic with the modulus size of 576 bits. Modern computers, as well\\nas the specific structure of the modulus inferred from RANLUX, allow for the\\ndevelopment of a fast modular multiplication -- the core of the procedure. This\\nwas previously believed to be slow and have too high cost in terms of computing\\nresources. Our tests show a significant gain in generation speed which is\\ncomparable with other fast, high quality random number generators. An\\nadditional feature is the fast skipping of generator states leading to a\\nseeding scheme which guarantees the uniqueness of random number sequences.\\n',\n",
       " \"  We consider the refined topological vertex of Iqbal et al, as a function of\\ntwo parameters (x, y), and deform it by introducing Macdonald parameters (q,\\nt), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald\\nrefined topological vertex'. In the limit q -> t, we recover the refined\\ntopological vertex of Iqbal et al. In the limit x -> y, we obtain a\\nqt-deformation of the topological vertex of Aganagic et al. Copies of the\\nvertex can be glued to obtain qt-deformed 5D instanton partition functions that\\nhave well-defined 4D limits and, for generic values of (q, t), contain\\ninfinite-towers of poles for every pole in the limit q -> t.\\n\",\n",
       " \"  We investigate bias voltage effects on the spin-dependent transport\\nproperties of Fe/MgAl${}_2$O${}_4$/Fe(001) magnetic tunneling junctions (MTJs)\\nby comparing them with those of Fe/MgO/Fe(001) MTJs. By means of the\\nnonequilibrium Green's function method and the density functional theory, we\\ncalculate bias voltage dependences of magnetoresistance (MR) ratios in both the\\nMTJs. We find that in both the MTJs, the MR ratio decreases as the bias voltage\\nincreases and finally vanishes at a critical bias voltage $V_{\\\\rm c}$. We also\\nfind that the critical bias voltage $V_{\\\\rm c}$ of the MgAl${}_2$O${}_4$-based\\nMTJ is clearly larger than that of the MgO-based MTJ. Since the in-plane\\nlattice constant of the Fe/MgAl${}_2$O${}_4$/Fe(001) supercell is twice that of\\nthe Fe/MgO/Fe(001) one, the Fe electrodes in the MgAl${}_2$O${}_4$-based MTJs\\nhave an identical band structure to that obtained by folding the Fe band\\nstructure of the MgO-based MTJs in the Brillouin zone of the in-plane wave\\nvector. We show that such a difference in the Fe band structure is the origin\\nof the difference in the critical bias voltage $V_{\\\\rm c}$ between the\\nMgAl${}_2$O${}_4$- and MgO-based MTJs.\\n\",\n",
       " '  Extensive efforts have been devoted to recognizing facial action units (AUs).\\nHowever, it is still challenging to recognize AUs from spontaneous facial\\ndisplays especially when they are accompanied with speech. Different from all\\nprior work that utilized visual observations for facial AU recognition, this\\npaper presents a novel approach that recognizes speech-related AUs exclusively\\nfrom audio signals based on the fact that facial activities are highly\\ncorrelated with voice during speech. Specifically, dynamic and physiological\\nrelationships between AUs and phonemes are modeled through a continuous time\\nBayesian network (CTBN); then AU recognition is performed by probabilistic\\ninference via the CTBN model.\\nA pilot audiovisual AU-coded database has been constructed to evaluate the\\nproposed audio-based AU recognition framework. The database consists of a\\n\"clean\" subset with frontal and neutral faces and a challenging subset\\ncollected with large head movements and occlusions. Experimental results on\\nthis database show that the proposed CTBN model achieves promising recognition\\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\\nvisual-based methods especially for those AUs that are activated at low\\nintensities or \"hardly visible\" in the visual channel. Furthermore, the CTBN\\nmodel yields more impressive recognition performance on the challenging subset,\\nwhere the visual-based approaches suffer significantly.\\n',\n",
       " '  We found an easy and quick post-learning method named \"Icing on the Cake\" to\\nenhance a classification performance in deep learning. The method is that we\\ntrain only the final classifier again after an ordinary training is done.\\n',\n",
       " '  As traditional neural network consumes a significant amount of computing\\nresources during back propagation, \\\\citet{Sun2017mePropSB} propose a simple yet\\neffective technique to alleviate this problem. In this technique, only a small\\nsubset of the full gradients are computed to update the model parameters. In\\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\\nto reduce calculation in back propagation, and the surprising results verify\\nits validity in CNN: only 5\\\\% of the gradients are passed back but the model\\nstill achieves the same effect as the traditional CNN, or even better. We also\\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\\nback propagation, which may bring significant computational benefits for high\\ncomputational complexity of convolution operation in CNN.\\n',\n",
       " '  Mobile robots are increasingly being used to assist with active pursuit and\\nlaw enforcement. One major limitation for such missions is the resource\\n(battery) allocated to the robot. Factors like nature and agility of evader,\\nterrain over which pursuit is being carried out, plausible traversal velocity\\nand the amount of necessary data to be collected all influence how long the\\nrobot can last in the field and how far it can travel. In this paper, we\\ndevelop an analytical model that analyzes the energy utilization for a variety\\nof components mounted on a robot to estimate the maximum operational range\\nachievable by the robot operating on a single battery discharge. We categorize\\nthe major consumers of energy as: 1.) ancillary robotic functions such as\\ncomputation, communication, sensing etc., and 2.) maneuvering which involves\\npropulsion, steering etc. Both these consumers draw power from the common power\\nsource but the achievable range is largely affected by the proportion of power\\navailable for maneuvering. For this case study, we performed experiments with\\nreal robots on planar and graded surfaces and evaluated the estimation error\\nfor each case.\\n',\n",
       " '  Mixed effects models are widely used to describe heterogeneity in a\\npopulation. A crucial issue when adjusting such a model to data consists in\\nidentifying fixed and random effects. From a statistical point of view, it\\nremains to test the nullity of the variances of a given subset of random\\neffects. Some authors have proposed to use the likelihood ratio test and have\\nestablished its asymptotic distribution in some particular cases. Nevertheless,\\nto the best of our knowledge, no general variance components testing procedure\\nhas been fully investigated yet. In this paper, we study the likelihood ratio\\ntest properties to test that the variances of a general subset of the random\\neffects are equal to zero in both linear and nonlinear mixed effects model,\\nextending the existing results. We prove that the asymptotic distribution of\\nthe test is a chi-bar-square distribution, that is to say a mixture of\\nchi-square distributions, and we identify the corresponding weights. We\\nhighlight in particular that the limiting distribution depends on the presence\\nof correlations between the random effects but not on the linear or nonlinear\\nstructure of the mixed effects model. We illustrate the finite sample size\\nproperties of the test procedure through simulation studies and apply the test\\nprocedure to two real datasets of dental growth and of coucal growth.\\n',\n",
       " '  We show that a positive Borel measure of positive finite total mass, on\\ncompact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic\\nsolution to the Monge-Ampere equation if and only if it is dominated locally by\\nMonge-Ampere measures of Holder continuous plurisubharmonic functions.\\n',\n",
       " \"  The Large European Array for Pulsars combines Europe's largest radio\\ntelescopes to form a tied-array telescope that provides high signal-to-noise\\nobservations of millisecond pulsars (MSPs) with the objective to increase the\\nsensitivity of detecting low-frequency gravitational waves. As part of this\\nendeavor we have developed a software correlator and beamformer which enables\\nthe formation of a tied-array beam from the raw voltages from each of\\ntelescopes. We explain the concepts and techniques involved in the process of\\nadding the raw voltages coherently. We further present the software processing\\npipeline that is specifically designed to deal with data from widely spaced,\\ninhomogeneous radio telescopes and describe the steps involved in preparing,\\ncorrelating and creating the tied-array beam. This includes polarization\\ncalibration, bandpass correction, frequency dependent phase correction,\\ninterference mitigation and pulsar gating. A link is provided where the\\nsoftware can be obtained.\\n\",\n",
       " '  In this paper, we discuss how a suitable family of tensor kernels can be used\\nto efficiently solve nonparametric extensions of $\\\\ell^p$ regularized learning\\nmethods. Our main contribution is proposing a fast dual algorithm, and showing\\nthat it allows to solve the problem efficiently. Our results contrast recent\\nfindings suggesting kernel methods cannot be extended beyond Hilbert setting.\\nNumerical experiments confirm the effectiveness of the method.\\n',\n",
       " '  Deep Learning models are vulnerable to adversarial examples, i.e.\\\\ images\\nobtained via deliberate imperceptible perturbations, such that the model\\nmisclassifies them with high confidence. However, class confidence by itself is\\nan incomplete picture of uncertainty. We therefore use principled Bayesian\\nmethods to capture model uncertainty in prediction for observing adversarial\\nmisclassification. We provide an extensive study with different Bayesian neural\\nnetworks attacked in both white-box and black-box setups. The behaviour of the\\nnetworks for noise, attacks and clean test data is compared. We observe that\\nBayesian neural networks are uncertain in their predictions for adversarial\\nperturbations, a behaviour similar to the one observed for random Gaussian\\nperturbations. Thus, we conclude that Bayesian neural networks can be\\nconsidered for detecting adversarial examples.\\n',\n",
       " '  The recent discovery of the planetary system hosted by the ultracool dwarf\\nstar TRAPPIST-1 could open new perspectives into the investigation of planetary\\nclimates of Earth-sized exoplanets, their atmospheres and their possible\\nhabitability. In this paper, we use a simple climate-vegetation energy-balance\\nmodel to study the climate of the seven TRAPPIST-1 planets and the climate\\ndependence on the global albedo, on the fraction of vegetation that could cover\\ntheir surfaces and on the different greenhouse conditions. The model allows us\\nto investigate whether liquid water could be maintained on the planetary\\nsurfaces (i.e., by defining a \"surface water zone\") in different planetary\\nconditions, with or without the presence of greenhouse effect.\\nIt is shown that planet TRAPPIST-1d seems to be the most stable from an\\nEarth-like perspective, since it resides in the surface water zone for a wide\\nrange of reasonable values of the model parameters. Moreover, according to the\\nmodel outer planets (f, g and h) cannot host liquid water on their surfaces,\\neven for Earth-like conditions, entering a snowball state. Although very\\nsimple, the model allows to extract the main features of the TRAPPIST-1\\nplanetary climates.\\n',\n",
       " '  We show that $\\\\mathbb{Q}$-Fano varieties of fixed dimension with\\nanti-canonical degrees and alpha-invariants bounded from below form a bounded\\nfamily. As a corollary, K-semistable $\\\\mathbb{Q}$-Fano varieties of fixed\\ndimension with anti-canonical degrees bounded from below form a bounded family.\\n',\n",
       " '  We consider quantum, nondterministic and probabilistic versions of known\\ncomputational model Ordered Read-$k$-times Branching Programs or Ordered Binary\\nDecision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We\\nshow width hierarchy for complexity classes of Boolean function computed by\\nthese models and discuss relation between different variants of $k$-OBDD.\\n',\n",
       " '  We study the relation between the microscopic properties of a many-body\\nsystem and the electron spectra, experimentally accessible by photoemission. In\\na recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the\\n\"fluctuation diagnostics\" approach, to extract the dominant wave vector\\ndependent bosonic fluctuations from the electronic self-energy. Here, we first\\nreformulate the theory in terms of fermionic modes, to render its connection\\nwith resonance valence bond (RVB) fluctuations more transparent. Secondly, by\\nusing a large-U expansion, where U is the Coulomb interaction, we relate the\\nfluctuations to real space correlations. Therefore, it becomes possible to\\nstudy how electron spectra are related to charge, spin, superconductivity and\\nRVB-like real space correlations, broadening the analysis of an earlier work\\n[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap\\nphysics of the two-dimensional Hubbard model, studied in the dynamical cluster\\napproximation. We perform calculations for embedded clusters with up to 32\\nsites, having three inequivalent K-points at the Fermi surface. We find that as\\nU is increased, correlation functions gradually attain values consistent with\\nan RVB state. This first happens for correlation functions involving the\\nantinodal point and gradually spreads to the nodal point along the Fermi\\nsurface. Simultaneously a pseudogap opens up along the Fermi surface. We relate\\nthis to a crossover from a Kondo-like state to an RVB-like localized cluster\\nstate and to the presence of RVB and spin fluctuations. These changes are\\ncaused by a strong momentum dependence in the cluster bath-couplings along the\\nFermi surface. We also show, from a more algorithmic perspective, how the\\ntime-consuming calculations in fluctuation diagnostics can be drastically\\nsimplified.\\n',\n",
       " '  We study a generic one-dimensional model for an intracellular cargo driven by\\nN motor proteins against an external applied force. The model includes\\nmotor-cargo and motor-motor interactions. The cargo motion is described by an\\nover-damped Langevin equation, while motor dynamics is specified by hopping\\nrates which follow a local detailed balance condition with respect to change in\\nenergy per hopping event. Based on this model, we show that the stall force,\\nthe mean external force corresponding to zero mean cargo velocity, is\\ncompletely independent of the details of the interactions and is, therefore,\\nalways equal to the sum of the stall forces of the individual motors. This\\nexact result is arrived on the basis of a simple assumption: the (macroscopic)\\nstate of stall of the cargo is analogous to a state of thermodynamic\\nequilibrium, and is characterized by vanishing net probability current between\\nany two microstates, with the latter specified by motor positions relative to\\nthe cargo. The corresponding probability distribution of the microstates under\\nstall is also determined. These predictions are in complete agreement with\\nnumerical simulations, carried out using specific forms of interaction\\npotentials.\\n',\n",
       " '  We report experiments on an agarose gel tablet loaded with camphoric acid\\n(c-boat) set into self-motion by interfacial tension gradients at the air-water\\ninterface. We observe three distinct modes of c-boat motion: harmonic mode\\nwhere the c-boat speed oscillates sinusoidally in time, a steady mode where the\\nc-boat maintains constant speed, and a relaxation oscillation mode where the\\nc-boat maintains near-zero speed between sudden jumps in speed and position at\\nregular time intervals. Whereas all three modes have been separately reported\\nbefore in different systems, we show they belong to a common description.\\nThrough control of the air-water surface tension with Sodium Dodecyl Sulfate\\n(SDS), we experimentally deduce the three self-propulsive modes result from\\nsurface tension difference between Camphoric Acid (CA) and the ambient\\nsurroundings.\\n',\n",
       " \"  Artificial intelligence is revolutionizing our lives at an ever increasing\\npace. At the heart of this revolution is the recent advancements in deep neural\\nnetworks (DNN), learning to perform sophisticated, high-level tasks. However,\\ntraining DNNs requires massive amounts of data and is very computationally\\nintensive. Gaining analytical understanding of the solutions found by DNNs can\\nhelp us devise more efficient training algorithms, replacing the commonly used\\nmthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and\\nshow that, indeed, direct computation of the solutions is possible in many\\ncases. We show that a high performing setup used in DNNs introduces a\\nseparation of time-scales in the training dynamics, allowing SGD to train\\nlayers from the lowest (closest to input) to the highest. We then show that for\\neach layer, the distribution of solutions found by SGD can be estimated using a\\nclass-based principal component analysis (PCA) of the layer's input. This\\nfinding allows us to forgo SGD entirely and directly derive the DNN parameters\\nusing this class-based PCA, which can be well estimated using significantly\\nless data than SGD. We implement these results on image datasets MNIST, CIFAR10\\nand CIFAR100 and find that, in fact, layers derived using our class-based PCA\\nperform comparable or superior to neural networks of the same size and\\narchitecture trained using SGD. We also confirm that the class-based PCA often\\nconverges using a fraction of the data required for SGD. Thus, using our method\\ntraining time can be reduced both by requiring less training data than SGD, and\\nby eliminating layers in the costly backpropagation step of the training.\\n\",\n",
       " '  In this letter, we define the homodyne $q$-deformed quadrature operator.\\nAnalytic expression for the wavefunctions of $q$-deformed oscillator in the\\nquadrature basis are found. Furthermore, we compute the explicit analytical\\nexpression for the tomogram of the $q$-deformed coherent states by finding the\\neigenstates of the $q$-deformed quadrature operator.\\n',\n",
       " '  Graph edit distance (GED) is an important similarity measure adopted in a\\nsimilarity-based analysis between two graphs, and computing GED is a primitive\\noperator in graph database analysis. Partially due to the NP-hardness, the\\nexisting techniques for computing GED are only able to process very small\\ngraphs with less than 30 vertices. Motivated by this, in this paper we\\nsystematically study the problems of both GED computation, and GED verification\\n(i.e., verify whether the GED between two graphs is no larger than a user-given\\nthreshold). Firstly, we develop a unified framework that can be instantiated\\ninto either a best-first search approach AStar+ or a depth-first search\\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\\ntechniques to compute tighter lower bounds for intermediate search states,\\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\\nsmaller search space. Extensive empirical studies validate that AStar+ performs\\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\\nstate-of-the-art technique by more than four orders of magnitude.\\n',\n",
       " '  We introduce canonical measures on a locally finite simplicial complex $K$\\nand study their asymptotic behavior under infinitely many barycentric\\nsubdivisions. We also compute the face polynomial of the asymptotic link and\\ndual block of a simplex in the $d^{th}$ barycentric subdivision $Sd^d(K)$ of\\n$K$, $d\\\\gg0$. It is almost everywhere constant. When $K$ is finite, we study\\nthe limit face polynomial of $Sd^d(K)$ after F.Brenti-V.Welker and\\nE.Delucchi-A.Pixton-L.Sabalka.\\n',\n",
       " '  Reusing passwords across multiple websites is a common practice that\\ncompromises security. Recently, Blum and Vempala have proposed password\\nstrategies to help people calculate, in their heads, passwords for different\\nsites without dependence on third-party tools or external devices. Thus far,\\nthe security and efficiency of these \"mental algorithms\" has been analyzed only\\ntheoretically. But are such methods usable? We present the first usability\\nstudy of humanly computable password strategies, involving a learning phase (to\\nlearn a password strategy), then a rehearsal phase (to login to a few\\nwebsites), and multiple follow-up tests. In our user study, with training,\\nparticipants were able to calculate a deterministic eight-character password\\nfor an arbitrary new website in under 20 seconds.\\n',\n",
       " '  As affordability pressures and tight rental markets in global cities mount,\\nonline shared accommodation sites proliferate. Home sharing arrangements\\npresent dilemmas for planning that aims to improve health and safety standards,\\nwhile supporting positives such as the usage of dormant stock and the relieving\\nof rental pressures on middle/lower income earners. Currently, no formal data\\nexists on this internationally growing trend. Here, we present a first\\nquantitative glance on shared accommodation practices across all major urban\\ncenters of Australia enabled via collection and analysis of thousands of online\\nlistings. We examine, countrywide, the spatial and short time scale temporal\\ncharacteristics of this market, along with preliminary analysis on rents,\\ndwelling types and other characteristics. Findings have implications for\\nhousing policy makers and planning practitioners seeking to monitor and respond\\nto housing policy and affordability pressures in formal and informal housing\\nmarkets.\\n',\n",
       " '  Interbank markets are often characterised in terms of a core-periphery\\nnetwork structure, with a highly interconnected core of banks holding the\\nmarket together, and a periphery of banks connected mostly to the core but not\\ninternally. This paradigm has recently been challenged for short time scales,\\nwhere interbank markets seem better characterised by a bipartite structure with\\nmore core-periphery connections than inside the core. Using a novel\\ncore-periphery detection method on the eMID interbank market, we enrich this\\npicture by showing that the network is actually characterised by multiple\\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\\nstructures occurs by shortening the temporal scale of data aggregation. We\\nfurther show how the global financial crisis transformed the market, in terms\\nof composition, multiplicity and internal organisation of core-periphery pairs.\\nBy unveiling such a fine-grained organisation and transformation of the\\ninterbank market, our method can find important applications in the\\nunderstanding of how distress can propagate over financial networks.\\n',\n",
       " \"  We present a novel algorithm that uses exact learning and abstraction to\\nextract a deterministic finite automaton describing the state dynamics of a\\ngiven trained RNN. We do this using Angluin's L* algorithm as a learner and the\\ntrained RNN as an oracle. Our technique efficiently extracts accurate automata\\nfrom trained RNNs, even when the state vectors are large and require fine\\ndifferentiation.\\n\",\n",
       " '  Everything in the world is being connected, and things are becoming\\ninteractive. The future of the interactive world depends on the future Internet\\nof Things (IoT). Software-defined networking (SDN) technology, a new paradigm\\nin the networking area, can be useful in creating an IoT because it can handle\\ninteractivity by controlling physical devices, transmission of data among them,\\nand data acquisition. However, digital signage can be one of the promising\\ntechnologies in this era of technology that is progressing toward the\\ninteractive world, connecting users to the IoT network through device-to-device\\ncommunication technology. This article illustrates a novel prototype that is\\nmainly focused on a smart digital signage system comprised of software-defined\\nIoT (SD-IoT) and invisible image sensor communication technology. We have\\nproposed an SDN scheme with a view to initiating its flexibility and\\ncompatibility for an IoT network-based smart digital signage system. The idea\\nof invisible communication can make the users of the technology trendier to it,\\nand the usage of unused resources such as images and videos can be ensured. In\\naddition, this communication has paved the way for interactivity between the\\nuser and digital signage, where the digital signage and the camera of a\\nsmartphone can be operated as a transmitter and a receiver, respectively. The\\nproposed scheme might be applicable to real-world applications because SDN has\\nthe flexibility to adapt with the alteration of network status without any\\nhardware modifications while displays and smartphones are available everywhere.\\nA performance analysis of this system showed the advantages of an SD-IoT\\nnetwork over an Internet protocol-based IoT network considering a queuing\\nanalysis for a dynamic link allocation process in the case of user access to\\nthe IoT network.\\n',\n",
       " '  Recently, an Atacama Large Millimeter/submillimeter Array (ALMA) observation\\nof the water snow line in the protoplanetary disk around the FU Orionis star\\nV883 Ori was reported. The radial variation of the spectral index at\\nmm-wavelengths around the snow line was interpreted as being due to a pileup of\\nparticles interior to the snow line. However, radial transport of solids in the\\nouter disk operates on timescales much longer than the typical timescale of an\\nFU Ori outburst ($10^{1}$--$10^{2}$ yr). Consequently, a steady-state pileup is\\nunlikely. We argue that it is only necessary to consider water evaporation and\\nre-coagulation of silicates to explain the recent ALMA observation of V883 Ori\\nbecause these processes are short enough to have had their impact since the\\noutburst. Our model requires the inner disk to have already been optically\\nthick before the outburst, and our results suggest that the carbon content of\\npebbles is low.\\n',\n",
       " \"  The existence of weak solutions to the stationary Navier-Stokes equations in\\nthe whole plane $\\\\mathbb{R}^2$ is proven. This particular geometry was the only\\ncase left open since the work of Leray in 1933. The reason is that due to the\\nabsence of boundaries the local behavior of the solutions cannot be controlled\\nby the enstrophy in two dimensions. We overcome this difficulty by constructing\\napproximate weak solutions having a prescribed mean velocity on some given\\nbounded set. As a corollary, we obtain infinitely many weak solutions in\\n$\\\\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the\\nexpected convergence of the velocity field at large distances to any prescribed\\nconstant vector field. This explicit parameterization of the weak solutions\\nallows us to prove a weak-strong uniqueness theorem for small data. The\\nquestion of the asymptotic behavior of the weak solutions remains however open,\\nwhen the uniqueness theorem doesn't apply.\\n\",\n",
       " '  New results on the Baire product problem are presented. It is shown that an\\narbitrary product of almost locally ccc Baire spaces is Baire; moreover, the\\nproduct of a Baire space and a 1st countable space which is $\\\\beta$-unfavorable\\nin the strong Choquet game is Baire.\\n',\n",
       " '  In the Ultimatum Game (UG) one player, named \"proposer\", has to decide how to\\nallocate a certain amount of money between herself and a \"responder\". If the\\noffer is greater than or equal to the responder\\'s minimum acceptable offer\\n(MAO), then the money is split as proposed, otherwise, neither the proposer nor\\nthe responder get anything. The UG has intrigued generations of behavioral\\nscientists because people in experiments blatantly violate the equilibrium\\npredictions that self-interested proposers offer the minimum available non-zero\\namount, and self-interested responders accept. Why are these predictions\\nviolated? Previous research has mainly focused on the role of social\\npreferences. Little is known about the role of general moral preferences for\\ndoing the right thing, preferences that have been shown to play a major role in\\nother social interactions (e.g., Dictator Game and Prisoner\\'s Dilemma). Here I\\ndevelop a theoretical model and an experiment designed to pit social\\npreferences against moral preferences. I find that, although people recognize\\nthat offering half and rejecting low offers are the morally right things to do,\\nmoral preferences have no causal impact on UG behavior. The experimental data\\nare indeed well fit by a model according to which: (i) high UG offers are\\nmotivated by inequity aversion and, to a lesser extent, self-interest; (ii)\\nhigh MAOs are motivated by inequity aversion.\\n',\n",
       " \"  In this work, we study the tradeoffs between the error probabilities of\\nclassical-quantum channels and the blocklength $n$ when the transmission rates\\napproach the channel capacity at a rate slower than $1/\\\\sqrt{n}$, a research\\ntopic known as moderate deviation analysis. We show that the optimal error\\nprobability vanishes under this rate convergence. Our main technical\\ncontributions are a tight quantum sphere-packing bound, obtained via Chaganty\\nand Sethuraman's concentration inequality in strong large deviation theory, and\\nasymptotic expansions of error-exponent functions. Moderate deviation analysis\\nfor quantum hypothesis testing is also established. The converse directly\\nfollows from our channel coding result, while the achievability relies on a\\nmartingale inequality.\\n\",\n",
       " '  Constructing tests or confidence regions that control over the error rates in\\nthe long-run is probably one of the most important problem in statistics. Yet,\\nthe theoretical justification for most methods in statistics is asymptotic. The\\nbootstrap for example, despite its simplicity and its widespread usage, is an\\nasymptotic method. There are in general no claim about the exactness of\\ninferential procedures in finite sample. In this paper, we propose an\\nalternative to the parametric bootstrap. We setup general conditions to\\ndemonstrate theoretically that accurate inference can be claimed in finite\\nsample.\\n',\n",
       " '  Generalized Bäcklund-Darboux transformations (GBDTs) of discrete\\nskew-selfadjoint Dirac systems have been successfully used for explicit solving\\nof direct and inverse problems of Weyl-Titchmarsh theory. During explicit\\nsolving of the direct and inverse problems, we considered GBDTs of the trivial\\ninitial systems. However, GBDTs of arbitrary discrete skew-selfadjoint Dirac\\nsystems are important as well and we introduce these transformations in the\\npresent paper. The obtained results are applied to the construction of explicit\\nsolutions of the interesting related non-stationary systems.\\n',\n",
       " '  We describe a procedure called panel collapse for replacing a CAT(0) cube\\ncomplex $\\\\Psi$ by a \"lower complexity\" CAT(0) cube complex $\\\\Psi_\\\\bullet$\\nwhenever $\\\\Psi$ contains a codimension-$2$ hyperplane that is extremal in one\\nof the codimension-$1$ hyperplanes containing it. Although $\\\\Psi_\\\\bullet$ is\\nnot in general a subcomplex of $\\\\Psi$, it is a subspace consisting of a\\nsubcomplex together with some cubes that sit inside $\\\\Psi$ \"diagonally\". The\\nhyperplanes of $\\\\Psi_\\\\bullet$ extend to hyperplanes of $\\\\Psi$. Applying this\\nprocedure, we prove: if a group $G$ acts cocompactly on a CAT(0) cube complex\\n$\\\\Psi$, then there is a CAT(0) cube complex $\\\\Omega$ so that $G$ acts\\ncocompactly on $\\\\Omega$ and for each hyperplane $H$ of $\\\\Omega$, the stabiliser\\nin $G$ of $H$ acts on $H$ essentially.\\nUsing panel collapse, we obtain a new proof of Stallings\\'s theorem on groups\\nwith more than one end. As another illustrative example, we show that panel\\ncollapse applies to the exotic cubulations of free groups constructed by Wise.\\nNext, we show that the CAT(0) cube complexes constructed by Cashen-Macura can\\nbe collapsed to trees while preserving all of the necessary group actions. (It\\nalso illustrates that our result applies to actions of some non-discrete\\ngroups.) We also discuss possible applications to quasi-isometric rigidity for\\ncertain classes of graphs of free groups with cyclic edge groups. Panel\\ncollapse is also used in forthcoming work of the first-named author and Wilton\\nto study fixed-point sets of finite subgroups of $\\\\mathrm{Out}(F_n)$ on the\\nfree splitting complex.\\n',\n",
       " '  The recently proposed self-ensembling methods have achieved promising results\\nin deep semi-supervised learning, which penalize inconsistent predictions of\\nunlabeled data under different perturbations. However, they only consider\\nadding perturbations to each single data point, while ignoring the connections\\nbetween data samples. In this paper, we propose a novel method, called Smooth\\nNeighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on\\nthe predictions of the teacher model, i.e., the implicit self-ensemble of\\nmodels. Then the graph serves as a similarity measure with respect to which the\\nrepresentations of \"similar\" neighboring points are learned to be smooth on the\\nlow-dimensional manifold. We achieve state-of-the-art results on\\nsemi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for\\nCIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,\\nthe improvements are significant when the labels are fewer. For the\\nnon-augmented MNIST with only 20 labels, the error rate is reduced from\\nprevious 4.81% to 1.36%. Our method also shows robustness to noisy labels.\\n',\n",
       " '  Efficient algorithms and techniques to detect and identify large flows in a\\nhigh throughput traffic stream in the SDN match-and-action model are presented.\\nThis is in contrast to previous work that either deviated from the match and\\naction model by requiring additional switch level capabilities or did not\\nexploit the SDN data plane. Our construction has two parts; (a) how to sample\\nin an SDN match and action model, (b) how to detect large flows efficiently and\\nin a scalable way, in the SDN model.\\nOur large flow detection methods provide high accuracy and present a good and\\npractical tradeoff between switch - controller traffic, and the number of\\nentries required in the switch flow table. Based on different parameters, we\\ndifferentiate between heavy flows, elephant flows and bulky flows and present\\nefficient algorithms to detect flows of the different types.\\nAdditionally, as part of our heavy flow detection scheme, we present sampling\\nmethods to sample packets with arbitrary probability $p$ per packet or per byte\\nthat traverses an SDN switch.\\nFinally, we show how our algorithms can be adapted to a distributed\\nmonitoring SDN setting with multiple switches, and easily scale with the number\\nof monitoring switches.\\n',\n",
       " '  With the tremendous increase of the Internet traffic, achieving the best\\nperformance with limited resources is becoming an extremely urgent problem. In\\norder to address this concern, in this paper, we build an optimization problem\\nwhich aims to maximize the total utility of traffic flows with the capacity\\nconstraint of nodes and links in the network. Based on Duality Theory, we\\npropose an iterative algorithm which adjusts the rates of traffic flows and\\ncapacity of nodes and links simultaneously to maximize the total utility.\\nSimulation results show that our algorithm performs better than the NUP\\nalgorithm on BA and ER network models, which has shown to get the best\\nperformance so far. Since our research combines the topology information with\\ncapacity constraint, it may give some insights for resource allocation in real\\ncommunication networks.\\n',\n",
       " '  We consider a fundamental integer programming (IP) model for cost-benefit\\nanalysis flood protection through dike building in the Netherlands, due to\\nVerweij and Zwaneveld.\\nExperimental analysis with data for the Ijsselmeer lead to integral optimal\\nsolution of the linear programming relaxation of the IP model.\\nThis naturally led to the question of integrality of the polytope associated\\nwith the IP model.\\nIn this paper we first give a negative answer to this question by\\nestablishing non-integrality of the polytope.\\nSecond, we establish natural conditions that guarantee the linear programming\\nrelaxation of the IP model to be integral.\\nWe then test the most recent data on flood probabilities, damage and\\ninvestment costs of the IJsselmeer for these conditions.\\nThird, we show that the IP model can be solved in polynomial time when the\\nnumber of dike segments, or the number of feasible barrier heights, are\\nconstant.\\n',\n",
       " '  In wireless communication, heterogeneous technologies such as WiFi, ZigBee\\nand BlueTooth operate in the same ISM band.With the exponential growth in the\\nnumber of wireless devices, the ISM band becomes more and more crowded. These\\nheterogeneous devices have to compete with each other to access spectrum\\nresources, generating cross-technology interference (CTI). Since CTI may\\ndestroy wireless communication, this field is facing an urgent and challenging\\nneed to investigate spectrum efficiency under CTI. In this paper, we introduce\\na novel framework to address this problem from two aspects. On the one hand,\\nfrom the perspective of each communication technology itself, we propose novel\\nchannel/link models to capture the channel/link status under CTI. On the other\\nhand, we investigate spectrum efficiency from the perspective by taking all\\nheterogeneous technologies as a whole and building crosstechnology\\ncommunication among them. The capability of direct communication among\\nheterogeneous devices brings great opportunities to harmoniously sharing the\\nspectrum with collaboration rather than competition.\\n',\n",
       " '  We have derived background corrected intensities of 3-50 MeV galactic\\nelectrons observed by Voyager 1 as it passes through the heliosheath from 95 to\\n122 AU. The overall intensity change of the background corrected data from the\\ninner to the outer boundary of the heliosheath is a maximum of a factor ~100 at\\n15 MeV. At lower energies this fractional change becomes less and the corrected\\nelectron spectra in the heliosheath becomes progressively steeper, reaching\\nvalues ~ -2.5 for the spectral index just outside of the termination shock. At\\nhigher energies the spectra of electrons has an exponent changing from the\\nnegative LIS spectral index of -1.3 to values approaching zero in the\\nheliosheath as a result of the solar modulation of the galactic electron\\ncomponent. The large modulation effects observed below ~100 MV are possible\\nevidence for enhanced diffusion as part of the modulation process for electrons\\nin the heliosheath.\\n',\n",
       " \"  The `beta' is one of the key quantities in the capital asset pricing model\\n(CAPM). In statistical language, the beta can be viewed as the slope of the\\nregression line fitted to financial returns on the market against the returns\\non the asset under consideration. The insurance counterpart of CAPM, called the\\nweighted insurance pricing model (WIPM), gives rise to the so-called\\nweighted-Gini beta. The aforementioned two betas may or may not coincide,\\ndepending on the form of the underlying regression function, and this has\\nprofound implications when designing portfolios and allocating risk capital. To\\nfacilitate these tasks, in this paper we develop large-sample statistical\\ninference results that, in a straightforward fashion, imply confidence\\nintervals for, and hypothesis tests about, the equality of the two betas.\\n\",\n",
       " \"  In partially observed environments, it can be useful for a human to provide\\nthe robot with declarative information that represents probabilistic relational\\nconstraints on properties of objects in the world, augmenting the robot's\\nsensory observations. For instance, a robot tasked with a search-and-rescue\\nmission may be informed by the human that two victims are probably in the same\\nroom. An important question arises: how should we represent the robot's\\ninternal knowledge so that this information is correctly processed and combined\\nwith raw sensory information? In this paper, we provide an efficient belief\\nstate representation that dynamically selects an appropriate factoring,\\ncombining aspects of the belief when they are correlated through information\\nand separating them when they are not. This strategy works in open domains, in\\nwhich the set of possible objects is not known in advance, and provides\\nsignificant improvements in inference time over a static factoring, leading to\\nmore efficient planning for complex partially observed tasks. We validate our\\napproach experimentally in two open-domain planning problems: a 2D discrete\\ngridworld task and a 3D continuous cooking task. A supplementary video can be\\nfound at this http URL.\\n\",\n",
       " \"  Open problems abound in the theory of complex networks, which has found\\nsuccessful application to diverse fields of science. With the aim of further\\nadvancing the understanding of the brain's functional connectivity, we propose\\nto evaluate a network metric which we term the geodesic entropy. This entropy,\\nin a way that can be made precise, quantifies the Shannon entropy of the\\ndistance distribution to a specific node from all other nodes. Measurements of\\ngeodesic entropy allow for the characterization of the structural information\\nof a network that takes into account the distinct role of each node into the\\nnetwork topology. The measurement and characterization of this structural\\ninformation has the potential to greatly improve our understanding of sustained\\nactivity and other emergent behaviors in networks, such as self-organized\\ncriticality sometimes seen in such contexts. We apply these concepts and\\nmethods to study the effects of how the psychedelic Ayahuasca affects the\\nfunctional connectivity of the human brain. We show that the geodesic entropy\\nis able to differentiate the functional networks of the human brain in two\\ndifferent states of consciousness in the resting state: (i) the ordinary waking\\nstate and (ii) a state altered by ingestion of the Ayahuasca. The entropy of\\nthe nodes of brain networks from subjects under the influence of Ayahuasca\\ndiverge significantly from those of the ordinary waking state. The functional\\nbrain networks from subjects in the altered state have, on average, a larger\\ngeodesic entropy compared to the ordinary state. We conclude that geodesic\\nentropy is a useful tool for analyzing complex networks and discuss how and why\\nit may bring even further valuable insights into the study of the human brain\\nand other empirical networks.\\n\",\n",
       " '  The high-energy non-thermal universe is dominated by power law-like spectra.\\nTherefore results in high-energy astronomy are often reported as parameters of\\npower law fits, or, in the case of a non-detection, as an upper limit assuming\\nthe underlying unseen spectrum behaves as a power law. In this paper I\\ndemonstrate a simple and powerful one-to-one relation of the integral upper\\nlimit in the two dimensional power law parameter space into the spectrum\\nparameter space and use this method to unravel the so far convoluted question\\nof the sensitivity of astroparticle telescopes.\\n',\n",
       " '  We develop estimates for the solutions and derive existence and uniqueness\\nresults of various local boundary value problems for Dirac equations that\\nimprove all relevant results known in the literature. With these estimates at\\nhand, we derive a general existence, uniqueness and regularity theorem for\\nsolutions of Dirac equations with such boundary conditions. We also apply these\\nestimates to a new nonlinear elliptic-parabolic problem, the Dirac-harmonic\\nheat flow on Riemannian spin manifolds. This problem is motivated by the\\nsupersymmetric nonlinear $\\\\sigma$-model and combines a harmonic heat flow type\\nequation with a Dirac equation that depends nonlinearly on the flow.\\n',\n",
       " \"  In modern election campaigns, political parties utilize social media to\\nadvertise their policies and candidates and to communicate to the electorate.\\nIn Japan's latest general election in 2017, the 48th general election for the\\nLower House, social media, especially Twitter, was actively used. In this\\npaper, we analyze the users who retweeted tweets of political parties on\\nTwitter during the election. Our aim is to clarify what kinds of users are\\ndiffusing (retweeting) tweets of political parties. The results indicate that\\nthe characteristics of retweeters of the largest ruling party (Liberal\\nDemocratic Party of Japan) and the largest opposition party (The Constitutional\\nDemocratic Party of Japan) were similar, even though the retweeters did not\\noverlap each other. We also found that a particular opposition party (Japanese\\nCommunist Party) had quite different characteristics from other political\\nparties.\\n\",\n",
       " \"  In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle\\nwith two equal bisectors is isosceles. But what about changing the bisectors to\\ncevians? Cevian is any line segment in a triangle with one endpoint on a vertex\\nof the triangle and other endpoint on the opposite side. Not for any pairs of\\nequal cevians the triangle is isosceles. Theorem. If for a triangle ABC there\\nare equal cevians issuing from A and B, which intersect on the bisector or on\\nthe median of the angle C, then AC=BC (so the triangle ABC is isosceles).\\nProposition. Let ABC be an isosceles triangle. Define circle C to be the circle\\nsymmetric relative to AB to the circumscribed circle of the triangle ABC. Then\\nthe locus of intersection points of pairs of equal cevians is the union of the\\nbase AB, the triangle's axis of symmetry, and the circle C.\\n\",\n",
       " '  Anomaly detecting as an important technical in cloud computing is applied to\\nsupport smooth running of the cloud platform. Traditional detecting methods\\nbased on statistic, analysis, etc. lead to the high false-alarm rate due to\\nnon-adaptive and sensitive parameters setting. We presented an online model for\\nanomaly detecting using machine learning theory. However, most existing methods\\nbased on machine learning linked all features from difference sub-systems into\\na long feature vector directly, which is difficult to both exploit the\\ncomplement information between sub-systems and ignore multi-view features\\nenhancing the classification performance. Aiming to this problem, the proposed\\nmethod automatic fuses multi-view features and optimize the discriminative\\nmodel to enhance the accuracy. This model takes advantage of extreme learning\\nmachine (ELM) to improve detection efficiency. ELM is the single hidden layer\\nneural network, which is transforming iterative solution the output weights to\\nsolution of linear equations and avoiding the local optimal solution. Moreover,\\nwe rank anomies according to the relationship between samples and the\\nclassification boundary, and then assigning weights for ranked anomalies,\\nretraining the classification model finally. Our method exploits the complement\\ninformation between sub-systems sufficiently, and avoids the influence from\\nimbalance dataset, therefore, deal with various challenges from the cloud\\ncomputing platform. We deploy the privately cloud platform by Openstack,\\nverifying the proposed model and comparing results to the state-of-the-art\\nmethods with better efficiency and simplicity.\\n',\n",
       " '  We survey the dimension theory of self-affine sets for general mathematical\\naudience. The article is in Finnish.\\n',\n",
       " '  Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a\\nhorizontal temperature gradient arises in a variety of situations. Recent\\nstudies have shown that the composition of the gas phase, which is typically a\\nmixture of vapour and air, has a noticeable effect on the critical Marangoni\\nnumber describing the onset of convection as well as on the observed convection\\npattern. Specifically, as the total pressure or, equivalently, the average\\nconcentration of air is decreased, the threshold of the instability leading to\\nthe emergence of convective rolls is found to increase rather significantly. We\\npresent a linear stability analysis of the problem which shows that this trend\\ncan be readily understood by considering the transport of heat and vapour\\nthrough the gas phase. In particular, we show that transport in the gas phase\\nhas a noticeable effect even at atmospheric conditions, when phase change is\\ngreatly suppressed.\\n',\n",
       " '  In the paper we consider a graph model of message passing processes and\\npresent a method verification of message passing processes. The method is\\nillustrated by an example of a verification of sliding window protocol.\\n',\n",
       " '  In this article, we derive a Bayesian model to learning the sparse and low\\nrank PARAFAC decomposition for the observed tensor with missing values via the\\nelastic net, with property to find the true rank and sparse factor matrix which\\nis robust to the noise. We formulate efficient block coordinate descent\\nalgorithm and admax stochastic block coordinate descent algorithm to solve it,\\nwhich can be used to solve the large scale problem. To choose the appropriate\\nrank and sparsity in PARAFAC decomposition, we will give a solution path by\\ngradually increasing the regularization to increase the sparsity and decrease\\nthe rank. When we find the sparse structure of the factor matrix, we can fixed\\nthe sparse structure, using a small to regularization to decreasing the\\nrecovery error, and one can choose the proper decomposition from the solution\\npath with sufficient sparse factor matrix with low recovery error. We test the\\npower of our algorithm on the simulation data and real data, which show it is\\npowerful.\\n',\n",
       " '  We introduce a new family of thermostat flows on the unit tangent bundle of\\nan oriented Riemannian $2$-manifold. Suitably reparametrised, these flows\\ninclude the geodesic flow of metrics of negative Gauss curvature and the\\ngeodesic flow induced by the Hilbert metric on the quotient surface of\\ndivisible convex sets. We show that the family of flows can be parametrised in\\nterms of certain weighted holomorphic differentials and investigate their\\nproperties. In particular, we prove that they admit a dominated splitting and\\nwe identify special cases in which the flows are Anosov. In the latter case, we\\nstudy when they admit an invariant measure in the Lebesgue class and the\\nregularity of the weak foliations.\\n',\n",
       " '  We introduce the shifted quantum affine algebras. They map homomorphically\\ninto the quantized $K$-theoretic Coulomb branches of $3d\\\\ {\\\\mathcal N}=4$ SUSY\\nquiver gauge theories. In type $A$, they are endowed with a coproduct, and they\\nact on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,\\nthey are closely related to the open relativistic quantum Toda lattice of type\\n$A$.\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72bdb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"  Predictive models allow subject-specific inference when analyzing disease\\nrelated alterations in neuroimaging data. Given a subject's data, inference can\\nbe made at two levels: global, i.e. identifiying condition presence for the\\nsubject, and local, i.e. detecting condition effect on each individual\\nmeasurement extracted from the subject's data. While global inference is widely\\nused, local inference, which can be used to form subject-specific effect maps,\\nis rarely used because existing models often yield noisy detections composed of\\ndispersed isolated islands. In this article, we propose a reconstruction\\nmethod, named RSM, to improve subject-specific detections of predictive\\nmodeling approaches and in particular, binary classifiers. RSM specifically\\naims to reduce noise due to sampling error associated with using a finite\\nsample of examples to train classifiers. The proposed method is a wrapper-type\\nalgorithm that can be used with different binary classifiers in a diagnostic\\nmanner, i.e. without information on condition presence. Reconstruction is posed\\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\\nestimated from training data in a classifier-specific fashion. Experimental\\nevaluation is performed on synthetically generated data and data from the\\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on\\nsynthetic data demonstrate that using RSM yields higher detection accuracy\\ncompared to using models directly or with bootstrap averaging. Analyses on the\\nADNI dataset show that RSM can also improve correlation between\\nsubject-specific detections in cortical thickness data and non-imaging markers\\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\\nand Cerebrospinal Fluid amyloid-$\\\\beta$ levels. Further reliability studies on\\nthe longitudinal ADNI dataset show improvement on detection reliability when\\nRSM is used.\\n\",\n",
       " '  Rotation invariance and translation invariance have great values in image\\nrecognition tasks. In this paper, we bring a new architecture in convolutional\\nneural network (CNN) named cyclic convolutional layer to achieve rotation\\ninvariance in 2-D symbol recognition. We can also get the position and\\norientation of the 2-D symbol by the network to achieve detection purpose for\\nmultiple non-overlap target. Last but not least, this architecture can achieve\\none-shot learning in some cases using those invariance.\\n',\n",
       " '  We introduce and develop the notion of spherical polyharmonics, which are a\\nnatural generalisation of spherical harmonics. In particular we study the\\ntheory of zonal polyharmonics, which allows us, analogously to zonal harmonics,\\nto construct Poisson kernels for polyharmonic functions on the union of rotated\\nballs. We find the representation of Poisson kernels and zonal polyharmonics in\\nterms of the Gegenbauer polynomials. We show the connection between the\\nclassical Poisson kernel for harmonic functions on the ball, Poisson kernels\\nfor polyharmonic functions on the union of rotated balls, and the Cauchy-Hua\\nkernel for holomorphic functions on the Lie ball.\\n',\n",
       " '  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the\\nMaxwell equations (the so called stochastic MLLG system) describes the creation\\nof domain walls and vortices (fundamental objects for the novel nanostructured\\nmagnetic memories). We first reformulate the stochastic LLG equation into an\\nequation with time-differentiable solutions. We then propose a convergent\\n$\\\\theta$-linear scheme to approximate the solutions of the reformulated system.\\nAs a consequence, we prove convergence of the approximate solutions, with no or\\nminor conditions on time and space steps (depending on the value of $\\\\theta$).\\nHence, we prove the existence of weak martingale solutions of the stochastic\\nMLLG system. Numerical results are presented to show applicability of the\\nmethod.\\n',\n",
       " '  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species\\nwere used to explore the influence of preprocessing and feature extraction on\\nefficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and\\nDiscrete Wavelet Transforms (DWT) were compared as feature extraction\\ntechniques for FTIR data of medicinal plants. Various combinations of signal\\nprocessing steps showed different behavior when applied to classification and\\nclustering tasks. Best results for WTT and DWT found through grid search were\\nsimilar, significantly improving quality of clustering as well as\\nclassification accuracy for tuned logistic regression in comparison to original\\nspectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a\\nmore versatile and easier to use as a data processing tool in various signal\\nprocessing applications.\\n',\n",
       " '  Let $\\\\Omega \\\\subset \\\\mathbb{R}^n$ be a bounded domain satisfying a\\nHayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain\\nreferred to as \"obstacle\". We are interested in the behaviour of the first\\nDirichlet eigenvalue $ \\\\lambda_1(\\\\Omega \\\\setminus (x+D)) $. First, we prove an\\nupper bound on $ \\\\lambda_1(\\\\Omega \\\\setminus (x+D)) $ in terms of the distance\\nof the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet\\nground state $ \\\\phi_{\\\\lambda_1} > 0 $ of $ \\\\Omega $. In short, a direct\\ncorollary is that if \\\\begin{equation} \\\\mu_\\\\Omega := \\\\max_{x}\\\\lambda_1(\\\\Omega\\n\\\\setminus (x+D)) \\\\end{equation} is large enough in terms of $ \\\\lambda_1(\\\\Omega)\\n$, then all maximizer sets $ x+D $ of $ \\\\mu_\\\\Omega $ are close to each maximum\\npoint $ x_0 $ of $ \\\\phi_{\\\\lambda_1} $.\\nSecond, we discuss the distribution of $ \\\\phi_{\\\\lambda_1(\\\\Omega)} $ and the\\npossibility to inscribe wavelength balls at a given point in $ \\\\Omega $.\\nFinally, we specify our observations to convex obstacles $ D $ and show that\\nif $ \\\\mu_\\\\Omega $ is sufficiently large with respect to $ \\\\lambda_1(\\\\Omega) $,\\nthen all maximizers $ x+D $ of $ \\\\mu_\\\\Omega $ contain all maximum points $ x_0\\n$ of $ \\\\phi_{\\\\lambda_1(\\\\Omega)} $.\\n',\n",
       " \"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017\\nU1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel\\nTelescope. From these observations, we derived a partial lightcurve with\\npeak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out\\nrotation periods less than 3 hr and suggests that the period is at least 5 hr.\\nOn the assumption that the variability is due to a changing cross section, the\\naxial ratio is at least 3:1. We saw no evidence for a coma or tail in either\\nindividual images or in a stacked image having an equivalent exposure time of\\n9000 s.\\n\",\n",
       " \"  The ability of metallic nanoparticles to supply heat to a liquid environment\\nunder exposure to an external optical field has attracted growing interest for\\nbiomedical applications. Controlling the thermal transport properties at a\\nsolid-liquid interface then appears to be particularly relevant. In this work,\\nwe address the thermal transport between water and a gold surface coated by a\\npolymer layer. Using molecular dynamics simulations, we demonstrate that\\nincreasing the polymer density displaces the domain resisting to the heat flow,\\nwhile it doesn't affect the final amount of thermal energy released in the\\nliquid. This unexpected behavior results from a trade-off established by the\\nincreasing polymer density which couples more efficiently with the solid but\\ninitiates a counterbalancing resistance with the liquid.\\n\",\n",
       " '  We model large-scale ($\\\\approx$2000km) impacts on a Mars-like planet using a\\nSmoothed Particle Hydrodynamics code. The effects of material strength and of\\nusing different Equations of State on the post-impact material and temperature\\ndistributions are investigated. The properties of the ejected material in terms\\nof escaping and disc mass are analysed as well. We also study potential\\nnumerical effects in the context of density discontinuities and rigid body\\nrotation. We find that in the large-scale collision regime considered here\\n(with impact velocities of 4km/s), the effect of material strength is\\nsubstantial for the post-impact distribution of the temperature and the\\nimpactor material, while the influence of the Equation of State is more subtle\\nand present only at very high temperatures.\\n',\n",
       " '  Time varying susceptibility of host at individual level due to waning and\\nboosting immunity is known to induce rich long-term behavior of disease\\ntransmission dynamics. Meanwhile, the impact of the time varying heterogeneity\\nof host susceptibility on the shot-term behavior of epidemics is not\\nwell-studied, even though the large amount of the available epidemiological\\ndata are the short-term epidemics. Here we constructed a parsimonious\\nmathematical model describing the short-term transmission dynamics taking into\\naccount natural-boosting immunity by reinfection, and obtained the explicit\\nsolution for our model. We found that our system show \"the delayed epidemic\",\\nthe epidemic takes off after negative slope of the epidemic curve at the\\ninitial phase of epidemic, in addition to the common classification in the\\nstandard SIR model, i.e., \"no epidemic\" as $\\\\mathcal{R}_{0}\\\\leq1$ or normal\\nepidemic as $\\\\mathcal{R}_{0}>1$. Employing the explicit solution we derived the\\ncondition for each classification.\\n',\n",
       " '  We present a systematic global sensitivity analysis using the Sobol method\\nwhich can be utilized to rank the variables that affect two quantity of\\ninterests -- pore pressure depletion and stress change -- around a\\nhydraulically-fractured horizontal well based on their degree of importance.\\nThese variables include rock properties and stimulation design variables. A\\nfully-coupled poroelastic hydraulic fracture model is used to account for pore\\npressure and stress changes due to production. To ease the computational cost\\nof a simulator, we also provide reduced order models (ROMs), which can be used\\nto replace the complex numerical model with a rather simple analytical model,\\nfor calculating the pore pressure and stresses at different locations around\\nhydraulic fractures. The main findings of this research are: (i) mobility,\\nproduction pressure, and fracture half-length are the main contributors to the\\nchanges in the quantities of interest. The percentage of the contribution of\\neach parameter depends on the location with respect to pre-existing hydraulic\\nfractures and the quantity of interest. (ii) As the time progresses, the effect\\nof mobility decreases and the effect of production pressure increases. (iii)\\nThese two variables are also dominant for horizontal stresses at large\\ndistances from hydraulic fractures. (iv) At zones close to hydraulic fracture\\ntips or inside the spacing area, other parameters such as fracture spacing and\\nhalf-length are the dominant factors that affect the minimum horizontal stress.\\nThe results of this study will provide useful guidelines for the stimulation\\ndesign of legacy wells and secondary operations such as refracturing and infill\\ndrilling.\\n',\n",
       " '  \"Three is a crowd\" is an old proverb that applies as much to social\\ninteractions, as it does to frustrated configurations in statistical physics\\nmodels. Accordingly, social relations within a triangle deserve special\\nattention. With this motivation, we explore the impact of topological\\nfrustration on the evolutionary dynamics of the snowdrift game on a triangular\\nlattice. This topology provides an irreconcilable frustration, which prevents\\nanti-coordination of competing strategies that would be needed for an optimal\\noutcome of the game. By using different strategy updating protocols, we observe\\ncomplex spatial patterns in dependence on payoff values that are reminiscent to\\na honeycomb-like organization, which helps to minimize the negative consequence\\nof the topological frustration. We relate the emergence of these patterns to\\nthe microscopic dynamics of the evolutionary process, both by means of\\nmean-field approximations and Monte Carlo simulations. For comparison, we also\\nconsider the same evolutionary dynamics on the square lattice, where of course\\nthe topological frustration is absent. However, with the deletion of diagonal\\nlinks of the triangular lattice, we can gradually bridge the gap to the square\\nlattice. Interestingly, in this case the level of cooperation in the system is\\na direct indicator of the level of topological frustration, thus providing a\\nmethod to determine frustration levels in an arbitrary interaction network.\\n',\n",
       " '  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se\\ndiluted-magnetic-semiconductor quantum wells using time-resolved\\nphotoluminescence (PL). The magnetic field and temperature dependencies of this\\ndynamics allow us to separate the non-magnetic and magnetic contributions to\\nthe exciton localization. We deduce the EMP energy of 14 meV, which is in\\nagreement with time-integrated measurements based on selective excitation and\\nthe magnetic field dependence of the PL circular polarization degree. The\\npolaron formation time of 500 ps is significantly longer than the corresponding\\nvalues reported earlier. We propose that this behavior is related to strong\\nself-localization of the EMP, accompanied with a squeezing of the heavy-hole\\nenvelope wavefunction. This conclusion is also supported by the decrease of the\\nexciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and\\ntemperature.\\n',\n",
       " '  The classical Eilenberg correspondence, based on the concept of the syntactic\\nmonoid, relates varieties of regular languages with pseudovarieties of finite\\nmonoids. Various modifications of this correspondence appeared, with more\\ngeneral classes of regular languages on one hand and classes of more complex\\nalgebraic structures on the other hand. For example, classes of languages need\\nnot be closed under complementation or all preimages under homomorphisms, while\\nmonoids can be equipped with a compatible order or they can have a\\ndistinguished set of generators. Such generalized varieties and pseudovarieties\\nalso have natural counterparts formed by classes of finite (ordered) automata.\\nIn this paper the previous approaches are combined. The notion of positive\\n$\\\\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final\\nstates are specified) is introduced and their correspondence with positive\\n$\\\\mathcal C$-varieties of languages is proved.\\n',\n",
       " '  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct\\nexperimental evidence for spontaneous vortex phase (SVP) formation in\\nEuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting\\n$T^{\\\\rm 0}_{\\\\rm SC}=23.6$~K and ferromagnetic $T_{\\\\rm FM}\\\\sim17.7$~K transition\\ntemperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the\\nvicinity of $T_{\\\\rm FM}$. Also, upon cooling cycle near $T_{\\\\rm FM}$ we observe\\nthe first-order transition from the short period domain structure, which\\nappears in the Meissner state, into the long period domain structure with\\nspontaneous vortices. It is the first experimental observation of this scenario\\nin the ferromagnetic superconductors. Low-temperature phase is characterized by\\nmuch larger domains in V-AV state and peculiar branched striped structures at\\nthe surface, which are typical for uniaxial ferromagnets with perpendicular\\nmagnetic anisotropy (PMA). The domain wall parameters at various temperatures\\nare estimated.\\n',\n",
       " '  The recent discovery that the exponent of matrix multiplication is determined\\nby the rank of the symmetrized matrix multiplication tensor has invigorated\\ninterest in better understanding symmetrized matrix multiplication. I present\\nan explicit rank 18 Waring decomposition of $sM_{\\\\langle 3\\\\rangle}$ and\\ndescribe its symmetry group.\\n',\n",
       " '  The process that leads to the formation of the bright star forming sites\\nobserved along prominent spiral arms remains elusive. We present results of a\\nmulti-wavelength study of a spiral arm segment in the nearby grand-design\\nspiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas\\nspurs. The combined observations of the(ionized, atomic, molecular, dusty)\\ninterstellar medium (ISM) with star formation tracers (HII regions, young\\n<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud\\n(GMC) properties between arm and gas spurs, (2) gas spurs and extinction\\nfeathers arising from the same structure with a close spatial relation between\\ngas spurs and ongoing/recent star formation (despite higher gas surface\\ndensities in the spiral arm), (3) no trend in star formation age either along\\nthe arm or along a spur, (4) evidence for strong star formation feedback in gas\\nspurs: (5) tentative evidence for star formation triggered by stellar feedback\\nfor one spur, and (6) GMC associations (GMAs) being no special entities but the\\nresult of blending of gas arm/spur cross-sections in lower resolution\\nobservations. We conclude that there is no evidence for a coherent star\\nformation onset mechanism that can be solely associated to the presence of the\\nspiral density wave. This suggests that other (more localized) mechanisms are\\nimportant to delay star formation such that it occurs in spurs. The evidence of\\nstar formation proceeding over several million years within individual spurs\\nimplies that the mechanism that leads to star formation acts or is sustained\\nover a longer time-scale.\\n',\n",
       " '  We describe a variant construction of the unstable Adams spectral the\\nsequence for a space $Y$, associated to any free simplicial resolution of\\n$H^*(Y;R)$ for $R=\\\\mathbb{F}_p$ or $\\\\mathbb{Q}$. We use this construction to\\ndescribe the differentials and filtration in the spectral sequence in terms of\\nappropriate systems of higher cohomology operations.\\n',\n",
       " '  When investigators seek to estimate causal effects, they often assume that\\nselection into treatment is based only on observed covariates. Under this\\nidentification strategy, analysts must adjust for observed confounders. While\\nbasic regression models have long been the dominant method of statistical\\nadjustment, more robust methods based on matching or weighting have become more\\ncommon. Of late, even more flexible methods based on machine learning methods\\nhave been developed for statistical adjustment. These machine learning methods\\nare designed to be black box methods with little input from the researcher.\\nRecent research used a data competition to evaluate various methods of\\nstatistical adjustment and found that black box methods out performed all other\\nmethods of statistical adjustment. Matching methods with covariate\\nprioritization are designed for direct input from substantive investigators in\\ndirect contrast to black methods. In this article, we use a different research\\ndesign to compare matching with covariate prioritization to black box methods.\\nWe use black box methods to replicate results from five studies where matching\\nwith covariate prioritization was used to customize the statistical adjustment\\nin direct response to substantive expertise. We find little difference across\\nthe methods. We conclude with advice for investigators.\\n',\n",
       " \"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the\\nthermoviscous wave equations (TWE) derived by transforming the linearized\\nNavier-Stokes equations (LNSE) to the frequency domain yields a so-called\\nHelmholtz solver, whose output is a discrete set of complex eigenfunction and\\neigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --\\nreverses such procedure by returning the value of acoustic impedance at one or\\nmore unknown impedance boundaries (IBs) of a given domain via spatial\\nintegration of the TWE for a given real-valued frequency with assigned\\nconditions on other boundaries. The iHS procedure is applied to a second-order\\nspatial discretization of the TWEs derived on an unstructured grid with\\nstaggered grid arrangement. The momentum equation only is extended to the\\ncenter of each IB face where pressure and velocity components are co-located\\nand treated as unknowns. One closure condition considered for the iHS is the\\nassignment of the surface gradient of pressure phase over the IBs,\\ncorresponding to assigning the shape of the acoustic waveform at the IB. The\\niHS procedure is carried out independently for each frequency in order to\\nreturn the complete broadband complex impedance distribution at the IBs in any\\ndesired frequency range. The iHS approach is first validated against Rott's\\ntheory for both inviscid and viscous, rectangular and circular ducts. The\\nimpedance of a geometrically complex toy cavity is then reconstructed and\\nverified against companion full compressible unstructured Navier-Stokes\\nsimulations resolving the cavity geometry and one-dimensional impedance test\\ntube calculations based on time-domain impedance boundary conditions (TDIBC).\\nThe iHS methodology is also shown to capture thermoacoustic effects, with\\nreconstructed impedance values quantitatively in agreement with thermoacoustic\\ngrowth rates.\\n\",\n",
       " '  The impact of random fluctuations on the dynamical behavior a complex\\nbiological systems is a longstanding issue, whose understanding would shed\\nlight on the evolutionary pressure that nature imposes on the intrinsic noise\\nlevels and would allow rationally designing synthetic networks with controlled\\nnoise. Using the Itō stochastic differential equation formalism, we performed\\nboth analytic and numerical analyses of several model systems containing\\ndifferent molecular species in contact with the environment and interacting\\nwith each other through mass-action kinetics. These systems represent for\\nexample biomolecular oligomerization processes, complex-breakage reactions,\\nsignaling cascades or metabolic networks. For chemical reaction networks with\\nzero deficiency values, which admit a detailed- or complex-balanced steady\\nstate, all molecular species are uncorrelated. The number of molecules of each\\nspecies follow a Poisson distribution and their Fano factors, which measure the\\nintrinsic noise, are equal to one. Systems with deficiency one have an\\nunbalanced non-equilibrium steady state and a non-zero S-flux, defined as the\\nflux flowing between the complexes multiplied by an adequate stoichiometric\\ncoefficient. In this case, the noise on each species is reduced if the flux\\nflows from the species of lowest to highest complexity, and is amplified is the\\nflux goes in the opposite direction. These results are generalized to systems\\nof deficiency two, which possess two independent non-vanishing S-fluxes, and we\\nconjecture that a similar relation holds for higher deficiency systems.\\n',\n",
       " '  Rare regions with weak disorder (Griffiths regions) have the potential to\\nspoil localization. We describe a non-perturbative construction of local\\nintegrals of motion (LIOMs) for a weakly interacting spin chain in one\\ndimension, under a physically reasonable assumption on the statistics of\\neigenvalues. We discuss ideas about the situation in higher dimensions, where\\none can no longer ensure that interactions involving the Griffiths regions are\\nmuch smaller than the typical energy-level spacing for such regions. We argue\\nthat ergodicity is restored in dimension d > 1, although equilibration should\\nbe extremely slow, similar to the dynamics of glasses.\\n',\n",
       " '  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB\\nfunctions for the analysis and solution of fault detection and model detection\\nproblems. The implemented functions are based on the computational procedures\\ndescribed in the Chapters 5, 6 and 7 of the book: \"A. Varga, Solving Fault\\nDiagnosis Problems - Linear Synthesis Techniques, Springer, 2017\". This\\ndocument is the User\\'s Guide for the version V1.0 of FDITOOLS. First, we\\npresent the mathematical background for solving several basic exact and\\napproximate synthesis problems of fault detection filters and model detection\\nfilters. Then, we give in-depth information on the command syntax of the main\\nanalysis and synthesis functions. Several examples illustrate the use of the\\nmain functions of FDITOOLS.\\n',\n",
       " '  Detectability of discrete event systems (DESs) is a question whether the\\ncurrent and subsequent states can be determined based on observations. Shu and\\nLin designed a polynomial-time algorithm to check strong (periodic)\\ndetectability and an exponential-time (polynomial-space) algorithm to check\\nweak (periodic) detectability. Zhang showed that checking weak (periodic)\\ndetectability is PSpace-complete. This intractable complexity opens a question\\nwhether there are structurally simpler DESs for which the problem is tractable.\\nIn this paper, we show that it is not the case by considering DESs represented\\nas deterministic finite automata without non-trivial cycles, which are\\nstructurally the simplest deadlock-free DESs. We show that even for such very\\nsimple DESs, checking weak (periodic) detectability remains intractable. On the\\ncontrary, we show that strong (periodic) detectability of DESs can be\\nefficiently verified on a parallel computer.\\n',\n",
       " \"  Let $X$ be a partially ordered set with the property that each family of\\norder intervals of the form $[a,b],[a,\\\\rightarrow )$ with the finite\\nintersection property has a nonempty intersection. We show that every directed\\nsubset of $X$ has a supremum. Then we apply the above result to prove that if\\n$X$ is a topological space with a partial order $\\\\preceq $ for which the order\\nintervals are compact, $\\\\mathcal{F}$ a nonempty commutative family of monotone\\nmaps from $X$ into $X$ and there exists $c\\\\in X$ such that $c\\\\preceq Tc$ for\\nevery $T\\\\in \\\\mathcal{F}$, then the set of common fixed points of $\\\\mathcal{F}$\\nis nonempty and has a maximal element. The result, specialized to the case of\\nBanach spaces gives a general fixed point theorem that drops almost all\\nassumptions from the recent results in this area. An application to the theory\\nof integral equations of Urysohn's type is also given.\\n\",\n",
       " '  Efficient methods are proposed, for computing integrals appeaing in\\nelectronic structure calculations. The methods consist of two parts: the first\\npart is to represent the integrals as contour integrals and the second one is\\nto evaluate the contour integrals by the Clenshaw-Curtis quadrature. The\\nefficiency of the proposed methods is demonstrated through numerical\\nexperiments.\\n',\n",
       " '  We present a novel sound localization algorithm for a non-line-of-sight\\n(NLOS) sound source in indoor environments. Our approach exploits the\\ndiffraction properties of sound waves as they bend around a barrier or an\\nobstacle in the scene. We combine a ray tracing based sound propagation\\nalgorithm with a Uniform Theory of Diffraction (UTD) model, which simulate\\nbending effects by placing a virtual sound source on a wedge in the\\nenvironment. We precompute the wedges of a reconstructed mesh of an indoor\\nscene and use them to generate diffraction acoustic rays to localize the 3D\\nposition of the source. Our method identifies the convergence region of those\\ngenerated acoustic rays as the estimated source position based on a particle\\nfilter. We have evaluated our algorithm in multiple scenarios consisting of a\\nstatic and dynamic NLOS sound source. In our tested cases, our approach can\\nlocalize a source position with an average accuracy error, 0.7m, measured by\\nthe L2 distance between estimated and actual source locations in a 7m*7m*3m\\nroom. Furthermore, we observe 37% to 130% improvement in accuracy over a\\nstate-of-the-art localization method that does not model diffraction effects,\\nespecially when a sound source is not visible to the robot.\\n',\n",
       " \"  In this paper we introduce the notion of $\\\\zeta$-crossbreeding in a set of\\n$\\\\zeta$-factorization formulas and also the notion of complete hybrid formula\\nas the final result of that crossbreeding. The last formula is used as a\\ncriterion for selection of families of $\\\\zeta$-kindred elements in class of\\nreal continuous functions.\\nDedicated to recalling of Gregory Mendel's pea-crossbreeding.\\n\",\n",
       " '  We consider the problem of estimating the $L_1$ distance between two discrete\\nprobability measures $P$ and $Q$ from empirical data in a nonasymptotic and\\nlarge alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,\\nwe show that for every $Q$, the minimax rate-optimal estimator with $n$ samples\\nachieves performance comparable to that of the maximum likelihood estimator\\n(MLE) with $n\\\\ln n$ samples. When both $P$ and $Q$ are unknown, we construct\\nminimax rate-optimal estimators whose worst case performance is essentially\\nthat of the known $Q$ case with $Q$ being uniform, implying that $Q$ being\\nuniform is essentially the most difficult case. The \\\\emph{effective sample size\\nenlargement} phenomenon, identified in Jiao \\\\emph{et al.} (2015), holds both in\\nthe known $Q$ case for every $Q$ and the $Q$ unknown case. However, the\\nconstruction of optimal estimators for $\\\\|P-Q\\\\|_1$ requires new techniques and\\ninsights beyond the approximation-based method of functional estimation in Jiao\\n\\\\emph{et al.} (2015).\\n',\n",
       " '  We investigate the density large deviation function for a multidimensional\\nconservation law in the vanishing viscosity limit, when the probability\\nconcentrates on weak solutions of a hyperbolic conservation law conservation\\nlaw. When the conductivity and dif-fusivity matrices are proportional, i.e. an\\nEinstein-like relation is satisfied, the problem has been solved in [4]. When\\nthis proportionality does not hold, we compute explicitly the large deviation\\nfunction for a step-like density profile, and we show that the associated\\noptimal current has a non trivial structure. We also derive a lower bound for\\nthe large deviation function, valid for a general weak solution, and leave the\\ngeneral large deviation function upper bound as a conjecture.\\n',\n",
       " '  Large deep neural networks are powerful, but exhibit undesirable behaviors\\nsuch as memorization and sensitivity to adversarial examples. In this work, we\\npropose mixup, a simple learning principle to alleviate these issues. In\\nessence, mixup trains a neural network on convex combinations of pairs of\\nexamples and their labels. By doing so, mixup regularizes the neural network to\\nfavor simple linear behavior in-between training examples. Our experiments on\\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\\nthat mixup improves the generalization of state-of-the-art neural network\\narchitectures. We also find that mixup reduces the memorization of corrupt\\nlabels, increases the robustness to adversarial examples, and stabilizes the\\ntraining of generative adversarial networks.\\n',\n",
       " \"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric\\nmeasure theory. There exist multiple variants of the original definition. Here\\nwe prove that most of them are indeed equal. One central point is to correct\\nthe proof of Brakke's §3.5, where he develops an estimate for the evolution\\nof the measure of time-dependent test functions.\\n\",\n",
       " '  With recent advancements in drone technology, researchers are now considering\\nthe possibility of deploying small cells served by base stations mounted on\\nflying drones. A major advantage of such drone small cells is that the\\noperators can quickly provide cellular services in areas of urgent demand\\nwithout having to pre-install any infrastructure. Since the base station is\\nattached to the drone, technically it is feasible for the base station to\\ndynamic reposition itself in response to the changing locations of users for\\nreducing the communication distance, decreasing the probability of signal\\nblocking, and ultimately increasing the spectral efficiency. In this paper, we\\nfirst propose distributed algorithms for autonomous control of drone movements,\\nand then model and analyse the spectral efficiency performance of a drone small\\ncell to shed new light on the fundamental benefits of dynamic repositioning. We\\nshow that, with dynamic repositioning, the spectral efficiency of drone small\\ncells can be increased by nearly 100\\\\% for realistic drone speed, height, and\\nuser traffic model and without incurring any major increase in drone energy\\nconsumption.\\n',\n",
       " '  Electronic health records (EHR) contain a large variety of information on the\\nclinical history of patients such as vital signs, demographics, diagnostic\\ncodes and imaging data. The enormous potential for discovery in this rich\\ndataset is hampered by its complexity and heterogeneity.\\nWe present the first study to assess unsupervised homogenization pipelines\\ndesigned for EHR clustering. To identify the optimal pipeline, we tested\\naccuracy on simulated data with varying amounts of redundancy, heterogeneity,\\nand missingness. We identified two optimal pipelines: 1) Multiple Imputation by\\nChained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,\\nZ-scoring, and Deep Autoencoders.\\n',\n",
       " '  Artificial Neural Network computation relies on intensive vector-matrix\\nmultiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array\\nshowed a feasibility of implementing such operations with high energy\\nefficiency, thus there are many works on efficiently utilizing emerging NVM\\ncrossbar array as analog vector-matrix multiplier. However, its nonlinear I-V\\ncharacteristics restrain critical design parameters, such as the read voltage\\nand weight range, resulting in substantial accuracy loss. In this paper,\\ninstead of optimizing hardware parameters to a given neural network, we propose\\na methodology of reconstructing a neural network itself optimized to resistive\\nmemory crossbar arrays. To verify the validity of the proposed method, we\\nsimulated various neural network with MNIST and CIFAR-10 dataset using two\\ndifferent specific Resistive Random Access Memory (RRAM) model. Simulation\\nresults show that our proposed neural network produces significantly higher\\ninference accuracies than conventional neural network when the synapse devices\\nhave nonlinear I-V characteristics.\\n',\n",
       " '  In this work, we establish a full single-letter characterization of the\\nrate-distortion region of an instance of the Gray-Wyner model with side\\ninformation at the decoders. Specifically, in this model an encoder observes a\\npair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and\\ncommunicates with two receivers over an error-free rate-limited link of\\ncapacity $R_0$, as well as error-free rate-limited individual links of\\ncapacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both\\nreceivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$\\nalso reproduces the source component $S^n_1$ lossily, to within some prescribed\\nfidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped\\nrespectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.\\nImportant in this setup, the side information sequences are arbitrarily\\ncorrelated among them, and with the source pair $(S^n_1,S^n_2)$; and are not\\nassumed to exhibit any particular ordering. Furthermore, by specializing the\\nmain result to two Heegard-Berger models with successive refinement and\\nscalable coding, we shed light on the roles of the common and private\\ndescriptions that the encoder should produce and what they should carry\\noptimally. We develop intuitions by analyzing the developed single-letter\\noptimal rate-distortion regions of these models, and discuss some insightful\\nbinary examples.\\n',\n",
       " '  This work discusses the numerical approximation of a nonlinear\\nreaction-advection-diffusion equation, which is a dimensionless form of the\\nWeertman equation. This equation models steadily-moving dislocations in\\nmaterials science. It reduces to the celebrated Peierls-Nabarro equation when\\nits advection term is set to zero. The approach rests on considering a\\ntime-dependent formulation, which admits the equation under study as its\\nlong-time limit. Introducing a Preconditioned Collocation Scheme based on\\nFourier transforms, the iterative numerical method presented solves the\\ntime-dependent problem, delivering at convergence the desired numerical\\nsolution to the Weertman equation. Although it rests on an explicit\\ntime-evolution scheme, the method allows for large time steps, and captures the\\nsolution in a robust manner. Numerical results illustrate the efficiency of the\\napproach for several types of nonlinearities.\\n',\n",
       " '  There are many web-based visualization systems available to date, each having\\nits strengths and limitations. The goals these systems set out to accomplish\\ninfluence design decisions and determine how reusable and scalable they are.\\nWeave is a new web-based visualization platform with the broad goal of enabling\\nvisualization of any available data by anyone for any purpose. Our open source\\nframework supports highly interactive linked visualizations for users of\\nvarying skill levels. What sets Weave apart from other systems is its\\nconsideration for real-time remote collaboration with session history. We\\nprovide a detailed account of the various framework designs we considered with\\ncomparisons to existing state-of-the-art systems.\\n',\n",
       " '  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using\\narchival multi-wavelength data. The Suzaku spectra are well described by\\ntwo-component thermal plasma models: The soft component is in ionization\\nequilibrium and has a temperature $\\\\sim$0.59 keV, while the hard component has\\ntemperature $\\\\sim$3.2 keV and ionization time-scale $\\\\sim$$2.6\\\\times10^{10}$\\ncm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\\\\sim$6.5 keV\\nfrom this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the\\nX-ray emission has an ejecta origin. The centroid energy of the Fe-K line\\nsupports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than\\na core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its\\nsurrounding were analyzed using about 6 years of Fermi data. We report about\\nthe non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray\\nsource in the south-west of G306.3$-$0.9 with a significance of\\n$\\\\sim$13$\\\\sigma$. We discuss several scenarios for these results with the help\\nof data from other wavebands to understand the SNR and its neighborhood.\\n',\n",
       " '  Previous approaches to training syntax-based sentiment classification models\\nrequired phrase-level annotated corpora, which are not readily available in\\nmany languages other than English. Thus, we propose the use of tree-structured\\nLong Short-Term Memory with an attention mechanism that pays attention to each\\nsubtree of the parse tree. Experimental results indicate that our model\\nachieves the state-of-the-art performance in a Japanese sentiment\\nclassification task.\\n',\n",
       " '  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior\\ninference technique that is increasingly popular due to its fast runtimes on\\nlarge-scale datasets. However, even when MFVB provides accurate posterior means\\nfor certain parameters, it often mis-estimates variances and covariances.\\nFurthermore, prior robustness measures have remained undeveloped for MFVB. By\\nderiving a simple formula for the effect of infinitesimal model perturbations\\non MFVB posterior means, we provide both improved covariance estimates and\\nlocal robustness measures for MFVB, thus greatly expanding the practical\\nusefulness of MFVB posterior approximations. The estimates for MFVB posterior\\ncovariances rely on a result from the classical Bayesian robustness literature\\nrelating derivatives of posterior expectations to posterior covariances and\\ninclude the Laplace approximation as a special case. Our key condition is that\\nthe MFVB approximation provides good estimates of a select subset of posterior\\nmeans---an assumption that has been shown to hold in many practical settings.\\nIn our experiments, we demonstrate that our methods are simple, general, and\\nfast, providing accurate posterior uncertainty estimates and robustness\\nmeasures with runtimes that can be an order of magnitude faster than MCMC.\\n',\n",
       " '  In this paper, we empirically study models for pricing Italian sovereign\\nbonds under a reduced form framework, by assuming different dynamics for the\\nshort-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek\\nmulti-factor models, with a focus on optimization algorithms applied in the\\ncalibration exercise. The Kalman filter algorithm together with a maximum\\nlikelihood estimation method are considered to fit the Italian term-structure\\nover a 12-year horizon, including the global financial crisis and the euro area\\nsovereign debt crisis. Analytic formulas for the gradient vector and the\\nHessian matrix of the likelihood function are provided.\\n',\n",
       " '  Ballistic point contact (BPC) with zigzag edges in graphene is a main\\ncandidate of a valley filter, in which the polarization of the valley degree of\\nfreedom can be selected by using a local gate voltage. Here, we propose to\\ndetect the valley filtering effect by Andreev reflection. Because electrons in\\nthe lowest conduction band and the highest valence band of the BPC possess\\nopposite chirality, the inter-band Andreev reflection is strongly suppressed,\\nafter multiple scattering and interference. We draw this conclusion by both the\\nscattering matrix analysis and the numerical simulation. The Andreev reflection\\nas a function of the incident energy of electrons and the local gate voltage at\\nthe BPC is obtained, by which the parameter region for a perfect valley filter\\nand the direction of valley polarization can be determined. The Andreev\\nreflection exhibits an oscillatory decay with the length of the BPC, indicating\\na negative correlation to valley polarization.\\n',\n",
       " '  Sparse superposition (SS) codes were originally proposed as a\\ncapacity-achieving communication scheme over the additive white Gaussian noise\\nchannel (AWGNC) [1]. Very recently, it was discovered that these codes are\\nuniversal, in the sense that they achieve capacity over any memoryless channel\\nunder generalized approximate message-passing (GAMP) decoding [2], although\\nthis decoder has never been stated for SS codes. In this contribution we\\nintroduce the GAMP decoder for SS codes, we confirm empirically the\\nuniversality of this communication scheme through its study on various channels\\nand we provide the main analysis tools: state evolution and potential. We also\\ncompare the performance of GAMP with the Bayes-optimal MMSE decoder. We\\nempirically illustrate that despite the presence of a phase transition\\npreventing GAMP to reach the optimal performance, spatial coupling allows to\\nboost the performance that eventually tends to capacity in a proper limit. We\\nalso prove that, in contrast with the AWGNC case, SS codes for binary input\\nchannels have a vanishing error floor in the limit of large codewords.\\nMoreover, the performance of Hadamard-based encoders is assessed for practical\\nimplementations.\\n',\n",
       " '  When developing general purpose robots, the overarching software architecture\\ncan greatly affect the ease of accomplishing various tasks. Initial efforts to\\ncreate unified robot systems in the 1990s led to hybrid architectures,\\nemphasizing a hierarchy in which deliberative plans direct the use of reactive\\nskills. However, since that time there has been significant progress in the\\nlow-level skills available to robots, including manipulation and perception,\\nmaking it newly feasible to accomplish many more tasks in real-world domains.\\nThere is thus renewed optimism that robots will be able to perform a wide array\\nof tasks while maintaining responsiveness to human operators. However, the top\\nlayer in traditional hybrid architectures, designed to achieve long-term goals,\\ncan make it difficult to react quickly to human interactions during goal-driven\\nexecution. To mitigate this difficulty, we propose a novel architecture that\\nsupports such transitions by adding a top-level reactive module which has\\nflexible access to both reactive skills and a deliberative control module. To\\nvalidate this architecture, we present a case study of its application on a\\ndomestic service robot platform.\\n',\n",
       " '  We propose an approach to estimate 3D human pose in real world units from a\\nsingle RGBD image and show that it exceeds performance of monocular 3D pose\\nestimation approaches from color as well as pose estimation exclusively from\\ndepth. Our approach builds on robust human keypoint detectors for color images\\nand incorporates depth for lifting into 3D. We combine the system with our\\nlearning from demonstration framework to instruct a service robot without the\\nneed of markers. Experiments in real world settings demonstrate that our\\napproach enables a PR2 robot to imitate manipulation actions observed from a\\nhuman teacher.\\n',\n",
       " '  We extend the work of Fouvry, Kowalski and Michel on correlation between\\nHecke eigenvalues of modular forms and algebraic trace functions in order to\\nestablish an asymptotic formula for a generalized cubic moment of modular\\nL-functions at the central point s = 1/2 and for prime moduli q. As an\\napplication, we exploit our recent result on the mollification of the fourth\\nmoment of Dirichlet L-functions to derive that for any pair\\n$(\\\\omega_1,\\\\omega_2)$ of multiplicative characters modulo q, there is a\\npositive proportion of $\\\\chi$ (mod q) such that $L(\\\\chi, 1/2 ), L(\\\\chi\\\\omega_1,\\n1/2 )$ and $L(\\\\chi\\\\omega_2, 1/2)$ are simultaneously not too small.\\n',\n",
       " '  Nonclassical states of a quantized light are described in terms of\\nGlauber-Sudarshan P distribution which is not a genuine classical probability\\ndistribution. Despite several attempts, defining a uniform measure of\\nnonclassicality (NC) for the single mode quantum states of light is yet an open\\ntask. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that\\nthe existing well-known measures fail to quantify the NC of single mode states\\nthat are generated under multiple NC-inducing operations. Recently, Ivan et.\\nal. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of\\nnon-Gaussian character of quantum optical states in terms of Wehrl entropy.\\nHere, we adopt this concept in the context of single mode NC. In this paper, we\\npropose a new quantification of NC for the single mode quantum states of light\\nas the difference between the total Wehrl entropy of the state and the maximum\\nWehrl entropy arising due to its classical characteristics. This we achieve by\\nsubtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any\\nclassical state that has same randomness as measured in terms of von-Neumann\\nentropy. We obtain analytic expressions of NC for most of the states, in\\nparticular, all pure states and Gaussian mixed states. However, the evaluation\\nof NC for the non-Gaussian mixed states is subject to extensive numerical\\ncomputation that lies beyond the scope of the current work. We show that, along\\nwith the states generated under single NC-inducing operations, also for the\\nbroader class of states that are generated under multiple NC-inducing\\noperations, our quantification enumerates the NC consistently.\\n',\n",
       " '  Following the recent progress in image classification and captioning using\\ndeep learning, we develop a novel natural language person retrieval system\\nbased on an attention mechanism. More specifically, given the description of a\\nperson, the goal is to localize the person in an image. To this end, we first\\nconstruct a benchmark dataset for natural language person retrieval. To do so,\\nwe generate bounding boxes for persons in a public image dataset from the\\nsegmentation masks, which are then annotated with descriptions and attributes\\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\\nFaster R-CNN as a candidate region generator. The cropped images based on the\\nregion proposals as well as the whole images with attention weights are fed\\ninto Convolutional Neural Networks for visual feature extraction, while the\\nnatural language expression and attributes are input to Bidirectional Long\\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\\ntext features are integrated to score region proposals, and the one with the\\nhighest score is retrieved as the output of our system. The experimental\\nresults show significant improvement over the state-of-the-art method for\\ngeneric object retrieval and this line of research promises to benefit search\\nin surveillance video footage.\\n',\n",
       " '  Real time large scale streaming data pose major challenges to forecasting, in\\nparticular defying the presence of human experts to perform the corresponding\\nanalysis. We present here a class of models and methods used to develop an\\nautomated, scalable and versatile system for large scale forecasting oriented\\ntowards safety and security monitoring. Our system provides short and long term\\nforecasts and uses them to detect safety and security issues in relation with\\nmultiple internet connected devices well in advance they might take place.\\n',\n",
       " '  Machine learning algorithms such as linear regression, SVM and neural network\\nhave played an increasingly important role in the process of scientific\\ndiscovery. However, none of them is both interpretable and accurate on\\nnonlinear datasets. Here we present contextual regression, a method that joins\\nthese two desirable properties together using a hybrid architecture of neural\\nnetwork embedding and dot product layer. We demonstrate its high prediction\\naccuracy and sensitivity through the task of predictive feature selection on a\\nsimulated dataset and the application of predicting open chromatin sites in the\\nhuman genome. On the simulated data, our method achieved high fidelity recovery\\nof feature contributions under random noise levels up to 200%. On the open\\nchromatin dataset, the application of our method not only outperformed the\\nstate of the art method in terms of accuracy, but also unveiled two previously\\nunfound open chromatin related histone marks. Our method can fill the blank of\\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\\n',\n",
       " '  We consider multi-time correlators for output signals from linear detectors,\\ncontinuously measuring several qubit observables at the same time. Using the\\nquantum Bayesian formalism, we show that for unital (symmetric) evolution in\\nthe absence of phase backaction, an $N$-time correlator can be expressed as a\\nproduct of two-time correlators when $N$ is even. For odd $N$, there is a\\nsimilar factorization, which also includes a single-time average. Theoretical\\npredictions agree well with experimental results for two detectors, which\\nsimultaneously measure non-commuting qubit observables.\\n',\n",
       " '  Constraint Handling Rules is an effective concurrent declarative programming\\nlanguage and a versatile computational logic formalism. CHR programs consist of\\nguarded reactive rules that transform multisets of constraints. One of the main\\nfeatures of CHR is its inherent concurrency. Intuitively, rules can be applied\\nto parts of a multiset in parallel. In this comprehensive survey, we give an\\noverview of concurrent and parallel as well as distributed CHR semantics,\\nstandard and more exotic, that have been proposed over the years at various\\nlevels of refinement. These semantics range from the abstract to the concrete.\\nThey are related by formal soundness results. Their correctness is established\\nas correspondence between parallel and sequential computations. We present\\ncommon concise sample CHR programs that have been widely used in experiments\\nand benchmarks. We review parallel CHR implementations in software and\\nhardware. The experimental results obtained show a consistent parallel speedup.\\nMost implementations are available online. The CHR formalism can also be used\\nto implement and reason with models for concurrency. To this end, the Software\\nTransaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus\\nhave been faithfully encoded in CHR. Under consideration in Theory and Practice\\nof Logic Programming (TPLP).\\n',\n",
       " '  Many people are suffering from voice disorders, which can adversely affect\\nthe quality of their lives. In response, some researchers have proposed\\nalgorithms for automatic assessment of these disorders, based on voice signals.\\nHowever, these signals can be sensitive to the recording devices. Indeed, the\\nchannel effect is a pervasive problem in machine learning for healthcare. In\\nthis study, we propose a detection system for pathological voice, which is\\nrobust against the channel effect. This system is based on a bidirectional LSTM\\nnetwork. To increase the performance robustness against channel mismatch, we\\nintegrate domain adversarial training (DAT) to eliminate the differences\\nbetween the devices. When we train on data recorded on a high-quality\\nmicrophone and evaluate on smartphone data without labels, our robust detection\\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\\nsample labels). To the best of our knowledge, this is the first study applying\\nunsupervised domain adaptation to pathological voice detection. Notably, our\\nsystem does not need target device sample labels, which allows for\\ngeneralization to many new devices.\\n',\n",
       " '  Computing a basis for the exponent lattice of algebraic numbers is a basic\\nproblem in the field of computational number theory with applications to many\\nother areas. The main cost of a well-known algorithm\\n\\\\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on\\ncomputing the primitive element of the extended field generated by the given\\nalgebraic numbers. When the extended field is of large degree, the problem\\nseems intractable by the tool implementing the algorithm. In this paper, a\\nspecial kind of exponent lattice basis is introduced. An important feature of\\nthe basis is that it can be inductively constructed, which allows us to deal\\nwith the given algebraic numbers one by one when computing the basis. Based on\\nthis, an effective framework for constructing exponent lattice basis is\\nproposed. Through computing a so-called pre-basis first and then solving some\\nlinear Diophantine equations, the basis can be efficiently constructed. A new\\ncertificate for multiplicative independence and some techniques for decreasing\\ndegrees of algebraic numbers are provided to speed up the computation. The new\\nalgorithm has been implemented with Mathematica and its effectiveness is\\nverified by testing various examples. Moreover, the algorithm is applied to\\nprogram verification for finding invariants of linear loops.\\n',\n",
       " '  Investigating the emergence of a particular cell type is a recurring theme in\\nmodels of growing cellular populations. The evolution of resistance to therapy\\nis a classic example. Common questions are: when does the cell type first\\noccur, and via which sequence of steps is it most likely to emerge? For growing\\npopulations, these questions can be formulated in a general framework of\\nbranching processes spreading through a graph from a root to a target vertex.\\nCells have a particular fitness value on each vertex and can transition along\\nedges at specific rates. Vertices represents cell states, say \\\\mic{genotypes\\n}or physical locations, while possible transitions are acquiring a mutation or\\ncell migration. We focus on the setting where cells at the root vertex have the\\nhighest fitness and transition rates are small. Simple formulas are derived for\\nthe time to reach the target vertex and for the probability that it is reached\\nalong a given path in the graph. We demonstrate our results on \\\\mic{several\\nscenarios relevant to the emergence of drug resistance}, including: the\\norderings of resistance-conferring mutations in bacteria and the impact of\\nimperfect drug penetration in cancer.\\n',\n",
       " '  Stimuli-responsive materials that modify their shape in response to changes\\nin environmental conditions -- such as solute concentration, temperature, pH,\\nand stress -- are widespread in nature and technology. Applications include\\nmicro- and nanoporous materials used in filtration and flow control. The\\nphysiochemical mechanisms that induce internal volume modifications have been\\nwidely studies. The coupling between induced volume changes and solute\\ntransport through porous materials, however, is not well understood. Here, we\\nconsider advective and diffusive transport through a small channel linking two\\nlarge reservoirs. A section of stimulus-responsive material regulates the\\nchannel permeability, which is a function of the local solute concentration. We\\nderive an exact solution to the coupled transport problem and demonstrate the\\nexistence of a flow regime in which the steady state is reached via a damped\\noscillation around the equilibrium concentration value. Finally, the\\nfeasibility of an experimental observation of the phenomena is discussed.\\nPlease note that this version of the paper has not been formally peer reviewed,\\nrevised or accepted by a journal.\\n',\n",
       " \"  Today's landscape of robotics is dominated by vertical integration where\\nsingle vendors develop the final product leading to slow progress, expensive\\nproducts and customer lock-in. Opposite to this, an horizontal integration\\nwould result in a rapid development of cost-effective mass-market products with\\nan additional consumer empowerment. The transition of an industry from vertical\\nintegration to horizontal integration is typically catalysed by de facto\\nindustry standards that enable a simplified and seamless integration of\\nproducts. However, in robotics there is currently no leading candidate for a\\nglobal plug-and-play standard.\\nThis paper tackles the problem of incompatibility between robot components\\nthat hinder the reconfigurability and flexibility demanded by the robotics\\nindustry. Particularly, it presents a model to create plug-and-play robot\\nhardware components. Rather than iteratively evolving previous ontologies, our\\nproposed model answers the needs identified by the industry while facilitating\\ninteroperability, measurability and comparability of robotics technology. Our\\napproach differs significantly with the ones presented before as it is\\nhardware-oriented and establishes a clear set of actions towards the\\nintegration of this model in real environments and with real manufacturers.\\n\",\n",
       " '  Machine learning models, especially based on deep architectures are used in\\neveryday applications ranging from self driving cars to medical diagnostics. It\\nhas been shown that such models are dangerously susceptible to adversarial\\nsamples, indistinguishable from real samples to human eye, adversarial samples\\nlead to incorrect classifications with high confidence. Impact of adversarial\\nsamples is far-reaching and their efficient detection remains an open problem.\\nWe propose to use direct density ratio estimation as an efficient model\\nagnostic measure to detect adversarial samples. Our proposed method works\\nequally well with single and multi-channel samples, and with different\\nadversarial sample generation methods. We also propose a method to use density\\nratio estimates for generating adversarial samples with an added constraint of\\npreserving density ratio.\\n',\n",
       " '  We study the query complexity of cake cutting and give lower and upper bounds\\nfor computing approximately envy-free, perfect, and equitable allocations with\\nthe minimum number of cuts. The lower bounds are tight for computing connected\\nenvy-free allocations among n=3 players and for computing perfect and equitable\\nallocations with minimum number of cuts between n=2 players.\\nWe also formalize moving knife procedures and show that a large subclass of\\nthis family, which captures all the known moving knife procedures, can be\\nsimulated efficiently with arbitrarily small error in the Robertson-Webb query\\nmodel.\\n',\n",
       " \"  This paper studies the emotion recognition from musical tracks in the\\n2-dimensional valence-arousal (V-A) emotional space. We propose a method based\\non convolutional (CNN) and recurrent neural networks (RNN), having\\nsignificantly fewer parameters compared with the state-of-the-art method for\\nthe same task. We utilize one CNN layer followed by two branches of RNNs\\ntrained separately for arousal and valence. The method was evaluated using the\\n'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\\narousal and 0.268 for valence, which is the best result reported on this\\ndataset.\\n\",\n",
       " '  We consider previous models of Timed, Probabilistic and Stochastic Timed\\nAutomata, we introduce our model of Timed Automata with Polynomial Delay and we\\ncharacterize the expressiveness of these models relative to each other.\\n',\n",
       " '  We present muon spin rotation measurements on superconducting Cu intercalated\\nBi$_2$Se$_3$, which was suggested as a realization of a topological\\nsuperconductor. We observe a clear evidence of the superconducting transition\\nbelow 4 K, where the width of magnetic field distribution increases as the\\ntemperature is decreased. The measured broadening at mK temperatures suggests a\\nlarge London penetration depth in the $ab$ plane ($\\\\lambda_{\\\\mathrm{eff}}\\\\sim\\n1.6$ $\\\\mathrm{\\\\mu}$m). We show that the temperature dependence of this\\nbroadening follows the BCS prediction, but could be consistent with several gap\\nsymmetries.\\n',\n",
       " '  Here we reveal details of the interaction between human lysozyme proteins,\\nboth native and fibrils, and their water environment by intense terahertz time\\ndomain spectroscopy. With the aid of a rigorous dielectric model, we determine\\nthe amplitude and phase of the oscillating dipole induced by the THz field in\\nthe volume containing the protein and its hydration water. At low\\nconcentrations, the amplitude of this induced dipolar response decreases with\\nincreasing concentration. Beyond a certain threshold, marking the onset of the\\ninteractions between the extended hydration shells, the amplitude remains fixed\\nbut the phase of the induced dipolar response, which is initially in phase with\\nthe applied THz field, begins to change. The changes observed in the THz\\nresponse reveal protein-protein interactions me-diated by extended hydration\\nlayers, which may control fibril formation and may have an important role in\\nchemical recognition phenomena.\\n',\n",
       " \"  We report on experimentally measured light shifts of superconducting flux\\nqubits deep-strongly coupled to LC oscillators, where the coupling constants\\nare comparable to the qubit and oscillator resonance frequencies. By using\\ntwo-tone spectroscopy, the energies of the six lowest levels of each circuit\\nare determined. We find huge Lamb shifts that exceed 90% of the bare qubit\\nfrequencies and inversions of the qubits' ground and excited states when there\\nare a finite number of photons in the oscillator. Our experimental results\\nagree with theoretical predictions based on the quantum Rabi model.\\n\",\n",
       " '  We describe a novel weakly supervised deep learning framework that combines\\nboth the discriminative and generative models to learn meaningful\\nrepresentation in the multiple instance learning (MIL) setting. MIL is a weakly\\nsupervised learning problem where labels are associated with groups of\\ninstances (referred as bags) instead of individual instances. To address the\\nessential challenge in MIL problems raised from the uncertainty of positive\\ninstances label, we use a discriminative model regularized by variational\\nautoencoders (VAEs) to maximize the differences between latent representations\\nof all instances and negative instances. As a result, the hidden layer of the\\nvariational autoencoder learns meaningful representation. This representation\\ncan effectively be used for MIL problems as illustrated by better performance\\non the standard benchmark datasets comparing to the state-of-the-art\\napproaches. More importantly, unlike most related studies, the proposed\\nframework can be easily scaled to large dataset problems, as illustrated by the\\naudio event detection and segmentation task. Visualization also confirms the\\neffectiveness of the latent representation in discriminating positive and\\nnegative classes.\\n',\n",
       " '  We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,\\nconfirming a conjecture of Berman.\\n',\n",
       " '  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary\\n$X$. Assume that $\\\\overline M$ admits a holomorphic $S^1$-action preserving the\\nboundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that\\nthe $\\\\overline\\\\partial$-Neumann Laplacian on $M$ is transversally elliptic and\\nas a consequence, the $m$-th Fourier component of the $q$-th Dolbeault\\ncohomology group $H^q_m(\\\\overline M)$ is finite dimensional, for every\\n$m\\\\in\\\\mathbb Z$ and every $q=0,1,\\\\ldots,n$. This enables us to define\\n$\\\\sum^{n}_{j=0}(-1)^j{\\\\rm dim\\\\,}H^q_m(\\\\overline M)$ the $m$-th Fourier\\ncomponent of the Euler characteristic on $M$ and to study large $m$-behavior of\\n$H^q_m(\\\\overline M)$. In this paper, we establish an index formula for\\n$\\\\sum^{n}_{j=0}(-1)^j{\\\\rm dim\\\\,}H^q_m(\\\\overline M)$ and Morse inequalities for\\n$H^q_m(\\\\overline M)$.\\n',\n",
       " '  Reinforcement learning methods require careful design involving a reward\\nfunction to obtain the desired action policy for a given task. In the absence\\nof hand-crafted reward functions, prior work on the topic has proposed several\\nmethods for reward estimation by using expert state trajectories and action\\npairs. However, there are cases where complete or good action information\\ncannot be obtained from expert demonstrations. We propose a novel reinforcement\\nlearning method in which the agent learns an internal model of observation on\\nthe basis of expert-demonstrated state trajectories to estimate rewards without\\ncompletely learning the dynamics of the external environment from state-action\\npairs. The internal model is obtained in the form of a predictive model for the\\ngiven expert state distribution. During reinforcement learning, the agent\\npredicts the reward as a function of the difference between the actual state\\nand the state predicted by the internal model. We conducted multiple\\nexperiments in environments of varying complexity, including the Super Mario\\nBros and Flappy Bird games. We show our method successfully trains good\\npolicies directly from expert game-play videos.\\n',\n",
       " '  In this paper we are interested in the class of n-ary operations on an\\narbitrary chain that are quasitrivial, symmetric, nondecreasing, and\\nassociative. We first provide a description of these operations. We then prove\\nthat associativity can be replaced with bisymmetry in the definition of this\\nclass. Finally we investigate the special situation where the chain is finite.\\n',\n",
       " '  We propose a new multivariate dependency measure. It is obtained by\\nconsidering a Gaussian kernel based distance between the copula transform of\\nthe given d-dimensional distribution and the uniform copula and then\\nappropriately normalizing it. The resulting measure is shown to satisfy a\\nnumber of desirable properties. A nonparametric estimate is proposed for this\\ndependency measure and its properties (finite sample as well as asymptotic) are\\nderived. Some comparative studies of the proposed dependency measure estimate\\nwith some widely used dependency measure estimates on artificial datasets are\\nincluded. A non-parametric test of independence between two or more random\\nvariables based on this measure is proposed. A comparison of the proposed test\\nwith some existing nonparametric multivariate test for independence is\\npresented.\\n',\n",
       " '  The pyrochlore metal Cd2Re2O7 has been recently investigated by\\nsecond-harmonic generation (SHG) reflectivity. In this paper, we develop a\\ngeneral formalism that allows for the identification of the relevant tensor\\ncomponents of the SHG from azimuthal scans. We demonstrate that the secondary\\norder parameter identified by SHG at the structural phase transition is the\\nx2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2\\nsymmetry of the atomic displacements associated with the I-4m2 crystal\\nstructure that was previously thought to be its origin. Within the same\\nformalism, we suggest that the primary order parameter detected in the SHG\\nexperiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the\\ngeneral mechanism driving the phase transition in our proposed framework, and\\nsuggest experiments, particularly resonant X-ray scattering ones, that could\\nclarify this issue.\\n',\n",
       " '  In evolutionary biology, the speciation history of living organisms is\\nrepresented graphically by a phylogeny, that is, a rooted tree whose leaves\\ncorrespond to current species and branchings indicate past speciation events.\\nPhylogenies are commonly estimated from molecular sequences, such as DNA\\nsequences, collected from the species of interest. At a high level, the idea\\nbehind this inference is simple: the further apart in the Tree of Life are two\\nspecies, the greater is the number of mutations to have accumulated in their\\ngenomes since their most recent common ancestor. In order to obtain accurate\\nestimates in phylogenetic analyses, it is standard practice to employ\\nstatistical approaches based on stochastic models of sequence evolution on a\\ntree. For tractability, such models necessarily make simplifying assumptions\\nabout the evolutionary mechanisms involved. In particular, commonly omitted are\\ninsertions and deletions of nucleotides -- also known as indels.\\nProperly accounting for indels in statistical phylogenetic analyses remains a\\nmajor challenge in computational evolutionary biology. Here we consider the\\nproblem of reconstructing ancestral sequences on a known phylogeny in a model\\nof sequence evolution incorporating nucleotide substitutions, insertions and\\ndeletions, specifically the classical TKF91 process. We focus on the case of\\ndense phylogenies of bounded height, which we refer to as the taxon-rich\\nsetting, where statistical consistency is achievable. We give the first\\npolynomial-time ancestral reconstruction algorithm with provable guarantees\\nunder constant rates of mutation. Our algorithm succeeds when the phylogeny\\nsatisfies the \"big bang\" condition, a necessary and sufficient condition for\\nstatistical consistency in this context.\\n',\n",
       " '  Subject of research is complex networks and network systems. The network\\nsystem is defined as a complex network in which flows are moved. Classification\\nof flows in the network is carried out on the basis of ordering and continuity.\\nIt is shown that complex networks with different types of flows generate\\nvarious network systems. Flow analogues of the basic concepts of the theory of\\ncomplex networks are introduced and the main problems of this theory in terms\\nof flow characteristics are formulated. Local and global flow characteristics\\nof networks bring closer the theory of complex networks to the systems theory\\nand systems analysis. Concept of flow core of network system is introduced and\\ndefined how it simplifies the process of its investigation. Concepts of kernel\\nand flow core of multiplex are determined. Features of operation of multiplex\\ntype systems are analyzed.\\n',\n",
       " '  We study the effect of domain growth on the orientation of striped phases in\\na Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter\\ndependence that allows stripe formation in a half plane, and suppresses\\npatterns in the complement, while the boundary of the pattern-forming region is\\npropagating with fixed normal velocity. We construct front solutions that leave\\nbehind stripes in the pattern-forming region that are parallel to or at a small\\noblique angle to the boundary.\\nTechnically, the construction of stripe formation parallel to the boundary\\nrelies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at\\na small oblique angle are constructed using a functional-analytic, perturbative\\napproach. Here, the main difficulties are the presence of continuous spectrum\\nand the fact that small oblique angles appear as a singular perturbation in a\\ntraveling-wave problem. We resolve the former difficulty using a farfield-core\\ndecomposition and Fredholm theory in weighted spaces. The singular perturbation\\nproblem is resolved using preconditioners and boot-strapping.\\n',\n",
       " '  This paper discusses minimum distance estimation method in the linear\\nregression model with dependent errors which are strongly mixing. The\\nregression parameters are estimated through the minimum distance estimation\\nmethod, and asymptotic distributional properties of the estimators are\\ndiscussed. A simulation study compares the performance of the minimum distance\\nestimator with other well celebrated estimator. This simulation study shows the\\nsuperiority of the minimum distance estimator over another estimator. KoulMde\\n(R package) which was used for the simulation study is available online. See\\nsection 4 for the detail.\\n',\n",
       " '  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,\\nby installing small cloud infrastructures at the network edge. This enables a\\nnew breed of real-time applications, such as instantaneous object recognition\\nand safety assistance in intelligent transportation systems, that require very\\nlow latency. One key issue that comes with proximity is how to ensure that\\nusers always receive good performance as they move across different locations.\\nMigrating services between MECs is seen as the means to achieve this. This\\narticle presents a layered framework for migrating active service applications\\nthat are encapsulated either in virtual machines (VMs) or containers. This\\nlayering approach allows a substantial reduction in service downtime. The\\nframework is easy to implement using readily available technologies, and one of\\nits key advantages is that it supports containers, which is a promising\\nemerging technology that offers tangible benefits over VMs. The migration\\nperformance of various real applications is evaluated by experiments under the\\npresented framework. Insights drawn from the experimentation results are\\ndiscussed.\\n',\n",
       " '  Analog black/white hole pairs, consisting of a region of supersonic flow,\\nhave been achieved in a recent experiment by J. Steinhauer using an elongated\\nBose-Einstein condensate. A growing standing density wave, and a checkerboard\\nfeature in the density-density correlation function, were observed in the\\nsupersonic region. We model the density-density correlation function, taking\\ninto account both quantum fluctuations and the shot-to-shot variation of atom\\nnumber normally present in ultracold-atom experiments. We find that quantum\\nfluctuations alone produce some, but not all, of the features of the\\ncorrelation function, whereas atom-number fluctuation alone can produce all the\\nobserved features, and agreement is best when both are included. In both cases,\\nthe density-density correlation is not intrinsic to the fluctuations, but\\nrather is induced by modulation of the standing wave caused by the\\nfluctuations.\\n',\n",
       " '  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and\\nlet $K_{\\\\infty}/K$ be a geometric extension with Galois group $\\\\mathbb{Z}_p$.\\nLet $K_n$ be the corresponding subextension with Galois group\\n$\\\\mathbb{Z}/p^n\\\\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple\\nexplicit formula $g_n$ in terms of an explicit Witt vector construction of the\\n$\\\\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which\\nis quadratic in $p^n$. Furthermore, we determine all $\\\\mathbb{Z}_p$-towers for\\nwhich the genus sequence is stable, in the sense that there are $a,b,c \\\\in\\n\\\\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus\\nstable towers are expected to have strong stable arithmetic properties for\\ntheir zeta functions. A key technical contribution of this work is a new\\nsimplified formula for the Schmid-Witt symbol coming from local class field\\ntheory.\\n',\n",
       " '  We study the evolution of spin-orbital correlations in an inhomogeneous\\nquantum system with an impurity replacing a doublon by a holon orbital degree\\nof freedom. Spin-orbital entanglement is large when spin correlations are\\nantiferromagnetic, while for a ferromagnetic host we obtain a pure orbital\\ndescription. In this regime the orbital model can be mapped on spinless\\nfermions and we uncover topological phases with zero energy modes at the edge\\nor at the domain between magnetically inequivalent regions.\\n',\n",
       " '  For autonomous agents to successfully operate in the real world, anticipation\\nof future events and states of their environment is a key competence. This\\nproblem has been formalized as a sequence extrapolation problem, where a number\\nof observations are used to predict the sequence into the future. Real-world\\nscenarios demand a model of uncertainty of such predictions, as predictions\\nbecome increasingly uncertain -- in particular on long time horizons. While\\nimpressive results have been shown on point estimates, scenarios that induce\\nmulti-modal distributions over future sequences remain challenging. Our work\\naddresses these challenges in a Gaussian Latent Variable model for sequence\\nprediction. Our core contribution is a \"Best of Many\" sample objective that\\nleads to more accurate and more diverse predictions that better capture the\\ntrue variations in real-world sequence data. Beyond our analysis of improved\\nmodel fit, our models also empirically outperform prior work on three diverse\\ntasks ranging from traffic scenes to weather data.\\n',\n",
       " '  End-to-end approaches have drawn much attention recently for significantly\\nsimplifying the construction of an automatic speech recognition (ASR) system.\\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\\nstudies have shown that RNN-T is difficult to train and a very complex training\\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\\naim to simplify the training process while maintaining performance. First, a\\nnew strategy of learning rate decay is proposed to accelerate the model\\nconvergence. Second, we find that adding convolutional layers at the beginning\\nof the network and using ordered data can discard the pre-training process of\\nthe encoder without loss of performance. Besides, we design experiments to find\\na balance among the usage of GPU memory, training circle and model performance.\\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\\n2% absolute improvement from a strong BLSTM CE system with language model\\ntrained on the same text corpus.\\n',\n",
       " '  Elasticity is a cloud property that enables applications and its execution\\nsystems to dynamically acquire and release shared computational resources on\\ndemand. Moreover, it unfolds the advantage of economies of scale in the cloud\\nthrough a drop in the average costs of these shared resources. However, it is\\nstill an open challenge to achieve a perfect match between resource demand and\\nprovision in autonomous elasticity management. Resource adaptation decisions\\nessentially involve a trade-off between economics and performance, which\\nproduces a gap between the ideal and actual resource provisioning. This gap, if\\nnot properly managed, can negatively impact the aggregate utility of a cloud\\ncustomer in the long run. To address this limitation, we propose a technical\\ndebt-aware learning approach for autonomous elasticity management based on a\\nreinforcement learning of elasticity debts in resource provisioning; the\\nadaptation pursues strategic decisions that trades off economics against\\nperformance. We extend CloudSim and Burlap to evaluate our approach. The\\nevaluation shows that a reinforcement learning of technical debts in elasticity\\nobtains a higher utility for a cloud customer, while conforming expected levels\\nof performance.\\n',\n",
       " \"  This is an exposition of homotopical results on the geometric realization of\\nsemi-simplicial spaces. We then use these to derive basic foundational results\\nabout classifying spaces of topological categories, possibly without units. The\\ntopics considered include: fibrancy conditions on topological categories; the\\neffect on classifying spaces of freely adjoining units; approximate notions of\\nunits; Quillen's Theorems A and B for non-unital topological categories; the\\neffect on classifying spaces of changing the topology on the space of objects;\\nthe Group-Completion Theorem.\\n\",\n",
       " '  Answer Set Programming (ASP) is a well-established declarative paradigm. One\\nof the successes of ASP is the availability of efficient systems.\\nState-of-the-art systems are based on the ground+solve approach. In some\\napplications this approach is infeasible because the grounding of one or few\\nconstraints is expensive. In this paper, we systematically compare alternative\\nstrategies to avoid the instantiation of problematic constraints, that are\\nbased on custom extensions of the solver. Results on real and synthetic\\nbenchmarks highlight some strengths and weaknesses of the different strategies.\\n(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)\\n',\n",
       " \"  The advances in geometric approaches to optical devices due to transformation\\noptics has led to the development of cloaks, concentrators, and other devices.\\nIt has also been shown that transformation optics can be used to gravitational\\nfields from general relativity. However, the technique is currently constrained\\nto linear devices, as a consistent approach to nonlinearity (including both the\\ncase of a nonlinear background medium and a nonlinear transformation) remains\\nan open question. Here we show that nonlinearity can be incorporated into\\ntransformation optics in a consistent way. We use this to illustrate a number\\nof novel effects, including cloaking an optical soliton, modeling nonlinear\\nsolutions to Einstein's field equations, controlling transport in a Debye\\nsolid, and developing a set of constitutive to relations for relativistic\\ncloaks in arbitrary nonlinear backgrounds.\\n\",\n",
       " '  We investigate crack propagation in a simple two-dimensional visco-elastic\\nmodel and find a scaling regime in the relation between the propagation\\nvelocity and energy release rate or fracture energy, together with lower and\\nupper bounds of the scaling regime. On the basis of our result, the existence\\nof the lower and upper bounds is expected to be universal or model-independent:\\nthe present simple simulation model provides generic insight into the physics\\nof crack propagation, and the model will be a first step towards the\\ndevelopment of a more refined coarse-grained model. Relatively abrupt changes\\nof velocity are predicted near the lower and upper bounds for the scaling\\nregime and the positions of the bounds could be good markers for the\\ndevelopment of tough polymers, for which we provide simple views that could be\\nuseful as guiding principles for toughening polymer-based materials.\\n',\n",
       " '  The fundamental group $\\\\pi$ of a Kodaira fibration is, by definition, the\\nextension of a surface group $\\\\Pi_b$ by another surface group $\\\\Pi_g$, i.e. \\\\[\\n1 \\\\rightarrow \\\\Pi_g \\\\rightarrow \\\\pi \\\\rightarrow \\\\Pi_b \\\\rightarrow 1. \\\\]\\nConversely, we can inquire about what conditions need to be satisfied by a\\ngroup of that sort in order to be the fundamental group of a Kodaira fibration.\\nIn this short note we collect some restriction on the image of the classifying\\nmap $m \\\\colon \\\\Pi_b \\\\to \\\\Gamma_g$ in terms of the coinvariant homology of\\n$\\\\Pi_g$. In particular, we observe that if $\\\\pi$ is the fundamental group of a\\nKodaira fibration with relative irregularity $g-s$, then $g \\\\leq 1+ 6s$, and we\\nshow that this effectively constrains the possible choices for $\\\\pi$, namely\\nthat there are group extensions as above that fail to satisfy this bound, hence\\ncannot be the fundamental group of a Kodaira fibration. In particular this\\nprovides examples of symplectic $4$--manifolds that fail to admit a Kähler\\nstructure for reasons that eschew the usual obstructions.\\n',\n",
       " '  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel\\nmaterial are used in a variety of electronics applications. However, a\\ncompetitive CNT-based technology requires the precise placement of CNTs at\\npredefined locations of a substrate. One promising placement approach is to use\\nchemical recognition to bind CNTs from solution at the desired locations on a\\nsurface. Producing the chemical pattern on the substrate is challenging. Here\\nwe describe a one-step patterning approach based on a highly photosensitive\\nsurface monolayer. The monolayer contains chromophopric group as light\\nsensitive body with heteroatoms as high quantum yield photolysis center. As\\ndeposited, the layer will bind CNTs from solution. However, when exposed to\\nultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for\\nconventional photoresists, the monolayer cleaves and no longer binds CNTs.\\nThese features allow standard, wafer-scale UV lithography processes to be used\\nto form a patterned chemical monolayer without the need for complex substrate\\npatterning or monolayer stamping.\\n',\n",
       " '  This paper derives two new optimization-driven Monte Carlo algorithms\\ninspired from variable splitting and data augmentation. In particular, the\\nformulation of one of the proposed approaches is closely related to the\\nalternating direction method of multipliers (ADMM) main steps. The proposed\\nframework enables to derive faster and more efficient sampling schemes than the\\ncurrent state-of-the-art methods and can embed the latter. By sampling\\nefficiently the parameter to infer as well as the hyperparameters of the\\nproblem, the generated samples can be used to approximate Bayesian estimators\\nof the parameters to infer. Additionally, the proposed approach brings\\nconfidence intervals at a low cost contrary to optimization methods.\\nSimulations on two often-studied signal processing problems illustrate the\\nperformance of the two proposed samplers. All results are compared to those\\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\\nsolve these problems.\\n',\n",
       " '  Yes, but only for a parameter value that makes it almost coincide with the\\nstandard model. We reconsider the cosmological dynamics of a generalized\\nChaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a\\ndark energy (DE) component with constant equation of state. This model, which\\nimplies a specific interaction between CDM and DE, has a $\\\\Lambda$CDM limit and\\nprovides the basis for studying deviations from the latter. Including matter\\nand radiation, we use the (modified) CLASS code \\\\cite{class} to construct the\\nCMB and matter power spectra in order to search for a gCg-based concordance\\nmodel that is in agreement with the SNIa data from the JLA sample and with\\nrecent Planck data. The results reveal that the gCg parameter $\\\\alpha$ is\\nrestricted to $|\\\\alpha|\\\\lesssim 0.05$, i.e., to values very close to the\\n$\\\\Lambda$CDM limit $\\\\alpha =0$. This excludes, in particular, models in which\\nDE decays linearly with the Hubble rate.\\n',\n",
       " '  The interest in the extracellular vesicles (EVs) is rapidly growing as they\\nbecame reliable biomarkers for many diseases. For this reason, fast and\\naccurate techniques of EVs size characterization are the matter of utmost\\nimportance. One increasingly popular technique is the Nanoparticle Tracking\\nAnalysis (NTA), in which the diameters of EVs are calculated from their\\ndiffusion constants. The crucial assumption here is that the diffusion in NTA\\nfollows the Stokes-Einstein relation, i.e. that the Mean Square Displacement\\n(MSD) of a particle grows linearly in time (MSD $\\\\propto t$). However, we show\\nthat NTA violates this assumption in both artificial and biological samples,\\ni.e. a large population of particles show a strongly sub-diffusive behaviour\\n(MSD $\\\\propto t^\\\\alpha$, $0<\\\\alpha<1$). To support this observation we present\\na range of experimental results for both polystyrene beads and EVs. This is\\nalso related to another problem: for the same samples there exists a huge\\ndiscrepancy (by the factor of 2-4) between the sizes measured with NTA and with\\nthe direct imaging methods, such as AFM. This can be remedied by e.g. the\\nFinite Track Length Adjustment (FTLA) method in NTA, but its applicability is\\nlimited in the biological and poly-disperse samples. On the other hand, the\\nmodels of sub-diffusion rarely provide the direct relation between the size of\\na particle and the generalized diffusion constant. However, we solve this last\\nproblem by introducing the logarithmic model of sub-diffusion, aimed at\\nretrieving the size data. In result, we propose a novel protocol of NTA data\\nanalysis. The accuracy of our method is on par with FTLA for small\\n($\\\\simeq$200nm) particles. We apply our method to study the EVs samples and\\ncorroborate the results with AFM.\\n',\n",
       " '  The processes of the averaged regression quantiles and of their modifications\\nprovide useful tools in the regression models when the covariates are not fully\\nunder our control. As an application we mention the probabilistic risk\\nassessment in the situation when the return depends on some exogenous\\nvariables. The processes enable to evaluate the expected $\\\\alpha$-shortfall\\n($0\\\\leq\\\\alpha\\\\leq 1$) and other measures of the risk, recently generally\\naccepted in the financial literature, but also help to measure the risk in\\nenvironment analysis and elsewhere.\\n',\n",
       " '  We study primordial perturbations from hyperinflation, proposed recently and\\nbased on a hyperbolic field-space. In the previous work, it was shown that the\\nfield-space angular momentum supported by the negative curvature modifies the\\nbackground dynamics and enhances fluctuations of the scalar fields\\nqualitatively, assuming that the inflationary background is almost de Sitter.\\nIn this work, we confirm and extend the analysis based on the standard approach\\nof cosmological perturbation in multi-field inflation. At the background level,\\nto quantify the deviation from de Sitter, we introduce the slow-varying\\nparameters and show that steep potentials, which usually can not drive\\ninflation, can drive inflation. At the linear perturbation level, we obtain the\\npower spectrum of primordial curvature perturbation and express the spectral\\ntilt and running in terms of the slow-varying parameters. We show that\\nhyperinflation with power-law type potentials has already been excluded by the\\nrecent Planck observations, while exponential-type potential with the exponent\\nof order unity can be made consistent with observations as far as the power\\nspectrum is concerned. We also argue that, in the context of a simple $D$-brane\\ninflation, the hyperinflation requires exponentially large hyperbolic extra\\ndimensions but that masses of Kaluza-Klein gravitons can be kept relatively\\nheavy.\\n',\n",
       " '  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,\\nexhibits interesting semiconductor to metal transition in the temperature range\\nof 530-560 K. The metallic behavior originates because of the reduction of V2O5\\nthrough oxygen vacancies. In the present report, V2O5 nanorods in the\\northorhombic phase with crystal orientation of (001) are grown using vapor\\ntransport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal\\nformula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the\\nformation of metallic phase above the transition temperature is established\\nfrom the temperature-dependent Raman spectroscopic studies. The origin of the\\nmetallic behavior of V2O5 is also understood due to the breakdown of pdpi bond\\nbetween OI and nearest V atom instigated by the formation of vanadyl OI\\nvacancy, confirmed from the downward shift of the bottom most split-off\\nconduction bands in the material with increasing temperature.\\n',\n",
       " '  In this paper, we presented a novel convolutional neural network framework\\nfor graph modeling, with the introduction of two new modules specially designed\\nfor graph-structured data: the $k$-th order convolution operator and the\\nadaptive filtering module. Importantly, our framework of High-order and\\nAdaptive Graph Convolutional Network (HA-GCN) is a general-purposed\\narchitecture that fits various applications on both node and graph centrics, as\\nwell as graph generative models. We conducted extensive experiments on\\ndemonstrating the advantages of our framework. Particularly, our HA-GCN\\noutperforms the state-of-the-art models on node classification and molecule\\nproperty prediction tasks. It also generates 32% more real molecules on the\\nmolecule generation task, both of which will significantly benefit real-world\\napplications such as material design and drug screening.\\n',\n",
       " '  A variety of representation learning approaches have been investigated for\\nreinforcement learning; much less attention, however, has been given to\\ninvestigating the utility of sparse coding. Outside of reinforcement learning,\\nsparse coding representations have been widely used, with non-convex objectives\\nthat result in discriminative representations. In this work, we develop a\\nsupervised sparse coding objective for policy evaluation. Despite the\\nnon-convexity of this objective, we prove that all local minima are global\\nminima, making the approach amenable to simple optimization strategies. We\\nempirically show that it is key to use a supervised objective, rather than the\\nmore straightforward unsupervised sparse coding approach. We compare the\\nlearned representations to a canonical fixed sparse representation, called\\ntile-coding, demonstrating that the sparse coding representation outperforms a\\nwide variety of tilecoding representations.\\n',\n",
       " \"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove\\nthat if a Riemannian manifold has Ricci curvature bounded below in a metric\\nball which moreover has almost maximal volume, then in a smaller ball (in a\\nquantified sense) it holds an almost-euclidean isoperimetric inequality. The\\nresult is actually established in the more general framework of non-smooth\\nspaces satisfying local Ricci curvature lower bounds in a synthetic sense via\\noptimal transportation.\\n\",\n",
       " '  We bound an exponential sum that appears in the study of irregularities of\\ndistribution (the low-frequency Fourier energy of the sum of several Dirac\\nmeasures) by geometric quantities: a special case is that for all $\\\\left\\\\{ x_1,\\n\\\\dots, x_N\\\\right\\\\} \\\\subset \\\\mathbb{T}^2$, $X \\\\geq 1$ and a universal $c>0$ $$\\n\\\\sum_{i,j=1}^{N}{ \\\\frac{X^2}{1 + X^4 \\\\|x_i -x_j\\\\|^4}} \\\\lesssim \\\\sum_{k \\\\in\\n\\\\mathbb{Z}^2 \\\\atop \\\\|k\\\\| \\\\leq X}{ \\\\left| \\\\sum_{n=1}^{N}{ e^{2 \\\\pi i\\n\\\\left\\\\langle k, x_n \\\\right\\\\rangle}}\\\\right|^2} \\\\lesssim \\\\sum_{i,j=1}^{N}{ X^2\\ne^{-c X^2\\\\|x_i -x_j\\\\|^2}}.$$ Since this exponential sum is intimately tied to\\nrather subtle distribution properties of the points, we obtain nonlocal\\nstructural statements for near-minimizers of the Riesz-type energy. In the\\nregime $X \\\\gtrsim N^{1/2}$ both upper and lower bound match for\\nmaximally-separated point sets satisfying $\\\\|x_i -x_j\\\\| \\\\gtrsim N^{-1/2}$.\\n',\n",
       " '  We investigate the effect of dimensional crossover in the ground state of the\\nantiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular\\nlattice that interpolates between the regime of weakly coupled Haldane chains\\n($J^{\\\\prime}\\\\! \\\\!\\\\ll\\\\!\\\\! J$) and the isotropic triangular lattice\\n($J^{\\\\prime}\\\\!\\\\!=\\\\!\\\\!J$). We use the density-matrix renormalization group\\n(DMRG) and Schwinger boson theory performed at the Gaussian correction level\\nabove the saddle-point solution. Our DMRG results show an abrupt transition\\nbetween decoupled spin chains and the spirally ordered regime at\\n$(J^{\\\\prime}/J)_c\\\\sim 0.42$, signaled by the sudden closing of the spin gap.\\nComing from the magnetically ordered side, the computation of the spin\\nstiffness within Schwinger boson theory predicts the instability of the spiral\\nmagnetic order toward a magnetically disordered phase with one-dimensional\\nfeatures at $(J^{\\\\prime}/J)_c \\\\sim 0.43$. The agreement of these complementary\\nmethods, along with the strong difference found between the intra- and the\\ninterchain DMRG short spin-spin correlations; for sufficiently large values of\\nthe interchain coupling, suggests that the interplay between the quantum\\nfluctuations and the dimensional crossover effects gives rise to the\\none-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.\\n',\n",
       " \"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\\noverwritten by new incoming information while important, frequently used\\nknowledge is prevented from being erased. In artificial learning systems,\\nlifelong learning so far has focused mainly on accumulating knowledge over\\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\\ngiven the limited model capacity and the unlimited new information to be\\nlearned, knowledge has to be preserved or erased selectively. Inspired by\\nneuroplasticity, we propose a novel approach for lifelong learning, coined\\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\\nneural network in an unsupervised and online manner. Given a new sample which\\nis fed to the network, MAS accumulates an importance measure for each parameter\\nof the network, based on how sensitive the predicted output function is to a\\nchange in this parameter. When learning a new task, changes to important\\nparameters can then be penalized, effectively preventing important knowledge\\nrelated to previous tasks from being overwritten. Further, we show an\\ninteresting connection between a local version of our method and Hebb's\\nrule,which is a model for the learning process in the brain. We test our method\\non a sequence of object recognition tasks and on the challenging problem of\\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\\nWe show state-of-the-art performance and, for the first time, the ability to\\nadapt the importance of the parameters based on unlabeled data towards what the\\nnetwork needs (not) to forget, which may vary depending on test conditions.\\n\",\n",
       " \"  In this paper, we study the generalized polynomial chaos (gPC) based\\nstochastic Galerkin method for the linear semiconductor Boltzmann equation\\nunder diffusive scaling and with random inputs from an anisotropic collision\\nkernel and the random initial condition. While the numerical scheme and the\\nproof of uniform-in-Knudsen-number regularity of the distribution function in\\nthe random space has been introduced in [Jin-Liu-16'], the main goal of this\\npaper is to first obtain a sharper estimate on the regularity of the\\nsolution-an exponential decay towards its local equilibrium, which then lead to\\nthe uniform spectral convergence of the stochastic Galerkin method for the\\nproblem under study.\\n\",\n",
       " '  Over the last decade, wireless networks have experienced an impressive growth\\nand now play a main role in many telecommunications systems. As a consequence,\\nscarce radio resources, such as frequencies, became congested and the need for\\neffective and efficient assignment methods arose. In this work, we present a\\nGenetic Algorithm for solving large instances of the Power, Frequency and\\nModulation Assignment Problem, arising in the design of wireless networks. To\\nour best knowledge, this is the first Genetic Algorithm that is proposed for\\nsuch problem. Compared to previous works, our approach allows a wider\\nexploration of the set of power solutions, while eliminating sources of\\nnumerical problems. The performance of the algorithm is assessed by tests over\\na set of large realistic instances of a Fixed WiMAX Network.\\n',\n",
       " '  We report on a combined study of the de Haas-van Alphen effect and angle\\nresolved photoemission spectroscopy on single crystals of the metallic\\ndelafossite PdRhO$_2$ rounded off by \\\\textit{ab initio} band structure\\ncalculations. A high sensitivity torque magnetometry setup with SQUID readout\\nand synchrotron-based photoemission with a light spot size of\\n$~50\\\\,\\\\mu\\\\mathrm{m}$ enabled high resolution data to be obtained from samples\\nas small as $150\\\\times100\\\\times20\\\\,(\\\\mu\\\\mathrm{m})^3$. The Fermi surface shape\\nis nearly cylindrical with a rounded hexagonal cross section enclosing a\\nLuttinger volume of 1.00(1) electrons per formula unit.\\n',\n",
       " \"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for\\nexponential integrals of bounded measurable functions in terms of Rényi\\ndivergences. We develop a variational characterization of the Rényi\\ndivergences between two probability distributions on a measurable sace in terms\\nof relative entropies. When combined with the elementary variational formula\\nfor exponential integrals of bounded measurable functions in terms of relative\\nentropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a\\ncorollary. We also develop an analogous variational characterization of the\\nRényi divergence rates between two stationary finite state Markov chains in\\nterms of relative entropy rates. When combined with Varadhan's variational\\ncharacterization of the spectral radius of square matrices with nonnegative\\nentries in terms of relative entropy, this yields an analog of the variational\\nformula of Atar, Chowdary and Dupuis in the framework of finite state Markov\\nchains.\\n\",\n",
       " '  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2\\nhave attracted much attention recently, particularly because of their type II\\nband alignments and the formation of interlayer exciton as the lowest-energy\\nexcitonic state. In this work, we calculate the electronic and optical\\nproperties of such heterostructures with the first-principles GW+Bethe-Salpeter\\nEquation (BSE) method and reveal the important role of interlayer coupling in\\ndeciding the excited-state properties, including the band alignment and\\nexcitonic properties. Our calculation shows that due to the interlayer\\ncoupling, the low energy excitons can be widely tunable by a vertical gate\\nfield. In particular, the dipole oscillator strength and radiative lifetime of\\nthe lowest energy exciton in these bilayer heterostructures is varied by over\\nan order of magnitude within a practical external gate field. We also build a\\nsimple model that captures the essential physics behind this tunability and\\nallows the extension of the ab initio results to a large range of electric\\nfields. Our work clarifies the physical picture of interlayer excitons in\\nbilayer vdW heterostructures and predicts a wide range of gate-tunable\\nexcited-state properties of 2D optoelectronic devices.\\n',\n",
       " \"  We construct the algebraic cobordism theory of bundles and divisors on\\nvarieties. It has a simple basis (over Q) from projective spaces and its rank\\nis equal to the number of Chern numbers. An application of this algebraic\\ncobordism theory is the enumeration of singular subvarieties with give tangent\\nconditions with a fixed smooth divisor, where the subvariety is the zero locus\\nof a section of a vector bundle. We prove that the generating series of numbers\\nof such subvarieties gives a homomorphism from the algebraic cobordism group to\\nthe power series ring. This implies that the enumeration of singular\\nsubvarieties with tangency conditions is governed by universal polynomials of\\nChern numbers, when the vector bundle is sufficiently ample. This result\\ncombines and generalizes the Caporaso-Harris recursive formula, Gottsche's\\nconjecture, classical De Jonquiere's Formula and node polynomials from tropical\\ngeometry.\\n\",\n",
       " \"  People with profound motor deficits could perform useful physical tasks for\\nthemselves by controlling robots that are comparable to the human body. Whether\\nthis is possible without invasive interfaces has been unclear, due to the\\nrobot's complexity and the person's limitations. We developed a novel,\\naugmented reality interface and conducted two studies to evaluate the extent to\\nwhich it enabled people with profound motor deficits to control robotic body\\nsurrogates. 15 novice users achieved meaningful improvements on a clinical\\nmanipulation assessment when controlling the robot in Atlanta from locations\\nacross the United States. Also, one expert user performed 59 distinct tasks in\\nhis own home over seven days, including self-care tasks such as feeding. Our\\nresults demonstrate that people with profound motor deficits can effectively\\ncontrol robotic body surrogates without invasive interfaces.\\n\",\n",
       " '  Object detection in wide area motion imagery (WAMI) has drawn the attention\\nof the computer vision research community for a number of years. WAMI proposes\\na number of unique challenges including extremely small object sizes, both\\nsparse and densely-packed objects, and extremely large search spaces (large\\nvideo frames). Nearly all state-of-the-art methods in WAMI object detection\\nreport that appearance-based classifiers fail in this challenging data and\\ninstead rely almost entirely on motion information in the form of background\\nsubtraction or frame-differencing. In this work, we experimentally verify the\\nfailure of appearance-based classifiers in WAMI, such as Faster R-CNN and a\\nheatmap-based fully convolutional neural network (CNN), and propose a novel\\ntwo-stage spatio-temporal CNN which effectively and efficiently combines both\\nappearance and motion information to significantly surpass the state-of-the-art\\nin WAMI object detection. To reduce the large search space, the first stage\\n(ClusterNet) takes in a set of extremely large video frames, combines the\\nmotion and appearance information within the convolutional architecture, and\\nproposes regions of objects of interest (ROOBI). These ROOBI can contain from\\none to clusters of several hundred objects due to the large video frame size\\nand varying object density in WAMI. The second stage (FoveaNet) then estimates\\nthe centroid location of all objects in that given ROOBI simultaneously via\\nheatmap estimation. The proposed method exceeds state-of-the-art results on the\\nWPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped\\nobjects, as well as being the first proposed method in wide area motion imagery\\nto detect completely stationary objects.\\n',\n",
       " '  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial\\nintelligence (e.g., the game of Go), is a well-known strategy for constructing\\napproximate solutions to sequential decision problems. Its primary innovation\\nis the use of a heuristic, known as a default policy, to obtain Monte Carlo\\nestimates of downstream values for states in a decision tree. This information\\nis used to iteratively expand the tree towards regions of states and actions\\nthat an optimal policy might visit. However, to guarantee convergence to the\\noptimal action, MCTS requires the entire tree to be expanded asymptotically. In\\nthis paper, we propose a new technique called Primal-Dual MCTS that utilizes\\nsampled information relaxation upper bounds on potential actions, creating the\\npossibility of \"ignoring\" parts of the tree that stem from highly suboptimal\\nchoices. This allows us to prove that despite converging to a partial decision\\ntree in the limit, the recommended action from Primal-Dual MCTS is optimal. The\\nnew approach shows significant promise when used to optimize the behavior of a\\nsingle driver navigating a graph while operating on a ride-sharing platform.\\nNumerical experiments on a real dataset of 7,000 trips in New Jersey suggest\\nthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decision\\ntrees and exhibits a reduced sensitivity to the size of the action space.\\n',\n",
       " '  We study the Fermi-edge singularity, describing the response of a degenerate\\nelectron system to optical excitation, in the framework of the functional\\nrenormalization group (fRG). Results for the (interband) particle-hole\\nsusceptibility from various implementations of fRG (one- and two-\\nparticle-irreducible, multi-channel Hubbard-Stratonovich, flowing\\nsusceptibility) are compared to the summation of all leading logarithmic (log)\\ndiagrams, achieved by a (first-order) solution of the parquet equations. For\\nthe (zero-dimensional) special case of the X-ray-edge singularity, we show that\\nthe leading log formula can be analytically reproduced in a consistent way from\\na truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic\\nstructure, we show that this derivation relies on fortuitous partial\\ncancellations special to the form of and accuracy applied to the X-ray-edge\\nsingularity and does not generalize.\\n',\n",
       " '  Retrosynthesis is a technique to plan the chemical synthesis of organic\\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\\nsearch tree is built by analysing molecules recursively and dissecting them\\ninto simpler molecular building blocks until one obtains a set of known\\nbuilding blocks. The search space is intractably large, and it is difficult to\\ndetermine the value of retrosynthetic positions. Here, we propose to model\\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\\nNetwork policy learned from essentially the complete published knowledge of\\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\\nexploratory studies, we demonstrate that MCTS with neural network policies\\noutperforms the traditionally used best-first search with hand-coded\\nheuristics.\\n',\n",
       " '  The class of stochastically self-similar sets contains many famous examples\\nof random sets, e.g. Mandelbrot percolation and general fractal percolation.\\nUnder the assumption of the uniform open set condition and some mild\\nassumptions on the iterated function systems used, we show that the\\nquasi-Assouad dimension of self-similar random recursive sets is almost surely\\nequal to the almost sure Hausdorff dimension of the set. We further comment on\\nrandom homogeneous and $V$-variable sets and the removal of overlap conditions.\\n',\n",
       " '  We report on the influence of spin-orbit coupling (SOC) in the Fe-based\\nsuperconductors (FeSCs) via application of circularly-polarized spin and\\nangle-resolved photoemission spectroscopy. We combine this technique in\\nrepresentative members of both the Fe-pnictides and Fe-chalcogenides with ab\\ninitio density functional theory and tight-binding calculations to establish an\\nubiquitous modification of the electronic structure in these materials imbued\\nby SOC. The influence of SOC is found to be concentrated on the hole pockets\\nwhere the superconducting gap is generally found to be largest. This result\\ncontests descriptions of superconductivity in these materials in terms of pure\\nspin-singlet eigenstates, raising questions regarding the possible pairing\\nmechanisms and role of SOC therein.\\n',\n",
       " '  In this work we examine how the updates addressing Meltdown and Spectre\\nvulnerabilities impact the performance of HPC applications. To study this we\\nuse the application kernel module of XDMoD to test the performance before and\\nafter the application of the vulnerability patches. We tested the performance\\ndifference for multiple application and benchmarks including: NWChem, NAMD,\\nHPCC, IOR, MDTest and IMB. The results show that although some specific\\nfunctions can have performance decreased by as much as 74%, the majority of\\nindividual metrics indicates little to no decrease in performance. The\\nreal-world applications show a 2-3% decrease in performance for single node\\njobs and a 5-11% decrease for parallel multi node jobs.\\n',\n",
       " '  Gene regulatory networks are powerful abstractions of biological systems.\\nSince the advent of high-throughput measurement technologies in biology in the\\nlate 90s, reconstructing the structure of such networks has been a central\\ncomputational problem in systems biology. While the problem is certainly not\\nsolved in its entirety, considerable progress has been made in the last two\\ndecades, with mature tools now available. This chapter aims to provide an\\nintroduction to the basic concepts underpinning network inference tools,\\nattempting a categorisation which highlights commonalities and relative\\nstrengths. While the chapter is meant to be self-contained, the material\\npresented should provide a useful background to the later, more specialised\\nchapters of this book.\\n',\n",
       " '  Glaucoma is the second leading cause of blindness all over the world, with\\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\\ntime, glaucoma causes irreversible damage to the optic nerve leading to\\nblindness. The optic nerve head examination, which involves measurement of\\ncup-to-disc ratio, is considered one of the most valuable methods of structural\\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\\nof optic disc and optic cup on eye fundus images and can be performed by modern\\ncomputer vision algorithms. This work presents universal approach for automatic\\noptic disc and cup segmentation, which is based on deep learning, namely,\\nmodification of U-Net convolutional neural network. Our experiments include\\ncomparison with the best known methods on publicly available databases\\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\\nour method achieves quality comparable to current state-of-the-art methods,\\noutperforming them in terms of the prediction time.\\n',\n",
       " '  The life of the modern world essentially depends on the work of the large\\nartificial homogeneous networks, such as wired and wireless communication\\nsystems, networks of roads and pipelines. The support of their effective\\ncontinuous functioning requires automatic screening and permanent optimization\\nwith processing of the huge amount of data by high-performance distributed\\nsystems. We propose new meta-algorithm of large homogeneous network analysis,\\nits decomposition into alternative sets of loosely connected subnets, and\\nparallel optimization of the most independent elements. This algorithm is based\\non a network-specific correlation function, Simulated Annealing technique, and\\nis adapted to work in the computer cluster. On the example of large wireless\\nnetwork, we show that proposed algorithm essentially increases speed of\\nparallel optimization. The elaborated general approach can be used for analysis\\nand optimization of the wide range of networks, including such specific types\\nas artificial neural networks or organized in networks physiological systems of\\nliving organisms.\\n',\n",
       " '  This paper considers the actor-critic contextual bandit for the mobile health\\n(mHealth) intervention. The state-of-the-art decision-making methods in mHealth\\ngenerally assume that the noise in the dynamic system follows the Gaussian\\ndistribution. Those methods use the least-square-based algorithm to estimate\\nthe expected reward, which is prone to the existence of outliers. To deal with\\nthe issue of outliers, we propose a novel robust actor-critic contextual bandit\\nmethod for the mHealth intervention. In the critic updating, the\\ncapped-$\\\\ell_{2}$ norm is used to measure the approximation error, which\\nprevents outliers from dominating our objective. A set of weights could be\\nachieved from the critic updating. Considering them gives a weighted objective\\nfor the actor updating. It provides the badly noised sample in the critic\\nupdating with zero weights for the actor updating. As a result, the robustness\\nof both actor-critic updating is enhanced. There is a key parameter in the\\ncapped-$\\\\ell_{2}$ norm. We provide a reliable method to properly set it by\\nmaking use of one of the most fundamental definitions of outliers in\\nstatistics. Extensive experiment results demonstrate that our method can\\nachieve almost identical results compared with the state-of-the-art methods on\\nthe dataset without outliers and dramatically outperform them on the datasets\\nnoised by outliers.\\n',\n",
       " \"  In 1933 Kolmogorov constructed a general theory that defines the modern\\nconcept of conditional expectation. In 1955 Renyi fomulated a new axiomatic\\ntheory for probability motivated by the need to include unbounded measures. We\\nintroduce a general concept of conditional expectation in Renyi spaces. In this\\ntheory improper priors are allowed, and the resulting posterior can also be\\nimproper.\\nIn 1965 Lindley published his classic text on Bayesian statistics using the\\ntheory of Renyi, but retracted this idea in 1973 due to the appearance of\\nmarginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes\\nare investigated, and the seemingly conflicting results are explained. The\\ntheory of Renyi can hence be used as an axiomatic basis for statistics that\\nallows use of unbounded priors.\\nKeywords: Haldane's prior; Poisson intensity; Marginalization paradox;\\nMeasure theory; conditional probability space; axioms for statistics;\\nconditioning on a sigma field; improper prior\\n\",\n",
       " '  Recently a new fault tolerant and simple mechanism was designed for solving\\ncommit consensus problem. It is based on replicated validation of messages sent\\nbetween transaction participants and a special dispatcher validator manager\\nnode. This paper presents a correctness, safety proofs and performance analysis\\nof this algorithm.\\n',\n",
       " '  This work presents a new method to quantify connectivity in transportation\\nnetworks. Inspired by the field of topological data analysis, we propose a\\nnovel approach to explore the robustness of road network connectivity in the\\npresence of congestion on the roadway. The robustness of the pattern is\\nsummarized in a congestion barcode, which can be constructed directly from\\ntraffic datasets commonly used for navigation. As an initial demonstration, we\\nillustrate the main technique on a publicly available traffic dataset in a\\nneighborhood in New York City.\\n',\n",
       " \"  The first transiting planetesimal orbiting a white dwarf was recently\\ndetected in K2 data of WD1145+017 and has been followed up intensively. The\\nmultiple, long, and variable transits suggest the transiting objects are dust\\nclouds, probably produced by a disintegrating asteroid. In addition, the system\\ncontains circumstellar gas, evident by broad absorption lines, mostly in the\\nu'-band, and a dust disc, indicated by an infrared excess. Here we present the\\nfirst detection of a change in colour of WD1145+017 during transits, using\\nsimultaneous multi-band fast-photometry ULTRACAM measurements over the\\nu'g'r'i'-bands. The observations reveal what appears to be 'bluing' during\\ntransits; transits are deeper in the redder bands, with a u'-r' colour\\ndifference of up to ~-0.05 mag. We explore various possible explanations for\\nthe bluing. 'Spectral' photometry obtained by integrating over bandpasses in\\nthe spectroscopic data in- and out-of-transit, compared to the photometric\\ndata, shows that the observed colour difference is most likely the result of\\nreduced circumstellar absorption in the spectrum during transits. This\\nindicates that the transiting objects and the gas share the same line-of-sight,\\nand that the gas covers the white dwarf only partially, as would be expected if\\nthe gas, the transiting debris, and the dust emitting the infrared excess, are\\npart of the same general disc structure (although possibly at different radii).\\nIn addition, we present the results of a week-long monitoring campaign of the\\nsystem.\\n\",\n",
       " '  In this review article, we discuss recent studies on drops and bubbles in\\nHele-Shaw cells, focusing on how scaling laws exhibit crossovers from the\\nthree-dimensional counterparts and focusing on topics in which viscosity plays\\nan important role. By virtue of progresses in analytical theory and high-speed\\nimaging, dynamics of drops and bubbles have actively been studied with the aid\\nof scaling arguments. However, compared with three dimensional problems,\\nstudies on the corresponding problems in Hele-Shaw cells are still limited.\\nThis review demonstrates that the effect of confinement in the Hele-Shaw cell\\nintroduces new physics allowing different scaling regimes to appear. For this\\npurpose, we discuss various examples that are potentially important for\\nindustrial applications handling drops and bubbles in confined spaces by\\nshowing agreement between experiments and scaling theories. As a result, this\\nreview provides a collection of problems in hydrodynamics that may be\\nanalytically solved or that may be worth studying numerically in the near\\nfuture.\\n',\n",
       " '  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural\\nnetwork (DNN) resemblance in terms of its very deep, feedforward network\\narchitecture. The typical S-DNN aggregates a variable number of individually\\nlearnable modules in series to assemble a DNN-alike alternative to the targeted\\nobject recognition tasks. This work likewise devises an S-DNN instantiation,\\ndubbed deep analytic network (DAN), on top of the spectral histogram (SH)\\nfeatures. The DAN learning principle relies on ridge regression, and some key\\nDNN constituents, specifically, rectified linear unit, fine-tuning, and\\nnormalization. The DAN aptitude is scrutinized on three repositories of varying\\ndomains, including FERET (faces), MNIST (handwritten digits), and CIFAR10\\n(natural objects). The empirical results unveil that DAN escalates the SH\\nbaseline performance over a sufficiently deep layer.\\n',\n",
       " '  In spite of Anderson\\'s theorem, disorder is known to affect superconductivity\\nin conventional s-wave superconductors. In most superconductors, the degree of\\ndisorder is fixed during sample preparation. Here we report measurements of the\\nsuperconducting properties of the two-dimensional gas that forms at the\\ninterface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal\\norientation, a system that permits \\\\emph{in situ} tuning of carrier density and\\ndisorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO\\ninterface, superconductivity at the (111) LAO/STO interface can be tuned by\\n$V_g$. In contrast to the (001) interface, superconductivity in these (111)\\nsamples is anisotropic, being different along different interface crystal\\ndirections, consistent with the strong anisotropy already observed other\\ntransport properties at the (111) LAO/STO interface. In addition, we find that\\nthe (111) interface samples \"remember\" the backgate voltage $V_F$ at which they\\nare cooled at temperatures near the superconducting transition temperature\\n$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low\\nenergy scale and other characteristics of this memory effect ($<1$ K)\\ndistinguish it from charge-trapping effects previously observed in (001)\\ninterface samples.\\n',\n",
       " '  We investigate beam loading and emittance preservation for a high-charge\\nelectron beam being accelerated in quasi-linear plasma wakefields driven by a\\nshort proton beam. The structure of the studied wakefields are similar to those\\nof a long, modulated proton beam, such as the AWAKE proton driver. We show that\\nby properly choosing the electron beam parameters and exploiting two well known\\neffects, beam loading of the wakefield and full blow out of plasma electrons by\\nthe accelerated beam, the electron beam can gain large amounts of energy with a\\nnarrow final energy spread (%-level) and without significant emittance growth.\\n',\n",
       " '  In this paper, we propose a practical receiver for multicarrier signals\\nsubjected to a strong memoryless nonlinearity. The receiver design is based on\\na generalized approximate message passing (GAMP) framework, and this allows\\nreal-time algorithm implementation in software or hardware with moderate\\ncomplexity. We demonstrate that the proposed receiver can provide more than a\\n2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range\\n$10^{-4}\\\\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to\\nclipping nonlinearity and the crest-factor of the clipped waveform is only\\n1.9dB. Simulation results also demonstrate that the proposed receiver provides\\nsignificant performance gain in frequency-selective multipath channels\\n',\n",
       " '  According to astrophysical observations value of recession velocity in a\\ncertain point is proportional to a distance to this point. The proportionality\\ncoefficient is the Hubble constant measured with 5% accuracy. It is used in\\nmany cosmological theories describing dark energy, dark matter, baryons, and\\ntheir relation with the cosmological constant introduced by Einstein.\\nIn the present work we have determined a limit value of the global Hubble\\nconstant (in a big distance from a point of observations) theoretically without\\nusing any empirical constants on the base of our own fractal model used for the\\ndescription a relation between distance to an observed galaxy and coordinate of\\nits center. The distance has been defined as a nonlinear fractal measure with\\nscale of measurement corresponding to a deviation of the measure from its fixed\\nvalue (zero-gravity radius). We have suggested a model of specific anisotropic\\nfractal for simulation a radial Universe expansion. Our theoretical results\\nhave shown existence of an inverse proportionality between accuracy of\\ndetermination the Hubble constant and accuracy of calculation a coordinates of\\ngalaxies leading to ambiguity results obtained at cosmological observations.\\n',\n",
       " '  Dynamic languages often employ reflection primitives to turn dynamically\\ngenerated text into executable code at run-time. These features make standard\\nstatic analysis extremely hard if not impossible because its essential data\\nstructures, i.e., the control-flow graph and the system of recursive equations\\nassociated with the program to analyse, are themselves dynamically mutating\\nobjects. We introduce SEA, an abstract interpreter for automatic sound string\\nexecutability analysis of dynamic languages employing bounded (i.e, finitely\\nnested) reflection and dynamic code generation. Strings are statically\\napproximated in an abstract domain of finite state automata with basic\\noperations implemented as symbolic transducers. SEA combines standard program\\nanalysis together with string executability analysis. The analysis of a call to\\nreflection determines a call to the same abstract interpreter over a code which\\nis synthesised directly from the result of the static string executability\\nanalysis at that program point. The use of regular languages for approximating\\ndynamically generated code structures allows SEA to soundly approximate safety\\nproperties of self modifying programs yet maintaining efficiency. Soundness\\nhere means that the semantics of the code synthesised by the analyser to\\nresolve reflection over-approximates the semantics of the code dynamically\\nbuilt at run-rime by the program at that point.\\n',\n",
       " '  Reductions for transition systems have been recently introduced as a uniform\\nand principled method for comparing the expressiveness of system models with\\nrespect to a range of properties, especially bisimulations. In this paper we\\nstudy the expressiveness (w.r.t. bisimulations) of models for quantitative\\ncomputations such as weighted labelled transition systems (WLTSs), uniform\\nlabelled transition systems (ULTraSs), and state-to-function transition systems\\n(FuTSs). We prove that there is a trade-off between labels and weights: at one\\nextreme lays the class of (unlabelled) weighted transition systems where\\ninformation is presented using weights only; at the other lays the class of\\nlabelled transition systems (LTSs) where information is shifted on labels.\\nThese categories of systems cannot be further reduced in any significant way\\nand subsume all the aforementioned models.\\n',\n",
       " \"  Poynting's theorem is used to obtain an expression for the turbulent\\npower-spectral density as function of frequency and wavenumber in low-frequency\\nmagnetic turbulence. No reference is made to Elsasser variables as is usually\\ndone in magnetohydrodynamic turbulence mixing mechanical and electromagnetic\\nturbulence. We rather stay with an implicit form of the mechanical part of\\nturbulence as suggested by electromagnetic theory in arbitrary media. All of\\nmechanics and flows is included into a turbulent response function which by\\nappropriate observations can be determined from knowledge of the turbulent\\nfluctuation spectra. This approach is not guided by the wish of developing a\\ncomplete theory of turbulence. It aims on the identification of the response\\nfunction from observations as input into a theory which afterwards attempts its\\ninterpretation. Combination of both the magnetic and electric power spectral\\ndensities leads to a representation of the turbulent response function, i.e.\\nthe turbulent conductivity spectrum $\\\\sigma_{\\\\omega k}$ as function of\\nfrequency $\\\\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to\\nelectric power spectral densities in frequency space. This knowledge allows for\\nformally writing down a turbulent dispersion relation. Power law inertial range\\nspectra result in a power law turbulent conductivity spectrum. These can be\\ncompared with observations in the solar wind. Keywords: MHD turbulence,\\nturbulent dispersion relation, turbulent response function, solar wind\\nturbulence\\n\",\n",
       " '  Let M be a compact Riemannian manifold and let $\\\\mu$,d be the associated\\nmeasure and distance on M. Robert McCann obtained, generalizing results for the\\nEuclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->\\nM pushing forward $\\\\mu$ to a measure $\\\\nu$: each S factors uniquely a.e. into\\nthe composition S = T \\\\circ U, where U : M -> M is volume preserving and T : M\\n-> M is the optimal map transporting $\\\\mu$ to $\\\\nu$ with respect to the cost\\nfunction d^2/2.\\nIn this article we study the polar factorization of conformal and projective\\nmaps of the sphere S^n. For conformal maps, which may be identified with\\nelements of the identity component of O(1,n+1), we prove that the polar\\nfactorization in the sense of optimal mass transport coincides with the\\nalgebraic polar factorization (Cartan decomposition) of this Lie group. For the\\nprojective case, where the group GL_+(n+1) is involved, we find necessary and\\nsufficient conditions for these two factorizations to agree.\\n',\n",
       " '  We examine the representation of numbers as the sum of two squares in\\n$\\\\mathbb{Z}_n$ for a general positive integer $n$. Using this information we\\nmake some comments about the density of positive integers which can be\\nrepresented as the sum of two squares and powers of $2$ in $\\\\mathbb{N}$.\\n',\n",
       " \"  Regression for spatially dependent outcomes poses many challenges, for\\ninference and for computation. Non-spatial models and traditional spatial\\nmixed-effects models each have their advantages and disadvantages, making it\\ndifficult for practitioners to determine how to carry out a spatial regression\\nanalysis. We discuss the data-generating mechanisms implicitly assumed by\\nvarious popular spatial regression models, and discuss the implications of\\nthese assumptions. We propose Bayesian spatial filtering as an approximate\\nmiddle way between non-spatial models and traditional spatial mixed models. We\\nshow by simulation that our Bayesian spatial filtering model has several\\ndesirable properties and hence may be a useful addition to a spatial\\nstatistician's toolkit.\\n\",\n",
       " '  One of the most important parameters in ionospheric plasma research also\\nhaving a wide practical application in wireless satellite telecommunications is\\nthe total electron content (TEC) representing the columnal electron number\\ndensity. The F region with high electron density provides the biggest\\ncontribution to TEC while the relatively weakly ionized plasma of the D region\\n(60 km - 90 km above Earths surface) is often considered as a negligible cause\\nof satellite signal disturbances. However, sudden intensive ionization\\nprocesses like those induced by solar X ray flares can cause relative increases\\nof electron density that are significantly larger in the D-region than in\\nregions at higher altitudes. Therefore, one cannot exclude a priori the D\\nregion from investigations of ionospheric influences on propagation of\\nelectromagnetic signals emitted by satellites. We discuss here this problem\\nwhich has not been sufficiently treated in literature so far. The obtained\\nresults are based on data collected from the D region monitoring by very low\\nfrequency radio waves and on vertical TEC calculations from the Global\\nNavigation Satellite System (GNSS) signal analyses, and they show noticeable\\nvariations in the D region electron content (TECD) during activity of a solar X\\nray flare (it rises by a factor of 136 in the considered case) when TECD\\ncontribution to TEC can reach several percent and which cannot be neglected in\\npractical applications like global positioning procedures by satellites.\\n',\n",
       " '  For the particles undergoing the anomalous diffusion with different waiting\\ntime distributions for different internal states, we derive the Fokker-Planck\\nand Feymann-Kac equations, respectively, describing positions of the particles\\nand functional distributions of the trajectories of particles; in particular,\\nthe equations governing the functional distribution of internal states are also\\nobtained. The dynamics of the stochastic processes are analyzed and the\\napplications, calculating the distribution of the first passage time and the\\ndistribution of the fraction of the occupation time, of the equations are\\ngiven.\\n',\n",
       " '  Stabilizing the magnetic signal of single adatoms is a crucial step towards\\ntheir successful usage in widespread technological applications such as\\nhigh-density magnetic data storage devices. The quantum mechanical nature of\\nthese tiny objects, however, introduces intrinsic zero-point spin-fluctuations\\nthat tend to destabilize the local magnetic moment of interest by dwindling the\\nmagnetic anisotropy potential barrier even at absolute zero temperature. Here,\\nwe elucidate the origins and quantify the effect of the fundamental ingredients\\ndetermining the magnitude of the fluctuations, namely the ($i$) local magnetic\\nmoment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner\\nexcitations. Based on a systematic first-principles study of 3d and 4d adatoms,\\nwe demonstrate that the transverse contribution of the fluctuations is\\ncomparable in size to the magnetic moment itself, leading to a remarkable\\n$\\\\gtrsim$50$\\\\%$ reduction of the magnetic anisotropy energy. Our analysis gives\\nrise to a comprehensible diagram relating the fluctuation magnitude to\\ncharacteristic features of adatoms, providing practical guidelines for\\ndesigning magnetically stable nanomagnets with minimal quantum fluctuations.\\n',\n",
       " '  We study a minimal model for the growth of a phenotypically heterogeneous\\npopulation of cells subject to a fluctuating environment in which they can\\nreplicate (by exploiting available resources) and modify their phenotype within\\na given landscape (thereby exploring novel configurations). The model displays\\nan exploration-exploitation trade-off whose specifics depend on the statistics\\nof the environment. Most notably, the phenotypic distribution corresponding to\\nmaximum population fitness (i.e. growth rate) requires a non-zero exploration\\nrate when the magnitude of environmental fluctuations changes randomly over\\ntime, while a purely exploitative strategy turns out to be optimal in two-state\\nenvironments, independently of the statistics of switching times. We obtain\\nanalytical insight into the limiting cases of very fast and very slow\\nexploration rates by directly linking population growth to the features of the\\nenvironment.\\n',\n",
       " '  Electronic Health Records (EHR) are data generated during routine clinical\\ncare. EHR offer researchers unprecedented phenotypic breadth and depth and have\\nthe potential to accelerate the pace of precision medicine at scale. A main EHR\\nuse-case is creating phenotyping algorithms to define disease status, onset and\\nseverity. Currently, no common machine-readable standard exists for defining\\nphenotyping algorithms which often are stored in human-readable formats. As a\\nresult, the translation of algorithms to implementation code is challenging and\\nsharing across the scientific community is problematic. In this paper, we\\nevaluate openEHR, a formal EHR data specification, for computable\\nrepresentations of EHR phenotyping algorithms.\\n',\n",
       " '  Mission critical data dissemination in massive Internet of things (IoT)\\nnetworks imposes constraints on the message transfer delay between devices. Due\\nto low power and communication range of IoT devices, data is foreseen to be\\nrelayed over multiple device-to-device (D2D) links before reaching the\\ndestination. The coexistence of a massive number of IoT devices poses a\\nchallenge in maximizing the successful transmission capacity of the overall\\nnetwork alongside reducing the multi-hop transmission delay in order to support\\nmission critical applications. There is a delicate interplay between the\\ncarrier sensing threshold of the contention based medium access protocol and\\nthe choice of packet forwarding strategy selected at each hop by the devices.\\nThe fundamental problem in optimizing the performance of such networks is to\\nbalance the tradeoff between conflicting performance objectives such as the\\nspatial frequency reuse, transmission quality, and packet progress towards the\\ndestination. In this paper, we use a stochastic geometry approach to quantify\\nthe performance of multi-hop massive IoT networks in terms of the spatial\\nfrequency reuse and the transmission quality under different packet forwarding\\nschemes. We also develop a comprehensive performance metric that can be used to\\noptimize the system to achieve the best performance. The results can be used to\\nselect the best forwarding scheme and tune the carrier sensing threshold to\\noptimize the performance of the network according to the delay constraints and\\ntransmission quality requirements.\\n',\n",
       " '  We develope a two-species exclusion process with a distinct pair of entry and\\nexit sites for each species of rigid rods. The relatively slower forward\\nstepping of the rods in an extended bottleneck region, located in between the\\ntwo entry sites, controls the extent of interference of the co-directional flow\\nof the two species of rods. The relative positions of the sites of entry of the\\ntwo species of rods with respect to the location of the bottleneck are\\nmotivated by a biological phenomenon. However, the primary focus of the study\\nhere is to explore the effects of the interference of the flow of the two\\nspecies of rods on their spatio-temporal organization and the regulations of\\nthis interference by the extended bottleneck. By a combination of mean-field\\ntheory and computer simulation we calculate the flux of both species of rods\\nand their density profiles as well as the composite phase diagrams of the\\nsystem. If the bottleneck is sufficiently stringent some of the phases become\\npractically unrealizable although not ruled out on the basis of any fundamental\\nphysical principle. Moreover the extent of suppression of flow of the\\ndownstream entrants by the flow of the upstream entrants can also be regulated\\nby the strength of the bottleneck. We speculate on the possible implications of\\nthe results in the context of the biological phenomenon that motivated the\\nformulation of the theoretical model.\\n',\n",
       " '  We introduce a large class of random Young diagrams which can be regarded as\\na natural one-parameter deformation of some classical Young diagram ensembles;\\na deformation which is related to Jack polynomials and Jack characters. We show\\nthat each such a random Young diagram converges asymptotically to some limit\\nshape and that the fluctuations around the limit are asymptotically Gaussian.\\n',\n",
       " '  We explicitly compute the critical exponents associated with logarithmic\\ncorrections (the so-called hatted exponents) starting from the renormalization\\ngroup equations and the mean field behavior for a wide class of models at the\\nupper critical behavior (for short and long range $\\\\phi^n$-theories) and below\\nit. This allows us to check the scaling relations among these critical\\nexponents obtained by analysing the complex singularities (Lee-Yang and Fisher\\nzeroes) of these models. Moreover, we have obtained an explicit method to\\ncompute the $\\\\hat{\\\\coppa}$ exponent [defined by $\\\\xi\\\\sim L (\\\\log\\nL)^{\\\\hat{\\\\coppa}}$] and, finally, we have found a new derivation of the scaling\\nlaw associated with it.\\n',\n",
       " '  We obtain a Bernstein-type inequality for sums of Banach-valued random\\nvariables satisfying a weak dependence assumption of general type and under\\ncertain smoothness assumptions of the underlying Banach norm. We use this\\ninequality in order to investigate in the asymptotical regime the error upper\\nbounds for the broad family of spectral regularization methods for reproducing\\nkernel decision rules, when trained on a sample coming from a $\\\\tau-$mixing\\nprocess.\\n',\n",
       " '  The temperature-dependent evolution of the Kondo lattice is a long-standing\\ntopic of theoretical and experimental investigation and yet it lacks a truly\\nmicroscopic description of the relation of the basic $f$-$d$ hybridization\\nprocesses to the fundamental temperature scales of Kondo screening and\\nFermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$\\nhybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo\\nlattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved\\nphotoemission (ARPES) with sufficient detail to allow direct comparison to\\nfirst principles dynamical mean field theory (DMFT) calculations containing\\nfull realism of crystalline electric field states. The ARPES results, for two\\northogonal (001) and (100) cleaved surfaces and three different $f$-$d$\\nhybridization scenarios, with additional microscopic insight provided by DMFT,\\nreveal $f$ participation in the Fermi surface at temperatures much higher than\\nthe lattice coherence temperature, $T^*\\\\approx$ 45 K, commonly believed to be\\nthe onset for such behavior. The identification of a $T$-dependent crystalline\\nelectric field degeneracy crossover in the DMFT theory $below$ $T^*$ is\\nspecifically highlighted.\\n',\n",
       " '  Hegarty conjectured for $n\\\\neq 2, 3, 5, 7$ that $\\\\mathbb{Z}/n\\\\mathbb{Z}$ has\\na permutation which destroys all arithmetic progressions mod $n$. For $n\\\\ge\\nn_0$, Hegarty and Martinsson demonstrated that $\\\\mathbb{Z}/n\\\\mathbb{Z}$ has an\\narithmetic-progression destroying permutation. However $n_0\\\\approx 1.4\\\\times\\n10^{14}$ and thus resolving the conjecture in full remained out of reach of any\\ncomputational techniques. However, this paper using constructions modeled after\\nthose used by Elkies and Swaminathan for the case of $\\\\mathbb{Z}/p\\\\mathbb{Z}$\\nwith $p$ being prime, establish the conjecture in full. Furthermore our results\\ndo not rely on the fact that it suffices to study when $n<n_0$ and thus our\\nresults completely independent of the proof given by Hegarty and Martinsson.\\n',\n",
       " '  An immersion $f : {\\\\mathcal D} \\\\rightarrow \\\\mathcal C$ between cell complexes\\nis a local homeomorphism onto its image that commutes with the characteristic\\nmaps of the cell complexes. We study immersions between finite-dimensional\\nconnected $\\\\Delta$-complexes by replacing the fundamental group of the base\\nspace by an appropriate inverse monoid. We show how conjugacy classes of the\\nclosed inverse submonoids of this inverse monoid may be used to classify\\nconnected immersions into the complex. This extends earlier results of Margolis\\nand Meakin for immersions between graphs and of Meakin and Szakács on\\nimmersions into $2$-dimensional $CW$-complexes.\\n',\n",
       " '  Resolving the relationship between biodiversity and ecosystem functioning has\\nbeen one of the central goals of modern ecology. Early debates about the\\nrelationship were finally resolved with the advent of a statistical\\npartitioning scheme that decomposed the biodiversity effect into a \"selection\"\\neffect and a \"complementarity\" effect. We prove that both the biodiversity\\neffect and its statistical decomposition into selection and complementarity are\\nfundamentally flawed because these methods use a naïve null expectation based\\non neutrality, likely leading to an overestimate of the net biodiversity\\neffect, and they fail to account for the nonlinear abundance-ecosystem\\nfunctioning relationships observed in nature. Furthermore, under such\\nnonlinearity no statistical scheme can be devised to partition the biodiversity\\neffects. We also present an alternative metric providing a more reasonable\\nestimate of biodiversity effect. Our results suggest that all studies conducted\\nsince the early 1990s likely overestimated the positive effects of biodiversity\\non ecosystem functioning.\\n',\n",
       " \"  The principle of democracy is that the people govern through elected\\nrepresentatives. Therefore, a democracy is healthy as long as the elected\\npoliticians do represent the people. We have analyzed data from the Brazilian\\nelectoral court (Tribunal Superior Eleitoral, TSE) concerning money donations\\nfor the electoral campaigns and the election results. Our work points to two\\ndisturbing conclusions: money is a determining factor on whether a candidate is\\nelected or not (as opposed to representativeness); secondly, the use of\\nBenford's Law to analyze the declared donations received by the parties and\\nelectoral campaigns shows evidence of fraud in the declarations. A better term\\nto define Brazil's government system is what we define as chrimatocracy (govern\\nby money).\\n\",\n",
       " \"  With the increasing commoditization of computer vision, speech recognition\\nand machine translation systems and the widespread deployment of learning-based\\nback-end technologies such as digital advertising and intelligent\\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\\nproduction. These changes have been made possible by unprecedented levels of\\ndata and computation, by methodological advances in machine learning, by\\ninnovations in systems software and architectures, and by the broad\\naccessibility of these technologies.\\nThe next generation of AI systems promises to accelerate these developments\\nand increasingly impact our lives via frequent interactions and making (often\\nmission-critical) decisions on our behalf, often in highly personalized\\ncontexts. Realizing this promise, however, raises daunting challenges. In\\nparticular, we need AI systems that make timely and safe decisions in\\nunpredictable environments, that are robust against sophisticated adversaries,\\nand that can process ever increasing amounts of data across organizations and\\nindividuals without compromising confidentiality. These challenges will be\\nexacerbated by the end of the Moore's Law, which will constrain the amount of\\ndata these technologies can store and process. In this paper, we propose\\nseveral open research directions in systems, architectures, and security that\\ncan address these challenges and help unlock AI's potential to improve lives\\nand society.\\n\",\n",
       " '  We rework and generalize equivariant infinite loop space theory, which shows\\nhow to construct G-spectra from G-spaces with suitable structure. There is a\\nnaive version which gives naive G-spectra for any topological group G, but our\\nfocus is on the construction of genuine G-spectra when G is finite.\\nWe give new information about the Segal and operadic equivariant infinite\\nloop space machines, supplying many details that are missing from the\\nliterature, and we prove by direct comparison that the two machines give\\nequivalent output when fed equivalent input. The proof of the corresponding\\nnonequivariant uniqueness theorem, due to May and Thomason, works for naive\\nG-spectra for general G but fails hopelessly for genuine G-spectra when G is\\nfinite. Even in the nonequivariant case, our comparison theorem is considerably\\nmore precise, giving a direct point-set level comparison.\\nWe have taken the opportunity to update this general area, equivariant and\\nnonequivariant, giving many new proofs, filling in some gaps, and giving some\\ncorrections to results in the literature.\\n',\n",
       " '  We prove that any open subset $U$ of a semi-simple simply connected\\nquasi-split linear algebraic group $G$ with ${codim} (G\\\\setminus U, G)\\\\geq 2$\\nover a number field satisfies strong approximation by establishing a fibration\\nof $G$ over a toric variety. We also prove a similar result of strong\\napproximation with Brauer-Manin obstruction for a partial equivariant smooth\\ncompactification of a homogeneous space where all invertible functions are\\nconstant and the semi-simple part of the linear algebraic group is quasi-split.\\nSome semi-abelian varieties of any given dimension where the complements of a\\nrational point do not satisfy strong approximation with Brauer-Manin\\nobstruction are given.\\n',\n",
       " '  We show that nonlocal minimal cones which are non-singular subgraphs outside\\nthe origin are necessarily halfspaces.\\nThe proof is based on classical ideas of~\\\\cite{DG1} and on the computation of\\nthe linearized nonlocal mean curvature operator, which is proved to satisfy a\\nsuitable maximum principle.\\nWith this, we obtain new, and somehow simpler, proofs of the Bernstein-type\\nresults for nonlocal minimal surfaces which have been recently established\\nin~\\\\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type\\nresult which classifies Lipschitz nonlocal minimal subgraphs outside a ball.\\n',\n",
       " \"  Let $f_1,\\\\ldots,f_k : \\\\mathbb{N} \\\\rightarrow \\\\mathbb{C}$ be multiplicative\\nfunctions taking values in the closed unit disc. Using an analytic approach in\\nthe spirit of Halász' mean value theorem, we compute multidimensional\\naverages of the shape $$x^{-l} \\\\sum_{\\\\mathbf{n} \\\\in [x]^l} \\\\prod_{1 \\\\leq j \\\\leq\\nk} f_j(L_j(\\\\mathbf{n}))$$ as $x \\\\rightarrow \\\\infty$, where $[x] := [1,x]$ and\\n$L_1,\\\\ldots, L_k$ are affine linear forms that satisfy some natural conditions.\\nOur approach gives a new proof of a result of Frantzikinakis and Host that is\\ndistinct from theirs, with \\\\emph{explicit} main and error terms. \\\\\\\\ As an\\napplication of our formulae, we establish a \\\\emph{local-to-global} principle\\nfor Gowers norms of multiplicative functions. We also compute the asymptotic\\ndensities of the sets of integers $n$ such that a given multiplicative function\\n$f: \\\\mathbb{N} \\\\rightarrow \\\\{-1, 1\\\\}$ yields a fixed sign pattern of length 3\\nor 4 on almost all 3- and 4-term arithmetic progressions, respectively, with\\nfirst term $n$.\\n\",\n",
       " \"  The apparent gas permeability of the porous medium is an important parameter\\nin the prediction of unconventional gas production, which was first\\ninvestigated systematically by Klinkenberg in 1941 and found to increase with\\nthe reciprocal mean gas pressure (or equivalently, the Knudsen number).\\nAlthough the underlying rarefaction effects are well-known, the reason that the\\ncorrection factor in Klinkenberg's famous equation decreases when the Knudsen\\nnumber increases has not been fully understood. Most of the studies idealize\\nthe porous medium as a bundle of straight cylindrical tubes, however, according\\nto the gas kinetic theory, this only results in an increase of the correction\\nfactor with the Knudsen number, which clearly contradicts Klinkenberg's\\nexperimental observations. Here, by solving the Bhatnagar-Gross-Krook equation\\nin simplified (but not simple) porous media, we identify, for the first time,\\ntwo key factors that can explain Klinkenberg's experimental results: the\\ntortuous flow path and the non-unitary tangential momentum accommodation\\ncoefficient for the gas-surface interaction. Moreover, we find that\\nKlinkenberg's results can only be observed when the ratio between the apparent\\nand intrinsic permeabilities is $\\\\lesssim30$; at large ratios (or Knudsen\\nnumbers) the correction factor increases with the Knudsen number. Our numerical\\nresults could also serve as benchmarking cases to assess the accuracy of\\nmacroscopic models and/or numerical schemes for the modeling/simulation of\\nrarefied gas flows in complex geometries over a wide range of gas rarefaction.\\n\",\n",
       " '  In previous papers, threshold probabilities for the properties of a random\\ndistance graph to contain strictly balanced graphs were found. We extend this\\nresult to arbitrary graphs and prove that the number of copies of a strictly\\nbalanced graph has asymptotically Poisson distribution at the threshold.\\n',\n",
       " '  Runtime enforcement can be effectively used to improve the reliability of\\nsoftware applications. However, it often requires the definition of ad hoc\\npolicies and enforcement strategies, which might be expensive to identify and\\nimplement. This paper discusses how to exploit lifecycle events to obtain\\nuseful enforcement strategies that can be easily reused across applications,\\nthus reducing the cost of adoption of the runtime enforcement technology. The\\npaper finally sketches how this idea can be used to define libraries that can\\nautomatically overcome problems related to applications misusing them.\\n',\n",
       " '  The atomic norm provides a generalization of the $\\\\ell_1$-norm to continuous\\nparameter spaces. When applied as a sparse regularizer for line spectral\\nestimation the solution can be obtained by solving a convex optimization\\nproblem. This problem is known as atomic norm soft thresholding (AST). It can\\nbe cast as a semidefinite program and solved by standard methods. In the\\nsemidefinite formulation there are $O(N^2)$ dual variables and a standard\\nprimal-dual interior point method requires at least $O(N^6)$ flops per\\niteration. That has lead researcher to consider alternating direction method of\\nmultipliers (ADMM) for the solution of AST, but this method is still somewhat\\nslow for large problem sizes. To obtain a faster algorithm we reformulate AST\\nas a non-symmetric conic program. That has two properties of key importance to\\nits numerical solution: the conic formulation has only $O(N)$ dual variables\\nand the Toeplitz structure inherent to AST is preserved. Based on it we derive\\nFastAST which is a primal-dual interior point method for solving AST. Two\\nvariants are considered with the fastest one requiring only $O(N^2)$ flops per\\niteration. Extensive numerical experiments demonstrate that FastAST solves AST\\nsignificantly faster than a state-of-the-art solver based on ADMM.\\n',\n",
       " '  We study the problem of causal structure learning over a set of random\\nvariables when the experimenter is allowed to perform at most $M$ experiments\\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\\nminimizing the portions of the structure that remains unknown given the limited\\nnumber of experiments in both Bayesian and minimax setting. We characterize the\\ntheoretical optimal solution and propose an algorithm, which designs the\\nexperiments efficiently in terms of time complexity. We show that for bounded\\ndegree graphs, in the minimax case and in the Bayesian case with uniform\\npriors, our proposed algorithm is a $\\\\rho$-approximation algorithm, where\\n$\\\\rho$ is independent of the order of the underlying graph. Simulations on both\\nsynthetic and real data show that the performance of our algorithm is very\\nclose to the optimal solution.\\n',\n",
       " '  We present a novel data-driven nested optimization framework that addresses\\nthe problem of coupling between plant and controller optimization. This\\noptimization strategy is tailored towards instances where a closed-form\\nexpression for the system dynamic response is unobtainable and simulations or\\nexperiments are necessary. Specifically, Bayesian Optimization, which is a\\ndata-driven technique for finding the optimum of an unknown and\\nexpensive-to-evaluate objective function, is employed to solve a nested\\noptimization problem. The underlying objective function is modeled by a\\nGaussian Process (GP); then, Bayesian Optimization utilizes the predictive\\nuncertainty information from the GP to determine the best subsequent control or\\nplant parameters. The proposed framework differs from the majority of co-design\\nliterature where there exists a closed-form model of the system dynamics.\\nFurthermore, we utilize the idea of Batch Bayesian Optimization at the plant\\noptimization level to generate a set of plant designs at each iteration of the\\noverall optimization process, recognizing that there will exist economies of\\nscale in running multiple experiments in each iteration of the plant design\\nprocess. We validate the proposed framework for a Buoyant Airborne Turbine\\n(BAT). We choose the horizontal stabilizer area, longitudinal center of mass\\nrelative to center of buoyancy (plant parameters), and the pitch angle\\nset-point (controller parameter) as our decision variables. Our results\\ndemonstrate that these plant and control parameters converge to their\\nrespective optimal values within only a few iterations.\\n',\n",
       " '  We explore the topological properties of quantum spin-1/2 chains with two\\nIsing symmetries. This class of models does not possess any of the symmetries\\nthat are required to protect the Haldane phase. Nevertheless, we show that\\nthere are 4 symmetry-protected topological phases, in addition to 6 phases that\\nspontaneously break one or both Ising symmetries. By mapping the model to\\none-dimensional interacting fermions with particle-hole and time-reversal\\nsymmetry, we obtain integrable parent Hamiltonians for the conventional and\\ntopological phases of the spin model. We use these Hamiltonians to characterize\\nthe physical properties of all 10 phases, identify their local and nonlocal\\norder parameters, and understand the effects of weak perturbations that respect\\nthe Ising symmetries. Our study provides the first explicit example of a class\\nof spin chains with several topologically non-trivial phases, and binds\\ntogether the topological classifications of interacting bosons and fermions.\\n',\n",
       " '  Most of the codes that have an algebraic decoding algorithm are derived from\\nthe Reed Solomon codes. They are obtained by taking equivalent codes, for\\nexample the generalized Reed Solomon codes, or by using the so-called subfield\\nsubcode method, which leads to Alternant codes and Goppa codes over the\\nunderlying prime field, or over some intermediate subfield. The main advantages\\nof these constructions is to preserve both the minimum distance and the\\ndecoding algorithm of the underlying Reed Solomon code. In this paper, we\\npropose a generalization of the subfield subcode construction by introducing\\nthe notion of subspace subcodes and a generalization of the equivalence of\\ncodes which leads to the notion of generalized subspace subcodes. When the\\ndimension of the selected subspaces is equal to one, we show that our approach\\ngives exactly the family of the codes obtained by equivalence and subfield\\nsubcode technique. However, our approach highlights the links between the\\nsubfield subcode of a code defined over an extension field and the operation of\\npuncturing the $q$-ary image of this code. When the dimension of the subspaces\\nis greater than one, we obtain codes whose alphabet is no longer a finite\\nfield, but a set of r-uples. We explain why these codes are practically as\\nefficient for applications as the codes defined on an extension of degree r. In\\naddition, they make it possible to obtain decodable codes over a large alphabet\\nhaving parameters previously inaccessible. As an application, we give some\\nexamples that can be used in public key cryptosystems such as McEliece.\\n',\n",
       " '  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of\\nGelfand-Cetlin systems over complex partial flag manifolds, we provide a\\ncomplete description of the topology of Gelfand-Cetlin fibers. We prove that\\nall fibers are \\\\emph{smooth} isotropic submanifolds and give a complete\\ndescription of the fiber to be Lagrangian in terms of combinatorics of\\nGelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian\\nfibers. After a few combinatorial and numercal tests for the displaceability,\\nusing the bulk-deformation of Floer cohomology by Schubert cycles, we prove\\nthat every full flag manifold $\\\\mathcal{F}(n)$ ($n \\\\geq 3$) with a monotone\\nKirillov-Kostant-Souriau symplectic form carries a continuum of\\nnon-displaceable Lagrangian tori which degenerates to a non-torus fiber in the\\nHausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\\\\mathcal{F}(3)$\\nis non-displaceable the question of which was raised by Nohara-Ueda who\\ncomputed its Floer cohomology to be vanishing.\\n',\n",
       " '  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)\\nare a key component of probabilistic weather forecasting. They represent the\\nuncertainty in the initial conditions by an ensemble which incorporates\\ninformation coming from the physical model with the latest observations.\\nHigh-resolution numerical weather prediction models ran at operational centers\\nare able to resolve non-linear and non-Gaussian physical phenomena such as\\nconvection. There is therefore a growing need to develop ensemble assimilation\\nalgorithms able to deal with non-Gaussianity while staying computationally\\nfeasible. In the present paper we address some of these needs by proposing a\\nnew hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully\\nformulated in ensemble space and uses a deterministic scheme such that it has\\nthe ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a\\nlimiting case. A new criterion for choosing the proportion of particle filter\\nand ETKF update is also proposed. The new algorithm is implemented in the COSMO\\nframework and numerical experiments in a quasi-operational convective-scale\\nsetup are conducted. The results show the feasibility of the new algorithm in\\npractice and indicate a strong potential for such local hybrid methods, in\\nparticular for forecasting non-Gaussian variables such as wind and hourly\\nprecipitation.\\n',\n",
       " '  In this paper, we consider the Tensor Robust Principal Component Analysis\\n(TRPCA) problem, which aims to exactly recover the low-rank and sparse\\ncomponents from their sum. Our model is based on the recently proposed\\ntensor-tensor product (or t-product) [13]. Induced by the t-product, we first\\nrigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor\\naverage rank, and show that the tensor nuclear norm is the convex envelope of\\nthe tensor average rank within the unit ball of the tensor spectral norm. These\\ndefinitions, their relationships and properties are consistent with matrix\\ncases. Equipped with the new tensor nuclear norm, we then solve the TRPCA\\nproblem by solving a convex program and provide the theoretical guarantee for\\nthe exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA\\nas a special case. Numerical experiments verify our results, and the\\napplications to image recovery and background modeling problems demonstrate the\\neffectiveness of our method.\\n',\n",
       " '  Galaxies in the local Universe are known to follow bimodal distributions in\\nthe global stellar populations properties. We analyze the distribution of the\\nlocal average stellar-population ages of 654,053 sub-galactic regions resolved\\non ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the\\nCALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.\\nWe find a bimodal local-age distribution, with an old and a young peak\\nprimarily due to regions in early-type galaxies and star-forming regions of\\nspirals, respectively. Within spiral galaxies, the older ages of bulges and\\ninter-arm regions relative to spiral arms support an internal age bimodality.\\nAlthough regions of higher stellar-mass surface-density, mu*, are typically\\nolder, mu* alone does not determine the stellar population age and a bimodal\\ndistribution is found at any fixed mu*. We identify an \"old ridge\" of regions\\nof age ~9 Gyr, independent of mu*, and a \"young sequence\" of regions with age\\nincreasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as\\nregions containing only old stars, and the latter as regions where the relative\\ncontamination of old stellar populations by young stars decreases as mu*\\nincreases. The reason why this bimodal age distribution is not inconsistent\\nwith the unimodal shape of the cosmic-averaged star-formation history is that\\ni) the dominating contribution by young stars biases the age low with respect\\nto the average epoch of star formation, and ii) the use of a single average age\\nper region is unable to represent the full time-extent of the star-formation\\nhistory of \"young-sequence\" regions.\\n',\n",
       " '  We introduce a minimal model for the evolution of functional\\nprotein-interaction networks using a sequence-based mutational algorithm, and\\napply the model to study neutral drift in networks that yield oscillatory\\ndynamics. Starting with a functional core module, random evolutionary drift\\nincreases network complexity even in the absence of specific selective\\npressures. Surprisingly, we uncover a hidden order in sequence space that gives\\nrise to long-term evolutionary memory, implying strong constraints on network\\nevolution due to the topology of accessible sequence space.\\n',\n",
       " \"  The handwritten string recognition is still a challengeable task, though the\\npowerful deep learning tools were introduced. In this paper, based on TAO-FCN,\\nwe proposed an end-to-end system for handwritten string recognition. Compared\\nwith the conventional methods, there is no preprocess nor manually designed\\nrules employed. With enough labelled data, it is easy to apply the proposed\\nmethod to different applications. Although the performance of the proposed\\nmethod may not be comparable with the state-of-the-art approaches, it's\\nusability and robustness are more meaningful for practical applications.\\n\",\n",
       " '  We note that the necessary and sufficient conditions established by Marcel\\nRiesz for the inclusion of regular Nörlund summation methods are in fact\\napplicable quite generally.\\n',\n",
       " \"  These lectures notes were written for a summer school on Mathematics for\\npost-quantum cryptography in Thiès, Senegal. They try to provide a guide for\\nMasters' students to get through the vast literature on elliptic curves,\\nwithout getting lost on their way to learning isogeny based cryptography. They\\nare by no means a reference text on the theory of elliptic curves, nor on\\ncryptography; students are encouraged to complement these notes with some of\\nthe books recommended in the bibliography.\\nThe presentation is divided in three parts, roughly corresponding to the\\nthree lectures given. In an effort to keep the reader interested, each part\\nalternates between the fundamental theory of elliptic curves, and applications\\nin cryptography. We often prefer to have the main ideas flow smoothly, rather\\nthan having a rigorous presentation as one would have in a more classical book.\\nThe reader will excuse us for the inaccuracies and the omissions.\\n\",\n",
       " \"  It has been shown recently that changing the fluidic properties of a drug can\\nimprove its efficacy in ablating solid tumors. We develop a modeling framework\\nfor tumor ablation, and present the simplest possible model for drug diffusion\\nin a spherical tumor with leaky boundaries and assuming cell death eventually\\nleads to ablation of that cell effectively making the two quantities\\nnumerically equivalent. The death of a cell after a given exposure time depends\\non both the concentration of the drug and the amount of oxygen available to the\\ncell. Higher oxygen availability leads to cell death at lower drug\\nconcentrations. It can be assumed that a minimum concentration is required for\\na cell to die, effectively connecting diffusion with efficacy. The\\nconcentration threshold decreases as exposure time increases, which allows us\\nto compute dose-response curves. Furthermore, these curves can be plotted at\\nmuch finer time intervals compared to that of experiments, which is used to\\nproduce a dose-threshold-response surface giving an observer a complete picture\\nof the drug's efficacy for an individual. In addition, since the diffusion,\\nleak coefficients, and the availability of oxygen is different for different\\nindividuals and tumors, we produce artificial replication data through\\nbootstrapping to simulate error. While the usual data-driven model with\\nSigmoidal curves use 12 free parameters, our mechanistic model only has two\\nfree parameters, allowing it to be open to scrutiny rather than forcing\\nagreement with data. Even so, the simplest model in our framework, derived\\nhere, shows close agreement with the bootstrapped curves, and reproduces well\\nestablished relations, such as Haber's rule.\\n\",\n",
       " '  To identify the estimand in missing data problems and observational studies,\\nit is common to base the statistical estimation on the \"missing at random\" and\\n\"no unmeasured confounder\" assumptions. However, these assumptions are\\nunverifiable using empirical data and pose serious threats to the validity of\\nthe qualitative conclusions of the statistical inference. A sensitivity\\nanalysis asks how the conclusions may change if the unverifiable assumptions\\nare violated to a certain degree. In this paper we consider a marginal\\nsensitivity model which is a natural extension of Rosenbaum\\'s sensitivity model\\nthat is widely used for matched observational studies. We aim to construct\\nconfidence intervals based on inverse probability weighting estimators, such\\nthat asymptotically the intervals have at least nominal coverage of the\\nestimand whenever the data generating distribution is in the collection of\\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\\nminimax/maximin inequality to transform this intractable problem to a linear\\nfractional programming problem, which can be solved very efficiently. We\\nillustrate our method using a real dataset to estimate the causal effect of\\nfish consumption on blood mercury level.\\n',\n",
       " '  In this paper, we provide an analysis of self-organized network management,\\nwith an end-to-end perspective of the network. Self-organization as applied to\\ncellular networks is usually referred to Self-organizing Networks (SONs), and\\nit is a key driver for improving Operations, Administration, and Maintenance\\n(OAM) activities. SON aims at reducing the cost of installation and management\\nof 4G and future 5G networks, by simplifying operational tasks through the\\ncapability to configure, optimize and heal itself. To satisfy 5G network\\nmanagement requirements, this autonomous management vision has to be extended\\nto the end to end network. In literature and also in some instances of products\\navailable in the market, Machine Learning (ML) has been identified as the key\\ntool to implement autonomous adaptability and take advantage of experience when\\nmaking decisions. In this paper, we survey how network management can\\nsignificantly benefit from ML solutions. We review and provide the basic\\nconcepts and taxonomy for SON, network management and ML. We analyse the\\navailable state of the art in the literature, standardization, and in the\\nmarket. We pay special attention to 3rd Generation Partnership Project (3GPP)\\nevolution in the area of network management and to the data that can be\\nextracted from 3GPP networks, in order to gain knowledge and experience in how\\nthe network is working, and improve network performance in a proactive way.\\nFinally, we go through the main challenges associated with this line of\\nresearch, in both 4G and in what 5G is getting designed, while identifying new\\ndirections for research.\\n',\n",
       " '  Understanding smart grid cyber attacks is key for developing appropriate\\nprotection and recovery measures. Advanced attacks pursue maximized impact at\\nminimized costs and detectability. This paper conducts risk analysis of\\ncombined data integrity and availability attacks against the power system state\\nestimation. We compare the combined attacks with pure integrity attacks - false\\ndata injection (FDI) attacks. A security index for vulnerability assessment to\\nthese two kinds of attacks is proposed and formulated as a mixed integer linear\\nprogramming problem. We show that such combined attacks can succeed with fewer\\nresources than FDI attacks. The combined attacks with limited knowledge of the\\nsystem model also expose advantages in keeping stealth against the bad data\\ndetection. Finally, the risk of combined attacks to reliable system operation\\nis evaluated using the results from vulnerability assessment and attack impact\\nanalysis. The findings in this paper are validated and supported by a detailed\\ncase study.\\n',\n",
       " '  We propose a family of near-metrics based on local graph diffusion to capture\\nsimilarity for a wide class of data sets. These quasi-metametrics, as their\\nnames suggest, dispense with one or two standard axioms of metric spaces,\\nspecifically distinguishability and symmetry, so that similarity between data\\npoints of arbitrary type and form could be measured broadly and effectively.\\nThe proposed near-metric family includes the forward k-step diffusion and its\\nreverse, typically on the graph consisting of data objects and their features.\\nBy construction, this family of near-metrics is particularly appropriate for\\ncategorical data, continuous data, and vector representations of images and\\ntext extracted via deep learning approaches. We conduct extensive experiments\\nto evaluate the performance of this family of similarity measures and compare\\nand contrast with traditional measures of similarity used for each specific\\napplication and with the ground truth when available. We show that for\\nstructured data including categorical and continuous data, the near-metrics\\ncorresponding to normalized forward k-step diffusion (k small) work as one of\\nthe best performing similarity measures; for vector representations of text and\\nimages including those extracted from deep learning, the near-metrics derived\\nfrom normalized and reverse k-step graph diffusion (k very small) exhibit\\noutstanding ability to distinguish data points from different classes.\\n',\n",
       " '  Recommender system is an important component of many web services to help\\nusers locate items that match their interests. Several studies showed that\\nrecommender systems are vulnerable to poisoning attacks, in which an attacker\\ninjects fake data to a given system such that the system makes recommendations\\nas the attacker desires. However, these poisoning attacks are either agnostic\\nto recommendation algorithms or optimized to recommender systems that are not\\ngraph-based. Like association-rule-based and matrix-factorization-based\\nrecommender systems, graph-based recommender system is also deployed in\\npractice, e.g., eBay, Huawei App Store. However, how to design optimized\\npoisoning attacks for graph-based recommender systems is still an open problem.\\nIn this work, we perform a systematic study on poisoning attacks to graph-based\\nrecommender systems. Due to limited resources and to avoid detection, we assume\\nthe number of fake users that can be injected into the system is bounded. The\\nkey challenge is how to assign rating scores to the fake users such that the\\ntarget item is recommended to as many normal users as possible. To address the\\nchallenge, we formulate the poisoning attacks as an optimization problem,\\nsolving which determines the rating scores for the fake users. We also propose\\ntechniques to solve the optimization problem. We evaluate our attacks and\\ncompare them with existing attacks under white-box (recommendation algorithm\\nand its parameters are known), gray-box (recommendation algorithm is known but\\nits parameters are unknown), and black-box (recommendation algorithm is\\nunknown) settings using two real-world datasets. Our results show that our\\nattack is effective and outperforms existing attacks for graph-based\\nrecommender systems. For instance, when 1% fake users are injected, our attack\\ncan make a target item recommended to 580 times more normal users in certain\\nscenarios.\\n',\n",
       " '  This paper describes the Stockholm University/University of Groningen\\n(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological\\ninflection. Our system is based on an attentional sequence-to-sequence neural\\nnetwork model using Long Short-Term Memory (LSTM) cells, with joint training of\\nmorphological inflection and the inverse transformation, i.e. lemmatization and\\nmorphological analysis. Our system outperforms the baseline with a large\\nmargin, and our submission ranks as the 4th best team for the track we\\nparticipate in (task 1, high-resource).\\n',\n",
       " \"  Neuroscientists classify neurons into different types that perform similar\\ncomputations at different locations in the visual field. Traditional methods\\nfor neural system identification do not capitalize on this separation of 'what'\\nand 'where'. Learning deep convolutional feature spaces that are shared among\\nmany neurons provides an exciting path forward, but the architectural design\\nneeds to account for data limitations: While new experimental techniques enable\\nrecordings from thousands of neurons, experimental time is limited so that one\\ncan sample only a small fraction of each neuron's response space. Here, we show\\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\\nneural data is the estimation of the individual receptive field locations, a\\nproblem that has been scratched only at the surface thus far. We propose a CNN\\narchitecture with a sparse readout layer factorizing the spatial (where) and\\nfeature (what) dimensions. Our network scales well to thousands of neurons and\\nshort recordings and can be trained end-to-end. We evaluate this architecture\\non ground-truth data to explore the challenges and limitations of CNN-based\\nsystem identification. Moreover, we show that our network model outperforms\\ncurrent state-of-the art system identification models of mouse primary visual\\ncortex.\\n\",\n",
       " '  The extremely low efficiency is regarded as the bottleneck of Wireless Power\\nTransfer (WPT) technology. To tackle this problem, either enlarging the\\ntransfer power or changing the infrastructure of WPT system could be an\\nintuitively proposed way. However, the drastically important issue on the user\\nexposure of electromagnetic radiation is rarely considered while we try to\\nimprove the efficiency of WPT. In this paper, a Distributed Antenna Power\\nBeacon (DA-PB) based WPT system where these antennas are uniformly distributed\\non a circle is analyzed and optimized with the safety electromagnetic radiation\\nlevel (SERL) requirement. In this model, three key questions are intended to be\\nanswered: 1) With the SERL, what is the performance of the harvested power at\\nthe users ? 2) How do we configure the parameters to maximize the efficiency of\\nWPT? 3) Under the same constraints, does the DA-PB still have performance gain\\nthan the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of\\nDA-PB is derived to make the radio frequency (RF) electromagnetic radiation\\npower density at any location of the charging cell lower than the SERL\\npublished by the Federal Communications Commission (FCC). Second, the\\nclosed-form expressions of average harvested Direct Current (DC) power per user\\nin the charging cell for pass-loss exponent 2 and 4 are also provided. In order\\nto maximize the average efficiency of WPT, the optimal radii for distributed\\nantennas elements (DAEs) are derived when the pass-loss exponent takes the\\ntypical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a\\nbenchmark. Simulation results verify our derived theoretical results. And it is\\nshown that the proposed DA-PB indeed achieves larger average harvested DC power\\nthan CA-PB and can improve the efficiency of WPT.\\n',\n",
       " '  A numerical method for particle-laden fluids interacting with a deformable\\nsolid domain and mobile rigid parts is proposed and implemented in a full\\nengineering system. The fluid domain is modeled with a lattice Boltzmann\\nrepresentation, the particles and rigid parts are modeled with a discrete\\nelement representation, and the deformable solid domain is modeled using a\\nLagrangian mesh. The main issue of this work, since separately each of these\\nmethods is a mature tool, is to develop coupling and model-reduction approaches\\nin order to efficiently simulate coupled problems of this nature, as occur in\\nvarious geological and engineering applications. The lattice Boltzmann method\\nincorporates a large-eddy simulation technique using the Smagorinsky turbulence\\nmodel. The discrete element method incorporates spherical and polyhedral\\nparticles for stiff contact interactions. A neo-Hookean hyperelastic model is\\nused for the deformable solid. We provide a detailed description of how to\\ncouple the three solvers within a unified algorithm. The technique we propose\\nfor rubber modeling/coupling exploits a simplification that prevents having to\\nsolve a finite-element problem each time step. We also develop a technique to\\nreduce the domain size of the full system by replacing certain zones with\\nquasi-analytic solutions, which act as effective boundary conditions for the\\nlattice Boltzmann method. The major ingredients of the routine are are\\nseparately validated. To demonstrate the coupled method in full, we simulate\\nslurry flows in two kinds of piston-valve geometries. The dynamics of the valve\\nand slurry are studied and reported over a large range of input parameters.\\n',\n",
       " \"  We construct a Schwinger-Keldysh effective field theory for relativistic\\nhydrodynamics for charged matter in a thermal background using a superspace\\nformalism. Superspace allows us to efficiently impose the symmetries of the\\nproblem and to obtain a simple expression for the effective action. We show\\nthat the theory we obtain is compatible with the Kubo-Martin-Schwinger\\ncondition, which in turn implies that Green's functions obey the\\nfluctuation-dissipation theorem. Our approach complements and extends existing\\nformulations found in the literature.\\n\",\n",
       " \"  Observables have a dual nature in both classical and quantum kinematics: they\\nare at the same time \\\\emph{quantities}, allowing to separate states by means of\\ntheir numerical values, and \\\\emph{generators of transformations}, establishing\\nrelations between different states. In this work, we show how this two-fold\\nrole of observables constitutes a key feature in the conceptual analysis of\\nclassical and quantum kinematics, shedding a new light on the distinguishing\\nfeature of the quantum at the kinematical level. We first take a look at the\\nalgebraic description of both classical and quantum observables in terms of\\nJordan-Lie algebras and show how the two algebraic structures are the precise\\nmathematical manifestation of the two-fold role of observables. Then, we turn\\nto the geometric reformulation of quantum kinematics in terms of Kähler\\nmanifolds. A key achievement of this reformulation is to show that the two-fold\\nrole of observables is the constitutive ingredient defining what an observable\\nis. Moreover, it points to the fact that, from the restricted point of view of\\nthe transformational role of observables, classical and quantum kinematics\\nbehave in exactly the same way. Finally, we present Landsman's general\\nframework of Poisson spaces with transition probability, which highlights with\\nunmatched clarity that the crucial difference between the two kinematics lies\\nin the way the two roles of observables are related to each other.\\n\",\n",
       " '  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with\\nsmooth boundary $\\\\partial M$. Suppose that $(M,g)$ admits a scalar-flat\\nconformal metric. We prove that the supremum of the isoperimetric quotient over\\nthe scalar-flat conformal class is strictly larger than the best constant of\\nthe isoperimetric inequality in the Euclidean space, and consequently is\\nachieved, if either (i) $n\\\\ge 12$ and $\\\\partial M$ has a nonumbilic point; or\\n(ii) $n\\\\ge 10$, $\\\\partial M$ is umbilic and the Weyl tensor does not vanish at\\nsome boundary point.\\n',\n",
       " '  Random feature maps are ubiquitous in modern statistical machine learning,\\nwhere they generalize random projections by means of powerful, yet often\\ndifficult to analyze nonlinear operators. In this paper, we leverage the\\n\"concentration\" phenomenon induced by random matrix theory to perform a\\nspectral analysis on the Gram matrix of these random feature maps, here for\\nGaussian mixture models of simultaneously large dimension and size. Our results\\nare instrumental to a deeper understanding on the interplay of the nonlinearity\\nand the statistics of the data, thereby allowing for a better tuning of random\\nfeature-based techniques.\\n',\n",
       " '  The calculation of minimum energy paths for transitions such as atomic and/or\\nspin re-arrangements is an important task in many contexts and can often be\\nused to determine the mechanism and rate of transitions. An important challenge\\nis to reduce the computational effort in such calculations, especially when ab\\ninitio or electron density functional calculations are used to evaluate the\\nenergy since they can require large computational effort. Gaussian process\\nregression is used here to reduce significantly the number of energy\\nevaluations needed to find minimum energy paths of atomic rearrangements. By\\nusing results of previous calculations to construct an approximate energy\\nsurface and then converge to the minimum energy path on that surface in each\\nGaussian process iteration, the number of energy evaluations is reduced\\nsignificantly as compared with regular nudged elastic band calculations. For a\\ntest problem involving rearrangements of a heptamer island on a crystal\\nsurface, the number of energy evaluations is reduced to less than a fifth. The\\nscaling of the computational effort with the number of degrees of freedom as\\nwell as various possible further improvements to this approach are discussed.\\n',\n",
       " '  Social media has changed the ways of communication, where everyone is\\nequipped with the power to express their opinions to others in online\\ndiscussion platforms. Previously, a number of stud- ies have been presented to\\nidentify opinion leaders in online discussion networks. Feng (\"Are you\\nconnected? Evaluating information cascade in online discussion about the\\n#RaceTogether campaign\", Computers in Human Behavior, 2016) identified five\\ntypes of central users and their communication patterns in an online\\ncommunication network of a limited time span. However, to trace the change in\\ncommunication pattern, a long-term analysis is required. In this study, we\\ncritically analyzed framework presented by Feng based on five types of central\\nusers in online communication network and their communication pattern in a\\nlong-term manner. We take another case study presented by Udnor et al.\\n(\"Determining social media impact on the politics of developing countries using\\nsocial network analytics\", Program, 2016) to further understand the dynamics as\\nwell as to perform validation . Results indicate that there may not exist all\\nof these central users in an online communication network in a long-term\\nmanner. Furthermore, we discuss the changing positions of opinion leaders and\\ntheir power to keep isolates interested in an online discussion network.\\n',\n",
       " '  Let $E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ denote the error of best approximation by\\npolynomials of degree at most $n$ in the space\\n$L^2(\\\\varpi_{\\\\alpha,\\\\beta,\\\\gamma})$ on the triangle $\\\\{(x,y): x, y \\\\ge 0, x+y\\n\\\\le 1\\\\}$, where $\\\\varpi_{\\\\alpha,\\\\beta,\\\\gamma}(x,y) := x^\\\\alpha y ^\\\\beta\\n(1-x-y)^\\\\gamma$ for $\\\\alpha,\\\\beta,\\\\gamma > -1$. Our main result gives a sharp\\nestimate of $E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ in terms of the error of best\\napproximation for higher order derivatives of $f$ in appropriate Sobolev\\nspaces. The result also leads to a characterization of\\n$E_n(f)_{\\\\alpha,\\\\beta,\\\\gamma}$ by a weighted $K$-functional.\\n',\n",
       " \"  Due to the increasing dependency of critical infrastructure on synchronized\\nclocks, network time synchronization protocols have become an attractive target\\nfor attackers. We identify data origin authentication as the key security\\nobjective and suggest to employ recently proposed high-performance digital\\nsignature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of\\nsecurity measures to secure multicast time synchronization. We conduct\\nexperiments to verify the computational and communication efficiency for using\\nthese signatures in the standard time synchronization protocols NTP and PTP. We\\npropose additional security measures to prevent replay attacks and to mitigate\\ndelay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we\\nextend our security measures specifically to 2-step mode (PTP) and show that\\nthey have no impact on time synchronization's precision.\\n\",\n",
       " \"  We implement an efficient numerical method to calculate response functions of\\ncomplex impurities based on the Density Matrix Renormalization Group (DMRG) and\\nuse it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This\\nmethod uses the correction vector to obtain precise Green's functions on the\\nreal frequency axis at zero temperature. By using a self-consistent bath\\nconfiguration with very low entanglement, we take full advantage of the DMRG to\\ncalculate dynamical response functions paving the way to treat large effective\\nimpurities such as those corresponding to multi-orbital interacting models and\\nmulti-site or multi-momenta clusters. This method leads to reliable\\ncalculations of non-local self energies at arbitrary dopings and interactions\\nand at any energy scale.\\n\",\n",
       " '  Bulk and surface electronic structures, calculated using density functional\\ntheory and a tight-binding model Hamiltonian, reveal the existence of two\\ntopologically invariant (TI) surface states in the family of cubic Bi\\nperovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI\\nstates, one lying in the valence band (TI-V) and other lying in the conduction\\nband (TI-C) are formed out of bonding and antibonding states of the\\nBi-$\\\\{$s,p$\\\\}$ - O-$\\\\{$p$\\\\}$ coordinated covalent interaction. Below a certain\\ncritical thickness of the film, which varies with A, TI states of top and\\nbottom surfaces couple to destroy the Dirac type linear dispersion and\\nconsequently to open surface energy gaps. The origin of s-p band inversion,\\nnecessary to form a TI state, classifies the family of ABiO$_3$ into two. For\\nclass-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,\\nis induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,\\nSr and Ba) the band inversion is induced through weak but sensitive second\\nneighbor Bi-Bi interactions.\\n',\n",
       " '  It is often recommended that identifiers for ontology terms should be\\nsemantics-free or meaningless. In practice, ontology developers tend to use\\nnumeric identifiers, starting at 1 and working upwards. In this paper we\\npresent a critique of current ontology semantics-free identifiers;\\nmonotonically increasing numbers have a number of significant usability flaws\\nwhich make them unsuitable as a default option, and we present a series of\\nalternatives. We have provide an implementation of these alternatives which can\\nbe freely combined.\\n',\n",
       " \"  Deep learning methods have achieved high performance in sound recognition\\ntasks. Deciding how to feed the training data is important for further\\nperformance improvement. We propose a novel learning method for deep sound\\nrecognition: Between-Class learning (BC learning). Our strategy is to learn a\\ndiscriminative feature space by recognizing the between-class sounds as\\nbetween-class sounds. We generate between-class sounds by mixing two sounds\\nbelonging to different classes with a random ratio. We then input the mixed\\nsound to the model and train the model to output the mixing ratio. The\\nadvantages of BC learning are not limited only to the increase in variation of\\nthe training data; BC learning leads to an enlargement of Fisher's criterion in\\nthe feature space and a regularization of the positional relationship among the\\nfeature distributions of the classes. The experimental results show that BC\\nlearning improves the performance on various sound recognition networks,\\ndatasets, and data augmentation schemes, in which BC learning proves to be\\nalways beneficial. Furthermore, we construct a new deep sound recognition\\nnetwork (EnvNet-v2) and train it with BC learning. As a result, we achieved a\\nperformance surpasses the human level.\\n\",\n",
       " \"  We propose a linear-time, single-pass, top-down algorithm for multiple\\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\\nedges specify a partial ordering in which hypotheses must be tested. The\\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\\n(FDR) while satisfying the logical constraint that a rejected node's parents\\nmust also be rejected. It is designed for sequential testing settings, when the\\nDAG structure is known a priori, but the $p$-values are obtained selectively\\n(such as in a sequence of experiments), but the algorithm is also applicable in\\nnon-sequential settings when all $p$-values can be calculated in advance (such\\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\\nEvolving Rejections on DAGs, provably controls the false discovery rate under\\nindependence, positive dependence or arbitrary dependence of the $p$-values.\\nThe DAGGER procedure specializes to known algorithms in the special cases of\\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\\nprocedure when the DAG has no edges. We explore the empirical performance of\\nDAGGER using simulations, as well as a real dataset corresponding to a gene\\nontology, showing favorable performance in terms of time and power.\\n\",\n",
       " '  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\\\\\"\\nodinger equation (NLS) and an ordinary differential equation (ODE). This system\\nis a simplified model of the NLS around soliton solutions. Following Nakanishi\\n\\\\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.\\nThe proof is based on Nakanishi\\'s framework and Fermi Golden Rule estimates on\\n$L^4$ in time norms.\\n',\n",
       " '  We relate the concepts used in decentralized ledger technology to studies of\\nepisodic memory in the mammalian brain. Specifically, we introduce the standard\\nconcepts of linked list, hash functions, and sharding, from computer science.\\nWe argue that these concepts may be more relevant to studies of the neural\\nmechanisms of memory than has been previously appreciated. In turn, we also\\nhighlight that certain phenomena studied in the brain, namely metacognition,\\nreality monitoring, and how perceptual conscious experiences come about, may\\ninspire development in blockchain technology too, specifically regarding\\nprobabilistic consensus protocols.\\n',\n",
       " '  Time-varying network topologies can deeply influence dynamical processes\\nmediated by them. Memory effects in the pattern of interactions among\\nindividuals are also known to affect how diffusive and spreading phenomena take\\nplace. In this paper we analyze the combined effect of these two ingredients on\\nepidemic dynamics on networks. We study the susceptible-infected-susceptible\\n(SIS) and the susceptible-infected-removed (SIR) models on the recently\\nintroduced activity-driven networks with memory. By means of an activity-based\\nmean-field approach we derive, in the long time limit, analytical predictions\\nfor the epidemic threshold as a function of the parameters describing the\\ndistribution of activities and the strength of the memory effects. Our results\\nshow that memory reduces the threshold, which is the same for SIS and SIR\\ndynamics, therefore favouring epidemic spreading. The theoretical approach\\nperfectly agrees with numerical simulations in the long time asymptotic regime.\\nStrong aging effects are present in the preasymptotic regime and the epidemic\\nthreshold is deeply affected by the starting time of the epidemics. We discuss\\nin detail the origin of the model-dependent preasymptotic corrections, whose\\nunderstanding could potentially allow for epidemic control on correlated\\ntemporal networks.\\n',\n",
       " '  A long-standing obstacle to progress in deep learning is the problem of\\nvanishing and exploding gradients. Although, the problem has largely been\\novercome via carefully constructed initializations and batch normalization,\\narchitectures incorporating skip-connections such as highway and resnets\\nperform much better than standard feedforward architectures despite well-chosen\\ninitialization and batch normalization. In this paper, we identify the\\nshattered gradients problem. Specifically, we show that the correlation between\\ngradients in standard feedforward networks decays exponentially with depth\\nresulting in gradients that resemble white noise whereas, in contrast, the\\ngradients in architectures with skip-connections are far more resistant to\\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\\nsupport of the analysis, on both fully-connected networks and convnets.\\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\\nshattering, with preliminary experiments showing the new initialization allows\\nto train very deep networks without the addition of skip-connections.\\n',\n",
       " '  We study the band structure topology and engineering from the interplay\\nbetween local moments and itinerant electrons in the context of pyrochlore\\niridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction\\nelectrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.\\nWhile the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned\\ninto an ordered spin ice with a finite ordering wavevector, dubbed\\n\"Melko-Hertog-Gingras\" state, by varying Ir and O contents. We point out that\\nthe ordered spin ice of the Pr local moments generates an internal magnetic\\nfield that reconstructs the band structure of the Luttinger semimetal. Besides\\nthe broad existence of Weyl nodes, we predict that the magnetic translation of\\nthe \"Melko-Hertog-Gingras\" state for the Pr moments protects the Dirac band\\ntouching at certain time reversal invariant momenta for the Ir conduction\\nelectrons. We propose the magnetic fields to control the Pr magnetic structure\\nand thereby indirectly influence the topological and other properties of the Ir\\nelectrons. Our prediction may be immediately tested in the ordered\\nPr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed\\nexamination of the band structure, magneto-transport, and other properties of\\nPr$_2$Ir$_2$O$_7$.\\n',\n",
       " '  Boundary value problems for Sturm-Liouville operators with potentials from\\nthe class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the\\npotentials are known on all the edges of the graph except two, and show that\\nthe potentials on the remaining edges can be constructed by fractional parts of\\ntwo spectra. A uniqueness theorem is proved, and an algorithm for the\\nconstructive solution of the partial inverse problem is provided. The main\\ningredient of the proofs is the Riesz-basis property of specially constructed\\nsystems of functions.\\n',\n",
       " '  The topological morphology--order of zeros at the positions of electrons with\\nrespect to a specific electron--of Laughlin state at filling fractions $1/m$\\n($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the\\npositions of other electrons. Although fairly accurate ground state wave\\nfunctions for most of the other quantum Hall states in the lowest Landau level\\nare quite well-known, it had been an open problem in expressing the ground\\nstate wave functions in terms of flux-attachment to particles, {\\\\em a la}, this\\nmorphology of Laughlin state. With a very general consideration of\\nflux-particle relations only, in spherical geometry, we here report a novel\\nmethod for determining morphologies of these states. Based on these, we\\nconstruct almost exact ground state wave-functions for the Coulomb interaction.\\nAlthough the form of interaction may change the ground state wave-function, the\\nsame morphology constructs the latter irrespective of the nature of the\\ninteraction between electrons.\\n',\n",
       " \"  The purpose of this paper is to formulate and study a common refinement of a\\nversion of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's\\n$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued\\nfunctions under a generalization of Yoshida's conjecture on the transcendental\\nparts of CM-periods. Then we conjecture a reciprocity law on their special\\nvalues concerning the absolute Frobenius action. We show that our conjecture\\nimplies a part of Stark's conjecture when the base field is an arbitrary real\\nfield and the splitting place is its real place. It also implies a refinement\\nof the Gross-Stark conjecture under a certain assumption. When the base field\\nis the rational number field, our conjecture follows from Coleman's formula on\\nFermat curves. We also prove some partial results in other cases.\\n\",\n",
       " \"  This paper considers the problem of autonomous multi-agent cooperative target\\nsearch in an unknown environment using a decentralized framework under a\\nno-communication scenario. The targets are considered as static targets and the\\nagents are considered to be homogeneous. The no-communication scenario\\ntranslates as the agents do not exchange either the information about the\\nenvironment or their actions among themselves. We propose an integrated\\ndecision and control theoretic solution for a search problem which generates\\nfeasible agent trajectories. In particular, a perception based algorithm is\\nproposed which allows an agent to estimate the probable strategies of other\\nagents' and to choose a decision based on such estimation. The algorithm shows\\nrobustness with respect to the estimation accuracy to a certain degree. The\\nperformance of the algorithm is compared with random strategies and numerical\\nsimulation shows considerable advantages.\\n\",\n",
       " '  This study explores the validity of chain effects of clean water, which are\\nknown as the \"Mills-Reincke phenomenon,\" in early twentieth-century Japan.\\nRecent studies have reported that water purifications systems are responsible\\nfor huge contributions to human capital. Although a few studies have\\ninvestigated the short-term effects of water-supply systems in pre-war Japan,\\nlittle is known about the benefits associated with these systems. By analyzing\\ncity-level cause-specific mortality data from the years 1922-1940, we found\\nthat eliminating typhoid fever infections decreased the risk of deaths due to\\nnon-waterborne diseases. Our estimates show that for one additional typhoid\\ndeath, there were approximately one to three deaths due to other causes, such\\nas tuberculosis and pneumonia. This suggests that the observed Mills-Reincke\\nphenomenon could have resulted from the prevention typhoid fever in a\\npreviously-developing Asian country.\\n',\n",
       " '  Developing neural network image classification models often requires\\nsignificant architecture engineering. In this paper, we study a method to learn\\nthe model architectures directly on the dataset of interest. As this approach\\nis expensive when the dataset is large, we propose to search for an\\narchitectural building block on a small dataset and then transfer the block to\\na larger dataset. The key contribution of this work is the design of a new\\nsearch space (the \"NASNet search space\") which enables transferability. In our\\nexperiments, we search for the best convolutional layer (or \"cell\") on the\\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\\ntogether more copies of this cell, each with their own parameters to design a\\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\\nnew regularization technique called ScheduledDropPath that significantly\\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\\nreduction of 28% in computational demand from the previous state-of-the-art\\nmodel. When evaluated at different levels of computational cost, accuracies of\\nNASNets exceed those of the state-of-the-art human-designed models. For\\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\\n3.1% better than equivalently-sized, state-of-the-art models for mobile\\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\\ndataset.\\n',\n",
       " '  We propose a new multi-frame method for efficiently computing scene flow\\n(dense depth and optical flow) and camera ego-motion for a dynamic scene\\nobserved from a moving stereo camera rig. Our technique also segments out\\nmoving objects from the rigid scene. In our method, we first estimate the\\ndisparity map and the 6-DOF camera motion using stereo matching and visual\\nodometry. We then identify regions inconsistent with the estimated camera\\nmotion and compute per-pixel optical flow only at these regions. This flow\\nproposal is fused with the camera motion-based flow proposal using fusion moves\\nto obtain the final optical flow and motion segmentation. This unified\\nframework benefits all four tasks - stereo, optical flow, visual odometry and\\nmotion segmentation leading to overall higher accuracy and efficiency. Our\\nmethod is currently ranked third on the KITTI 2015 scene flow benchmark.\\nFurthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3\\norders of magnitude faster than the top six methods. We also report a thorough\\nevaluation on challenging Sintel sequences with fast camera and object motion,\\nwhere our method consistently outperforms OSF [Menze and Geiger, 2015], which\\nis currently ranked second on the KITTI benchmark.\\n',\n",
       " '  Let $\\\\K$ be an algebraically closed field of positive characteristic $p$. We\\nmainly classify pointed Hopf algebras over $\\\\K$ of dimension $p^2q$, $pq^2$ and\\n$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete\\nclassification of such Hopf algebras except two subcases when they are not\\ngenerated by the first terms of coradical filtration. In particular, we obtain\\nmany new examples of non-commutative and non-cocommutative finite-dimensional\\nHopf algebras.\\n',\n",
       " '  We present the mixed Galerkin discretization of distributed parameter\\nport-Hamiltonian systems. On the prototypical example of hyperbolic systems of\\ntwo conservation laws in arbitrary spatial dimension, we derive the main\\ncontributions: (i) A weak formulation of the underlying geometric\\n(Stokes-Dirac) structure with a segmented boundary according to the causality\\nof the boundary ports. (ii) The geometric approximation of the Stokes-Dirac\\nstructure by a finite-dimensional Dirac structure is realized using a mixed\\nGalerkin approach and power-preserving linear maps, which define minimal\\ndiscrete power variables. (iii) With a consistent approximation of the\\nHamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.\\nBy the degrees of freedom in the power-preserving maps, the resulting family of\\nstructure-preserving schemes allows for trade-offs between centered\\napproximations and upwinding. We illustrate the method on the example of\\nWhitney finite elements on a 2D simplicial triangulation and compare the\\neigenvalue approximation in 1D with a related approach.\\n',\n",
       " '  The regularity of earthquakes, their destructive power, and the nuisance of\\nground vibration in urban environments, all motivate designs of defence\\nstructures to lessen the impact of seismic and ground vibration waves on\\nbuildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and\\nup to a few tens of Hz for vibrations generated by human activities, cause a\\nlarge amount of damage, or inconvenience, depending on the geological\\nconditions they can travel considerable distances and may match the resonant\\nfundamental frequency of buildings. The ultimate aim of any seismic\\nmetamaterial, or any other seismic shield, is to protect over this entire range\\nof frequencies, the long wavelengths involved, and low frequency, have meant\\nthis has been unachievable to date.\\nElastic flexural waves, applicable in the mechanical vibrations of thin\\nelastic plates, can be designed to have a broad zero-frequency stop-band using\\na periodic array of very small clamped circles. Inspired by this experimental\\nand theoretical observation, all be it in a situation far removed from seismic\\nwaves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)\\nand body (pressure P and shear S) wave reflectors at very large wavelengths in\\nstructured soils modelled as a fully elastic layer periodically clamped to\\nbedrock.\\nWe identify zero frequency stop-bands that only exist in the limit of columns\\nof concrete clamped at their base to the bedrock. In a realistic configuration\\nof a sedimentary basin 15 meters deep we observe a zero frequency stop-band\\ncovering a broad frequency range of $0$ to $30$ Hz.\\n',\n",
       " \"  In this paper, we prove some difference analogue of second main theorems of\\nmeromorphic mapping from Cm into an algebraic variety V intersecting a finite\\nset of fixed hypersurfaces in subgeneral position. As an application, we prove\\na result on algebraically degenerate of holomorphic curves intersecting\\nhypersurfaces and difference analogue of Picard's theorem on holomorphic\\ncurves. Furthermore, we obtain a second main theorem of meromorphic mappings\\nintersecting hypersurfaces in N-subgeneral position for Veronese embedding in\\nPn(C) and a uniqueness theorem sharing hypersurfaces.\\n\",\n",
       " \"  Large-scale datasets have played a significant role in progress of neural\\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\\ngeneral multi-label video classification. It was created from over 7 million\\nYouTube videos (450,000 hours of video) and includes video labels from a\\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\\npre-extracted audio & visual features from every second of video (3.2 billion\\nfeature vectors in total). Google cloud recently released the datasets and\\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\\nCompetitors are challenged to develop classification algorithms that assign\\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\\nby the competition, we started exploration of audio understanding and\\nclassification using deep learning algorithms and ensemble methods. We built\\nseveral baseline predictions according to the benchmark paper and public github\\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\\nbase level 77% to 80.7% through approaches of ensemble.\\n\",\n",
       " '  Observational data collected during experiments, such as the planned Fire and\\nSmoke Model Evaluation Experiment (FASMEE), are critical for progressing and\\ntransitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM\\ninto operational use. Historical meteorological data, representing typical\\nweather conditions for the anticipated burn locations and times, have been\\nprocessed to initialize and run a set of simulations representing the planned\\nexperimental burns. Based on an analysis of these numerical simulations, this\\npaper provides recommendations on the experimental setup that include the\\nignition procedures, size and duration of the burns, and optimal sensor\\nplacement. New techniques are developed to initialize coupled fire-atmosphere\\nsimulations with weather conditions typical of the planned burn locations and\\ntime of the year. Analysis of variation and sensitivity analysis of simulation\\ndesign to model parameters by repeated Latin Hypercube Sampling are used to\\nassess the locations of the sensors. The simulations provide the locations of\\nthe measurements that maximize the expected variation of the sensor outputs\\nwith the model parameters.\\n',\n",
       " '  For a knot $K$ in a homology $3$-sphere $\\\\Sigma$, let $M$ be the result of\\n$2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our\\nfirst theorem is that if the first homology of $X$ is finite cyclic and $M$ is\\na Seifert fibered space with $N\\\\ge 3$ singular fibers, then $N\\\\ge 4$ if and\\nonly if the first homology of the universal abelian covering of $X$ is\\ninfinite. Our second theorem is that under an appropriate assumption on the\\nAlexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\\\\pm 1$\\n(i.e.\\\\ integral surgery).\\n',\n",
       " \"  Sparse feature selection is necessary when we fit statistical models, we have\\naccess to a large group of features, don't know which are relevant, but assume\\nthat most are not. Alternatively, when the number of features is larger than\\nthe available data the model becomes over parametrized and the sparse feature\\nselection task involves selecting the most informative variables for the model.\\nWhen the model is a simple location model and the number of relevant features\\ndoes not grow with the total number of features, sparse feature selection\\ncorresponds to sparse mean estimation. We deal with a simplified mean\\nestimation problem consisting of an additive model with gaussian noise and mean\\nthat is in a restricted, finite hypothesis space. This restriction simplifies\\nthe mean estimation problem into a selection problem of combinatorial nature.\\nAlthough the hypothesis space is finite, its size is exponential in the\\ndimension of the mean. In limited data settings and when the size of the\\nhypothesis space depends on the amount of data or on the dimension of the data,\\nchoosing an approximation set of hypotheses is a desirable approach. Choosing a\\nset of hypotheses instead of a single one implies replacing the bias-variance\\ntrade off with a resolution-stability trade off. Generalization capacity\\nprovides a resolution selection criterion based on allowing the learning\\nalgorithm to communicate the largest amount of information in the data to the\\nlearner without error. In this work the theory of approximation set coding and\\ngeneralization capacity is explored in order to understand this approach. We\\nthen apply the generalization capacity criterion to the simplified sparse mean\\nestimation problem and detail an importance sampling algorithm which at once\\nsolves the difficulty posed by large hypothesis spaces and the slow convergence\\nof uniform sampling algorithms.\\n\",\n",
       " '  In this letter, we consider the joint power and admission control (JPAC)\\nproblem by assuming that only the channel distribution information (CDI) is\\navailable. Under this assumption, we formulate a new chance (probabilistic)\\nconstrained JPAC problem, where the signal to interference plus noise ratio\\n(SINR) outage probability of the supported links is enforced to be not greater\\nthan a prespecified tolerance. To efficiently deal with the chance SINR\\nconstraint, we employ the sample approximation method to convert them into\\nfinitely many linear constraints. Then, we propose a convex approximation based\\ndeflation algorithm for solving the sample approximation JPAC problem. Compared\\nto the existing works, this letter proposes a novel two-timescale JPAC\\napproach, where admission control is performed by the proposed deflation\\nalgorithm based on the CDI in a large timescale and transmission power is\\nadapted instantly with fast fadings in a small timescale. The effectiveness of\\nthe proposed algorithm is illustrated by simulations.\\n',\n",
       " '  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest\\nstar, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness\\nwere similar to coronally active late-type dwarf members. Later, in 2010, a\\nHubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found\\nfar-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious\\noffset of the ROSAT source, suggested that a late-type companion might be\\nresponsible for the X-rays. Recently, a multi-faceted program tested that\\npremise. Groundbased optical coronography, and near-UV imaging with HST Wide\\nField Camera 3, searched for any close-in faint candidate coronal objects, but\\nwithout success. Then, a Chandra pointing found the X-ray source single and\\ncoincident with the bright star. Significantly, the SiIV emissions of Alpha\\nPersei, in a deeper FUV spectrum collected by HST COS as part of the joint\\nprogram, aligned well with chromospheric atomic oxygen (which must be intrinsic\\nto the luminous star), within the context of cooler late-F and early-G\\nsupergiants, including Cepheid variables. This pointed to the X-rays as the\\nfundamental anomaly. The over-luminous X-rays still support the case for a\\nhyperactive dwarf secondary, albeit now spatially unresolved. However, an\\nalternative is that Alpha Persei represents a novel class of coronal source.\\nResolving the first possibility now has become more difficult, because the easy\\nsolution -- a well separated companion -- has been eliminated. Testing the\\nother possibility will require a broader high-energy census of the early-F\\nsupergiants.\\n',\n",
       " '  Realistic music generation is a challenging task. When building generative\\nmodels of music that are learnt from data, typically high-level representations\\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\\nparticular performance. But these nuances are very important for our perception\\nof musicality and realism, so in this work we embark on modelling music in the\\nraw audio domain. It has been shown that autoregressive models excel at\\ngenerating raw audio waveforms of speech, but when applied to music, we find\\nthem biased towards capturing local signal structure at the expense of\\nmodelling long-range correlations. This is problematic because music exhibits\\nstructure at many different timescales. In this work, we explore autoregressive\\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\\ncapture long-range correlations in waveforms. We find that they allow us to\\nunconditionally generate piano music directly in the raw audio domain, which\\nshows stylistic consistency across tens of seconds.\\n',\n",
       " '  Young asteroid families are unique sources of information about fragmentation\\nphysics and the structure of their parent bodies, since their physical\\nproperties have not changed much since their birth. Families have different\\nproperties such as age, size, taxonomy, collision severity and others, and\\nunderstanding the effect of those properties on our observations of the\\nsize-frequency distribution (SFD) of family fragments can give us important\\ninsights into the hypervelocity collision processes at scales we cannot achieve\\nin our laboratories. Here we take as an example the very young Datura family,\\nwith a small 8-km parent body, and compare its size distribution to other\\nfamilies, with both large and small parent bodies, and created by both\\ncatastrophic and cratering formation events. We conclude that most likely\\nexplanation for the shallower size distribution compared to larger families is\\na more pronounced observational bias because of its small size. Its size\\ndistribution is perfectly normal when its parent body size is taken into\\naccount. We also discuss some other possibilities. In addition, we study\\nanother common feature: an offset or \"bump\" in the distribution occurring for a\\nfew of the larger elements. We hypothesize that it can be explained by a newly\\ndescribed regime of cratering, \"spall cratering\", which controls the majority\\nof impact craters on the surface of small asteroids like Datura.\\n',\n",
       " '  We provide a graph formula which describes an arbitrary monomial in {\\\\omega}\\nclasses (also referred to as stable {\\\\psi} classes) in terms of a simple family\\nof dual graphs (pinwheel graphs) with edges decorated by rational functions in\\n{\\\\psi} classes. We deduce some numerical consequences and in particular a\\ncombinatorial formula expressing top intersections of \\\\k{appa} classes on Mg in\\nterms of top intersections of {\\\\psi} classes.\\n',\n",
       " '  Tomography has made a radical impact on diverse fields ranging from the study\\nof 3D atomic arrangements in matter to the study of human health in medicine.\\nDespite its very diverse applications, the core of tomography remains the same,\\nthat is, a mathematical method must be implemented to reconstruct the 3D\\nstructure of an object from a number of 2D projections. In many scientific\\napplications, however, the number of projections that can be measured is\\nlimited due to geometric constraints, tolerable radiation dose and/or\\nacquisition speed. Thus it becomes an important problem to obtain the\\nbest-possible reconstruction from a limited number of projections. Here, we\\npresent the mathematical implementation of a tomographic algorithm, termed\\nGENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between\\nreal and reciprocal space, GENFIRE searches for a global solution that is\\nconcurrently consistent with the measured data and general physical\\nconstraints. The algorithm requires minimal human intervention and also\\nincorporates angular refinement to reduce the tilt angle error. We demonstrate\\nthat GENFIRE can produce superior results relative to several other popular\\ntomographic reconstruction techniques by numerical simulations, and by\\nexperimentally by reconstructing the 3D structure of a porous material and a\\nfrozen-hydrated marine cyanobacterium. Equipped with a graphical user\\ninterface, GENFIRE is freely available from our website and is expected to find\\nbroad applications across different disciplines.\\n',\n",
       " '  Generative Adversarial Networks (GANs) excel at creating realistic images\\nwith complex models for which maximum likelihood is infeasible. However, the\\nconvergence of GAN training has still not been proved. We propose a two\\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\\nfor both the discriminator and the generator. Using the theory of stochastic\\napproximation, we prove that the TTUR converges under mild assumptions to a\\nstationary local Nash equilibrium. The convergence carries over to the popular\\nAdam optimization, for which we prove that it follows the dynamics of a heavy\\nball with friction and thus prefers flat minima in the objective landscape. For\\nthe evaluation of the performance of GANs at image generation, we introduce the\\n\"Fréchet Inception Distance\" (FID) which captures the similarity of generated\\nimages to real ones better than the Inception Score. In experiments, TTUR\\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\\nBedrooms, and the One Billion Word Benchmark.\\n',\n",
       " '  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we\\npresent new measurements of activity and magnetic field proxies of 442 low-mass\\nK5-M7 dwarfs. The objects were analysed as potential targets to search for\\nplanetary-mass companions with the new spectropolarimeter and high-precision\\nvelocimeter, SPIRou. We have analysed their high-resolution spectra in an\\nhomogeneous way: circular polarisation, chromospheric features, and Zeeman\\nbroadening of the FeH infrared line. The complex relationship between these\\nactivity indicators is analysed: while no strong connection is found between\\nthe large-scale and small-scale magnetic fields, the latter relates with the\\nnon-thermal flux originating in the chromosphere.\\nWe then examine the relationship between various activity diagnostics and the\\noptical radial-velocity jitter available in the literature, especially for\\nplanet host stars. We use this to derive for all stars an activity merit\\nfunction (higher for quieter stars) with the goal of identifying the most\\nfavorable stars where the radial-velocity jitter is low enough for planet\\nsearches. We find that the main contributors to the RV jitter are the\\nlarge-scale magnetic field and the chromospheric non-thermal emission.\\nIn addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed\\nalong their rotation using the spectropolarimetric mode, and we derive their\\nmagnetic topology. These very slow rotators are good representatives of future\\nSPIRou targets. They are compared to other stars where the magnetic topology is\\nalso known. The poloidal component of the magnetic field is predominent in all\\nthree stars.\\n',\n",
       " '  Inferring directional connectivity from point process data of multiple\\nelements is desired in various scientific fields such as neuroscience,\\ngeography, economics, etc. Here, we propose an inference procedure for this\\ngoal based on the kinetic Ising model. The procedure is composed of two steps:\\n(1) determination of the time-bin size for transforming the point-process data\\nto discrete time binary data and (2) screening of relevant couplings from the\\nestimated networks. For these, we develop simple methods based on information\\ntheory and computational statistics. Applications to data from artificial and\\n\\\\textit{in vitro} neuronal networks show that the proposed procedure performs\\nfairly well when identifying relevant couplings, including the discrimination\\nof their signs, with low computational cost. These results highlight the\\npotential utility of the kinetic Ising model to analyze real interacting\\nsystems with event occurrences.\\n',\n",
       " '  Support vector machines (SVMs) are an important tool in modern data analysis.\\nTraditionally, support vector machines have been fitted via quadratic\\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\\nan alternative approach to SVM fitting via the majorization--minimization (MM)\\nparadigm. Algorithms that are derived via MM algorithm constructions can be\\nshown to monotonically decrease their objectives at each iteration, as well as\\nbe globally convergent to stationary points. We demonstrate the construction of\\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\\nfor SVM risk minimization problems involving the hinge, least-square,\\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\\npenalizations. Successful implementations of our algorithms are presented via\\nsome numerical examples.\\n',\n",
       " '  Estimating vaccination uptake is an integral part of ensuring public health.\\nIt was recently shown that vaccination uptake can be estimated automatically\\nfrom web data, instead of slowly collected clinical records or population\\nsurveys. All prior work in this area assumes that features of vaccination\\nuptake collected from the web are temporally regular. We present the first ever\\nmethod to remove this assumption from vaccination uptake estimation: our method\\ndynamically adapts to temporal fluctuations in time series web data used to\\nestimate vaccination uptake. We show our method to outperform the state of the\\nart compared to competitive baselines that use not only web data but also\\ncurated clinical data. This performance improvement is more pronounced for\\nvaccines whose uptake has been irregular due to negative media attention (HPV-1\\nand HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of\\n12 years old (whose vaccination is more irregular compared to younger\\nchildren).\\n',\n",
       " '  We show that every invertible strong mixing transformation on a Lebesgue\\nspace has strictly over-recurrent sets. Also, we give an explicit procedure for\\nconstructing strong mixing transformations with no under-recurrent sets. This\\nanswers both parts of a question of V. Bergelson.\\nWe define $\\\\epsilon$-over-recurrence and show that given $\\\\epsilon > 0$, any\\nergodic measure preserving invertible transformation (including discrete\\nspectrum) has $\\\\epsilon$-over-recurrent sets of arbitrarily small measure.\\nDiscrete spectrum transformations and rotations do not have over-recurrent\\nsets, but we construct a weak mixing rigid transformation with strictly\\nover-recurrent sets.\\n',\n",
       " \"  Development of a mesoscale neural circuitry map of the common marmoset is an\\nessential task due to the ideal characteristics of the marmoset as a model\\norganism for neuroscience research. To facilitate this development there is a\\nneed for new computational tools to cross-register multi-modal data sets\\ncontaining MRI volumes as well as multiple histological series, and to register\\nthe combined data set to a common reference atlas. We present a fully automatic\\npipeline for same-subject-MRI guided reconstruction of image volumes from a\\nseries of histological sections of different modalities, followed by\\ndiffeomorphic mapping to a reference atlas. We show registration results for\\nNissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo\\nMRI as our reference and show that our method achieves accurate registration\\nand eliminates artifactual warping that may be result from the absence of a\\nreference MRI data set. Examination of the determinant of the local metric\\ntensor of the diffeomorphic mapping between each subject's ex-vivo MRI and\\nresultant Nissl reconstruction allows an unprecedented local quantification of\\ngeometrical distortions resulting from the histological processing, showing a\\nslight shrinkage, a median linear scale change of ~-1% in going from the\\nex-vivo MRI to the tape-transfer generated histological image data.\\n\",\n",
       " '  The system that we study in this paper contains a set of users that observe a\\ndiscrete memoryless multiple source and communicate via noise-free channels\\nwith the aim of attaining omniscience, the state that all users recover the\\nentire multiple source. We adopt the concept of successive omniscience (SO),\\ni.e., letting the local omniscience in some user subset be attained before the\\nglobal omniscience in the entire system, and consider the problem of how to\\nefficiently attain omniscience in a successive manner. Based on the existing\\nresults on SO, we propose a CompSetSO algorithm for determining a complimentary\\nset, a user subset in which the local omniscience can be attained first without\\nincreasing the sum-rate, the total number of communications, for the global\\nomniscience. We also derive a sufficient condition for a user subset to be\\ncomplimentary so that running the CompSetSO algorithm only requires a lower\\nbound, instead of the exact value, of the minimum sum-rate for attaining global\\nomniscience. The CompSetSO algorithm returns a complimentary user subset in\\npolynomial time. We show by example how to recursively apply the CompSetSO\\nalgorithm so that the global omniscience can be attained by multi-stages of SO.\\n',\n",
       " '  In this paper we present a novel methodology for identifying scholars with a\\nTwitter account. By combining bibliometric data from Web of Science and Twitter\\nusers identified by Altmetric.com we have obtained the largest set of\\nindividual scholars matched with Twitter users made so far. Our methodology\\nconsists of a combination of matching algorithms, considering different\\nlinguistic elements of both author names and Twitter names; followed by a\\nrule-based scoring system that weights the common occurrence of several\\nelements related with the names, individual elements and activities of both\\nTwitter users and scholars matched. Our results indicate that about 2% of the\\noverall population of scholars in the Web of Science is active on Twitter. By\\ndomain we find a strong presence of researchers from the Social Sciences and\\nthe Humanities. Natural Sciences is the domain with the lowest level of\\nscholars on Twitter. Researchers on Twitter also tend to be younger than those\\nthat are not on Twitter. As this is a bibliometric-based approach, it is\\nimportant to highlight the reliance of the method on the number of publications\\nproduced and tweeted by the scholars, thus the share of scholars on Twitter\\nranges between 1% and 5% depending on their level of productivity. Further\\nresearch is suggested in order to improve and expand the methodology.\\n',\n",
       " '  As a measure for the centrality of a point in a set of multivariate data,\\nstatistical depth functions play important roles in multivariate analysis,\\nbecause one may conveniently construct descriptive as well as inferential\\nprocedures relying on them. Many depth notions have been proposed in the\\nliterature to fit to different applications. However, most of them are mainly\\ndeveloped for the location setting. In this paper, we discuss the possibility\\nof extending some of them into the regression setting. A general concept of\\nregression depth function is also provided.\\n',\n",
       " \"  When a two-dimensional electron gas is exposed to a perpendicular magnetic\\nfield and an in-plane electric field, its conductance becomes quantized in the\\ntransverse in-plane direction: this is known as the quantum Hall (QH) effect.\\nThis effect is a result of the nontrivial topology of the system's electronic\\nband structure, where an integer topological invariant known as the first Chern\\nnumber leads to the quantization of the Hall conductance. Interestingly, it was\\nshown that the QH effect can be generalized mathematically to four spatial\\ndimensions (4D), but this effect has never been realized for the obvious reason\\nthat experimental systems are bound to three spatial dimensions. In this work,\\nwe harness the high tunability and control offered by photonic waveguide arrays\\nto experimentally realize a dynamically-generated 4D QH system using a 2D array\\nof coupled optical waveguides. The inter-waveguide separation is constructed\\nsuch that the propagation of light along the device samples over\\nhigher-dimensional momenta in the directions orthogonal to the two physical\\ndimensions, thus realizing a 2D topological pump. As a result, the device's\\nband structure is associated with 4D topological invariants known as second\\nChern numbers which support a quantized bulk Hall response with a 4D symmetry.\\nIn a finite-sized system, the 4D topological bulk response is carried by\\nlocalized edges modes that cross the sample as a function of of the modulated\\nauxiliary momenta. We directly observe this crossing through photon pumping\\nfrom edge-to-edge and corner-to-corner of our system. These are equivalent to\\nthe pumping of charge across a 4D system from one 3D hypersurface to the\\nopposite one and from one 2D hyperedge to another, and serve as first\\nexperimental realization of higher-dimensional topological physics.\\n\",\n",
       " '  In many applications involving large dataset or online updating, stochastic\\ngradient descent (SGD) provides a scalable way to compute parameter estimates\\nand has gained increasing popularity due to its numerical convenience and\\nmemory efficiency. While the asymptotic properties of SGD-based estimators have\\nbeen established decades ago, statistical inference such as interval estimation\\nremains much unexplored. The traditional resampling method such as the\\nbootstrap is not computationally feasible since it requires to repeatedly draw\\nindependent samples from the entire dataset. The plug-in method is not\\napplicable when there are no explicit formulas for the covariance matrix of the\\nestimator. In this paper, we propose a scalable inferential procedure for\\nstochastic gradient descent, which, upon the arrival of each observation,\\nupdates the SGD estimate as well as a large number of randomly perturbed SGD\\nestimates. The proposed method is easy to implement in practice. We establish\\nits theoretical properties for a general class of models that includes\\ngeneralized linear models and quantile regression models as special cases. The\\nfinite-sample performance and numerical utility is evaluated by simulation\\nstudies and two real data applications.\\n',\n",
       " '  In the work of Peng et al. in 2012, a new measure was proposed for fault\\ndiagnosis of systems: namely, g-good-neighbor conditional diagnosability, which\\nrequires that any fault-free vertex has at least g fault-free neighbors in the\\nsystem. In this paper, we establish the g-good-neighbor conditional\\ndiagnosability of locally twisted cubes under the PMC model and the MM^* model.\\n',\n",
       " \"  Categories of polymorphic lenses in computer science, and of open games in\\ncompositional game theory, have a curious structure that is reminiscent of\\ncompact closed categories, but differs in some crucial ways. Specifically they\\nhave a family of morphisms that behave like the counits of a compact closed\\ncategory, but have no corresponding units; and they have a `partial' duality\\nthat behaves like transposition in a compact closed category when it is\\ndefined. We axiomatise this structure, which we refer to as a `teleological\\ncategory'. We precisely define a diagrammatic language suitable for these\\ncategories, and prove a coherence theorem for them. This underpins the use of\\ndiagrammatic reasoning in compositional game theory, which has previously been\\nused only informally.\\n\",\n",
       " '  We present an efficient algorithm to compute Euler characteristic curves of\\ngray scale images of arbitrary dimension. In various applications the Euler\\ncharacteristic curve is used as a descriptor of an image.\\nOur algorithm is the first streaming algorithm for Euler characteristic\\ncurves. The usage of streaming removes the necessity to store the entire image\\nin RAM. Experiments show that our implementation handles terabyte scale images\\non commodity hardware. Due to lock-free parallelism, it scales well with the\\nnumber of processor cores. Our software---CHUNKYEuler---is available as open\\nsource on Bitbucket.\\nAdditionally, we put the concept of the Euler characteristic curve in the\\nwider context of computational topology. In particular, we explain the\\nconnection with persistence diagrams.\\n',\n",
       " '  We give a new example of an automata group of intermediate growth. It is\\ngenerated by an automaton with 4 states on an alphabet with 8 letters. This\\nautomata group has exponential activity and its limit space is not simply\\nconnected.\\n',\n",
       " '  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to\\nBose-Einstein condensation (BEC) is difficult to realize in quantum materials\\nbecause, unlike in ultracold atoms, one cannot tune the pairing interaction. We\\nrealize the BCS-BEC crossover in a nearly compensated semimetal\\nFe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\\\\epsilon_F$, via\\nchemical doping, which permits us to systematically change $\\\\Delta /\\n\\\\epsilon_F$ from 0.16 to 0.5 were $\\\\Delta$ is the superconducting (SC) gap. We\\nuse angle-resolved photoemission spectroscopy to measure the Fermi energy, the\\nSC gap and characteristic changes in the SC state electronic dispersion as the\\nsystem evolves from a BCS to a BEC regime. Our results raise important\\nquestions about the crossover in multiband superconductors which go beyond\\nthose addressed in the context of cold atoms.\\n',\n",
       " '  Model compression is essential for serving large deep neural nets on devices\\nwith limited resources or applications that require real-time responses. As a\\ncase study, a state-of-the-art neural language model usually consists of one or\\nmore recurrent layers sandwiched between an embedding layer used for\\nrepresenting input tokens and a softmax layer for generating output tokens. For\\nproblems with a very large vocabulary size, the embedding and the softmax\\nmatrices can account for more than half of the model size. For instance, the\\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\\nmethod for neural language models, based on vocabulary-partition (block) based\\nlow-rank matrix approximation and the inherent frequency distribution of tokens\\n(the power-law distribution of words). The experimental results show our method\\ncan significantly outperform traditional compression methods such as low-rank\\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\\ncompression rate for the embedding and softmax matrices, and when combined with\\nquantization, our method can achieve 26 times compression rate, which\\ntranslates to a factor of 12.8 times compression for the entire model with very\\nlittle degradation in perplexity.\\n',\n",
       " '  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy\\nwere implanted into SiO2 matrix with Different fluences. The implanted samples\\nwere annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of\\nimplanted as well as annealed samples were captured by the atomic force\\nmicroscopy (AFM). Two dimension (2D) multifractal detrended fluctuation\\nanalysis (MFDFA) based on the partition function approach has been used to\\nstudy the surfaces of ion implanted and annealed samples. The partition\\nfunction is used to calculate generalized Hurst exponent with the segment size.\\nMoreover, it is seen that the generalized Hurst exponents vary nonlinearly with\\nthe moment, thereby exhibiting the multifractal nature. The multifractality of\\nsurface is pronounced after annealing for the surface implanted with fluence\\n7.5X1016 ions/cm^2.\\n',\n",
       " '  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)\\nmaterial with liquid metal, Lead Lithium ( Pb-Li) has been studied under static\\ncondition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000\\nand 9000 hours. Corrosion rate was calculated from weight loss measurements.\\nMicrostructure analysis was carried out using SEM and chemical composition by\\nSEM-EDX measurements. Micro Vickers hardness and tensile testing were also\\ncarried out. Chromium was found leaching from the near surface regions and\\nsurface hardness was found to decrease in all the three cases. Grain boundaries\\nwere affected. Some grains got detached from the surface giving rise to pebble\\nlike structures in the surface micrographs. There was no significant reduction\\nin the tensile strength, after exposure to liquid metal. This paper discusses\\nthe experimental details and the results obtained.\\n',\n",
       " '  This paper presents an overview and discussion of magnetocapillary\\nself-assemblies. New results are presented, in particular concerning the\\npossible development of future applications. These self-organizing structures\\npossess the notable ability to move along an interface when powered by an\\noscillatory, uniform magnetic field. The system is constructed as follows. Soft\\nmagnetic particles are placed on a liquid interface, and submitted to a\\nmagnetic induction field. An attractive force due to the curvature of the\\ninterface around the particles competes with an interaction between magnetic\\ndipoles. Ordered structures can spontaneously emerge from these conditions.\\nFurthermore, time-dependent magnetic fields can produce a wide range of dynamic\\nbehaviours, including non-time-reversible deformation sequences that produce\\ntranslational motion at low Reynolds number. In other words, due to a\\nspontaneous breaking of time-reversal symmetry, the assembly can turn into a\\nsurface microswimmer. Trajectories have been shown to be precisely\\ncontrollable. As a consequence, this system offers a way to produce microrobots\\nable to perform different tasks. This is illustrated in this paper by the\\ncapture, transport and release of a floating cargo, and the controlled mixing\\nof fluids at low Reynolds number.\\n',\n",
       " '  For the problem of nonparametric detection of signal in Gaussian white noise\\nwe point out strong asymptotically minimax tests. The sets of alternatives are\\na ball in Besov space $B^r_{2\\\\infty}$ with \"small\" balls in $L_2$ removed.\\n',\n",
       " '  Metabolic flux balance analyses are a standard tool in analysing metabolic\\nreaction rates compatible with measurements, steady-state and the metabolic\\nreaction network stoichiometry. Flux analysis methods commonly place\\nunrealistic assumptions on fluxes due to the convenience of formulating the\\nproblem as a linear programming model, and most methods ignore the notable\\nuncertainty in flux estimates. We introduce a novel paradigm of Bayesian\\nmetabolic flux analysis that models the reactions of the whole genome-scale\\ncellular system in probabilistic terms, and can infer the full flux vector\\ndistribution of genome-scale metabolic systems based on exchange and\\nintracellular (e.g. 13C) flux measurements, steady-state assumptions, and\\ntarget function assumptions. The Bayesian model couples all fluxes jointly\\ntogether in a simple truncated multivariate posterior distribution, which\\nreveals informative flux couplings. Our model is a plug-in replacement to\\nconventional metabolic balance methods, such as flux balance analysis (FBA).\\nOur experiments indicate that we can characterise the genome-scale flux\\ncovariances, reveal flux couplings, and determine more intracellular unobserved\\nfluxes in C. acetobutylicum from 13C data than flux variability analysis. The\\nCOBRA compatible software is available at github.com/markusheinonen/bamfa\\n',\n",
       " '  We introduce a robust estimator of the location parameter for the\\nchange-point in the mean based on the Wilcoxon statistic and establish its\\nconsistency for $L_1$ near epoch dependent processes. It is shown that the\\nconsistency rate depends on the magnitude of change. A simulation study is\\nperformed to evaluate finite sample properties of the Wilcoxon-type estimator\\nin standard cases, as well as under heavy-tailed distributions and disturbances\\nby outliers, and to compare it with a CUSUM-type estimator. It shows that the\\nWilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard\\ncases, but outperforms the CUSUM-type estimator in presence of heavy tails or\\noutliers in the data.\\n',\n",
       " '  In glass forming liquids close to the glass transition point, even a very\\nslight increase in the macroscopic density results in a dramatic slowing down\\nof the macroscopic relaxation. Concomitantly, the local density itself\\nfluctuates in space. Therefore, one can imagine that even very small local\\ndensity variations control the local glassy nature. Based on this perspective,\\na model for describing growing length scale accompanying the vitrification is\\nintroduced, in which we assume that in a subsystem whose density is above a\\ncertain threshold value, $\\\\rho_{\\\\rm c}$, owing to steric constraints, particle\\nrearrangements are highly suppressed for a sufficiently long time period\\n($\\\\sim$ structural relaxation time). We regard such a subsystem as a glassy\\ncluster. Then, based on the statistics of the subsystem-density, we predict\\nthat with compression (increasing average density $\\\\rho$) at a fixed\\ntemperature $T$ in supercooled states, the characteristic length of the\\nclusters, $\\\\xi$, diverges as $\\\\xi\\\\sim(\\\\rho_{\\\\rm c}-\\\\rho)^{-2/d}$, where $d$ is\\nthe spatial dimensionality. This $\\\\xi$ measures the average persistence length\\nof the steric constraints in blocking the rearrangement motions and is\\ndetermined by the subsystem density. Additionally, with decreasing $T$ at a\\nfixed $\\\\rho$, the length scale diverges in the same manner as $\\\\xi\\\\sim(T-T_{\\\\rm\\nc})^{-2/d}$, for which $\\\\rho$ is identical to $\\\\rho_{\\\\rm c}$ at $T=T_{\\\\rm c}$.\\nThe exponent describing the diverging length scale is the same as the one\\npredicted by some theoretical models and indeed has been observed in some\\nsimulations and experiments. However, the basic mechanism for this divergence\\nis different; that is, we do not invoke thermodynamic anomalies associated with\\nthe thermodynamic phase transition as the origin of the growing length scale.\\nWe further present arguements for the cooperative properties based on the\\nclusters.\\n',\n",
       " '  We propose a new Pareto Local Search Algorithm for the many-objective\\ncombinatorial optimization. Pareto Local Search proved to be a very effective\\ntool in the case of the bi-objective combinatorial optimization and it was used\\nin a number of the state-of-the-art algorithms for problems of this kind. On\\nthe other hand, the standard Pareto Local Search algorithm becomes very\\ninefficient for problems with more than two objectives. We build an effective\\nMany-Objective Pareto Local Search algorithm using three new mechanisms: the\\nefficient update of large Pareto archives with ND-Tree data structure, a new\\nmechanism for the selection of the promising solutions for the neighborhood\\nexploration, and a partial exploration of the neighborhoods. We apply the\\nproposed algorithm to the instances of two different problems, i.e. the\\ntraveling salesperson problem and the traveling salesperson problem with\\nprofits with up to 5 objectives showing high effectiveness of the proposed\\nalgorithm.\\n',\n",
       " '  We identify the components of bio-inspired artificial camouflage systems\\nincluding actuation, sensing, and distributed computation. After summarizing\\nrecent results in understanding the physiology and system-level performance of\\na variety of biological systems, we describe computational algorithms that can\\ngenerate similar patterns and have the potential for distributed\\nimplementation. We find that the existing body of work predominately treats\\ncomponent technology in an isolated manner that precludes a material-like\\nimplementation that is scale-free and robust. We conclude with open research\\nchallenges towards the realization of integrated camouflage solutions.\\n',\n",
       " '  In the present work we study Bayesian nonparametric inference for the\\ncontinuous-time M/G/1 queueing system. In the focus of the study is the\\nunobservable service time distribution. We assume that the only available data\\nof the system are the marked departure process of customers with the marks\\nbeing the queue lengths just after departure instants. These marks constitute\\nan embedded Markov chain whose distribution may be parametrized by stochastic\\nmatrices of a special delta form. We develop the theory in order to obtain\\nintegral mixtures of Markov measures with respect to suitable prior\\ndistributions. We have found a sufficient statistic with a distribution of a\\nso-called S-structure sheding some new light on the inner statistical structure\\nof the M/G/1 queue. Moreover, it allows to update suitable prior distributions\\nto the posterior. Our inference methods are validated by large sample results\\nas posterior consistency and posterior normality.\\n',\n",
       " '  We will show that $(1-q)(1-q^2)\\\\dots (1-q^m)$ is a polynomial in $q$ with\\ncoefficients from $\\\\{-1,0,1\\\\}$ iff $m=1,\\\\ 2,\\\\ 3,$ or $5$ and explore some\\ninteresting consequences of this result. We find explicit formulas for the\\n$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\\\\dots$ and\\n$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\\\\dots$. In doing so, we extend certain\\nobservations made by Sudler in 1964. We also discuss the classification of the\\nproducts $(1-q)(1-q^2)\\\\dots (1-q^m)$ and some related series with respect to\\ntheir absolute largest coefficients.\\n',\n",
       " '  In this paper, we develop a position estimation system for Unmanned Aerial\\nVehicles formed by hardware and software. It is based on low-cost devices: GPS,\\ncommercial autopilot sensors and dense optical flow algorithm implemented in an\\nonboard microcomputer. Comparative tests were conducted using our approach and\\nthe conventional one, where only fusion of GPS and inertial sensors are used.\\nExperiments were conducted using a quadrotor in two flying modes: hovering and\\ntrajectory tracking in outdoor environments. Results demonstrate the\\neffectiveness of the proposed approach in comparison with the conventional\\napproaches presented in the vast majority of commercial drones.\\n',\n",
       " '  We study the decomposition of a multivariate Hankel matrix H\\\\_$\\\\sigma$ as a\\nsum of Hankel matrices of small rank in correlation with the decomposition of\\nits symbol $\\\\sigma$ as a sum of polynomial-exponential series. We present a new\\nalgorithm to compute the low rank decomposition of the Hankel operator and the\\ndecomposition of its symbol exploiting the properties of the associated\\nArtinian Gorenstein quotient algebra A\\\\_$\\\\sigma$. A basis of A\\\\_$\\\\sigma$ is\\ncomputed from the Singular Value Decomposition of a sub-matrix of the Hankel\\nmatrix H\\\\_$\\\\sigma$. The frequencies and the weights are deduced from the\\ngeneralized eigenvectors of pencils of shifted sub-matrices of H $\\\\sigma$.\\nExplicit formula for the weights in terms of the eigenvectors avoid us to solve\\na Vandermonde system. This new method is a multivariate generalization of the\\nso-called Pencil method for solving Prony-type decomposition problems. We\\nanalyse its numerical behaviour in the presence of noisy input moments, and\\ndescribe a rescaling technique which improves the numerical quality of the\\nreconstruction for frequencies of high amplitudes. We also present a new Newton\\niteration, which converges locally to the closest multivariate Hankel matrix of\\nlow rank and show its impact for correcting errors on input moments.\\n',\n",
       " '  Linear time-periodic (LTP) dynamical systems frequently appear in the\\nmodeling of phenomena related to fluid dynamics, electronic circuits, and\\nstructural mechanics via linearization centered around known periodic orbits of\\nnonlinear models. Such LTP systems can reach orders that make repeated\\nsimulation or other necessary analysis prohibitive, motivating the need for\\nmodel reduction.\\nWe develop here an algorithmic framework for constructing reduced models that\\nretains the linear time-periodic structure of the original LTP system. Our\\napproach generalizes optimal approaches that have been established previously\\nfor linear time-invariant (LTI) model reduction problems. We employ an\\nextension of the usual H2 Hardy space defined for the LTI setting to\\ntime-periodic systems and within this broader framework develop an a posteriori\\nerror bound expressible in terms of related LTI systems. Optimization of this\\nbound motivates our algorithm. We illustrate the success of our method on two\\nnumerical examples.\\n',\n",
       " '  Broad efforts are underway to capture metadata about research software and\\nretain it across services; notable in this regard is the CodeMeta project. What\\nmetadata are important to have about (research) software? What metadata are\\nuseful for searching for codes? What would you like to learn about astronomy\\nsoftware? This BoF sought to gather information on metadata most desired by\\nresearchers and users of astro software and others interested in registering,\\nindexing, capturing, and doing research on this software. Information from this\\nBoF could conceivably result in changes to the Astrophysics Source Code Library\\n(ASCL) or other resources for the benefit of the community or provide input\\ninto other projects concerned with software metadata.\\n',\n",
       " \"  Recently, digital music libraries have been developed and can be plainly\\naccessed. Latest research showed that current organization and retrieval of\\nmusic tracks based on album information are inefficient. Moreover, they\\ndemonstrated that people use emotion tags for music tracks in order to search\\nand retrieve them. In this paper, we discuss separability of a set of emotional\\nlabels, proposed in the categorical emotion expression, using Fisher's\\nseparation theorem. We determine a set of adjectives to tag music parts: happy,\\nsad, relaxing, exciting, epic and thriller. Temporal, frequency and energy\\nfeatures have been extracted from the music parts. It could be seen that the\\nmaximum separability within the extracted features occurs between relaxing and\\nepic music parts. Finally, we have trained a classifier using Support Vector\\nMachines to automatically recognize and generate emotional labels for a music\\npart. Accuracy for recognizing each label has been calculated; where the\\nresults show that epic music can be recognized more accurately (77.4%),\\ncomparing to the other types of music.\\n\",\n",
       " '  One key requirement for effective supply chain management is the quality of\\nits inventory management. Various inventory management methods are typically\\nemployed for different types of products based on their demand patterns,\\nproduct attributes, and supply network. In this paper, our goal is to develop\\nrobust demand prediction methods for weather sensitive products at retail\\nstores. We employ historical datasets from Walmart, whose customers and markets\\nare often exposed to extreme weather events which can have a huge impact on\\nsales regarding the affected stores and products. We want to accurately predict\\nthe sales of 111 potentially weather-sensitive products around the time of\\nmajor weather events at 45 of Walmart retails locations in the U.S.\\nIntuitively, we may expect an uptick in the sales of umbrellas before a big\\nthunderstorm, but it is difficult for replenishment managers to predict the\\nlevel of inventory needed to avoid being out-of-stock or overstock during and\\nafter that storm. While they rely on a variety of vendor tools to predict sales\\naround extreme weather events, they mostly employ a time-consuming process that\\nlacks a systematic measure of effectiveness. We employ all the methods critical\\nto any analytics project and start with data exploration. Critical features are\\nextracted from the raw historical dataset for demand forecasting accuracy and\\nrobustness. In particular, we employ Artificial Neural Network for forecasting\\ndemand for each product sold around the time of major weather events. Finally,\\nwe evaluate our model to evaluate their accuracy and robustness.\\n',\n",
       " '  We propose a deformable generator model to disentangle the appearance and\\ngeometric information from images into two independent latent vectors. The\\nappearance generator produces the appearance information, including color,\\nillumination, identity or category, of an image. The geometric generator\\nproduces displacement of the coordinates of each pixel and performs geometric\\nwarping, such as stretching and rotation, on the appearance generator to obtain\\nthe final synthesized image. The proposed model can learn both representations\\nfrom image data in an unsupervised manner. The learned geometric generator can\\nbe conveniently transferred to the other image datasets to facilitate\\ndownstream AI tasks.\\n',\n",
       " '  The Gaussian kernel is a very popular kernel function used in many\\nmachine-learning algorithms, especially in support vector machines (SVM). For\\nnonlinear training instances in machine learning, it often outperforms\\npolynomial kernels in model accuracy. We use Gaussian kernel profoundly in\\nformulating nonlinear classical SVM. In the recent research, P. Rebentrost\\net.al. discuss a very elegant quantum version of least square support vector\\nmachine using the quantum version of polynomial kernel, which is exponentially\\nfaster than the classical counterparts. In this paper, we have demonstrated a\\nquantum version of the Gaussian kernel and analyzed its complexity in the\\ncontext of quantum SVM. Our analysis shows that the computational complexity of\\nthe quantum Gaussian kernel is O(\\\\epsilon^(-1)logN) with N-dimensional\\ninstances and \\\\epsilon with a Taylor remainder error term |R_m (\\\\epsilon^(-1)\\nlogN)|.\\n',\n",
       " '  Security, privacy, and fairness have become critical in the era of data\\nscience and machine learning. More and more we see that achieving universally\\nsecure, private, and fair systems is practically impossible. We have seen for\\nexample how generative adversarial networks can be used to learn about the\\nexpected private training data; how the exploitation of additional data can\\nreveal private information in the original one; and how what looks like\\nunrelated features can teach us about each other. Confronted with this\\nchallenge, in this paper we open a new line of research, where the security,\\nprivacy, and fairness is learned and used in a closed environment. The goal is\\nto ensure that a given entity (e.g., the company or the government), trusted to\\ninfer certain information with our data, is blocked from inferring protected\\ninformation from it. For example, a hospital might be allowed to produce\\ndiagnosis on the patient (the positive task), without being able to infer the\\ngender of the subject (negative task). Similarly, a company can guarantee that\\ninternally it is not using the provided data for any undesired task, an\\nimportant goal that is not contradicting the virtually impossible challenge of\\nblocking everybody from the undesired task. We design a system that learns to\\nsucceed on the positive task while simultaneously fail at the negative one, and\\nillustrate this with challenging cases where the positive task is actually\\nharder than the negative one being blocked. Fairness, to the information in the\\nnegative task, is often automatically obtained as a result of this proposed\\napproach. The particular framework and examples open the door to security,\\nprivacy, and fairness in very important closed scenarios, ranging from private\\ndata accumulation companies like social networks to law-enforcement and\\nhospitals.\\n',\n",
       " '  The difficulty of modeling energy consumption in communication systems leads\\nto challenges in energy harvesting (EH) systems, in which nodes scavenge energy\\nfrom their environment. An EH receiver must harvest enough energy for\\ndemodulating and decoding. The energy required depends upon factors, like code\\nrate and signal-to-noise ratio, which can be adjusted dynamically. We consider\\na receiver which harvests energy from ambient sources and the transmitter,\\nmeaning the received signal is used for both EH and information decoding.\\nAssuming a generalized function for energy consumption, we maximize the total\\nnumber of information bits decoded, under both average and peak power\\nconstraints at the transmitter, by carefully optimizing the power used for EH,\\npower used for information transmission, fraction of time for EH, and code\\nrate. For transmission over a single block, we find there exist problem\\nparameters for which either maximizing power for information transmission or\\nmaximizing power for EH is optimal. In the general case, the optimal solution\\nis a tradeoff of the two. For transmission over multiple blocks, we give an\\nupper bound on performance and give sufficient and necessary conditions to\\nachieve this bound. Finally, we give some numerical results to illustrate our\\nresults and analysis.\\n',\n",
       " \"  This paper studies a recently proposed continuous-time distributed\\nself-appraisal model with time-varying interactions among a network of $n$\\nindividuals which are characterized by a sequence of time-varying relative\\ninteraction matrices. The model describes the evolution of the\\nsocial-confidence levels of the individuals via a reflected appraisal mechanism\\nin real time. We first show by example that when the relative interaction\\nmatrices are stochastic (not doubly stochastic), the social-confidence levels\\nof the individuals may not converge to a steady state. We then show that when\\nthe relative interaction matrices are doubly stochastic, the $n$ individuals'\\nself-confidence levels will all converge to $1/n$, which indicates a democratic\\nstate, exponentially fast under appropriate assumptions, and provide an\\nexplicit expression of the convergence rate.\\n\",\n",
       " '  When the brain receives input from multiple sensory systems, it is faced with\\nthe question of whether it is appropriate to process the inputs in combination,\\nas if they originated from the same event, or separately, as if they originated\\nfrom distinct events. Furthermore, it must also have a mechanism through which\\nit can keep sensory inputs calibrated to maintain the accuracy of its internal\\nrepresentations. We have developed a neural network architecture capable of i)\\napproximating optimal multisensory spatial integration, based on Bayesian\\ncausal inference, and ii) recalibrating the spatial encoding of sensory\\nsystems. The architecture is based on features of the dorsal processing\\nhierarchy, including the spatial tuning properties of unisensory neurons and\\nthe convergence of different sensory inputs onto multisensory neurons.\\nFurthermore, we propose that these unisensory and multisensory neurons play\\ndual roles in i) encoding spatial location as separate or integrated estimates\\nand ii) accumulating evidence for the independence or relatedness of\\nmultisensory stimuli. We further propose that top-down feedback connections\\nspanning the dorsal pathway play key a role in recalibrating spatial encoding\\nat the level of early unisensory cortices. Our proposed architecture provides\\npossible explanations for a number of human electrophysiological and\\nneuroimaging results and generates testable predictions linking neurophysiology\\nwith behaviour.\\n',\n",
       " \"  A common problem in large-scale data analysis is to approximate a matrix\\nusing a combination of specifically sampled rows and columns, known as CUR\\ndecomposition. Unfortunately, in many real-world environments, the ability to\\nsample specific individual rows or columns of the matrix is limited by either\\nsystem constraints or cost. In this paper, we consider matrix approximation by\\nsampling predefined \\\\emph{blocks} of columns (or rows) from the matrix. We\\npresent an algorithm for sampling useful column blocks and provide novel\\nguarantees for the quality of the approximation. This algorithm has application\\nin problems as diverse as biometric data analysis to distributed computing. We\\ndemonstrate the effectiveness of the proposed algorithms for computing the\\nBlock CUR decomposition of large matrices in a distributed setting with\\nmultiple nodes in a compute cluster, where such blocks correspond to columns\\n(or rows) of the matrix stored on the same node, which can be retrieved with\\nmuch less overhead than retrieving individual columns stored across different\\nnodes. In the biometric setting, the rows correspond to different users and\\ncolumns correspond to users' biometric reaction to external stimuli, {\\\\em\\ne.g.,}~watching video content, at a particular time instant. There is\\nsignificant cost in acquiring each user's reaction to lengthy content so we\\nsample a few important scenes to approximate the biometric response. An\\nindividual time sample in this use case cannot be queried in isolation due to\\nthe lack of context that caused that biometric reaction. Instead, collections\\nof time segments ({\\\\em i.e.,} blocks) must be presented to the user. The\\npractical application of these algorithms is shown via experimental results\\nusing real-world user biometric data from a content testing environment.\\n\",\n",
       " '  The unusually high surface tension of room temperature liquid metal is\\nmolding it as unique material for diverse newly emerging areas. However, unlike\\nits practices on earth, such metal fluid would display very different behaviors\\nwhen working in space where gravity disappears and surface property dominates\\nthe major physics. So far, few direct evidences are available to understand\\nsuch effect which would impede further exploration of liquid metal use for\\nspace. Here to preliminarily probe into this intriguing issue, a low cost\\nexperimental strategy to simulate microgravity environment on earth was\\nproposed through adopting bridges with high enough free falling distance as the\\ntest platform. Then using digital cameras amounted along x, y, z directions on\\noutside wall of the transparent container with liquid metal and allied solution\\ninside, synchronous observations on the transient flow and transformational\\nactivities of liquid metal were performed. Meanwhile, an unmanned aerial\\nvehicle was adopted to record the whole free falling dynamics of the test\\ncapsule from the far end which can help justify subsequent experimental\\nprocedures. A series of typical fundamental phenomena were thus observed as:\\n(a) A relatively large liquid metal object would spontaneously transform from\\nits original planar pool state into a sphere and float in the container if\\ninitiating the free falling; (b) The liquid metal changes its three-dimensional\\nshape due to dynamic microgravity strength due to free falling and rebound of\\nthe test capsule; and (c) A quick spatial transformation of liquid metal\\nimmersed in the solution can easily be induced via external electrical fields.\\nThe mechanisms of the surface tension driven liquid metal actuation in space\\nwere interpreted. All these findings indicated that microgravity effect should\\nbe fully treated in developing future generation liquid metal space\\ntechnologies.\\n',\n",
       " '  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)\\nmethod for performing approximate inference in complex probabilistic models of\\ncontinuous variables. In common with many MCMC methods, however, the standard\\nHMC approach performs poorly in distributions with multiple isolated modes. We\\npresent a method for augmenting the Hamiltonian system with an extra continuous\\ntemperature control variable which allows the dynamic to bridge between\\nsampling a complex target distribution and a simpler unimodal base\\ndistribution. This augmentation both helps improve mixing in multimodal targets\\nand allows the normalisation constant of the target distribution to be\\nestimated. The method is simple to implement within existing HMC code,\\nrequiring only a standard leapfrog integrator. We demonstrate experimentally\\nthat the method is competitive with annealed importance sampling and simulating\\ntempering methods at sampling from challenging multimodal distributions and\\nestimating their normalising constants.\\n',\n",
       " \"  We present a new method for the automated synthesis of digital controllers\\nwith formal safety guarantees for systems with nonlinear dynamics, noisy output\\nmeasurements, and stochastic disturbances. Our method derives digital\\ncontrollers such that the corresponding closed-loop system, modeled as a\\nsampled-data stochastic control system, satisfies a safety specification with\\nprobability above a given threshold. The proposed synthesis method alternates\\nbetween two steps: generation of a candidate controller pc, and verification of\\nthe candidate. pc is found by maximizing a Monte Carlo estimate of the safety\\nprobability, and by using a non-validated ODE solver for simulating the system.\\nSuch a candidate is therefore sub-optimal but can be generated very rapidly. To\\nrule out unstable candidate controllers, we prove and utilize Lyapunov's\\nindirect method for instability of sampled-data nonlinear systems. In the\\nsubsequent verification step, we use a validated solver based on SMT\\n(Satisfiability Modulo Theories) to compute a numerically and statistically\\nvalid confidence interval for the safety probability of pc. If the probability\\nso obtained is not above the threshold, we expand the search space for\\ncandidates by increasing the controller degree. We evaluate our technique on\\nthree case studies: an artificial pancreas model, a powertrain control model,\\nand a quadruple-tank process.\\n\",\n",
       " '  In the present paper we consider numerical methods to solve the discrete\\nSchrödinger equation with a time dependent Hamiltonian (motivated by problems\\nencountered in the study of spin systems). We will consider both short-range\\ninteractions, which lead to evolution equations involving sparse matrices, and\\nlong-range interactions, which lead to dense matrices. Both of these settings\\nshow very different computational characteristics. We use Magnus integrators\\nfor time integration and employ a framework based on Leja interpolation to\\ncompute the resulting action of the matrix exponential. We consider both\\ntraditional Magnus integrators (which are extensively used for these types of\\nproblems in the literature) as well as the recently developed commutator-free\\nMagnus integrators and implement them on modern CPU and GPU (graphics\\nprocessing unit) based systems.\\nWe find that GPUs can yield a significant speed-up (up to a factor of $10$ in\\nthe dense case) for these types of problems. In the sparse case GPUs are only\\nadvantageous for large problem sizes and the achieved speed-ups are more\\nmodest. In most cases the commutator-free variant is superior but especially on\\nthe GPU this advantage is rather small. In fact, none of the advantage of\\ncommutator-free methods on GPUs (and on multi-core CPUs) is due to the\\nelimination of commutators. This has important consequences for the design of\\nmore efficient numerical methods.\\n',\n",
       " '  This paper re-investigates the estimation of multiple factor models relaxing\\nthe convention that the number of factors is small and using a new approach for\\nidentifying factors. We first obtain the collection of all possible factors and\\nthen provide a simultaneous test, security by security, of which factors are\\nsignificant. Since the collection of risk factors is large and highly\\ncorrelated, high-dimension methods (including the LASSO and prototype\\nclustering) have to be used. The multi-factor model is shown to have a\\nsignificantly better fit than the Fama-French 5-factor model. Robustness tests\\nare also provided.\\n',\n",
       " '  We experimentally confirmed the threshold behavior and scattering length\\nscaling law of the three-body loss coefficients in an ultracold spin-polarized\\ngas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the\\nthree-body loss coefficients as functions of temperature and scattering volume,\\nand found that the threshold law and the scattering length scaling law hold in\\nlimited temperature and magnetic field regions. We also found that the\\nbreakdown of the scaling laws is due to the emergence of the effective-range\\nterm. This work is an important first step toward full understanding of the\\nloss of identical fermions with $p$-wave interactions.\\n',\n",
       " \"  The paper proposes an expanded version of the Local Variance Gamma model of\\nCarr and Nadtochiy by adding drift to the governing underlying process. Still\\nin this new model it is possible to derive an ordinary differential equation\\nfor the option price which plays a role of Dupire's equation for the standard\\nlocal volatility model. It is shown how calibration of multiple smiles (the\\nwhole local volatility surface) can be done in such a case. Further, assuming\\nthe local variance to be a piecewise linear function of strike and piecewise\\nconstant function of time this ODE is solved in closed form in terms of\\nConfluent hypergeometric functions. Calibration of the model to market smiles\\ndoes not require solving any optimization problem and, in contrast, can be done\\nterm-by-term by solving a system of non-linear algebraic equations for each\\nmaturity, which is fast.\\n\",\n",
       " '  In processing human produced text using natural language processing (NLP)\\ntechniques, two fundamental subtasks that arise are (i) segmentation of the\\nplain text into meaningful subunits (e.g., entities), and (ii) dependency\\nparsing, to establish relations between subunits. In this paper, we develop a\\nrelatively simple and effective neural joint model that performs both\\nsegmentation and dependency parsing together, instead of one after the other as\\nin most state-of-the-art works. We will focus in particular on the real estate\\nad setting, aiming to convert an ad to a structured description, which we name\\nproperty tree, comprising the tasks of (1) identifying important entities of a\\nproperty (e.g., rooms) from classifieds and (2) structuring them into a tree\\nformat. In this work, we propose a new joint model that is able to tackle the\\ntwo tasks simultaneously and construct the property tree by (i) avoiding the\\nerror propagation that would arise from the subtasks one after the other in a\\npipelined fashion, and (ii) exploiting the interactions between the subtasks.\\nFor this purpose, we perform an extensive comparative study of the pipeline\\nmethods and the new proposed joint model, reporting an improvement of over\\nthree percentage points in the overall edge F1 score of the property tree.\\nAlso, we propose attention methods, to encourage our model to focus on salient\\ntokens during the construction of the property tree. Thus we experimentally\\ndemonstrate the usefulness of attentive neural architectures for the proposed\\njoint model, showcasing a further improvement of two percentage points in edge\\nF1 score for our application.\\n',\n",
       " '  The asymptotic variance of the maximum likelihood estimate is proved to\\ndecrease when the maximization is restricted to a subspace that contains the\\ntrue parameter value. Maximum likelihood estimation allows a systematic fitting\\nof covariance models to the sample, which is important in data assimilation.\\nThe hierarchical maximum likelihood approach is applied to the spectral\\ndiagonal covariance model with different parameterizations of eigenvalue decay,\\nand to the sparse inverse covariance model with specified parameter values on\\ndifferent sets of nonzero entries. It is shown computationally that using\\nsmaller sets of parameters can decrease the sampling noise in high dimension\\nsubstantially.\\n',\n",
       " '  Fully automating machine learning pipelines is one of the key challenges of\\ncurrent artificial intelligence research, since practical machine learning\\noften requires costly and time-consuming human-powered processes such as model\\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\\nverify that automated architecture search synergizes with the effect of\\ngradient-based meta learning. We adopt the progressive neural architecture\\nsearch \\\\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\\narchitectures for meta-learners. The gradient based meta-learner whose\\narchitecture was automatically found achieved state-of-the-art results on the\\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\\\%$ accuracy,\\nwhich is $11.54\\\\%$ improvement over the result obtained by the first\\ngradient-based meta-learner called MAML\\n\\\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\\nthe first successful neural architecture search implementation in the context\\nof meta learning.\\n',\n",
       " '  We provide a comprehensive study of the convergence of forward-backward\\nalgorithm under suitable geometric conditions leading to fast rates. We present\\nseveral new results and collect in a unified view a variety of results\\nscattered in the literature, often providing simplified proofs. Novel\\ncontributions include the analysis of infinite dimensional convex minimization\\nproblems, allowing the case where minimizers might not exist. Further, we\\nanalyze the relation between different geometric conditions, and discuss novel\\nconnections with a priori conditions in linear inverse problems, including\\nsource conditions, restricted isometry properties and partial smoothness.\\n',\n",
       " '  Magnetic Particle Imaging (MPI) is a novel imaging modality with important\\napplications such as angiography, stem cell tracking, and cancer imaging.\\nRecently, there have been efforts to increase the functionality of MPI via\\nmulti-color imaging methods that can distinguish the responses of different\\nnanoparticles, or nanoparticles in different environmental conditions. The\\nproposed techniques typically rely on extensive calibrations that capture the\\ndifferences in the harmonic responses of the nanoparticles. In this work, we\\npropose a method to directly estimate the relaxation time constant of the\\nnanoparticles from the MPI signal, which is then used to generate a multi-color\\nrelaxation map. The technique is based on the underlying mirror symmetry of the\\nadiabatic MPI signal when the same region is scanned back and forth. We\\nvalidate the proposed method via extensive simulations, and via experiments on\\nour in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our\\nin-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be\\nsuccessfully distinguished with the proposed technique, without any calibration\\nor prior knowledge about the nanoparticles.\\n',\n",
       " '  Draft of textbook chapter on neural machine translation. a comprehensive\\ntreatment of the topic, ranging from introduction to neural networks,\\ncomputation graphs, description of the currently dominant attentional\\nsequence-to-sequence model, recent refinements, alternative architectures and\\nchallenges. Written as chapter for the textbook Statistical Machine\\nTranslation. Used in the JHU Fall 2017 class on machine translation.\\n',\n",
       " '  Let $D$ be a bounded domain $D$ in $\\\\mathbb R^n $ with infinitely smooth\\nboundary and $n$ is odd. We prove that if the volume cut off from the domain by\\na hyperplane is an algebraic function of the hyperplane, free of real singular\\npoints, then the domain is an ellipsoid. This partially answers a question of\\nV.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically\\nintegrable domains?\\n',\n",
       " '  The novel unseen classes can be formulated as the extreme values of known\\nclasses. This inspired the recent works on open-set recognition\\n\\\\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no\\nway of naming the novel unseen classes. To solve this problem, we propose the\\nExtreme Value Learning (EVL) formulation to learn the mapping from visual\\nfeature to semantic space. To model the margin and coverage distributions of\\neach class, the Vocabulary-informed Learning (ViL) is adopted by using vast\\nopen vocabulary in the semantic space. Essentially, by incorporating the EVL\\nand ViL, we for the first time propose a novel semantic embedding paradigm --\\nVocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual\\nfeatures into semantic space in a probabilistic way. The learned embedding can\\nbe directly used to solve supervised learning, zero-shot and open set\\nrecognition simultaneously. Experiments on two benchmark datasets demonstrate\\nthe effectiveness of proposed frameworks.\\n',\n",
       " \"  In this paper, we study the performance of two cross-layer optimized dynamic\\nrouting techniques for radio interference mitigation across multiple coexisting\\nwireless body area networks (BANs), based on real-life measurements. At the\\nnetwork layer, the best route is selected according to channel state\\ninformation from the physical layer, associated with low duty cycle TDMA at the\\nMAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel\\ncooperative multi-path routing (CMR) incorporating 3-branch selection\\ncombining) perform real-time and reliable data transfer across BANs operating\\nnear the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'\\nmixed-activities is used for analyzing the proposed cross-layer optimization.\\nWe show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and\\neven 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage\\nprobability at a realistic signal-to-interference-plus-noise ratio (SINR).\\nAcceptable packet delivery ratios (PDR) and spectral efficiencies are obtained\\nfrom SPR and CMR with reasonably sensitive receivers across a range of TDMA low\\nduty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The\\ndistribution fits for received SINR through routing are also derived and\\nvalidated with theoretical analysis.\\n\",\n",
       " '  In recent years, the proliferation of online resumes and the need to evaluate\\nlarge populations of candidates for on-site and virtual teams have led to a\\ngrowing interest in automated team-formation. Given a large pool of candidates,\\nthe general problem requires the selection of a team of experts to complete a\\ngiven task. Surprisingly, while ongoing research has studied numerous\\nvariations with different constraints, it has overlooked a factor with a\\nwell-documented impact on team cohesion and performance: team faultlines.\\nAddressing this gap is challenging, as the available measures for faultlines in\\nexisting teams cannot be efficiently applied to faultline optimization. In this\\nwork, we meet this challenge with a new measure that can be efficiently used\\nfor both faultline measurement and minimization. We then use the measure to\\nsolve the problem of automatically partitioning a large population into\\nlow-faultline teams. By introducing faultlines to the team-formation\\nliterature, our work creates exciting opportunities for algorithmic work on\\nfaultline optimization, as well as on work that combines and studies the\\nconnection of faultlines with other influential team characteristics.\\n',\n",
       " '  Deep convolutional neural networks (CNNs) have recently achieved great\\nsuccess in many visual recognition tasks. However, existing deep neural network\\nmodels are computationally expensive and memory intensive, hindering their\\ndeployment in devices with low memory resources or in applications with strict\\nlatency requirements. Therefore, a natural thought is to perform model\\ncompression and acceleration in deep networks without significantly decreasing\\nthe model performance. During the past few years, tremendous progress has been\\nmade in this area. In this paper, we survey the recent advanced techniques for\\ncompacting and accelerating CNNs model developed. These techniques are roughly\\ncategorized into four schemes: parameter pruning and sharing, low-rank\\nfactorization, transferred/compact convolutional filters, and knowledge\\ndistillation. Methods of parameter pruning and sharing will be described at the\\nbeginning, after that the other techniques will be introduced. For each scheme,\\nwe provide insightful analysis regarding the performance, related applications,\\nadvantages, and drawbacks etc. Then we will go through a few very recent\\nadditional successful methods, for example, dynamic capacity networks and\\nstochastic depths networks. After that, we survey the evaluation matrix, the\\nmain datasets used for evaluating the model performance and recent benchmarking\\nefforts. Finally, we conclude this paper, discuss remaining challenges and\\npossible directions on this topic.\\n',\n",
       " '  The task of calibration is to retrospectively adjust the outputs from a\\nmachine learning model to provide better probability estimates on the target\\nvariable. While calibration has been investigated thoroughly in classification,\\nit has not yet been well-established for regression tasks. This paper considers\\nthe problem of calibrating a probabilistic regression model to improve the\\nestimated probability densities over the real-valued targets. We propose to\\ncalibrate a regression model through the cumulative probability density, which\\ncan be derived from calibrating a multi-class classifier. We provide three\\nnon-parametric approaches to solve the problem, two of which provide empirical\\nestimates and the third providing smooth density estimates. The proposed\\napproaches are experimentally evaluated to show their ability to improve the\\nperformance of regression models on the predictive likelihood.\\n',\n",
       " \"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of\\nX-ray photons off quantized plasma electrons in the strong magnetic field (of\\nthe order 10^12 G) close to the surface of an accreting X-ray pulsar. The line\\nprofiles of CRSFs cannot be described by an analytic expression. Numerical\\nmethods such as Monte Carlo (MC) simulations of the scattering processes are\\nrequired in order to predict precise line shapes for a given physical setup,\\nwhich can be compared to observations to gain information about the underlying\\nphysics in these systems.\\nA versatile simulation code is needed for the generation of synthetic\\ncyclotron lines. Sophisticated geometries should be investigatable by making\\ntheir simulation possible for the first time.\\nThe simulation utilizes the mean free path tables described in the first\\npaper of this series for the fast interpolation of propagation lengths. The\\ncode is parallelized to make the very time consuming simulations possible on\\nconvenient time scales. Furthermore, it can generate responses to\\nmono-energetic photon injections, producing Green's functions, which can be\\nused later to generate spectra for arbitrary continua.\\nWe develop a new simulation code to generate synthetic cyclotron lines for\\ncomplex scenarios, allowing for unprecedented physical interpretation of the\\nobserved data. An associated XSPEC model implementation is used to fit\\nsynthetic line profiles to NuSTAR data of Cep X-4. The code has been developed\\nwith the main goal of overcoming previous geometrical constraints in MC\\nsimulations of CRSFs. By applying this code also to more simple, classic\\ngeometries used in previous works, we furthermore address issues of code\\nverification and cross-comparison of various models. The XSPEC model and the\\nGreen's function tables are available online at\\nthis http URL .\\n\",\n",
       " '  This paper is devoted to the study of the construction of new quantum MDS\\ncodes. Based on constacyclic codes over Fq2 , we derive four new families of\\nquantum MDS codes, one of which is an explicit generalization of the\\nconstruction given in Theorem 7 in [22]. We also extend the result of Theorem\\n3:3 given in [17].\\n',\n",
       " '  We present a unified categorical treatment of completeness theorems for\\nseveral classical and intuitionistic infinitary logics with a proposed\\naxiomatization. This provides new completeness theorems and subsumes previous\\nones by Gödel, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an\\napplication we prove, using large cardinals assumptions, the disjunction and\\nexistence properties for infinitary intuitionistic first-order logics.\\n',\n",
       " '  Recent advances in stochastic gradient techniques have made it possible to\\nestimate posterior distributions from large datasets via Markov Chain Monte\\nCarlo (MCMC). However, when the target posterior is multimodal, mixing\\nperformance is often poor. This results in inadequate exploration of the\\nposterior distribution. A framework is proposed to improve the sampling\\nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A\\ngeneralized kinetic function is leveraged, delivering superior stationary\\nmixing, especially for multimodal distributions. Techniques are also discussed\\nto overcome the practical issues introduced by this generalization. It is shown\\nthat the proposed approach is better at exploring complex multimodal posterior\\ndistributions, as demonstrated on multiple applications and in comparison with\\nother stochastic gradient MCMC methods.\\n',\n",
       " '  We study the problems related to the estimation of the Gini index in presence\\nof a fat-tailed data generating process, i.e. one in the stable distribution\\nclass with finite mean but infinite variance (i.e. with tail index\\n$\\\\alpha\\\\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be\\nreliably estimated using conventional nonparametric methods, because of a\\ndownward bias that emerges under fat tails. This has important implications for\\nthe ongoing discussion about economic inequality.\\nWe start by discussing how the nonparametric estimator of the Gini index\\nundergoes a phase transition in the symmetry structure of its asymptotic\\ndistribution, as the data distribution shifts from the domain of attraction of\\na light-tailed distribution to that of a fat-tailed one, especially in the case\\nof infinite variance. We also show how the nonparametric Gini bias increases\\nwith lower values of $\\\\alpha$. We then prove that maximum likelihood estimation\\noutperforms nonparametric methods, requiring a much smaller sample size to\\nreach efficiency.\\nFinally, for fat-tailed data, we provide a simple correction mechanism to the\\nsmall sample bias of the nonparametric estimator based on the distance between\\nthe mode and the mean of its asymptotic distribution.\\n',\n",
       " '  Training a neural network using backpropagation algorithm requires passing\\nerror gradients sequentially through the network. The backward locking prevents\\nus from updating network layers in parallel and fully leveraging the computing\\nresources. Recently, there are several works trying to decouple and parallelize\\nthe backpropagation algorithm. However, all of them suffer from severe accuracy\\nloss or memory explosion when the neural network is deep. To address these\\nchallenging issues, we propose a novel parallel-objective formulation for the\\nobjective function of the neural network. After that, we introduce features\\nreplay algorithm and prove that it is guaranteed to converge to critical points\\nfor the non-convex problem under certain conditions. Finally, we apply our\\nmethod to training deep convolutional neural networks, and the experimental\\nresults show that the proposed method achieves {faster} convergence, {lower}\\nmemory consumption, and {better} generalization error than compared methods.\\n',\n",
       " '  We study a diagrammatic categorification (the \"anti-spherical category\") of\\nthe anti-spherical module for any Coxeter group. We deduce that Deodhar\\'s\\n(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,\\nand that a monotonicity conjecture of Brenti\\'s holds. The main technical\\nobservation is a localisation procedure for the anti-spherical category, from\\nwhich we construct a \"light leaves\" basis of morphisms. Our techniques may be\\nused to calculate many new elements of the $p$-canonical basis in the\\nanti-spherical module. The results use generators and relations for Soergel\\nbimodules (\"Soergel calculus\") in a crucial way.\\n',\n",
       " '  Recently, we have predicted that the modulation instability of optical vortex\\nsolitons propagating in nonlinear colloidal suspensions with exponential\\nsaturable nonlinearity leads to formation of necklace beams (NBs)\\n[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \\\\textbf{40},\\n5714 (2015)]. Here, we investigate the dynamics of NB formation and\\npropagation, and show that the distance at which the NB is formed depends on\\nthe input power of the vortex beam. Moreover, we show that the NB trajectories\\nare not necessarily tangent to the initial vortex ring, and that their\\nvelocities have components stemming both from the beam diffraction and from the\\nbeam orbital angular momentum. We also demonstrate the generation of twisted\\nsolitons and analyze the influence of losses on their propagation. Finally, we\\ninvestigate the conservation of the orbital angular momentum in necklace and\\ntwisted beams. Our studies, performed in ideal lossless media and in realistic\\ncolloidal suspensions with losses, provide a detailed description of NB\\ndynamics and may be useful in studies of light propagation in highly scattering\\ncolloids and biological samples.\\n',\n",
       " '  A three-dimensional spin current solver based on a generalised spin\\ndrift-diffusion description, including the spin Hall effect, is integrated with\\na magnetisation dynamics solver. The resulting model is shown to simultaneously\\nreproduce the spin-orbit torques generated using the spin Hall effect, spin\\npumping torques generated by magnetisation dynamics in multilayers, as well as\\nthe spin transfer torques acting on magnetisation regions with spatial\\ngradients, whilst field-like and spin-like torques are reproduced in a spin\\nvalve geometry. Two approaches to modelling interfaces are analysed, one based\\non the spin mixing conductance and the other based on continuity of spin\\ncurrents where the spin dephasing length governs the absorption of transverse\\nspin components. In both cases analytical formulas are derived for the\\nspin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in\\ngeneral both field-like and damping-like torques are generated. The limitations\\nof the analytical approach are discussed, showing that even in a simple bilayer\\ngeometry, due to the non-uniformity of the spin currents, a full\\nthree-dimensional treatment is required. Finally the model is applied to the\\nquantitative analysis of the spin Hall angle in Pt by reproducing published\\nexperimental data on the ferromagnetic resonance linewidth in the bilayer\\ngeometry.\\n',\n",
       " '  This paper aims to explore models based on the extreme gradient boosting\\n(XGBoost) approach for business risk classification. Feature selection (FS)\\nalgorithms and hyper-parameter optimizations are simultaneously considered\\nduring model training. The five most commonly used FS methods including weight\\nby Gini, weight by Chi-square, hierarchical variable clustering, weight by\\ncorrelation, and weight by information are applied to alleviate the effect of\\nredundant features. Two hyper-parameter optimization approaches, random search\\n(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in\\nXGBoost. The effect of different FS and hyper-parameter optimization methods on\\nthe model performance are investigated by the Wilcoxon Signed Rank Test. The\\nperformance of XGBoost is compared to the traditionally utilized logistic\\nregression (LR) model in terms of classification accuracy, area under the curve\\n(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results\\nshow that hierarchical clustering is the optimal FS method for LR while weight\\nby Chi-square achieves the best performance in XG-Boost. Both TPE and RS\\noptimization in XGBoost outperform LR significantly. TPE optimization shows a\\nsuperiority over RS since it results in a significantly higher accuracy and a\\nmarginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE\\ntuning shows a lower variability than the RS method. Finally, the ranking of\\nfeature importance based on XGBoost enhances the model interpretation.\\nTherefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an\\noperative while powerful approach for business risk modeling.\\n',\n",
       " '  Immiscible fluids flowing at high capillary numbers in porous media may be\\ncharacterized by an effective viscosity. We demonstrate that the effective\\nviscosity is well described by the Lichtenecker-Rother equation. The exponent\\n$\\\\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in\\nthree-dimensional systems depending on the pore geometry. Our arguments are\\nbased on analytical and numerical methods.\\n',\n",
       " \"  Quantum charge pumping phenomenon connects band topology through the dynamics\\nof a one-dimensional quantum system. In terms of a microscopic model, the\\nSu-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful\\nstarting point for many considerations of topological physics. Here we present\\na generalized Creutz scheme as a distinct two-band quantum pump model. By\\nnoting that it undergoes two kinds of topological band transitions accompanying\\nwith a Zak-phase-difference of $\\\\pi$ and $2\\\\pi$, respectively, various charge\\npumping schemes are studied by applying an elaborate Peierl's phase\\nsubstitution. Translating into real space, the transportation of quantized\\ncharges is a result of cooperative quantum interference effect. In particular,\\nan all-flux quantum pump emerges which operates with time-varying fluxes only\\nand transports two charge units. This puts cold atoms with artificial gauge\\nfields as an unique system where this kind of phenomena can be realized.\\n\",\n",
       " '  Heart disease is the leading cause of death, and experts estimate that\\napproximately half of all heart attacks and strokes occur in people who have\\nnot been flagged as \"at risk.\" Thus, there is an urgent need to improve the\\naccuracy of heart disease diagnosis. To this end, we investigate the potential\\nof using data analysis, and in particular the design and use of deep neural\\nnetworks (DNNs) for detecting heart disease based on routine clinical data. Our\\nmain contribution is the design, evaluation, and optimization of DNN\\narchitectures of increasing depth for heart disease diagnosis. This work led to\\nthe discovery of a novel five layer DNN architecture - named Heart Evaluation\\nfor Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields\\nbest prediction accuracy. HEARO-5\\'s design employs regularization optimization\\nand automatically deals with missing data and/or data outliers. To evaluate and\\ntune the architectures we use k-way cross-validation as well as Matthews\\ncorrelation coefficient (MCC) to measure the quality of our classifications.\\nThe study is performed on the publicly available Cleveland dataset of medical\\ninformation, and we are making our developments open source, to further\\nfacilitate openness and research on the use of DNNs in medicine. The HEARO-5\\narchitecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms\\ncurrently published research in the area.\\n',\n",
       " '  The theory of integral quadratic constraints (IQCs) allows verification of\\nstability and gain-bound properties of systems containing nonlinear or\\nuncertain elements. Gain bounds often imply exponential stability, but it can\\nbe challenging to compute useful numerical bounds on the exponential decay\\nrate. This work presents a generalization of the classical IQC results of\\nMegretski and Rantzer that leads to a tractable computational procedure for\\nfinding exponential rate certificates that are far less conservative than ones\\ncomputed from $L_2$ gain bounds alone. An expanded library of IQCs for\\ncertifying exponential stability is also provided and the effectiveness of the\\ntechnique is demonstrated via numerical examples.\\n',\n",
       " \"  Foreshock transients upstream of Earth's bow shock have been recently\\nobserved to accelerate electrons to many times their thermal energy. How such\\nacceleration occurs is unknown, however. Using THEMIS case studies, we examine\\na subset of acceleration events (31 of 247 events) in foreshock transients with\\ncores that exhibit gradual electron energy increases accompanied by low\\nbackground magnetic field strength and large-amplitude magnetic fluctuations.\\nUsing the evolution of electron distributions and the energy increase rates at\\nmultiple spacecraft, we suggest that Fermi acceleration between a converging\\nforeshock transient's compressional boundary and the bow shock is responsible\\nfor the observed electron acceleration. We then show that a one-dimensional\\ntest particle simulation of an ideal Fermi acceleration model in fluctuating\\nfields prescribed by the observations can reproduce the observed evolution of\\nelectron distributions, energy increase rate, and pitch-angle isotropy,\\nproviding further support for our hypothesis. Thus, Fermi acceleration is\\nlikely the principal electron acceleration mechanism in at least this subset of\\nforeshock transient cores.\\n\",\n",
       " '  The quantum speed limit (QSL), or the energy-time uncertainty relation,\\ndescribes the fundamental maximum rate for quantum time evolution and has been\\nregarded as being unique in quantum mechanics. In this study, we obtain a\\nclassical speed limit corresponding to the QSL using the Hilbert space for the\\nclassical Liouville equation. Thus, classical mechanics has a fundamental speed\\nlimit, and QSL is not a purely quantum phenomenon but a universal dynamical\\nproperty of the Hilbert space. Furthermore, we obtain similar speed limits for\\nthe imaginary-time Schroedinger equations such as the master equation.\\n',\n",
       " '  This paper mainly discusses the diffusion on complex networks with\\ntime-varying couplings. We propose a model to describe the adaptive diffusion\\nprocess of local topological and dynamical information, and find that the\\nBarabasi-Albert scale-free network (BA network) is beneficial to the diffusion\\nand leads nodes to arrive at a larger state value than other networks do. The\\nability of diffusion for a node is related to its own degree. Specifically,\\nnodes with smaller degrees are more likely to change their states and reach\\nlarger values, while those with larger degrees tend to stick to their original\\nstates. We introduce state entropy to analyze the thermodynamic mechanism of\\nthe diffusion process, and interestingly find that this kind of diffusion\\nprocess is a minimization process of state entropy. We use the inequality\\nconstrained optimization method to reveal the restriction function of the\\nminimization and find that it has the same form as the Gibbs free energy. The\\nthermodynamical concept allows us to understand dynamical processes on complex\\nnetworks from a brand-new perspective. The result provides a convenient means\\nof optimizing relevant dynamical processes on practical circuits as well as\\nrelated complex systems.\\n',\n",
       " '  This paper proposes a non-parallel many-to-many voice conversion (VC) method\\nusing a variant of the conditional variational autoencoder (VAE) called an\\nauxiliary classifier VAE (ACVAE). The proposed method has three key features.\\nFirst, it adopts fully convolutional architectures to construct the encoder and\\ndecoder networks so that the networks can learn conversion rules that capture\\ntime dependencies in the acoustic feature sequences of source and target\\nspeech. Second, it uses an information-theoretic regularization for the model\\ntraining to ensure that the information in the attribute class label will not\\nbe lost in the conversion process. With regular CVAEs, the encoder and decoder\\nare free to ignore the attribute class label input. This can be problematic\\nsince in such a situation, the attribute class label will have little effect on\\ncontrolling the voice characteristics of input speech at test time. Such\\nsituations can be avoided by introducing an auxiliary classifier and training\\nthe encoder and decoder so that the attribute classes of the decoder outputs\\nare correctly predicted by the classifier. Third, it avoids producing\\nbuzzy-sounding speech at test time by simply transplanting the spectral details\\nof the input speech into its converted version. Subjective evaluation\\nexperiments revealed that this simple method worked reasonably well in a\\nnon-parallel many-to-many speaker identity conversion task.\\n',\n",
       " '  Informed by LES data and resolvent analysis of the mean flow, we examine the\\nstructure of turbulence in jets in the subsonic, transonic, and supersonic\\nregimes. Spectral (frequency-space) proper orthogonal decomposition is used to\\nextract energy spectra and decompose the flow into energy-ranked coherent\\nstructures. The educed structures are generally well predicted by the resolvent\\nanalysis. Over a range of low frequencies and the first few azimuthal mode\\nnumbers, these jets exhibit a low-rank response characterized by\\nKelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer\\nup to the end of the potential core and that are excited by forcing in the\\nvery-near-nozzle shear layer. These modes too the have been experimentally\\nobserved before and predicted by quasi-parallel stability theory and other\\napproximations--they comprise a considerable portion of the total turbulent\\nenergy. At still lower frequencies, particularly for the axisymmetric mode, and\\nagain at high frequencies for all azimuthal wavenumbers, the response is not\\nlow rank, but consists of a family of similarly amplified modes. These modes,\\nwhich are primarily active downstream of the potential core, are associated\\nwith the Orr mechanism. They occur also as sub-dominant modes in the range of\\nfrequencies dominated by the KH response. Our global analysis helps tie\\ntogether previous observations based on local spatial stability theory, and\\nexplains why quasi-parallel predictions were successful at some frequencies and\\nazimuthal wavenumbers, but failed at others.\\n',\n",
       " '  One of the popular approaches for low-rank tensor completion is to use the\\nlatent trace norm regularization. However, most existing works in this\\ndirection learn a sparse combination of tensors. In this work, we fill this gap\\nby proposing a variant of the latent trace norm that helps in learning a\\nnon-sparse combination of tensors. We develop a dual framework for solving the\\nlow-rank tensor completion problem. We first show a novel characterization of\\nthe dual solution space with an interesting factorization of the optimal\\nsolution. Overall, the optimal solution is shown to lie on a Cartesian product\\nof Riemannian manifolds. Furthermore, we exploit the versatile Riemannian\\noptimization framework for proposing computationally efficient trust region\\nalgorithm. The experiments illustrate the efficacy of the proposed algorithm on\\nseveral real-world datasets across applications.\\n',\n",
       " \"  Nous tentons dans cet article de proposer une thèse cohérente concernant\\nla formation de la notion d'involution dans le Brouillon Project de Desargues.\\nPour cela, nous donnons une analyse détaillée des dix premières pages\\ndudit Brouillon, comprenant les développements de cas particuliers qui aident\\nà comprendre l'intention de Desargues. Nous mettons cette analyse en regard\\nde la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis\\nCharitables.\\nThe purpose of this article is to propose a coherent thesis on how Girard\\nDesargues arrived at the notion of involution in his Brouillon Project of 1639.\\nTo this purpose we give a detailed analysis of the ten first pages of the\\nBrouillon, including developments of particular cases which help to understand\\nthe goal of Desargues, as well as to clarify the links between the notion of\\ninvolution and that of harmonic division. We compare the conclusions of this\\nanalysis with the very critical reading Jean de Beaugrand made of the Brouillon\\nProject in the Advis Charitables of 1640.\\n\",\n",
       " '  X-ray computed tomography (CT) using sparse projection views is a recent\\napproach to reduce the radiation dose. However, due to the insufficient\\nprojection views, an analytic reconstruction approach using the filtered back\\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\\napproaches using large receptive field neural networks such as U-Net have\\ndemonstrated impressive performance for sparse- view CT reconstruction.\\nHowever, theoretical justification is still lacking. Inspired by the recent\\ntheory of deep convolutional framelets, the main goal of this paper is,\\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\\ndeep learning schemes. In particular, we show that the alternative U- Net\\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\\nframe condition which make them better for effective recovery of high frequency\\nedges in sparse view- CT. Using extensive experiments with real patient data\\nset, we demonstrate that the new network architectures provide better\\nreconstruction performance.\\n',\n",
       " '  A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a\\nsubsheaf of the sheaf $\\\\mathfrak{X}$ of vector fields on $M$. We show that if\\nthis singular foliation admits a resolution (in the sense of sheaves)\\nconsisting of sections of a graded vector bundle of finite type, then one can\\nlift the Lie bracket of vector fields to a Lie $\\\\infty$-algebroid structure on\\nthis resolution, that we call a universal Lie $\\\\infty$-algebroid associated to\\nthe foliation. The name is justified because it is isomorphic (up to homotopy)\\nto any other Lie $\\\\infty$-algebroid structure built on any other resolution of\\nthe given singular foliation.\\n',\n",
       " '  The Weyl semimetal phase is a recently discovered topological quantum state\\nof matter characterized by the presence of topologically protected degeneracies\\nnear the Fermi level. These degeneracies are the source of exotic phenomena,\\nincluding the realization of chiral Weyl fermions as quasiparticles in the bulk\\nand the formation of Fermi arc states on the surfaces. Here, we demonstrate\\nthat these two key signatures show distinct evolutions with the bulk band\\ntopology by performing angle-resolved photoemission spectroscopy, supported by\\nfirst-principle calculations, on transition-metal monophosphides. While Weyl\\nfermion quasiparticles exist only when the chemical potential is located\\nbetween two saddle points of the Weyl cone features, the Fermi arc states\\nextend in a larger energy scale and are robust across the bulk Lifshitz\\ntransitions associated with the recombination of two non-trivial Fermi surfaces\\nenclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl\\npoints of opposite chirality. Therefore, in some systems (e.g. NbP),\\ntopological Fermi arc states are preserved even if Weyl fermion quasiparticles\\nare absent in the bulk. Our findings not only provide insight into the\\nrelationship between the exotic physical phenomena and the intrinsic bulk band\\ntopology in Weyl semimetals, but also resolve the apparent puzzle of the\\ndifferent magneto-transport properties observed in TaAs, TaP and NbP, where the\\nFermi arc states are similar.\\n',\n",
       " \"  A sequence of pathological changes takes place in Alzheimer's disease, which\\ncan be assessed in vivo using various brain imaging methods. Currently, there\\nis no appropriate statistical model available that can easily integrate\\nmultiple imaging modalities, being able to utilize the additional information\\nprovided from the combined data. We applied Gaussian graphical models (GGMs)\\nfor analyzing the conditional dependency networks of multimodal neuroimaging\\ndata and assessed alterations of the network structure in mild cognitive\\nimpairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy\\ncontrols.\\nData from N=667 subjects were obtained from the Alzheimer's Disease\\nNeuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism\\n(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.\\nSeparate GGMs were estimated using a Bayesian framework for the combined\\nmultimodal data for each diagnostic category. Graph-theoretical statistics were\\ncalculated to determine network alterations associated with disease severity.\\nNetwork measures clustering coefficient, path length and small-world\\ncoefficient were significantly altered across diagnostic groups, with a\\nbiphasic u-shape trajectory, i.e. increased small-world coefficient in early\\nMCI, intermediate values in late MCI, and decreased values in AD patients\\ncompared to controls. In contrast, no group differences were found for\\nclustering coefficient and small-world coefficient when estimating conditional\\ndependency networks on single imaging modalities.\\nGGMs provide a useful methodology to analyze the conditional dependency\\nnetworks of multimodal neuroimaging data.\\n\",\n",
       " '  This work bridges the technical concepts underlying distributed computing and\\nblockchain technologies with their profound socioeconomic and sociopolitical\\nimplications, particularly on academic research and the healthcare industry.\\nSeveral examples from academia, industry, and healthcare are explored\\nthroughout this paper. The limiting factor in contemporary life sciences\\nresearch is often funding: for example, to purchase expensive laboratory\\nequipment and materials, to hire skilled researchers and technicians, and to\\nacquire and disseminate data through established academic channels. In the case\\nof the U.S. healthcare system, hospitals generate massive amounts of data, only\\na small minority of which is utilized to inform current and future medical\\npractice. Similarly, corporations too expend large amounts of money to collect,\\nsecure and transmit data from one centralized source to another. In all three\\nscenarios, data moves under the traditional paradigm of centralization, in\\nwhich data is hosted and curated by individuals and organizations and of\\nbenefit to only a small subset of people.\\n',\n",
       " '  Fragility curves are commonly used in civil engineering to assess the\\nvulnerability of structures to earthquakes. The probability of failure\\nassociated with a prescribed criterion (e.g. the maximal inter-storey drift of\\na building exceeding a certain threshold) is represented as a function of the\\nintensity of the earthquake ground motion (e.g. peak ground acceleration or\\nspectral acceleration). The classical approach relies on assuming a lognormal\\nshape of the fragility curves; it is thus parametric. In this paper, we\\nintroduce two non-parametric approaches to establish the fragility curves\\nwithout employing the above assumption, namely binned Monte Carlo simulation\\nand kernel density estimation. As an illustration, we compute the fragility\\ncurves for a three-storey steel frame using a large number of synthetic ground\\nmotions. The curves obtained with the non-parametric approaches are compared\\nwith respective curves based on the lognormal assumption. A similar comparison\\nis presented for a case when a limited number of recorded ground motions is\\navailable. It is found that the accuracy of the lognormal curves depends on the\\nground motion intensity measure, the failure criterion and most importantly, on\\nthe employed method for estimating the parameters of the lognormal shape.\\n',\n",
       " '  We consider continuous-time Markov chains which display a family of wells at\\nthe same depth. We provide sufficient conditions which entail the convergence\\nof the finite-dimensional distributions of the order parameter to the ones of a\\nfinite state Markov chain. We also show that the state of the process can be\\nrepresented as a time-dependent convex combination of metastable states, each\\nof which is supported on one well.\\n',\n",
       " '  We construct embedded minimal surfaces which are $n$-periodic in\\n$\\\\mathbb{R}^n$. They are new for codimension $n-2\\\\ge 2$. We start with a Jordan\\ncurve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk\\nwhich Schwarz reflection extends to a complete minimal surface. Studying the\\ngroup of Schwarz reflections, we can characterize those Jordan curves for which\\nthe complete surface is embedded. For example, for $n=4$ exactly five such\\nJordan curves generate embedded surfaces. Our results apply to surface classes\\nother than minimal as well, for instance polygonal surfaces.\\n',\n",
       " '  We report the discovery of three small transiting planets orbiting GJ 9827, a\\nbright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\\\\pm0.11$\\n$R_{\\\\rm \\\\oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$\\n$R_{\\\\rm \\\\oplus}$ super Earth on a 3.6 day period, and a $2.07\\\\pm0.14$ $R_{\\\\rm\\n\\\\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting\\nGJ 9827 span the transition between predominantly rocky and gaseous planets,\\nand GJ 9827 b and c fall in or close to the known gap in the radius\\ndistribution of small planets between these populations. At a distance of 30\\nparsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making\\nthese planets well-suited for atmospheric studies with the upcoming James Webb\\nSpace Telescope. The GJ 9827 system provides a valuable opportunity to\\ncharacterize interior structure and atmospheric properties of coeval planets\\nspanning the rocky to gaseous transition.\\n',\n",
       " '  We define a family of quantum invariants of closed oriented $3$-manifolds\\nusing spherical multi-fusion categories. The state sum nature of this invariant\\nleads directly to $(2+1)$-dimensional topological quantum field theories\\n($\\\\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury\\n($\\\\text{TVBW}$) $\\\\text{TQFT}$s from spherical fusion categories. The invariant\\nis given as a state sum over labeled triangulations, which is mostly parallel\\nto, but richer than the $\\\\text{TVBW}$ approach in that here the labels live not\\nonly on $1$-simplices but also on $0$-simplices. It is shown that a\\nmulti-fusion category in general cannot be a spherical fusion category in the\\nusual sense. Thus we introduce the concept of a spherical multi-fusion category\\nby imposing a weakened version of sphericity. Besides containing the\\n$\\\\text{TVBW}$ theory, our construction also includes the recent higher gauge\\ntheory $(2+1)$-$\\\\text{TQFT}$s given by Kapustin and Thorngren, which was not\\nknown to have a categorical origin before.\\n',\n",
       " '  We propose Sparse Neural Network architectures that are based on random or\\nstructured bipartite graph topologies. Sparse architectures provide compression\\nof the models learned and speed-ups of computations, they can also surpass\\ntheir unstructured or fully connected counterparts. As we show, even more\\ncompact topologies of the so-called SNN (Sparse Neural Network) can be achieved\\nwith the use of structured graphs of connections between consecutive layers of\\nneurons. In this paper, we investigate how the accuracy and training speed of\\nthe models depend on the topology and sparsity of the neural network. Previous\\napproaches using sparcity are all based on fully connected neural network\\nmodels and create sparcity during training phase, instead we explicitly define\\na sparse architectures of connections before the training. Building compact\\nneural network models is coherent with empirical observations showing that\\nthere is much redundancy in learned neural network models. We show\\nexperimentally that the accuracy of the models learned with neural networks\\ndepends on expander-like properties of the underlying topologies such as the\\nspectral gap and algebraic connectivity rather than the density of the graphs\\nof connections.\\n',\n",
       " '  Computer vision has made remarkable progress in recent years. Deep neural\\nnetwork (DNN) models optimized to identify objects in images exhibit\\nunprecedented task-trained accuracy and, remarkably, some generalization\\nability: new visual problems can now be solved more easily based on previous\\nlearning. Biological vision (learned in life and through evolution) is also\\naccurate and general-purpose. Is it possible that these different learning\\nregimes converge to similar problem-dependent optimal computations? We\\ntherefore asked whether the human system-level computation of visual perception\\nhas DNN correlates and considered several anecdotal test cases. We found that\\nperceptual sensitivity to image changes has DNN mid-computation correlates,\\nwhile sensitivity to segmentation, crowding and shape has DNN end-computation\\ncorrelates. Our results quantify the applicability of using DNN computation to\\nestimate perceptual loss, and are consistent with the fascinating theoretical\\nview that properties of human perception are a consequence of\\narchitecture-independent visual learning.\\n',\n",
       " '  A numerical analysis of heat conduction through the cover plate of a heat\\npipe is carried out to determine the temperature of the working substance,\\naverage temperature of heating and cooling surfaces, heat spread in the\\ntransmitter, and the heat bypass through the cover plate. Analysis has been\\nextended for the estimation of heat transfer requirements at the outer surface\\nof the con- denser under different heat load conditions using Genetic\\nAlgorithm. This paper also presents the estimation of an average heat transfer\\ncoefficient for the boiling and condensation of the working substance inside\\nthe microgrooves corresponding to a known temperature of the heat source. The\\nequation of motion of the working fluid in the meniscus of an equilateral\\ntriangular groove has been presented from which a new term called the minimum\\nsurface tension required for avoiding the dry out condition is defined.\\nQuantitative results showing the effect of thickness of cover plate, heat load,\\nangle of inclination and viscosity of the working fluid on the different\\naspects of the heat transfer, minimum surface tension required to avoid dry\\nout, velocity distribution of the liquid, and radius of liquid meniscus inside\\nthe micro-grooves have been presented and discussed.\\n',\n",
       " \"  This paper provides short proofs of two fundamental theorems of finite\\nsemigroup theory whose previous proofs were significantly longer, namely the\\ntwo-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike\\ntheorem, using a new algebraic technique that we call the merge decomposition.\\nA prototypical application of this technique decomposes a semigroup $T$ into a\\ntwo-sided semidirect product whose components are built from two subsemigroups\\n$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their\\nsetwise product $T_1T_2$. In this sense we decompose $T$ by merging the\\nsubsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup\\nhomomorphisms from free semigroups.\\n\",\n",
       " \"  Nefarious actors on social media and other platforms often spread rumors and\\nfalsehoods through images whose metadata (e.g., captions) have been modified to\\nprovide visual substantiation of the rumor/falsehood. This type of modification\\nis referred to as image repurposing, in which often an unmanipulated image is\\npublished along with incorrect or manipulated metadata to serve the actor's\\nulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)\\ndataset, a substantially challenging dataset over that which has been\\npreviously available to support research into image repurposing detection. The\\nnew dataset includes location, person, and organization manipulations on\\nreal-world data sourced from Flickr. We also present a novel, end-to-end, deep\\nmultimodal learning model for assessing the integrity of an image by combining\\ninformation extracted from the image with related information from a knowledge\\nbase. The proposed method is compared against state-of-the-art techniques on\\nexisting datasets as well as MEIR, where it outperforms existing methods across\\nthe board, with AUC improvement up to 0.23.\\n\",\n",
       " '  Recent advances in adversarial Deep Learning (DL) have opened up a largely\\nunexplored surface for malicious attacks jeopardizing the integrity of\\nautonomous DL systems. With the wide-spread usage of DL in critical and\\ntime-sensitive applications, including unmanned vehicles, drones, and video\\nsurveillance systems, online detection of malicious inputs is of utmost\\nimportance. We propose DeepFense, the first end-to-end automated framework that\\nsimultaneously enables efficient and safe execution of DL models. DeepFense\\nformalizes the goal of thwarting adversarial attacks as an optimization problem\\nthat minimizes the rarely observed regions in the latent feature space spanned\\nby a DL network. To solve the aforementioned minimization problem, a set of\\ncomplementary but disjoint modular redundancies are trained to validate the\\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\\nleverages hardware/software/algorithm co-design and customized acceleration to\\nachieve just-in-time performance in resource-constrained settings. The proposed\\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\\nto train modular redundancies. We further provide an accompanying API to reduce\\nthe non-recurring engineering cost and ensure automated adaptation to various\\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\\nof magnitude performance improvement while enabling online adversarial sample\\ndetection.\\n',\n",
       " '  Users form information trails as they browse the web, checkin with a\\ngeolocation, rate items, or consume media. A common problem is to predict what\\na user might do next for the purposes of guidance, recommendation, or\\nprefetching. First-order and higher-order Markov chains have been widely used\\nmethods to study such sequences of data. First-order Markov chains are easy to\\nestimate, but lack accuracy when history matters. Higher-order Markov chains,\\nin contrast, have too many parameters and suffer from overfitting the training\\ndata. Fitting these parameters with regularization and smoothing only offers\\nmild improvements. In this paper we propose the retrospective higher-order\\nMarkov process (RHOMP) as a low-parameter model for such sequences. This model\\nis a special case of a higher-order Markov chain where the transitions depend\\nretrospectively on a single history state instead of an arbitrary combination\\nof history states. There are two immediate computational advantages: the number\\nof parameters is linear in the order of the Markov chain and the model can be\\nfit to large state spaces. Furthermore, by providing a specific structure to\\nthe higher-order chain, RHOMPs improve the model accuracy by efficiently\\nutilizing history states without risks of overfitting the data. We demonstrate\\nhow to estimate a RHOMP from data and we demonstrate the effectiveness of our\\nmethod on various real application datasets spanning geolocation data, review\\nsequences, and business locations. The RHOMP model uniformly outperforms\\nhigher-order Markov chains, Kneser-Ney regularization, and tensor\\nfactorizations in terms of prediction accuracy.\\n',\n",
       " '  We analyze two novel randomized variants of the Frank-Wolfe (FW) or\\nconditional gradient algorithm. While classical FW algorithms require solving a\\nlinear minimization problem over the domain at each iteration, the proposed\\nmethod only requires to solve a linear minimization problem over a small\\n\\\\emph{subset} of the original domain. The first algorithm that we propose is a\\nrandomized variant of the original FW algorithm and achieves a\\n$\\\\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic\\ncounterpart. The second algorithm is a randomized variant of the Away-step FW\\nalgorithm, and again as its deterministic counterpart, reaches linear (i.e.,\\nexponential) convergence rate making it the first provably convergent\\nrandomized variant of Away-step FW. In both cases, while subsampling reduces\\nthe convergence rate by a constant factor, the linear minimization step can be\\na fraction of the cost of that of the deterministic versions, especially when\\nthe data is streamed. We illustrate computational gains of the algorithms on\\nregression problems, involving both $\\\\ell_1$ and latent group lasso penalties.\\n',\n",
       " '  In this paper, we prove a mean value formula for bounded subharmonic\\nHermitian matrix valued function on a complete Riemannian manifold with\\nnonnegative Ricci curvature. As its application, we obtain a Liouville type\\ntheorem for the complex Monge-Ampère equation on product manifolds.\\n',\n",
       " '  In this work, we investigate the value of uncertainty modeling in 3D\\nsuper-resolution with convolutional neural networks (CNNs). Deep learning has\\nshown success in a plethora of medical image transformation problems, such as\\nsuper-resolution (SR) and image synthesis. However, the highly ill-posed nature\\nof such problems results in inevitable ambiguity in the learning of networks.\\nWe propose to account for intrinsic uncertainty through a per-patch\\nheteroscedastic noise model and for parameter uncertainty through approximate\\nBayesian inference in the form of variational dropout. We show that the\\ncombined benefits of both lead to the state-of-the-art performance SR of\\ndiffusion MR brain images in terms of errors compared to ground truth. We\\nfurther show that the reduced error scores produce tangible benefits in\\ndownstream tractography. In addition, the probabilistic nature of the methods\\nnaturally confers a mechanism to quantify uncertainty over the super-resolved\\noutput. We demonstrate through experiments on both healthy and pathological\\nbrains the potential utility of such an uncertainty measure in the risk\\nassessment of the super-resolved images for subsequent clinical use.\\n',\n",
       " '  Superconductor-Ferromagnet (SF) heterostructures are of interest due to\\nnumerous phenomena related to the spin-dependent interaction of Cooper pairs\\nwith the magnetization. Here we address the effects of a magnetic insulator on\\nthe density of states of a superconductor based on a recently developed\\nboundary condition for strongly spin-dependent interfaces. We show that the\\nboundary to a magnetic insulator has a similar effect like the presence of\\nmagnetic impurities. In particular we find that the impurity effects of\\nstrongly scattering localized spins leading to the formation of Shiba bands can\\nbe mapped onto the boundary problem.\\n',\n",
       " '  We present the first general purpose framework for marginal maximum a\\nposteriori estimation of probabilistic program variables. By using a series of\\ncode transformations, the evidence of any probabilistic program, and therefore\\nof any graphical model, can be optimized with respect to an arbitrary subset of\\nits sampled variables. To carry out this optimization, we develop the first\\nBayesian optimization package to directly exploit the source code of its\\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\\noptimization, and implicit constraint satisfaction; delivering significant\\nperformance improvements over prominent existing packages. We present\\napplications of our method to a number of tasks including engineering design\\nand parameter optimization.\\n',\n",
       " '  Long-term load forecasting plays a vital role for utilities and planners in\\nterms of grid development and expansion planning. An overestimate of long-term\\nelectricity load will result in substantial wasted investment in the\\nconstruction of excess power facilities, while an underestimate of future load\\nwill result in insufficient generation and unmet demand. This paper presents\\nfirst-of-its-kind approach to use multiplicative error model (MEM) in\\nforecasting load for long-term horizon. MEM originates from the structure of\\nautoregressive conditional heteroscedasticity (ARCH) model where conditional\\nvariance is dynamically parameterized and it multiplicatively interacts with an\\ninnovation term of time-series. Historical load data, accessed from a U.S.\\nregional transmission operator, and recession data for years 1993-2016 is used\\nin this study. The superiority of considering volatility is proven by\\nout-of-sample forecast results as well as directional accuracy during the great\\neconomic recession of 2008. To incorporate future volatility, backtesting of\\nMEM model is performed. Two performance indicators used to assess the proposed\\nmodel are mean absolute percentage error (for both in-sample model fit and\\nout-of-sample forecasts) and directional accuracy.\\n',\n",
       " '  Many empirical studies document power law behavior in size distributions of\\neconomic interest such as cities, firms, income, and wealth. One mechanism for\\ngenerating such behavior combines independent and identically distributed\\nGaussian additive shocks to log-size with a geometric age distribution. We\\ngeneralize this mechanism by allowing the shocks to be non-Gaussian (but\\nlight-tailed) and dependent upon a Markov state variable. Our main results\\nprovide sharp bounds on tail probabilities and simple formulas for Pareto\\nexponents. We present two applications: (i) we show that the tails of the\\nwealth distribution in a heterogeneous-agent dynamic general equilibrium model\\nwith idiosyncratic endowment risk decay exponentially, unlike models with\\ninvestment risk where the tails may be Paretian, and (ii) we show that a random\\ngrowth model for the population dynamics of Japanese prefectures is consistent\\nwith the observed Pareto exponent but only after allowing for Markovian\\ndynamics.\\n',\n",
       " '  In topological quantum computing, information is encoded in \"knotted\" quantum\\nstates of topological phases of matter, thus being locked into topology to\\nprevent decay. Topological precision has been confirmed in quantum Hall liquids\\nby experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum\\nmemory. In this survey, we discuss the conceptual development of this\\ninterdisciplinary field at the juncture of mathematics, physics and computer\\nscience. Our focus is on computing and physical motivations, basic mathematical\\nnotions and results, open problems and future directions related to and/or\\ninspired by topological quantum computing.\\n',\n",
       " '  $ \\\\def\\\\vecc#1{\\\\boldsymbol{#1}} $We design a polynomial time algorithm that\\nfor any weighted undirected graph $G = (V, E,\\\\vecc w)$ and sufficiently large\\n$\\\\delta > 1$, partitions $V$ into subsets $V_1, \\\\ldots, V_h$ for some $h\\\\geq\\n1$, such that\\n$\\\\bullet$ at most $\\\\delta^{-1}$ fraction of the weights are between clusters,\\ni.e. \\\\[ w(E - \\\\cup_{i = 1}^h E(V_i)) \\\\lesssim \\\\frac{w(E)}{\\\\delta};\\\\]\\n$\\\\bullet$ the effective resistance diameter of each of the induced subgraphs\\n$G[V_i]$ is at most $\\\\delta^3$ times the average weighted degree, i.e. \\\\[\\n\\\\max_{u, v \\\\in V_i} \\\\mathsf{Reff}_{G[V_i]}(u, v) \\\\lesssim \\\\delta^3 \\\\cdot\\n\\\\frac{|V|}{w(E)} \\\\quad \\\\text{ for all } i=1, \\\\ldots, h.\\\\]\\nIn particular, it is possible to remove one percent of weight of edges of any\\ngiven graph such that each of the resulting connected components has effective\\nresistance diameter at most the inverse of the average weighted degree.\\nOur proof is based on a new connection between effective resistance and low\\nconductance sets. We show that if the effective resistance between two vertices\\n$u$ and $v$ is large, then there must be a low conductance cut separating $u$\\nfrom $v$. This implies that very mildly expanding graphs have constant\\neffective resistance diameter. We believe that this connection could be of\\nindependent interest in algorithm design.\\n',\n",
       " '  Self-supervised learning (SSL) is a reliable learning mechanism in which a\\nrobot enhances its perceptual capabilities. Typically, in SSL a trusted,\\nprimary sensor cue provides supervised training data to a secondary sensor cue.\\nIn this article, a theoretical analysis is performed on the fusion of the\\nprimary and secondary cue in a minimal model of SSL. A proof is provided that\\ndetermines the specific conditions under which it is favorable to perform\\nfusion. In short, it is favorable when (i) the prior on the target value is\\nstrong or (ii) the secondary cue is sufficiently accurate. The theoretical\\nfindings are validated with computational experiments. Subsequently, a\\nreal-world case study is performed to investigate if fusion in SSL is also\\nbeneficial when assumptions of the minimal model are not met. In particular, a\\nflying robot learns to map pressure measurements to sonar height measurements\\nand then fuses the two, resulting in better height estimation. Fusion is also\\nbeneficial in the opposite case, when pressure is the primary cue. The analysis\\nand results are encouraging to study SSL fusion also for other robots and\\nsensors.\\n',\n",
       " '  Deep reinforcement learning on Atari games maps pixel directly to actions;\\ninternally, the deep neural network bears the responsibility of both extracting\\nuseful information and making decisions based on it. Aiming at devoting entire\\ndeep networks to decision making alone, we propose a new method for learning\\npolicies and compact state representations separately but simultaneously for\\npolicy approximation in reinforcement learning. State representations are\\ngenerated by a novel algorithm based on Vector Quantization and Sparse Coding,\\ntrained online along with the network, and capable of growing its dictionary\\nsize over time. We also introduce new techniques allowing both the neural\\nnetwork and the evolution strategy to cope with varying dimensions. This\\nenables networks of only 6 to 18 neurons to learn to play a selection of Atari\\ngames with performance comparable---and occasionally superior---to\\nstate-of-the-art techniques using evolution strategies on deep networks two\\norders of magnitude larger.\\n',\n",
       " '  We consider the problem of isotonic regression, where the underlying signal\\n$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the\\ncone $\\\\{ x\\\\in\\\\mathbb{R}^n : x_1 \\\\leq \\\\dots \\\\leq x_n\\\\}$. We study the isotonic\\nprojection operator (projection to this cone), and find a necessary and\\nsufficient condition characterizing all norms with respect to which this\\nprojection is contractive. This enables a simple and non-asymptotic analysis of\\nthe convergence properties of isotonic regression, yielding uniform confidence\\nbands that adapt to the local Lipschitz properties of the signal.\\n',\n",
       " '  Heating, Ventilation, and Cooling (HVAC) systems are often the most\\nsignificant contributor to the energy usage, and the operational cost, of large\\noffice buildings. Therefore, to understand the various factors affecting the\\nenergy usage, and to optimize the operational efficiency of building HVAC\\nsystems, energy analysts and architects often create simulations (e.g.,\\nEnergyPlus or DOE-2), of buildings prior to construction or renovation to\\ndetermine energy savings and quantify the Return-on-Investment (ROI). While\\nuseful, these simulations usually use static HVAC control strategies such as\\nlowering room temperature at night, or reactive control based on simulated room\\noccupancy. Recently, advances have been made in HVAC control algorithms that\\npredict room occupancy. However, these algorithms depend on costly sensor\\ninstallations and the tradeoffs between predictive accuracy, energy savings,\\ncomfort and expenses are not well understood. Current simulation frameworks do\\nnot support easy analysis of these tradeoffs. Our contribution is a simulation\\nframework that can be used to explore this design space by generating objective\\nestimates of the energy savings and occupant comfort for different levels of\\nHVAC prediction and control performance. We validate our framework on a\\nreal-world occupancy dataset spanning 6 months for 235 rooms in a large\\nuniversity office building. Using the gold standard of energy use modeling and\\nsimulation (Revit and Energy Plus), we compare the energy consumption and\\noccupant comfort in 29 independent simulations that explore our parameter\\nspace. Our results highlight a number of potentially useful tradeoffs with\\nrespect to energy savings, comfort, and algorithmic performance among\\npredictive, reactive, and static schedules, for a stakeholder of our building.\\n',\n",
       " '  In this paper, we consider the problem of identifying the type (local\\nminimizer, maximizer or saddle point) of a given isolated real critical point\\n$c$, which is degenerate, of a multivariate polynomial function $f$. To this\\nend, we introduce the definition of faithful radius of $c$ by means of the\\ncurve of tangency of $f$. We show that the type of $c$ can be determined by the\\nglobal extrema of $f$ over the Euclidean ball centered at $c$ with a faithful\\nradius.We propose algorithms to compute a faithful radius of $c$ and determine\\nits type.\\n',\n",
       " '  Identification of patients at high risk for readmission could help reduce\\nmorbidity and mortality as well as healthcare costs. Most of the existing\\nstudies on readmission prediction did not compare the contribution of data\\ncategories. In this study we analyzed relative contribution of 90,101 variables\\nacross 398,884 admission records corresponding to 163,468 patients, including\\npatient demographics, historical hospitalization information, discharge\\ndisposition, diagnoses, procedures, medications and laboratory test results. We\\nestablished an interpretable readmission prediction model based on Logistic\\nRegression in scikit-learn, and added the available variables to the model one\\nby one in order to analyze the influences of individual data categories on\\nreadmission prediction accuracy. Diagnosis related groups (c-statistic\\nincrement of 0.0933) and discharge disposition (c-statistic increment of\\n0.0269) were the strongest contributors to model accuracy. Additionally, we\\nalso identified the top ten contributing variables in every data category.\\n',\n",
       " '  Tropical recurrent sequences are introduced satisfying a given vector (being\\na tropical counterpart of classical linear recurrent sequences). We consider\\nthe case when Newton polygon of the vector has a single (bounded) edge. In this\\ncase there are periodic tropical recurrent sequences which are similar to\\nclassical linear recurrent sequences. A question is studied when there exists a\\nnon-periodic tropical recurrent sequence satisfying a given vector, and partial\\nanswers are provided to this question. Also an algorithm is designed which\\ntests existence of non-periodic tropical recurrent sequences satisfying a given\\nvector with integer coordinates. Finally, we introduce a tropical entropy of a\\nvector and provide some bounds on it.\\n',\n",
       " '  An interesting approach to analyzing neural networks that has received\\nrenewed attention is to examine the equivalent kernel of the neural network.\\nThis is based on the fact that a fully connected feedforward network with one\\nhidden layer, a certain weight distribution, an activation function, and an\\ninfinite number of neurons can be viewed as a mapping into a Hilbert space. We\\nderive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for\\nall rotationally-invariant weight distributions, generalizing a previous result\\nthat required Gaussian weight distributions. Additionally, the Central Limit\\nTheorem is used to show that for certain activation functions, kernels\\ncorresponding to layers with weight distributions having $0$ mean and finite\\nabsolute third moment are asymptotically universal, and are well approximated\\nby the kernel corresponding to layers with spherical Gaussian weights. In deep\\nnetworks, as depth increases the equivalent kernel approaches a pathological\\nfixed point, which can be used to argue why training randomly initialized\\nnetworks can be difficult. Our results also have implications for weight\\ninitialization.\\n',\n",
       " '  We explore whether useful temporal neural generative models can be learned\\nfrom sequential data without back-propagation through time. We investigate the\\nviability of a more neurocognitively-grounded approach in the context of\\nunsupervised generative modeling of sequences. Specifically, we build on the\\nconcept of predictive coding, which has gained influence in cognitive science,\\nin a neural framework. To do so we develop a novel architecture, the Temporal\\nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The\\nunderlying directed generative model is fully recurrent, meaning that it\\nemploys structural feedback connections and temporal feedback connections,\\nyielding information propagation cycles that create local learning signals.\\nThis facilitates a unified bottom-up and top-down approach for information\\ntransfer inside the architecture. Our proposed algorithm shows promise on the\\nbouncing balls generative modeling problem. Further experiments could be\\nconducted to explore the strengths and weaknesses of our approach.\\n',\n",
       " '  Kitaev quantum spin liquid is a topological magnetic quantum state\\ncharacterized by Majorana fermions of fractionalized spin excitations, which\\nare identical to their own antiparticles. Here, we demonstrate emergence of\\nMajorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice\\n{\\\\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage\\nrelease of magnetic entropy involving localized and itinerant Majorana\\nfermions. The inelastic neutron scattering results further corroborate these\\ntwo distinct fermions by exhibiting quasielastic excitations at low energies\\naround the Brillouin zone center and Y-shaped magnetic continuum at high\\nenergies, which are evident for the ferromagnetic Kitaev model. Our results\\nprovide an opportunity to build a unified conceptual framework of\\nfractionalized excitations, applicable also for the quantum Hall states,\\nsuperconductors, and frustrated magnets.\\n',\n",
       " '  Identifying the mechanism by which high energy Lyman continuum (LyC) photons\\nescaped from early galaxies is one of the most pressing questions in cosmic\\nevolution. Haro 11 is the best known local LyC leaking galaxy, providing an\\nimportant opportunity to test our understanding of LyC escape. The observed LyC\\nemission in this galaxy presumably originates from one of the three bright,\\nphotoionizing knots known as A, B, and C. It is known that Knot C has strong\\nLy$\\\\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray\\nsource, which may be a low-luminosity AGN. To clarify the LyC source, we carry\\nout ionization-parameter mapping (IPM) by obtaining narrow-band imaging from\\nthe Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved\\nratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization\\nstructure of the interstellar medium and allows us to identify optically thin\\nregions. To optimize the continuum subtraction, we introduce a new method for\\ndetermining the best continuum scale factor derived from the mode of the\\ncontinuum-subtracted, image flux distribution. We find no conclusive evidence\\nof LyC escape from Knots B or C, but instead, we identify a high-ionization\\nregion extending over at least 1 kpc from Knot A. Knot A shows evidence of an\\nextremely young age ($\\\\lesssim 1$ Myr), perhaps containing very massive stars\\n($>100$ M$_\\\\odot$). It is weak in Ly$\\\\alpha$, so if it is confirmed as the LyC\\nsource, our results imply that LyC emission may be independent of Ly$\\\\alpha$\\nemission.\\n',\n",
       " '  Dependently typed languages such as Coq are used to specify and verify the\\nfull functional correctness of source programs. Type-preserving compilation can\\nbe used to preserve these specifications and proofs of correctness through\\ncompilation into the generated target-language programs. Unfortunately,\\ntype-preserving compilation of dependent types is hard. In essence, the problem\\nis that dependent type systems are designed around high-level compositional\\nabstractions to decide type checking, but compilation interferes with the\\ntype-system rules for reasoning about run-time terms.\\nWe develop a type-preserving closure-conversion translation from the Calculus\\nof Constructions (CC) with strong dependent pairs ($\\\\Sigma$ types)---a subset\\nof the core language of Coq---to a type-safe, dependently typed compiler\\nintermediate language named CC-CC. The central challenge in this work is how to\\ntranslate the source type-system rules for reasoning about functions into\\ntarget type-system rules for reasoning about closures. To justify these rules,\\nwe prove soundness of CC-CC by giving a model in CC. In addition to type\\npreservation, we prove correctness of separate compilation.\\n',\n",
       " '  Any generic closed curve in the plane can be transformed into a simple closed\\ncurve by a finite sequence of local transformations called homotopy moves. We\\nprove that simplifying a planar closed curve with $n$ self-crossings requires\\n$\\\\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the\\nbest previous upper bound $O(n^2)$, which is already implicit in the classical\\nwork of Steinitz; the matching lower bound follows from the construction of\\nclosed curves with large defect, a topological invariant of generic closed\\ncurves introduced by Aicardi and Arnold. Our lower bound also implies that\\n$\\\\Omega(n^{3/2})$ facial electrical transformations are required to reduce any\\nplane graph with treewidth $\\\\Omega(\\\\sqrt{n})$ to a single vertex, matching\\nknown upper bounds for rectangular and cylindrical grid graphs. More generally,\\nwe prove that transforming one immersion of $k$ circles with at most $n$\\nself-crossings into another requires $\\\\Theta(n^{3/2} + nk + k^2)$ homotopy\\nmoves in the worst case. Finally, we prove that transforming one\\nnoncontractible closed curve to another on any orientable surface requires\\n$\\\\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if\\nthe curve is homotopic to a simple closed curve.\\n',\n",
       " '  Consider a channel with a given input distribution. Our aim is to degrade it\\nto a channel with at most L output letters. One such degradation method is the\\nso called \"greedy-merge\" algorithm. We derive an upper bound on the reduction\\nin mutual information between input and output. For fixed input alphabet size\\nand variable L, the upper bound is within a constant factor of an\\nalgorithm-independent lower bound. Thus, we establish that greedy-merge is\\noptimal in the power-law sense.\\n',\n",
       " '  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let\\n${\\\\mathcal F}$ be the family of all functions holomorphic in the unit disk\\n${\\\\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on\\n$L_1$. It is shown that ${\\\\mathcal F}$ is normal in ${\\\\mathbb\\nD}\\\\backslash\\\\{0\\\\}$. The case where $L_0$ is the positive real axis and $L_1$ is\\nthe negative real axis is studied in more detail.\\n',\n",
       " '  A habitable exoplanet is a world that can maintain stable liquid water on its\\nsurface. Techniques and approaches to characterizing such worlds are essential,\\nas performing a census of Earth-like planets that may or may not have life will\\ninform our understanding of how frequently life originates and is sustained on\\nworlds other than our own. Observational techniques like high contrast imaging\\nand transit spectroscopy can reveal key indicators of habitability for\\nexoplanets. Both polarization measurements and specular reflectance from oceans\\n(also known as \"glint\") can provide direct evidence for surface liquid water,\\nwhile constraining surface pressure and temperature (from moderate resolution\\nspectra) can indicate liquid water stability. Indirect evidence for\\nhabitability can come from a variety of sources, including observations of\\nvariability due to weather, surface mapping studies, and/or measurements of\\nwater vapor or cloud profiles that indicate condensation near a surface.\\nApproaches to making the types of measurements that indicate habitability are\\ndiverse, and have different considerations for the required wavelength range,\\nspectral resolution, maximum noise levels, stellar host temperature, and\\nobserving geometry.\\n',\n",
       " '  In this paper, we evaluate the accuracy of deep learning approaches on\\ngeospatial vector geometry classification tasks. The purpose of this evaluation\\nis to investigate the ability of deep learning models to learn from geometry\\ncoordinates directly. Previous machine learning research applied to geospatial\\npolygon data did not use geometries directly, but derived properties thereof.\\nThese are produced by way of extracting geometry properties such as Fourier\\ndescriptors. Instead, our introduced deep neural net architectures are able to\\nlearn on sequences of coordinates mapped directly from polygons. In three\\nclassification tasks we show that the deep learning architectures are\\ncompetitive with common learning algorithms that require extracted features.\\n',\n",
       " \"  The distance standard deviation, which arises in distance correlation\\nanalysis of multivariate data, is studied as a measure of spread. New\\nrepresentations for the distance standard deviation are obtained in terms of\\nGini's mean difference and in terms of the moments of spacings of order\\nstatistics. Inequalities for the distance variance are derived, proving that\\nthe distance standard deviation is bounded above by the classical standard\\ndeviation and by Gini's mean difference. Further, it is shown that the distance\\nstandard deviation satisfies the axiomatic properties of a measure of spread.\\nExplicit closed-form expressions for the distance variance are obtained for a\\nbroad class of parametric distributions. The asymptotic distribution of the\\nsample distance variance is derived.\\n\",\n",
       " '  One of the most basic skills a robot should possess is predicting the effect\\nof physical interactions with objects in the environment. This enables optimal\\naction selection to reach a certain goal state. Traditionally, dynamics are\\napproximated by physics-based analytical models. These models rely on specific\\nstate representations that may be hard to obtain from raw sensory data,\\nespecially if no knowledge of the object shape is assumed. More recently, we\\nhave seen learning approaches that can predict the effect of complex physical\\ninteractions directly from sensory input. It is however an open question how\\nfar these models generalize beyond their training data. In this work, we\\ninvestigate the advantages and limitations of neural network based learning\\napproaches for predicting the effects of actions based on sensory input and\\nshow how analytical and learned models can be combined to leverage the best of\\nboth worlds. As physical interaction task, we use planar pushing, for which\\nthere exists a well-known analytical model and a large real-world dataset. We\\npropose to use a convolutional neural network to convert raw depth images or\\norganized point clouds into a suitable representation for the analytical model\\nand compare this approach to using neural networks for both, perception and\\nprediction. A systematic evaluation of the proposed approach on a very large\\nreal-world dataset shows two main advantages of the hybrid architecture.\\nCompared to a pure neural network, it significantly (i) reduces required\\ntraining data and (ii) improves generalization to novel physical interaction.\\n',\n",
       " '  In this paper, we give a complete characterization of Leavitt path algebras\\nwhich are graded $\\\\Sigma $-$V$ rings, that is, rings over which a direct sum of\\narbitrary copies of any graded simple module is graded injective. Specifically,\\nwe show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded\\n$\\\\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of\\narbitrary size but with finitely many non-zero entries over $K$ or\\n$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical\\ncharacterization of such a graded $\\\\Sigma $-$V$ ring $L$% . When the graph $E$\\nis finite, we show that $L$ is a graded $\\\\Sigma $-$V$ ring $\\\\Longleftrightarrow\\nL$ is graded directly-finite $\\\\Longleftrightarrow L $ has bounded index of\\nnilpotence $\\\\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that\\nthe equivalence of these properties in the preceding statement no longer holds\\nwhen the graph $E$ is infinite. Following this, we also characterize Leavitt\\npath algebras $L$ which are non-graded $\\\\Sigma $-$V$ rings. Graded rings which\\nare graded directly-finite are explored and it is shown that if a Leavitt path\\nalgebra $L$ is a graded $\\\\Sigma$-$V$ ring, then $L$ is always graded\\ndirectly-finite. Examples show the subtle differences between graded and\\nnon-graded directly-finite rings. Leavitt path algebras which are graded\\ndirectly-finite are shown to be directed unions of graded semisimple rings.\\nUsing this, we give an alternative proof of a theorem of Vaš \\\\cite{V} on\\ndirectly-finite Leavitt path algebras.\\n',\n",
       " '  We derive the mean squared error convergence rates of kernel density-based\\nplug-in estimators of mutual information measures between two multidimensional\\nrandom variables $\\\\mathbf{X}$ and $\\\\mathbf{Y}$ for two cases: 1) $\\\\mathbf{X}$\\nand $\\\\mathbf{Y}$ are both continuous; 2) $\\\\mathbf{X}$ is continuous and\\n$\\\\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble\\nestimator of these information measures for the second case by taking a\\nweighted sum of the plug-in estimators with varied bandwidths. The resulting\\nensemble estimator achieves the $1/N$ parametric convergence rate when the\\nconditional densities of the continuous variables are sufficiently smooth. To\\nthe best of our knowledge, this is the first nonparametric mutual information\\nestimator known to achieve the parametric convergence rate for this case, which\\nfrequently arises in applications (e.g. variable selection in classification).\\nThe estimator is simple to implement as it uses the solution to an offline\\nconvex optimization problem and simple plug-in estimators. A central limit\\ntheorem is also derived for the ensemble estimator. Ensemble estimators that\\nachieve the parametric rate are also derived for the first case ($\\\\mathbf{X}$\\nand $\\\\mathbf{Y}$ are both continuous) and another case 3) $\\\\mathbf{X}$ and\\n$\\\\mathbf{Y}$ may have any mixture of discrete and continuous components.\\n',\n",
       " '  Widespread use of social media has led to the generation of substantial\\namounts of information about individuals, including health-related information.\\nSocial media provides the opportunity to study health-related information about\\nselected population groups who may be of interest for a particular study. In\\nthis paper, we explore the possibility of utilizing social media to perform\\ntargeted data collection and analysis from a particular population group --\\npregnant women. We hypothesize that we can use social media to identify cohorts\\nof pregnant women and follow them over time to analyze crucial health-related\\ninformation. To identify potentially pregnant women, we employ simple\\nrule-based searches that attempt to detect pregnancy announcements with\\nmoderate precision. To further filter out false positives and noise, we employ\\na supervised classifier using a small number of hand-annotated data. We then\\ncollect their posts over time to create longitudinal health timelines and\\nattempt to divide the timelines into different pregnancy trimesters. Finally,\\nwe assess the usefulness of the timelines by performing a preliminary analysis\\nto estimate drug intake patterns of our cohort at different trimesters. Our\\nrule-based cohort identification technique collected 53,820 users over thirty\\nmonths from Twitter. Our pregnancy announcement classification technique\\nachieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user\\ntimelines. Analysis of the timelines revealed that pertinent health-related\\ninformation, such as drug-intake and adverse reactions can be mined from the\\ndata. Our approach to using user timelines in this fashion has produced very\\nencouraging results and can be employed for other important tasks where\\ncohorts, for which health-related information may not be available from other\\nsources, are required to be followed over time to derive population-based\\nestimates.\\n',\n",
       " '  The vortex method is a common numerical and theoretical approach used to\\nimplement the motion of an ideal flow, in which the vorticity is approximated\\nby a sum of point vortices, so that the Euler equations read as a system of\\nordinary differential equations. Such a method is well justified in the full\\nplane, thanks to the explicit representation formulas of Biot and Savart. In an\\nexterior domain, we also replace the impermeable boundary by a collection of\\npoint vortices generating the circulation around the obstacle. The density of\\nthese point vortices is chosen in order that the flow remains tangent at\\nmidpoints between adjacent vortices. In this work, we provide a rigorous\\njustification for this method in exterior domains. One of the main mathematical\\ndifficulties being that the Biot-Savart kernel defines a singular integral\\noperator when restricted to a curve. For simplicity and clarity, we only treat\\nthe case of the unit disk in the plane approximated by a uniformly distributed\\nmesh of point vortices. The complete and general version of our work is\\navailable in [arXiv:1707.01458].\\n',\n",
       " \"  Let $R$ be an associative ring with unit and denote by $K({\\\\rm R\\n\\\\mbox{-}Proj})$ the homotopy category of complexes of projective left\\n$R$-modules. Neeman proved the theorem that $K({\\\\rm R \\\\mbox{-}Proj})$ is\\n$\\\\aleph_1$-compactly generated, with the category $K^+ ({\\\\rm R \\\\mbox{-}proj})$\\nof left bounded complexes of finitely generated projective $R$-modules\\nproviding an essentially small class of such generators. Another proof of\\nNeeman's theorem is explained, using recent ideas of Christensen and Holm, and\\nEmmanouil. The strategy of the proof is to show that every complex in $K({\\\\rm R\\n\\\\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\\\\rm R\\n\\\\mbox{-}Flat})/\\\\langle K^+ ({\\\\rm R \\\\mbox{-}proj}) \\\\rangle.$\\n\",\n",
       " '  For an arbitrary finite family of semi-algebraic/definable functions, we\\nconsider the corresponding inequality constraint set and we study qualification\\nconditions for perturbations of this set. In particular we prove that all\\npositive diagonal perturbations, save perhaps a finite number of them, ensure\\nthat any point within the feasible set satisfies Mangasarian-Fromovitz\\nconstraint qualification. Using the Milnor-Thom theorem, we provide a bound for\\nthe number of singular perturbations when the constraints are polynomial\\nfunctions. Examples show that the order of magnitude of our exponential bound\\nis relevant. Our perturbation approach provides a simple protocol to build\\nsequences of \"regular\" problems approximating an arbitrary\\nsemi-algebraic/definable problem. Applications to sequential quadratic\\nprogramming methods and sum of squares relaxation are provided.\\n',\n",
       " '  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits\\nof the hamiltonian $H=\\\\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a\\nsymplectic manifold (\\\\Gamma,\\\\omega), a hamiltonian $H:\\\\Gamma\\\\to\\\\mathbb{R}$ and\\na Lagrangian sub-manifold $M\\\\subset\\\\Gamma$ we find a generalization of the\\nnotion of curvature. The particular case\\n$H=\\\\frac{1}{2}g^{ij}\\\\left[p_{i}-A_{i}\\\\right]\\\\left[p_{j}-A_{j}\\\\right]+\\\\phi $ of\\na particle moving in a gravitational, electromagnetic and scalar fields is\\nstudied in more detail. The integral of the generalized Ricci tensor w.r.t. the\\nBoltzmann weight reduces to the action principle\\n$\\\\int\\\\left[R+\\\\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\\\\partial_{i}\\\\phi\\\\partial_{j}\\\\phi\\\\right]e^{-\\\\phi}\\\\sqrt{g}d^{n}q$\\nfor the scalar, vector and tensor fields.\\n',\n",
       " '  We investigate how a neural network can learn perception actions loops for\\nnavigation in unknown environments. Specifically, we consider how to learn to\\nnavigate in environments populated with cul-de-sacs that represent convex local\\nminima that the robot could fall into instead of finding a set of feasible\\nactions that take it to the goal. Traditional methods rely on maintaining a\\nglobal map to solve the problem of over coming a long cul-de-sac. However, due\\nto errors induced from local and global drift, it is highly challenging to\\nmaintain such a map for long periods of time. One way to mitigate this problem\\nis by using learning techniques that do not rely on hand engineered map\\nrepresentations and instead output appropriate control policies directly from\\ntheir sensory input. We first demonstrate that such a problem cannot be solved\\ndirectly by deep reinforcement learning due to the sparse reward structure of\\nthe environment. Further, we demonstrate that deep supervised learning also\\ncannot be used directly to solve this problem. We then investigate network\\nmodels that offer a combination of reinforcement learning and supervised\\nlearning and highlight the significance of adding fully differentiable memory\\nunits to such networks. We evaluate our networks on their ability to generalize\\nto new environments and show that adding memory to such networks offers huge\\njumps in performance\\n',\n",
       " '  In this paper, we present a combinatorial approach to the opposite 2-variable\\nbi-free partial $S$-transforms where the opposite multiplication is used on the\\nright. In addition, extensions of this partial $S$-transforms to the\\nconditional bi-free and operator-valued bi-free settings are discussed.\\n',\n",
       " '  Many giant exoplanets are found near their Roche limit and in mildly\\neccentric orbits. In this study we examine the fate of such planets through\\nRoche-lobe overflow as a function of the physical properties of the binary\\ncomponents, including the eccentricity and the asynchronicity of the rotating\\nplanet. We use a direct three-body integrator to compute the trajectories of\\nthe lost mass in the ballistic limit and investigate the possible outcomes. We\\nfind three different outcomes for the mass transferred through the Lagrangian\\npoint $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the\\nstellar surface, (iii) disk formation around the star. We explore the parameter\\nspace of the three different regimes and find that at low eccentricities,\\n$e\\\\lesssim 0.2$, mass overflow leads to disk formation for most systems, while\\nfor higher eccentricities or retrograde orbits self-accretion is the only\\npossible outcome. We conclude that the assumption often made in previous work\\nthat when a planet overflows its Roche lobe it is quickly disrupted and\\naccreted by the star is not always valid.\\n',\n",
       " '  A new Short-Orbit Spectrometer (SOS) has been constructed and installed\\nwithin the experimental facility of the A1 collaboration at Mainz Microtron\\n(MAMI), with the goal to detect low-energy pions. It is equipped with a\\nBrowne-Buechner magnet and a detector system consisting of two helium-ethane\\nbased drift chambers and a scintillator telescope made of five layers. The\\ndetector system allows detection of pions in the momentum range of 50 - 147\\nMeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can\\nbe placed at a distance range of 54 - 66 cm from the target center. Two\\ncollimators are available for the measurements, one having 1.8 msr aperture and\\nthe other having 7 msr aperture. The Short-Orbit Spectrometer has been\\nsuccessfully calibrated and used in coincidence measurements together with the\\nstandard magnetic spectrometers of the A1 collaboration.\\n',\n",
       " '  Capacitive deionization (CDI) is a fast-emerging water desalination\\ntechnology in which a small cell voltage of ~1 V across porous carbon\\nelectrodes removes salt from feedwaters via electrosorption. In flow-through\\nelectrode (FTE) CDI cell architecture, feedwater is pumped through macropores\\nor laser perforated channels in porous electrodes, enabling highly compact\\ncells with parallel flow and electric field, as well as rapid salt removal. We\\nhere present a one-dimensional model describing water desalination by FTE CDI,\\nand a comparison to data from a custom-built experimental cell. The model\\nemploys simple cell boundary conditions derived via scaling arguments. We show\\ngood model-to-data fits with reasonable values for fitting parameters such as\\nthe Stern layer capacitance, micropore volume, and attraction energy. Thus, we\\ndemonstrate that from an engineering modeling perspective, an FTE CDI cell may\\nbe described with simpler one-dimensional models, unlike more typical\\nflow-between electrodes architecture where 2D models are required.\\n',\n",
       " '  We present bilateral teleoperation system for task learning and robot motion\\ngeneration. Our system includes a bilateral teleoperation platform and a deep\\nlearning software. The deep learning software refers to human demonstration\\nusing the bilateral teleoperation platform to collect visual images and robotic\\nencoder values. It leverages the datasets of images and robotic encoder\\ninformation to learn about the inter-modal correspondence between visual images\\nand robot motion. In detail, the deep learning software uses a combination of\\nDeep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent\\nNeural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor\\nangles, to learn motion taught be human teleoperation. The learnt models are\\nused to predict new motion trajectories for similar tasks. Experimental results\\nshow that our system has the adaptivity to generate motion for similar scooping\\ntasks. Detailed analysis is performed based on failure cases of the\\nexperimental results. Some insights about the cans and cannots of the system\\nare summarized.\\n',\n",
       " '  We propose two multimodal deep learning architectures that allow for\\ncross-modal dataflow (XFlow) between the feature extractors, thereby extracting\\nmore interpretable features and obtaining a better representation than through\\nunimodal learning, for the same amount of training data. These models can\\nusefully exploit correlations between audio and visual data, which have a\\ndifferent dimensionality and are therefore nontrivially exchangeable. Our work\\nimproves on existing multimodal deep learning metholodogies in two essential\\nways: (1) it presents a novel method for performing cross-modality (before\\nfeatures are learned from individual modalities) and (2) extends the previously\\nproposed cross-connections, which only transfer information between streams\\nthat process compatible data. Both cross-modal architectures outperformed their\\nbaselines (by up to 7.5%) when evaluated on the AVletters dataset.\\n',\n",
       " '  Educational research has shown that narratives are useful tools that can help\\nyoung students make sense of scientific phenomena. Based on previous research,\\nI argue that narratives can also become tools for high school students to make\\nsense of concepts such as the electric field. In this paper I examine high\\nschool students visual and oral narratives in which they describe the\\ninteraction among electric charges as if they were characters of a cartoon\\nseries. The study investigates: given the prompt to produce narratives for\\nelectrostatic phenomena during a classroom activity prior to receiving formal\\ninstruction, (1) what ideas of electrostatics do students attend to in their\\nnarratives?; (2) what role do students narratives play in their understanding\\nof electrostatics? The participants were a group of high school students\\nengaged in an open-ended classroom activity prior to receiving formal\\ninstruction about electrostatics. During the activity, the group was asked to\\ndraw comic strips for electric charges. In addition to individual work,\\nstudents shared their work within small groups as well as with the whole group.\\nPost activity, six students from a small group were interviewed individually\\nabout their work. In this paper I present two cases in which students produced\\nnarratives to express their ideas about electrostatics in different ways. In\\neach case, I present student work for the comic strip activity (visual\\nnarratives), their oral descriptions of their work (oral narratives) during the\\ninterview and/or to their peers during class, and the their ideas of the\\nelectric interactions expressed through their narratives.\\n',\n",
       " \"  Hospital acquired infections (HAI) are infections acquired within the\\nhospital from healthcare workers, patients or from the environment, but which\\nhave no connection to the initial reason for the patient's hospital admission.\\nHAI are a serious world-wide problem, leading to an increase in mortality\\nrates, duration of hospitalisation as well as significant economic burden on\\nhospitals. Although clear preventive guidelines exist, studies show that\\ncompliance to them is frequently poor. This paper details the software\\nperspective for an innovative, business process software based cyber-physical\\nsystem that will be implemented as part of a European Union-funded research\\nproject. The system is composed of a network of sensors mounted in different\\nsites around the hospital, a series of wearables used by the healthcare workers\\nand a server side workflow engine. For better understanding, we describe the\\nsystem through the lens of a single, simple clinical workflow that is\\nresponsible for a significant portion of all hospital infections. The goal is\\nthat when completed, the system will be configurable in the sense of\\nfacilitating the creation and automated monitoring of those clinical workflows\\nthat when combined, account for over 90\\\\% of hospital infections.\\n\",\n",
       " '  We advocate the use of curated, comprehensive benchmark suites of machine\\nlearning datasets, backed by standardized OpenML-based interfaces and\\ncomplementary software toolkits written in Python, Java and R. Major\\ndistinguishing features of OpenML benchmark suites are (a) ease of use through\\nstandardized data formats, APIs, and existing client libraries; (b)\\nmachine-readable meta-information regarding the contents of the suite; and (c)\\nonline sharing of results, enabling large scale comparisons. As a first such\\nsuite, we propose the OpenML100, a machine learning benchmark suite of\\n100~classification datasets carefully curated from the thousands of datasets\\navailable on OpenML.org.\\n',\n",
       " '  In this article, we study orbifold constructions associated with the Leech\\nlattice vertex operator algebra. As an application, we prove that the structure\\nof a strongly regular holomorphic vertex operator algebra of central charge\\n$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra\\nhas the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,\\n$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse\\norbifold construction. Our result also provides alternative constructions of\\nthese vertex operator algebras (except for the case $A_{6,7}$) from the Leech\\nlattice vertex operator algebra.\\n',\n",
       " '  Deconstruction of the theme of the 2017 FQXi essay contest is already an\\ninteresting exercise in its own right: Teleology is rarely useful in physics\\n--- the only known mainstream physics example (black hole event horizons) has a\\nvery mixed score-card --- so the \"goals\" and \"aims and intentions\" alluded to\\nin the theme of the 2017 FQXi essay contest are already somewhat pushing the\\nlimits. Furthermore, \"aims and intentions\" certainly carries the implication of\\nconsciousness, and opens up a whole can of worms related to the mind-body\\nproblem. As for \"mindless mathematical laws\", that allusion is certainly in\\ntension with at least some versions of the \"mathematical universe hypothesis\".\\nFinally \"wandering towards a goal\" again carries the implication of\\nconsciousness, with all its attendant problems.\\nIn this essay I will argue, simply because we do not yet have any really good\\nmathematical or physical theory of consciousness, that the theme of this essay\\ncontest is premature, and unlikely to lead to any resolution that would be\\nwidely accepted in the mathematics or physics communities.\\n',\n",
       " '  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water\\nvapour radiometers (WVR), which monitor the atmospheric water vapour line at\\n183 GHz along the line of sight above each antenna to correct for phase delays\\nintroduced by the wet component of the troposphere. The application of WVR\\nderived phase corrections improve the image quality and facilitate successful\\nobservations in weather conditions that were classically marginal or poor. We\\npresent work to indicate that a scaling factor applied to the WVR solutions can\\nact to further improve the phase stability and image quality of ALMA data. We\\nfind reduced phase noise statistics for 62 out of 75 datasets from the\\nlong-baseline science verification campaign after a WVR scaling factor is\\napplied. The improvement of phase noise translates to an expected coherence\\nimprovement in 39 datasets. When imaging the bandpass source, we find 33 of the\\n39 datasets show an improvement in the signal-to-noise ratio (S/N) between a\\nfew to ~30 percent. There are 23 datasets where the S/N of the science image is\\nimproved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies\\nstudied (band 6 and band 7) are those most improved, specifically datasets with\\nlow precipitable water vapour (PWV), <1mm, where the dominance of the wet\\ncomponent is reduced. Although these improvements are not profound, phase\\nstability improvements via the WVR scaling factor come into play for the higher\\nfrequency (>450 GHz) and long-baseline (>5km) observations. These inherently\\nhave poorer phase stability and are taken in low PWV (<1mm) conditions for\\nwhich we find the scaling to be most effective. A promising explanation for the\\nscaling factor is the mixing of dry and wet air components, although other\\norigins are discussed. We have produced a python code to allow ALMA users to\\nundertake WVR scaling tests and make improvements to their data.\\n',\n",
       " '  In this paper, we study the $\\\\mu$-ordinary locus of a Shimura variety with\\nparahoric level structure. Under the axioms in \\\\cite{HR}, we show that\\n$\\\\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport\\nstrata introduced in \\\\cite{HR} and we give criteria on the density of the\\n$\\\\mu$-ordinary locus.\\n',\n",
       " '  Charts are an excellent way to convey patterns and trends in data, but they\\ndo not facilitate further modeling of the data or close inspection of\\nindividual data points. We present a fully automated system for extracting the\\nnumerical values of data points from images of scatter plots. We use deep\\nlearning techniques to identify the key components of the chart, and optical\\ncharacter recognition together with robust regression to map from pixels to the\\ncoordinate system of the chart. We focus on scatter plots with linear scales,\\nwhich already have several interesting challenges. Previous work has done fully\\nautomatic extraction for other types of charts, but to our knowledge this is\\nthe first approach that is fully automatic for scatter plots. Our method\\nperforms well, achieving successful data extraction on 89% of the plots in our\\ntest set.\\n',\n",
       " '  We consider a modification to the standard cosmological history consisting of\\nintroducing a new species $\\\\phi$ whose energy density red-shifts with the scale\\nfactor $a$ like $\\\\rho_\\\\phi \\\\propto a^{-(4+n)}$. For $n>0$, such a red-shift is\\nfaster than radiation, hence the new species dominates the energy budget of the\\nuniverse at early times while it is completely negligible at late times. If\\nequality with the radiation energy density is achieved at low enough\\ntemperatures, dark matter can be produced as a thermal relic during the new\\ncosmological phase. Dark matter freeze-out then occurs at higher temperatures\\ncompared to the standard case, implying that reproducing the observed abundance\\nrequires significantly larger annihilation rates. Here, we point out a\\ncompletely new phenomenon, which we refer to as $\\\\textit{relentless}$ dark\\nmatter: for large enough $n$, unlike the standard case where annihilation ends\\nshortly after the departure from thermal equilibrium, dark matter particles\\nkeep annihilating long after leaving chemical equilibrium, with a significant\\ndepletion of the final relic abundance. Relentless annihilation occurs for $n\\n\\\\geq 2$ and $n \\\\geq 4$ for s-wave and p-wave annihilation, respectively, and it\\nthus occurs in well motivated scenarios such as a quintessence with a kination\\nphase. We discuss a few microscopic realizations for the new cosmological\\ncomponent and highlight the phenomenological consequences of our calculations\\nfor dark matter searches.\\n',\n",
       " '  We describe the configuration space $\\\\mathbf{S}$ of polygons with prescribed\\nedge slopes, and study the perimeter $\\\\mathcal{P}$ as a Morse function on\\n$\\\\mathbf{S}$. We characterize critical points of $\\\\mathcal{P}$ (these are\\n\\\\textit{tangential} polygons) and compute their Morse indices. This setup is\\nmotivated by a number of results about critical points and Morse indices of the\\noriented area function defined on the configuration space of polygons with\\nprescribed edge lengths (flexible polygons). As a by-product, we present an\\nindependent computation of the Morse index of the area function (obtained\\nearlier by G. Panina and A. Zhukova).\\n',\n",
       " '  This paper discusses a Metropolis-Hastings algorithm developed by\\n\\\\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is\\nproven that the algorithm becomes more efficient with more data and meets the\\ngrowing demands of large scale educational measurement.\\n',\n",
       " '  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National\\nUniversity of Singapore. In this contribution he describes the power of\\noptofluidics as a research tool and reviews new insights within the areas of\\nsingle cell analysis, microphysiological analysis, and integrated systems.\\n',\n",
       " '  Topology has appeared in different physical contexts. The most prominent\\napplication is topologically protected edge transport in condensed matter\\nphysics. The Chern number, the topological invariant of gapped Bloch\\nHamiltonians, is an important quantity in this field. Another example of\\ntopology, in polarization physics, are polarization singularities, called L\\nlines and C points. By establishing a connection between these two theories, we\\ndevelop a novel technique to visualize and potentially measure the Chern\\nnumber: it can be expressed either as the winding of the polarization azimuth\\nalong L lines in reciprocal space, or in terms of the handedness and the index\\nof the C points. For mechanical systems, this is directly connected to the\\nvisible motion patterns.\\n',\n",
       " '  We study the scale and tidy subgroups of an endomorphism of a totally\\ndisconnected locally compact group using a geometric framework. This leads to\\nnew interpretations of tidy subgroups and the scale function. Foremost, we\\nobtain a geometric tidying procedure which applies to endomorphisms as well as\\na geometric proof of the fact that tidiness is equivalent to being minimizing\\nfor a given endomorphism. Our framework also yields an endomorphism version of\\nthe Baumgartner-Willis tree representation theorem. We conclude with a\\nconstruction of new endomorphisms of totally disconnected locally compact\\ngroups from old via HNN-extensions.\\n',\n",
       " '  The coupled exciton-vibrational dynamics of a three-site model of the FMO\\ncomplex is investigated using the Multi-layer Multi-configuration\\nTime-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of\\nthe spectral density on the exciton state populations as well as on the\\nvibrational and vibronic non-equilibrium excitations. Models which use either a\\nsingle or site-specific spectral densities are contrasted to a spectral density\\nadapted from experiment. For the transfer efficiency, the total integrated\\nHuang-Rhys factor is found to be more important than details of the spectral\\ndistributions. However, the latter are relevant for the obtained\\nnon-equilibrium vibrational and vibronic distributions and thus influence the\\nactual pattern of population relaxation.\\n',\n",
       " '  In the first part of this work we show the convergence with respect to an\\nasymptotic parameter {\\\\epsilon} of a delayed heat equation. It represents a\\nmathematical extension of works considered previously by the authors [Milisic\\net al. 2011, Milisic et al. 2016]. Namely, this is the first result involving\\ndelay operators approximating protein linkages coupled with a spatial elliptic\\nsecond order operator. For the sake of simplicity we choose the Laplace\\noperator, although more general results could be derived. The main arguments\\nare (i) new energy estimates and (ii) a stability result extended from the\\nprevious work to this more involved context. They allow to prove convergence of\\nthe delay operator to a friction term together with the Laplace operator in the\\nsame asymptotic regime considered without the space dependence in [Milisic et\\nal, 2011]. In a second part we extend fixed-point results for the fully\\nnon-linear model introduced in [Milisic et al, 2016] and prove global existence\\nin time. This shows that the blow-up scenario observed previously does not\\noccur. Since the latter result was interpreted as a rupture of adhesion forces,\\nwe discuss the possibility of bond breaking both from the analytic and\\nnumerical point of view.\\n',\n",
       " '  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\\nestimating Gaussian location mixture densities in $d$-dimensions from\\nindependent observations. Unlike usual likelihood-based methods for fitting\\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\\nmultiplicative factors) when the true density is a discrete Gaussian mixture\\nwithout any prior information on the number of mixture components. NPMLEs can\\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\\nestimator. Here our results imply that the empirical Bayes estimator performs\\nat nearly the optimal level (up to logarithmic multiplicative factors) for\\ndenoising in clustering situations without any prior knowledge of the number of\\nclusters.\\n',\n",
       " '  Parametric resonance is among the most efficient phenomena generating\\ngravitational waves (GWs) in the early Universe. The dynamics of parametric\\nresonance, and hence of the GWs, depend exclusively on the resonance parameter\\n$q$. The latter is determined by the properties of each scenario: the initial\\namplitude and potential curvature of the oscillating field, and its coupling to\\nother species. Previous works have only studied the GW production for fixed\\nvalue(s) of $q$. We present an analytical derivation of the GW amplitude\\ndependence on $q$, valid for any scenario, which we confront against numerical\\nresults. By running lattice simulations in an expanding grid, we study for a\\nwide range of $q$ values, the production of GWs in post-inflationary preheating\\nscenarios driven by parametric resonance. We present simple fits for the final\\namplitude and position of the local maxima in the GW spectrum. Our\\nparametrization allows to predict the location and amplitude of the GW\\nbackground today, for an arbitrary $q$. The GW signal can be rather large, as\\n$h^2\\\\Omega_{\\\\rm GW}(f_p) \\\\lesssim 10^{-11}$, but it is always peaked at high\\nfrequencies $f_p \\\\gtrsim 10^{7}$ Hz. We also discuss the case of\\nspectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,\\nor the Standard Model Higgs.\\n',\n",
       " '  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed\\nbands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a\\ntentative assignment is given to the 2-photon transition from $X$ to the\\nn=12-13 $[X^{2}{\\\\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate\\nthe IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been\\nidentified as a molecular candidate amenable to laser control. Our work allows\\nus to identify an efficient method for loading cold SiO$^{+}$ from an ablated\\nsample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.\\n',\n",
       " '  Robots and automated systems are increasingly being introduced to unknown and\\ndynamic environments where they are required to handle disturbances, unmodeled\\ndynamics, and parametric uncertainties. Robust and adaptive control strategies\\nare required to achieve high performance in these dynamic environments. In this\\npaper, we propose a novel adaptive model predictive controller that combines\\nmodel predictive control (MPC) with an underlying $\\\\mathcal{L}_1$ adaptive\\ncontroller to improve trajectory tracking of a system subject to unknown and\\nchanging disturbances. The $\\\\mathcal{L}_1$ adaptive controller forces the\\nsystem to behave in a predefined way, as specified by a reference model. A\\nhigher-level model predictive controller then uses this reference model to\\ncalculate the optimal reference input based on a cost function, while taking\\ninto account input and state constraints. We focus on the experimental\\nvalidation of the proposed approach and demonstrate its effectiveness in\\nexperiments on a quadrotor. We show that the proposed approach has a lower\\ntrajectory tracking error compared to non-predictive, adaptive approaches and a\\npredictive, non-adaptive approach, even when external wind disturbances are\\napplied.\\n',\n",
       " '  With the use of ontologies in several domains such as semantic web,\\ninformation retrieval, artificial intelligence, the concept of similarity\\nmeasuring has become a very important domain of research. Therefore, in the\\ncurrent paper, we propose our method of similarity measuring which uses the\\nDijkstra algorithm to define and compute the shortest path. Then, we use this\\none to compute the semantic distance between two concepts defined in the same\\nhierarchy of ontology. Afterward, we base on this result to compute the\\nsemantic similarity. Finally, we present an experimental comparison between our\\nmethod and other methods of similarity measuring.\\n',\n",
       " '  In this short note, we present a novel method for computing exact lower and\\nupper bounds of eigenvalues of a symmetric tridiagonal interval matrix.\\nCompared to the known methods, our approach is fast, simple to present and to\\nimplement, and avoids any assumptions. Our construction explicitly yields those\\nmatrices for which particular lower and upper bounds are attained.\\n',\n",
       " '  Visualizing a complex network is computationally intensive process and\\ndepends heavily on the number of components in the network. One way to solve\\nthis problem is not to render the network in real time. PRE-render Content\\nUsing Tiles (PRECUT) is a process to convert any complex network into a\\npre-rendered network. Tiles are generated from pre-rendered images at different\\nzoom levels, and navigating the network simply becomes delivering relevant\\ntiles. PRECUT is exemplified by performing large-scale compound-target\\nrelationship analyses. Matched molecular pair (MMP) networks were created using\\ncompounds and the target class description found in the ChEMBL database. To\\nvisualize MMP networks, the MMP network viewer has been implemented in COMBINE\\nand as a web application, hosted at this http URL.\\n',\n",
       " '  We prove that every triangle-free graph with maximum degree $\\\\Delta$ has list\\nchromatic number at most $(1+o(1))\\\\frac{\\\\Delta}{\\\\ln \\\\Delta}$. This matches the\\nbest-known bound for graphs of girth at least 5. We also provide a new proof\\nthat for any $r\\\\geq 4$ every $K_r$-free graph has list-chromatic number at most\\n$200r\\\\frac{\\\\Delta\\\\ln\\\\ln\\\\Delta}{\\\\ln\\\\Delta}$.\\n',\n",
       " '  We study the two-dimensional topology of the galactic distribution when\\nprojected onto two-dimensional spherical shells. Using the latest Horizon Run 4\\nsimulation data, we construct the genus of the two-dimensional field and\\nconsider how this statistic is affected by late-time nonlinear effects --\\nprincipally gravitational collapse and redshift space distortion (RSD). We also\\nconsider systematic and numerical artifacts such as shot noise, galaxy bias,\\nand finite pixel effects. We model the systematics using a Hermite polynomial\\nexpansion and perform a comprehensive analysis of known effects on the\\ntwo-dimensional genus, with a view toward using the statistic for cosmological\\nparameter estimation. We find that the finite pixel effect is dominated by an\\namplitude drop and can be made less than $1\\\\%$ by adopting pixels smaller than\\n$1/3$ of the angular smoothing length. Nonlinear gravitational evolution\\nintroduces time-dependent coefficients of the zeroth, first, and second Hermite\\npolynomials, but the genus amplitude changes by less than $1\\\\%$ between $z=1$\\nand $z=0$ for smoothing scales $R_{\\\\rm G} > 9 {\\\\rm Mpc/h}$. Non-zero terms are\\nmeasured up to third order in the Hermite polynomial expansion when studying\\nRSD. Differences in shapes of the genus curves in real and redshift space are\\nsmall when we adopt thick redshift shells, but the amplitude change remains a\\nsignificant $\\\\sim {\\\\cal O}(10\\\\%)$ effect. The combined effects of galaxy\\nbiasing and shot noise produce systematic effects up to the second Hermite\\npolynomial. It is shown that, when sampling, the use of galaxy mass cuts\\nsignificantly reduces the effect of shot noise relative to random sampling.\\n',\n",
       " '  The new index of the author\\'s popularity estimation is represented in the\\npaper. The index is calculated on the basis of Wikipedia encyclopedia analysis\\n(Wikipedia Index - WI). Unlike the conventional existed citation indices, the\\nsuggested mark allows to evaluate not only the popularity of the author, as it\\ncan be done by means of calculating the general citation number or by the\\nHirsch index, which is often used to measure the author\\'s research rate. The\\nindex gives an opportunity to estimate the author\\'s popularity, his/her\\ninfluence within the sought-after area \"knowledge area\" in the Internet - in\\nthe Wikipedia. The suggested index is supposed to be calculated in frames of\\nthe subject domain, and it, on the one hand, avoids the mistaken computation of\\nthe homonyms, and on the other hand - provides the entirety of the subject\\narea. There are proposed algorithms and the technique of the Wikipedia Index\\ncalculation through the network encyclopedia sounding, the exemplified\\ncalculations of the index for the prominent researchers, and also the methods\\nof the information networks formation - models of the subject domains by the\\nautomatic monitoring and networks information reference resources analysis. The\\nconsidered in the paper notion network corresponds the terms-heads of the\\nWikipedia articles.\\n',\n",
       " '  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree\\n266 and describe the applied method. This yields explicit polynomials having J1\\nas a Galois group over K(t), [K:Q] = 7.\\n',\n",
       " '  Our goal is to find classes of convolution semigroups on Lie groups $G$ that\\ngive rise to interesting processes in symmetric spaces $G/K$. The\\n$K$-bi-invariant convolution semigroups are a well-studied example. An\\nappealing direction for the next step is to generalise to right $K$-invariant\\nconvolution semigroups, but recent work of Liao has shown that these are in\\none-to-one correspondence with $K$-bi-invariant convolution semigroups. We\\ninvestigate a weaker notion of right $K$-invariance, but show that this is, in\\nfact, the same as the usual notion. Another possible approach is to use\\ngeneralised notions of negative definite functions, but this also leads to\\nnothing new. We finally find an interesting class of convolution semigroups\\nthat are obtained by making use of the Cartan decomposition of a semisimple Lie\\ngroup, and the solution of certain stochastic differential equations. Examples\\nsuggest that these are well-suited for generating random motion along geodesics\\nin symmetric spaces.\\n',\n",
       " '  CMO Council reports that 71\\\\% of internet users in the U.S. were influenced\\nby coupons and discounts when making their purchase decisions. It has also been\\nshown that offering coupons to a small fraction of users (called seed users)\\nmay affect the purchase decisions of many other users in a social network. This\\nmotivates us to study the optimal coupon allocation problem, and our objective\\nis to allocate coupons to a set of users so as to maximize the expected\\ncascade. Different from existing studies on influence maximizaton (IM), our\\nframework allows a general utility function and a more complex set of\\nconstraints. In particular, we formulate our problem as an approximate\\nsubmodular maximization problem subject to matroid and knapsack constraints.\\nExisting techniques relying on the submodularity of the utility function, such\\nas greedy algorithm, can not work directly on a non-submodular function. We use\\n$\\\\epsilon$ to measure the difference between our function and its closest\\nsubmodular function and propose a novel approximate algorithm with\\napproximation ratio $\\\\beta(\\\\epsilon)$ with $\\\\lim_{\\\\epsilon\\\\rightarrow\\n0}\\\\beta(\\\\epsilon)=1-1/e$. This is the best approximation guarantee for\\napproximate submodular maximization subject to a partition matroid and knapsack\\nconstraints, our results apply to a broad range of optimization problems that\\ncan be formulated as an approximate submodular maximization problem.\\n',\n",
       " '  We prove the Lefschetz duality for intersection (co)homology in the framework\\nof $\\\\partial$-pesudomanifolds. We work with general perversities and without\\nrestriction on the coefficient ring.\\n',\n",
       " '  All possible removals of $n=5$ nodes from networks of size $N=100$ are\\nperformed in order to find the optimal set of nodes which fragments the\\noriginal network into the smallest largest connected component. The resulting\\nattacks are ordered according to the size of the largest connected component\\nand compared with the state of the art methods of network attacks. We chose\\nattacks of size $5$ on relative small networks of size $100$ because the number\\nof all possible attacks, ${100}\\\\choose{5}$ $\\\\approx 10^8$, is at the verge of\\nthe possible to compute with the available standard computers. Besides, we\\napplied the procedure in a series of networks with controlled and varied\\nmodularity, comparing the resulting statistics with the effect of removing the\\nsame amount of vertices, according to the known most efficient disruption\\nstrategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index\\nattack (CI), and Modular Based Attack (MBA). Results show that modularity has\\nan inverse relation with robustness, with $Q_c \\\\approx 0.7$ being the critical\\nvalue. For modularities lower than $Q_c$, all heuristic method gives mostly the\\nsame results than with random attacks, while for bigger $Q$, networks are less\\nrobust and highly vulnerable to malicious attacks.\\n',\n",
       " \"  In this paper, we study stochastic non-convex optimization with non-convex\\nrandom functions. Recent studies on non-convex optimization revolve around\\nestablishing second-order convergence, i.e., converging to a nearly\\nsecond-order optimal stationary points. However, existing results on stochastic\\nnon-convex optimization are limited, especially with a high probability\\nsecond-order convergence. We propose a novel updating step (named NCG-S) by\\nleveraging a stochastic gradient and a noisy negative curvature of a stochastic\\nHessian, where the stochastic gradient and Hessian are based on a proper\\nmini-batch of random functions. Building on this step, we develop two\\nalgorithms and establish their high probability second-order convergence. To\\nthe best of our knowledge, the proposed stochastic algorithms are the first\\nwith a second-order convergence in {\\\\it high probability} and a time complexity\\nthat is {\\\\it almost linear} in the problem's dimensionality.\\n\",\n",
       " \"  Lower bounds on the smallest eigenvalue of a symmetric positive definite\\nmatrices $A\\\\in\\\\mathbb{R}^{m\\\\times m}$ play an important role in condition\\nnumber estimation and in iterative methods for singular value computation. In\\nparticular, the bounds based on ${\\\\rm Tr}(A^{-1})$ and ${\\\\rm Tr}(A^{-2})$\\nattract attention recently because they can be computed in $O(m)$ work when $A$\\nis tridiagonal. In this paper, we focus on these bounds and investigate their\\nproperties in detail. First, we consider the problem of finding the optimal\\nbound that can be computed solely from ${\\\\rm Tr}(A^{-1})$ and ${\\\\rm\\nTr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one\\nin terms of sharpness. Next, we study the gap between the Laguerre bound and\\nthe smallest eigenvalue. We characterize the situation in which the gap becomes\\nlargest in terms of the eigenvalue distribution of $A$ and show that the gap\\nbecomes smallest when ${\\\\rm Tr}(A^{-2})/\\\\{{\\\\rm Tr}(A^{-1})\\\\}^2$ approaches 1 or\\n$\\\\frac{1}{m}$. These results will be useful, for example, in designing\\nefficient shift strategies for singular value computation algorithms.\\n\",\n",
       " \"  This paper describes the development of a magnetic attitude control subsystem\\nfor a 2U cubesat. Due to the presence of gravity gradient torques, the\\nsatellite dynamics are open-loop unstable near the desired pointing\\nconfiguration. Nevertheless the linearized time-varying system is completely\\ncontrollable, under easily verifiable conditions, and the system's disturbance\\nrejection capabilities can be enhanced by adding air drag panels exemplifying a\\nbeneficial interplay between hardware design and control. In the paper,\\nconditions for the complete controllability for the case of a magnetically\\ncontrolled satellite with passive air drag panels are developed, and simulation\\ncase studies with the LQR and MPC control designs applied in combination with a\\nnonlinear time-varying input transformation are presented to demonstrate the\\nability of the closed-loop system to satisfy mission objectives despite\\ndisturbance torques.\\n\",\n",
       " '  One advantage of decision tree based methods like random forests is their\\nability to natively handle categorical predictors without having to first\\ntransform them (e.g., by using feature engineering techniques). However, in\\nthis paper, we show how this capability can lead to an inherent \"absent levels\"\\nproblem for decision tree based methods that has never been thoroughly\\ndiscussed, and whose consequences have never been carefully explored. This\\nproblem occurs whenever there is an indeterminacy over how to handle an\\nobservation that has reached a categorical split which was determined when the\\nobservation in question\\'s level was absent during training. Although these\\nincidents may appear to be innocuous, by using Leo Breiman and Adele Cutler\\'s\\nrandom forests FORTRAN code and the randomForest R package (Liaw and Wiener,\\n2002) as motivating case studies, we examine how overlooking the absent levels\\nproblem can systematically bias a model. Furthermore, by using three real data\\nexamples, we illustrate how absent levels can dramatically alter a model\\'s\\nperformance in practice, and we empirically demonstrate how some simple\\nheuristics can be used to help mitigate the effects of the absent levels\\nproblem until a more robust theoretical solution is found.\\n',\n",
       " '  For decades, conventional computers based on the von Neumann architecture\\nhave performed computation by repeatedly transferring data between their\\nprocessing and their memory units, which are physically separated. As\\ncomputation becomes increasingly data-centric and as the scalability limits in\\nterms of performance and power are being reached, alternative computing\\nparadigms are searched for in which computation and storage are collocated. A\\nfascinating new approach is that of computational memory where the physics of\\nnanoscale memory devices are used to perform certain computational tasks within\\nthe memory unit in a non-von Neumann manner. Here we present a large-scale\\nexperimental demonstration using one million phase-change memory devices\\norganized to perform a high-level computational primitive by exploiting the\\ncrystallization dynamics. Also presented is an application of such a\\ncomputational memory to process real-world data-sets. The results show that\\nthis co-existence of computation and storage at the nanometer scale could be\\nthe enabler for new, ultra-dense, low power, and massively parallel computing\\nsystems.\\n',\n",
       " '  We generalise some well-known graph parameters to operator systems by\\nconsidering their underlying quantum channels. In particular, we introduce the\\nquantum complexity as the dimension of the smallest co-domain Hilbert space a\\nquantum channel requires to realise a given operator system as its\\nnon-commutative confusability graph. We describe quantum complexity as a\\ngeneralised minimum semidefinite rank and, in the case of a graph operator\\nsystem, as a quantum intersection number. The quantum complexity and a closely\\nrelated quantum version of orthogonal rank turn out to be upper bounds for the\\nShannon zero-error capacity of a quantum channel, and we construct examples for\\nwhich these bounds beat the best previously known general upper bound for the\\ncapacity of quantum channels, given by the quantum Lovász theta number.\\n',\n",
       " '  During the ionization of atoms irradiated by linearly polarized intense laser\\nfields, we find for the first time that the transverse momentum distribution of\\nphotoelectrons can be well fitted by a squared zeroth-order Bessel function\\nbecause of the quantum interference effect of Glory rescattering. The\\ncharacteristic of the Bessel function is determined by the common angular\\nmomentum of a bunch of semiclassical paths termed as Glory trajectories, which\\nare launched with different nonzero initial transverse momenta distributed on a\\nspecific circle in the momentum plane and finally deflected to the same\\nasymptotic momentum, which is along the polarization direction, through\\npost-tunneling rescattering. Glory rescattering theory (GRT) based on the\\nsemiclassical path-integral formalism is developed to address this effect\\nquantitatively. Our theory can resolve the long-standing discrepancies between\\nexisting theories and experiments on the fringe location, predict the sudden\\ntransition of the fringe structure in holographic patterns, and shed light on\\nthe quantum interference aspects of low-energy structures in strong-field\\natomic ionization.\\n',\n",
       " '  We are interested in extending operators defined on positive measures, called\\nhere transfunctions, to signed measures and vector measures. Our methods use a\\nsomewhat nonstandard approach to measures and vector measures. The necessary\\nbackground, including proofs of some auxiliary results, is included.\\n',\n",
       " \"  Convolutional Neural Networks (CNNs) can learn effective features, though\\nhave been shown to suffer from a performance drop when the distribution of the\\ndata changes from training to test data. In this paper we analyze the internal\\nrepresentations of CNNs and observe that the representations of unseen data in\\neach class, spread more (with higher variance) in the embedding space of the\\nCNN compared to representations of the training data. More importantly, this\\ndifference is more extreme if the unseen data comes from a shifted\\ndistribution. Based on this observation, we objectively evaluate the degree of\\nrepresentation's variance in each class via eigenvalue decomposition on the\\nwithin-class covariance of the internal representations of CNNs and observe the\\nsame behaviour. This can be problematic as larger variances might lead to\\nmis-classification if the sample crosses the decision boundary of its class. We\\napply nearest neighbor classification on the representations and empirically\\nshow that the embeddings with the high variance actually have significantly\\nworse KNN classification performances, although this could not be foreseen from\\ntheir end-to-end classification results. To tackle this problem, we propose\\nDeep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that\\nsignificantly reduces the within-class covariance of a DNN's representation,\\nimproving performance on unseen test data from a shifted distribution. We\\nempirically evaluate DWCCA on two datasets for Acoustic Scene Classification\\n(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA\\nsignificantly improve the network's internal representation, it also increases\\nthe end-to-end classification accuracy, especially when the test set exhibits a\\ndistribution shift. By adding DWCCA to a VGG network, we achieve around 6\\npercentage points improvement in the case of a distribution mismatch.\\n\",\n",
       " '  We present an efficient second-order algorithm with\\n$\\\\tilde{O}(\\\\frac{1}{\\\\eta}\\\\sqrt{T})$ regret for the bandit online multiclass\\nproblem. The regret bound holds simultaneously with respect to a family of loss\\nfunctions parameterized by $\\\\eta$, for a range of $\\\\eta$ restricted by the norm\\nof the competitor. The family of loss functions ranges from hinge loss\\n($\\\\eta=0$) to squared hinge loss ($\\\\eta=1$). This provides a solution to the\\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\\n$\\\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\\nalgorithm experimentally, showing that it also performs favorably against\\nearlier algorithms.\\n',\n",
       " '  Swarm systems constitute a challenging problem for reinforcement learning\\n(RL) as the algorithm needs to learn decentralized control policies that can\\ncope with limited local sensing and communication abilities of the agents.\\nWhile it is often difficult to directly define the behavior of the agents,\\nsimple communication protocols can be defined more easily using prior knowledge\\nabout the given task. In this paper, we propose a number of simple\\ncommunication protocols that can be exploited by deep reinforcement learning to\\nfind decentralized control policies in a multi-robot swarm environment. The\\nprotocols are based on histograms that encode the local neighborhood relations\\nof the agents and can also transmit task-specific information, such as the\\nshortest distance and direction to a desired target. In our framework, we use\\nan adaptation of Trust Region Policy Optimization to learn complex\\ncollaborative tasks, such as formation building and building a communication\\nlink. We evaluate our findings in a simulated 2D-physics environment, and\\ncompare the implications of different communication protocols.\\n',\n",
       " '  We describe the design and implementation of an extremely scalable real-time\\nRFI mitigation method, based on the offline AOFlagger. All algorithms scale\\nlinearly in the number of samples. We describe how we implemented the flagger\\nin the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we\\nintroduce a novel simple history-based flagger that helps reduce the impact of\\nour small window on the data.\\nBy examining an observation of a known pulsar, we demonstrate that our\\nflagger can achieve much higher quality than a simple thresholder, even when\\nrunning in real time, on a distributed system. The flagger works on visibility\\ndata, but also on raw voltages, and beam formed data. The algorithms are\\nscale-invariant, and work on microsecond to second time scales. We are\\ncurrently implementing a prototype for the time domain pipeline of the SKA\\ncentral signal processor.\\n',\n",
       " '  Controlling embodied agents with many actuated degrees of freedom is a\\nchallenging task. We propose a method that can discover and interpolate between\\ncontext dependent high-level actions or body-affordances. These provide an\\nabstract, low-dimensional interface indexing high-dimensional and time-\\nextended action policies. Our method is related to recent ap- proaches in the\\nmachine learning literature but is conceptually simpler and easier to\\nimplement. More specifically our method requires the choice of a n-dimensional\\ntarget sensor space that is endowed with a distance metric. The method then\\nlearns an also n-dimensional embedding of possibly reactive body-affordances\\nthat spread as far as possible throughout the target sensor space.\\n',\n",
       " '  Let $n >3$ and $ 0< k < \\\\frac{n}{2} $ be integers. In this paper, we\\ninvestigate some algebraic properties of the line graph of the graph $\\n{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$\\nwhich is induced by the set of vertices of weights $k$ and $k+1$. In the first\\nstep, we determine the automorphism groups of these graphs for all values of\\n$k$. In the second step, we study Cayley properties of the line graph of these\\ngraphs. In particular, we show that for $ k>2, $ if $ 2k+1 \\\\neq n$, then the\\nline graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley\\ngraph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a\\nCayley graph if and only if $ n$ is a power of a prime $p$.\\n',\n",
       " '  Recently software development companies started to embrace Machine Learning\\n(ML) techniques for introducing a series of advanced functionality in their\\nproducts such as personalisation of the user experience, improved search,\\ncontent recommendation and automation. The technical challenges for tackling\\nthese problems are heavily researched in literature. A less studied area is a\\npragmatic approach to the role of humans in a complex modern industrial\\nenvironment where ML based systems are developed. Key stakeholders affect the\\nsystem from inception and up to operation and maintenance. Product managers\\nwant to embed \"smart\" experiences for their users and drive the decisions on\\nwhat should be built next; software engineers are challenged to build or\\nutilise ML software tools that require skills that are well outside of their\\ncomfort zone; legal and risk departments may influence design choices and data\\naccess; operations teams are requested to maintain ML systems which are\\nnon-stationary in their nature and change behaviour over time; and finally ML\\npractitioners should communicate with all these stakeholders to successfully\\nbuild a reliable system. This paper discusses some of the challenges we faced\\nin Atlassian as we started investing more in the ML space.\\n',\n",
       " '  Generative Adversarial Networks (GANs) produce systematically better quality\\nsamples when class label information is provided., i.e. in the conditional GAN\\nsetup. This is still observed for the recently proposed Wasserstein GAN\\nformulation which stabilized adversarial training and allows considering high\\ncapacity network architectures such as ResNet. In this work we show how to\\nboost conditional GAN by augmenting available class labels. The new classes\\ncome from clustering in the representation space learned by the same GAN model.\\nThe proposed strategy is also feasible when no class information is available,\\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\\nunsupervised setup.\\n',\n",
       " '  We study the phase space dynamics of cosmological models in the theoretical\\nformulations of non-minimal metric-torsion couplings with a scalar field, and\\ninvestigate in particular the critical points which yield stable solutions\\nexhibiting cosmic acceleration driven by the {\\\\em dark energy}. The latter is\\ndefined in a way that it effectively has no direct interaction with the\\ncosmological fluid, although in an equivalent scalar-tensor cosmological setup\\nthe scalar field interacts with the fluid (which we consider to be the\\npressureless dust). Determining the conditions for the existence of the stable\\ncritical points we check their physical viability, in both Einstein and Jordan\\nframes. We also verify that in either of these frames, the evolution of the\\nuniverse at the corresponding stable points matches with that given by the\\nrespective exact solutions we have found in an earlier work (arXiv: 1611.00654\\n[gr-qc]). We not only examine the regions of physical relevance for the\\ntrajectories in the phase space when the coupling parameter is varied, but also\\ndemonstrate the evolution profiles of the cosmological parameters of interest\\nalong fiducial trajectories in the effectively non-interacting scenarios, in\\nboth Einstein and Jordan frames.\\n',\n",
       " '  In this work, we propose an end-to-end deep architecture that jointly learns\\nto detect obstacles and estimate their depth for MAV flight applications. Most\\nof the existing approaches either rely on Visual SLAM systems or on depth\\nestimation models to build 3D maps and detect obstacles. However, for the task\\nof avoiding obstacles this level of complexity is not required. Recent works\\nhave proposed multi task architectures to both perform scene understanding and\\ndepth estimation. We follow their track and propose a specific architecture to\\njointly estimate depth and obstacles, without the need to compute a global map,\\nbut maintaining compatibility with a global SLAM system if needed. The network\\narchitecture is devised to exploit the joint information of the obstacle\\ndetection task, that produces more reliable bounding boxes, with the depth\\nestimation one, increasing the robustness of both to scenario changes. We call\\nthis architecture J-MOD$^{2}$. We test the effectiveness of our approach with\\nexperiments on sequences with different appearance and focal lengths and\\ncompare it to SotA multi task methods that jointly perform semantic\\nsegmentation and depth estimation. In addition, we show the integration in a\\nfull system using a set of simulated navigation experiments where a MAV\\nexplores an unknown scenario and plans safe trajectories by using our detection\\nmodel.\\n',\n",
       " '  In this paper, we prove that there exists a dimensional constant $\\\\delta > 0$\\nsuch that given any background Kähler metric $\\\\omega$, the Calabi flow with\\ninitial data $u_0$ satisfying \\\\begin{equation*} \\\\partial \\\\bar \\\\partial u_0 \\\\in\\nL^\\\\infty (M) \\\\text{ and } (1- \\\\delta )\\\\omega < \\\\omega_{u_0} < (1+\\\\delta\\n)\\\\omega, \\\\end{equation*} admits a unique short time solution and it becomes\\nsmooth immediately, where $\\\\omega_{u_0} : = \\\\omega +\\\\sqrt{-1}\\\\partial\\n\\\\bar\\\\partial u_0$. The existence time depends on initial data $u_0$ and the\\nmetric $\\\\omega$. As a corollary, we get that Calabi flow has short time\\nexistence for any initial data satisfying \\\\begin{equation*} \\\\partial \\\\bar\\n\\\\partial u_0 \\\\in C^0(M) \\\\text{ and } \\\\omega_{u_0} > 0, \\\\end{equation*} which\\nshould be interpreted as a \"continuous Kähler metric\". A main technical\\ningredient is Schauder-type estimates for biharmonic heat equation on\\nRiemannian manifolds with time weighted Hölder norms.\\n',\n",
       " '  To probe the star-formation (SF) processes, we present results of an analysis\\nof the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency\\nobservations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.\\nAn almost horseshoe-like structure embedded within the MCG35.2 is evident in\\nthe infrared and millimeter images and harbors the previously known sites,\\nultra-compact/hyper-compact G35.20$-$0.74N H\\\\,{\\\\sc ii} region, Ap2-1, and\\nMercer 14 at its base. The site, Ap2-1 is found to be excited by a radio\\nspectral type of B0.5V star where the distribution of 20 cm and H$\\\\alpha$\\nemission is surrounded by the extended molecular hydrogen emission. Using the\\n{\\\\it Herschel} 160-500 $\\\\mu$m and photometric 1-24 $\\\\mu$m data analysis,\\nseveral embedded clumps and clusters of young stellar objects (YSOs) are\\ninvestigated within the MCG35.2, revealing the SF activities. Majority of the\\nYSOs clusters and massive clumps (500-4250 M$_{\\\\odot}$) are seen toward the\\nhorseshoe-like structure. The position-velocity analysis of $^{13}$CO emission\\nshows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km\\ns$^{-1}$) interconnected by lower intensity intermediated velocity emission,\\ntracing a broad bridge feature. The presence of such broad bridge feature\\nsuggests the onset of a collision between molecular components in the MCG35.2.\\nA noticeable change in the H-band starlight mean polarization angles has also\\nbeen observed in the MCG35.2, probably tracing the interaction between\\nmolecular components. Taken together, it seems that the cloud-cloud collision\\nprocess has influenced the birth of massive stars and YSOs clusters in the\\nMCG35.2.\\n',\n",
       " \"  We present novel oblivious routing algorithms for both splittable and\\nunsplittable multicommodity flow. Our algorithm for minimizing congestion for\\n\\\\emph{unsplittable} multicommodity flow is the first oblivious routing\\nalgorithm for this setting. As an intermediate step towards this algorithm, we\\npresent a novel generalization of Valiant's classical load balancing scheme for\\npacket-switched networks to arbitrary graphs, which is of independent interest.\\nOur algorithm for minimizing congestion for \\\\emph{splittable} multicommodity\\nflow improves upon the state-of-the-art, in terms of both running time and\\nperformance, for graphs that exhibit good expansion guarantees. Our algorithms\\nrely on diffusing traffic via iterative applications of the random walk\\noperator. Consequently, the performance guarantees of our algorithms are\\nderived from the convergence of the random walk operator to the stationary\\ndistribution and are expressed in terms of the spectral gap of the graph (which\\ndominates the mixing time).\\n\",\n",
       " '  We study functional graphs generated by quadratic polynomials over prime\\nfields. We introduce efficient algorithms for methodical computations and\\nprovide the values of various direct and cumulative statistical parameters of\\ninterest. These include: the number of connected functional graphs, the number\\nof graphs having a maximal cycle, the number of cycles of fixed size, the\\nnumber of components of fixed size, as well as the shape of trees extracted\\nfrom functional graphs. We particularly focus on connected functional graphs,\\nthat is, the graphs which contain only one component (and thus only one cycle).\\nBased on the results of our computations, we formulate several conjectures\\nhighlighting the similarities and differences between these functional graphs\\nand random mappings.\\n',\n",
       " \"  Helmholtz decomposition theorem for vector fields is usually presented with\\ntoo strong restrictions on the fields and only for time independent fields.\\nBlumenthal showed in 1905 that decomposition is possible for any asymptotically\\nweakly decreasing vector field. He used a regularization method in his proof\\nwhich can be extended to prove the theorem even for vector fields\\nasymptotically increasing sublinearly. Blumenthal's result is then applied to\\nthe time-dependent fields of the dipole radiation and an artificial sublinearly\\nincreasing field.\\n\",\n",
       " \"  We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy\\ndecomposition of the fibre $\\\\Omega^3S^{17}\\\\{2\\\\}$ of the $H$-space squaring map\\non the triple loop space of the $17$-sphere. This induces a splitting of the\\nmod-$2$ homotopy groups $\\\\pi_\\\\ast(S^{17}; \\\\mathbb{Z}/2\\\\mathbb{Z})$ in terms of\\nthe integral homotopy groups of the fibre of the double suspension\\n$E^2:S^{2n-1} \\\\to \\\\Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,\\nwho gave similar decompositions for $S^5$ and $S^9$. We relate these\\ndecompositions to various Whitehead products in the homotopy groups of mod-$2$\\nMoore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},\\ni_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$\\nis divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.\\n\",\n",
       " '  We show that certain orderable groups admit no isolated left orders. The\\ngroups we consider are cyclic amalgamations of a free group with a general\\norderable group, the HNN extensions of free groups over cyclic subgroups, and a\\nparticular class of one-relator groups. In order to prove the results about\\norders, we develop perturbation techniques for actions of these groups on the\\nline.\\n',\n",
       " '  Machine learning classifiers are known to be vulnerable to inputs maliciously\\nconstructed by adversaries to force misclassification. Such adversarial\\nexamples have been extensively studied in the context of computer vision\\napplications. In this work, we show adversarial attacks are also effective when\\ntargeting neural network policies in reinforcement learning. Specifically, we\\nshow existing adversarial example crafting techniques can be used to\\nsignificantly degrade test-time performance of trained policies. Our threat\\nmodel considers adversaries capable of introducing small perturbations to the\\nraw input of the policy. We characterize the degree of vulnerability across\\ntasks and training algorithms, for a subclass of adversarial-example attacks in\\nwhite-box and black-box settings. Regardless of the learned task or training\\nalgorithm, we observe a significant drop in performance, even with small\\nadversarial perturbations that do not interfere with human perception. Videos\\nare available at this http URL.\\n',\n",
       " '  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy\\noffer a unique way to constrain the shape of galactic gravitational potentials.\\nSuch streams can be used as leaning tower gravitational experiments on galactic\\nscales. The most well motivated modification of gravity proposed as an\\nalternative to dark matter on galactic scales is Milgromian dynamics (MOND),\\nand we present here the first ever N-body simulations of the dynamical\\nevolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a\\nrealistic baryonic mass model for the Milky Way, we attempt to reproduce the\\npresent-day spatial and kinematic structure of the Sagittarius dwarf and its\\nimmense tidal stream that wraps around the Milky Way. With very little freedom\\non the original structure of the progenitor, constrained by the total\\nluminosity of the Sagittarius structure and by the observed stellar mass-size\\nrelation for isolated dwarf galaxies, we find reasonable agreement between our\\nsimulations and observations of this system. The observed stellar velocities in\\nthe leading arm can be reproduced if we include a massive hot gas corona around\\nthe Milky Way that is flattened in the direction of the principal plane of its\\nsatellites. This is the first time that tidal dissolution in MOND has been\\ntested rigorously at these mass and acceleration scales.\\n',\n",
       " '  The response of an electron system to electromagnetic fields with sharp\\nspatial variations is strongly dependent on quantum electronic properties, even\\nin ambient conditions, but difficult to access experimentally. We use\\npropagating graphene plasmons, together with an engineered dielectric-metallic\\nenvironment, to probe the graphene electron liquid and unveil its detailed\\nelectronic response at short wavelengths.The near-field imaging experiments\\nreveal a parameter-free match with the full theoretical quantum description of\\nthe massless Dirac electron gas, in which we identify three types of quantum\\neffects as keys to understanding the experimental response of graphene to\\nshort-ranged terahertz electric fields. The first type is of single-particle\\nnature and is related to shape deformations of the Fermi surface during a\\nplasmon oscillations. The second and third types are a many-body effect\\ncontrolled by the inertia and compressibility of the interacting electron\\nliquid in graphene. We demonstrate how, in principle, our experimental approach\\ncan determine the full spatiotemporal response of an electron system.\\n',\n",
       " \"  A new generation of solar instruments provides improved spectral, spatial,\\nand temporal resolution, thus facilitating a better understanding of dynamic\\nprocesses on the Sun. High-resolution observations often reveal\\nmultiple-component spectral line profiles, e.g., in the near-infrared He I\\n10830 \\\\AA\\\\ triplet, which provides information about the chromospheric velocity\\nand magnetic fine structure. We observed an emerging flux region, including two\\nsmall pores and an arch filament system, on 2015 April 17 with the 'very fast\\nspectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the\\n1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We\\ndiscuss this method of obtaining fast (one per minute) spectral scans of the\\nsolar surface and its potential to follow dynamic processes on the Sun. We\\ndemonstrate the performance of the 'very fast spectroscopic mode' by tracking\\nchromospheric high-velocity features in the arch filament system.\\n\",\n",
       " '  In the past decade, the information security and threat landscape has grown\\nsignificantly making it difficult for a single defender to defend against all\\nattacks at the same time. This called for introduc- ing information sharing, a\\nparadigm in which threat indicators are shared in a community of trust to\\nfacilitate defenses. Standards for representation, exchange, and consumption of\\nindicators are pro- posed in the literature, although various issues are\\nundermined. In this paper, we rethink information sharing for actionable\\nintelli- gence, by highlighting various issues that deserve further explo-\\nration. We argue that information sharing can benefit from well- defined use\\nmodels, threat models, well-understood risk by mea- surement and robust\\nscoring, well-understood and preserved pri- vacy and quality of indicators and\\nrobust mechanism to avoid free riding behavior of selfish agent. We call for\\nusing the differential nature of data and community structures for optimizing\\nsharing.\\n',\n",
       " '  Permutation polynomials over finite fields have wide applications in many\\nareas of science and engineering. In this paper, we present six new classes of\\npermutation trinomials over $\\\\mathbb{F}_{2^n}$ which have explicit forms by\\ndetermining the solutions of some equations.\\n',\n",
       " '  Ben-David and Shelah proved that if $\\\\lambda$ is a singular strong-limit\\ncardinal and $2^\\\\lambda=\\\\lambda^+$, then $\\\\square^*_\\\\lambda$ entails the\\nexistence of a normal $\\\\lambda$-distributive $\\\\lambda^+$-Aronszajn tree. Here,\\nit is proved that the same conclusion remains valid after replacing the\\nhypothesis $\\\\square^*_\\\\lambda$ by $\\\\square(\\\\lambda^+,{<}\\\\lambda)$.\\nAs $\\\\square(\\\\lambda^+,{<}\\\\lambda)$ does not impose a bound on the order-type\\nof the witnessing clubs, our construction is necessarily different from that of\\nBen-David and Shelah, and instead uses walks on ordinals augmented with club\\nguessing.\\nA major component of this work is the study of postprocessing functions and\\ntheir effect on square sequences. A byproduct of this study is the finding that\\nfor $\\\\kappa$ regular uncountable, $\\\\square(\\\\kappa)$ entails the existence of a\\npartition of $\\\\kappa$ into $\\\\kappa$ many fat sets. When contrasted with a\\nclassic model of Magidor, this shows that it is equiconsistent with the\\nexistence of a weakly compact cardinal that $\\\\omega_2$ cannot be split into two\\nfat sets.\\n',\n",
       " '  The real Scarf II potential is discussed as a radial problem. This potential\\nhas been studied extensively as a one-dimensional problem, and now these\\nresults are used to construct its bound and resonance solutions for $l=0$ by\\nsetting the origin at some arbitrary value of the coordinate. The solutions\\nwith appropriate boundary conditions are composed as the linear combination of\\nthe two independent solutions of the Schrödinger equation. The asymptotic\\nexpression of these solutions is used to construct the $S_0(k)$ s-wave\\n$S$-matrix, the poles of which supply the $k$ values corresponding to the\\nbound, resonance and anti-bound solutions. The location of the discrete energy\\neigenvalues is analyzed, and the relation of the solutions of the radial and\\none-dimensional Scarf II potentials is discussed. It is shown that the\\ngeneralized Woods--Saxon potential can be generated from the Rosen--Morse II\\npotential in the same way as the radial Scarf II potential is obtained from its\\none-dimensional correspondent. Based on this analogy, possible applications are\\nalso pointed out.\\n',\n",
       " '  This paper presents a novel model for multimodal learning based on gated\\nneural networks. The Gated Multimodal Unit (GMU) model is intended to be used\\nas an internal unit in a neural network architecture whose purpose is to find\\nan intermediate representation based on a combination of data from different\\nmodalities. The GMU learns to decide how modalities influence the activation of\\nthe unit using multiplicative gates. It was evaluated on a multilabel scenario\\nfor genre classification of movies using the plot and the poster. The GMU\\nimproved the macro f-score performance of single-modality approaches and\\noutperformed other fusion strategies, including mixture of experts models.\\nAlong with this work, the MM-IMDb dataset is released which, to the best of our\\nknowledge, is the largest publicly available multimodal dataset for genre\\nprediction on movies.\\n',\n",
       " '  In a single winner election with several candidates and ranked choice or\\nrating scale ballots, a Condorcet winner is one who wins all their two way\\nraces by majority rule or MR. A voting system has Condorcet consistency or CC\\nif it names any Condorcet winner the winner. Many voting systems lack CC, but a\\nthree step line of reasoning is used here to show why it is necessary. In step\\n1 we show that we can dismiss all the electoral criteria which conflict with\\nCC. In step 2 we point out that CC follows almost automatically if we can agree\\nthat MR is the only acceptable system for elections with two candidates. In\\nstep 3 we make that argument for MR. This argument itself has three parts.\\nFirst, in races with two candidates, the only well known alternatives to MR can\\nsometimes name as winner a candidate who is preferred over their opponent by\\nonly one voter, with all others preferring the opponent. That is unacceptable.\\nSecond, those same systems are also extremely susceptible to strategic\\ninsincere voting. Third, in simulation studies using spatial models with two\\ncandidates, the best known alternative to MR picks the best or most centrist\\ncandidate significantly less often than MR does.\\n',\n",
       " '  In the framework of the Einstein-Maxwell-aether theory we study the\\nbirefringence effect, which can occur in the pp-wave symmetric dynamic aether.\\nThe dynamic aether is considered to be latently birefringent quasi-medium,\\nwhich displays this hidden property if and only if the aether motion is\\nnon-uniform, i.e., when the aether flow is characterized by the non-vanishing\\nexpansion, shear, vorticity or acceleration. In accordance with the\\ndynamo-optical scheme of description of the interaction between electromagnetic\\nwaves and the dynamic aether, we shall model the susceptibility tensors by the\\nterms linear in the covariant derivative of the aether velocity four-vector.\\nWhen the pp-wave modes appear in the dynamic aether, we deal with a\\ngravitationally induced degeneracy removal with respect to hidden\\nsusceptibility parameters. As a consequence, the phase velocities of\\nelectromagnetic waves possessing orthogonal polarizations do not coincide, thus\\ndisplaying the birefringence effect. Two electromagnetic field configurations\\nare studied in detail: longitudinal and transversal with respect to the aether\\npp-wave front. For both cases the solutions are found, which reveal anomalies\\nin the electromagnetic response on the action of the pp-wave aether mode.\\n',\n",
       " \"  The $p$-set, which is in a simple analytic form, is well distributed in unit\\ncubes. The well-known Weil's exponential sum theorem presents an upper bound of\\nthe exponential sum over the $p$-set. Based on the result, one shows that the\\n$p$-set performs well in numerical integration, in compressed sensing as well\\nas in UQ. However, $p$-set is somewhat rigid since the cardinality of the\\n$p$-set is a prime $p$ and the set only depends on the prime number $p$. The\\npurpose of this paper is to present generalizations of $p$-sets, say\\n$\\\\mathcal{P}_{d,p}^{{\\\\mathbf a},\\\\epsilon}$, which is more flexible.\\nParticularly, when a prime number $p$ is given, we have many different choices\\nof the new $p$-sets. Under the assumption that Goldbach conjecture holds, for\\nany even number $m$, we present a point set, say ${\\\\mathcal L}_{p,q}$, with\\ncardinality $m-1$ by combining two different new $p$-sets, which overcomes a\\nmajor bottleneck of the $p$-set. We also present the upper bounds of the\\nexponential sums over $\\\\mathcal{P}_{d,p}^{{\\\\mathbf a},\\\\epsilon}$ and ${\\\\mathcal\\nL}_{p,q}$, which imply these sets have many potential applications.\\n\",\n",
       " '  This paper presents the design and implementation of a Human Interface for a\\nhousekeeper robot. It bases on the idea of making the robot understand the\\nhuman needs without making the human go through the details of robots work, for\\nexample, the way that the robot implements the work or the method that the\\nrobot uses to plan the path in order to reach the work area. The interface\\ncommands based on idioms of the natural human language and designed in a manner\\nthat the user gives the robot several commands with their execution date/time.\\n',\n",
       " '  This work proposes a new algorithm for training a re-weighted L2 Support\\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès\\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\\nparticular, the margin required for each training vector is set independently,\\ndefining a new weighted SVM model. These weights are selected to be binary, and\\nthey are automatically adapted during the training of the model, resulting in a\\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\\ncomputational complexity as the original algorithm. As shown experimentally,\\nthis algorithm is computationally cheaper to apply since it requires less\\niterations to converge, and it produces models with a sparser representation in\\nterms of support vectors and which are more stable with respect to the\\nselection of the regularization hyper-parameter.\\n',\n",
       " '  Learning-based approaches to robotic manipulation are limited by the\\nscalability of data collection and accessibility of labels. In this paper, we\\npresent a multi-task domain adaptation framework for instance grasping in\\ncluttered scenes by utilizing simulated robot experiments. Our neural network\\ntakes monocular RGB images and the instance segmentation mask of a specified\\ntarget object as inputs, and predicts the probability of successfully grasping\\nthe specified object for each candidate motor command. The proposed transfer\\nlearning framework trains a model for instance grasping in simulation and uses\\na domain-adversarial loss to transfer the trained model to real robots using\\nindiscriminate grasping data, which is available both in simulation and the\\nreal world. We evaluate our model in real-world robot experiments, comparing it\\nwith alternative model architectures as well as an indiscriminate grasping\\nbaseline.\\n',\n",
       " \"  A 1-ended finitely presented group has semistable fundamental group at\\n$\\\\infty$ if it acts geometrically on some (equivalently any) simply connected\\nand locally finite complex $X$ with the property that any two proper rays in\\n$X$ are properly homotopic. If $G$ has semistable fundamental group at $\\\\infty$\\nthen one can unambiguously define the fundamental group at $\\\\infty$ for $G$.\\nThe problem, asking if all finitely presented groups have semistable\\nfundamental group at $\\\\infty$ has been studied for over 40 years. If $G$ is an\\nascending HNN extension of a finitely presented group then indeed, $G$ has\\nsemistable fundamental group at $\\\\infty$, but since the early 1980's it has\\nbeen suggested that the finitely presented groups that are ascending HNN\\nextensions of {\\\\it finitely generated} groups may include a group with\\nnon-semistable fundamental group at $\\\\infty$. Ascending HNN extensions\\nnaturally break into two classes, those with bounded depth and those with\\nunbounded depth. Our main theorem shows that bounded depth finitely presented\\nascending HNN extensions of finitely generated groups have semistable\\nfundamental group at $\\\\infty$. Semistability is equivalent to two weaker\\nasymptotic conditions on the group holding simultaneously. We show one of these\\nconditions holds for all ascending HNN extensions, regardless of depth. We give\\na technique for constructing ascending HNN extensions with unbounded depth.\\nThis work focuses attention on a class of groups that may contain a group with\\nnon-semistable fundamental group at $\\\\infty$.\\n\",\n",
       " '  Developing and testing algorithms for autonomous vehicles in real world is an\\nexpensive and time consuming process. Also, in order to utilize recent advances\\nin machine intelligence and deep learning we need to collect a large amount of\\nannotated training data in a variety of conditions and environments. We present\\na new simulator built on Unreal Engine that offers physically and visually\\nrealistic simulations for both of these goals. Our simulator includes a physics\\nengine that can operate at a high frequency for real-time hardware-in-the-loop\\n(HITL) simulations with support for popular protocols (e.g. MavLink). The\\nsimulator is designed from the ground up to be extensible to accommodate new\\ntypes of vehicles, hardware platforms and software protocols. In addition, the\\nmodular design enables various components to be easily usable independently in\\nother projects. We demonstrate the simulator by first implementing a quadrotor\\nas an autonomous vehicle and then experimentally comparing the software\\ncomponents with real-world flights.\\n',\n",
       " '  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power\\nseries. The associated metric and Hausdorff dimension function give rise to the\\nHausdorff spectrum, which consists of the Hausdorff dimensions of closed\\nsubgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff\\ndimension function is well understood; in particular, the Hausdorff spectrum\\nconsists of finitely many rational numbers closely linked to the analytic\\ndimensions of subgroups of $G$.\\nConversely, it is a long-standing open question whether the finiteness of the\\nHausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the\\nanswer is yes, in a strong sense, under the extra condition that $G$ is\\nsoluble.\\nFurthermore, we explore the problem and related questions also for other\\nfiltration series, such as the lower $p$-series, the Frattini series, the\\nmodular dimension subgroup series and quite general filtration series. For\\ninstance, we prove, for odd primes $p$, that every countably based pro-$p$\\ngroup $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers\\nadmits a filtration series such that the corresponding Hausdorff spectrum\\ncontains an infinite real interval.\\n',\n",
       " \"  Brain-Machine Interaction (BMI) system motivates interesting and promising\\nresults in forward/feedback control consistent with human intention. It holds\\ngreat promise for advancements in patient care and applications to\\nneurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic\\nplatform using a personalized social robot in order to assist patients having\\ncognitive deficits through bilateral rehabilitation and mental training. For\\ninitial testing of the platform, electroencephalography (EEG) brainwaves of a\\nhuman user were collected in real time during tasks of imaginary movements.\\nFirst, the brainwaves associated with imagined body kinematics parameters were\\ndecoded to control a cursor on a computer screen in training protocol. Then,\\nthe experienced subject was able to interact with a social robot via our\\nreal-time BMI robotic platform. Corresponding to subject's imagery performance,\\nhe/she received specific gesture movements and eye color changes as\\nneural-based feedback from the robot. This hands-free neurofeedback interaction\\nnot only can be used for mind control of a social robot's movements, but also\\nsets the stage for application to enhancing and recovering mental abilities\\nsuch as attention via training in humans by providing real-time neurofeedback\\nfrom a social robot.\\n\",\n",
       " '  Road networks in cities are massive and is a critical component of mobility.\\nFast response to defects, that can occur not only due to regular wear and tear\\nbut also because of extreme events like storms, is essential. Hence there is a\\nneed for an automated system that is quick, scalable and cost-effective for\\ngathering information about defects. We propose a system for city-scale road\\naudit, using some of the most recent developments in deep learning and semantic\\nsegmentation. For building and benchmarking the system, we curated a dataset\\nwhich has annotations required for road defects. However, many of the labels\\nrequired for road audit have high ambiguity which we overcome by proposing a\\nlabel hierarchy. We also propose a multi-step deep learning model that segments\\nthe road, subdivide the road further into defects, tags the frame for each\\ndefect and finally localizes the defects on a map gathered using GPS. We\\nanalyze and evaluate the models on image tagging as well as segmentation at\\ndifferent levels of the label hierarchy.\\n',\n",
       " \"  In this Letter, we study the motion and wake-patterns of freely rising and\\nfalling cylinders in quiescent fluid. We show that the amplitude of oscillation\\nand the overall system-dynamics are intricately linked to two parameters: the\\nparticle's mass-density relative to the fluid $m^* \\\\equiv \\\\rho_p/\\\\rho_f$ and\\nits relative moment-of-inertia $I^* \\\\equiv {I}_p/{I}_f$. This supersedes the\\ncurrent understanding that a critical mass density ($m^*\\\\approx$ 0.54) alone\\ntriggers the sudden onset of vigorous vibrations. Using over 144 combinations\\nof ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering\\nvery heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire\\ndata collapses into two scaling regimes demarcated by a transitional Strouhal\\nnumber, $St_t \\\\approx 0.17$. $St_t$ separates a mass-dominated regime from a\\nregime dominated by the particle's moment of inertia. A shift from one regime\\nto the other also marks a gradual transition in the wake-shedding pattern: from\\nthe classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.\\nThus, auto-rotation can have a significant influence on the trajectories and\\nwakes of freely rising isotropic bodies.\\n\",\n",
       " \"  Theory of Mind is the ability to attribute mental states (beliefs, intents,\\nknowledge, perspectives, etc.) to others and recognize that these mental states\\nmay differ from one's own. Theory of Mind is critical to effective\\ncommunication and to teams demonstrating higher collective performance. To\\neffectively leverage the progress in Artificial Intelligence (AI) to make our\\nlives more productive, it is important for humans and AI to work well together\\nin a team. Traditionally, there has been much emphasis on research to make AI\\nmore accurate, and (to a lesser extent) on having it better understand human\\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\\nmore human-like and having it develop a theory of our minds. In this work, we\\nargue that for human-AI teams to be effective, humans must also develop a\\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\\nand quirks. We instantiate these ideas within the domain of Visual Question\\nAnswering (VQA). We find that using just a few examples (50), lay people can be\\ntrained to better predict responses and oncoming failures of a complex VQA\\nmodel. We further evaluate the role existing explanation (or interpretability)\\nmodalities play in helping humans build ToAIM. Explainable AI has received\\nconsiderable scientific and popular attention in recent times. Surprisingly, we\\nfind that having access to the model's internal states - its confidence in its\\ntop-k predictions, explicit or implicit attention maps which highlight regions\\nin the image (and words in the question) the model is looking at (and listening\\nto) while answering a question about an image - do not help people better\\npredict its behavior.\\n\",\n",
       " '  We study families of varieties endowed with polarized canonical eigensystems\\nof several maps, inducing canonical heights on the dominating variety as well\\nas on the \"good\" fibers of the family. We show explicitely the dependence on\\nthe parameter for global and local canonical heights defined by Kawaguchi when\\nthe fibers change, extending previous works of J. Silverman and others.\\nFinally, fixing an absolute value $v \\\\in K$ and a variety $V/K$, we descript\\nthe Kawaguchi`s canonical local height $\\\\hat{\\\\lambda}_{V,E,\\\\mathcal{Q},}(.,v)$\\nas an intersection number, provided that the polarized system $(V,\\\\mathcal{Q})$\\nhas a certain weak Néron model over Spec$(\\\\mathcal{O}_v)$ to be defined and\\nunder some conditions depending on the special fiber. With this we extend\\nNéron\\'s work strengthening Silverman\\'s results, which were for systems\\nhaving only one map.\\n',\n",
       " '  Large-scale extragalactic magnetic fields may induce conversions between\\nvery-high-energy photons and axionlike particles (ALPs), thereby shielding the\\nphotons from absorption on the extragalactic background light. However, in\\nsimplified \"cell\" models, used so far to represent extragalactic magnetic\\nfields, this mechanism would be strongly suppressed by current astrophysical\\nbounds. Here we consider a recent model of extragalactic magnetic fields\\nobtained from large-scale cosmological simulations. Such simulated magnetic\\nfields would have large enhancement in the filaments of matter. As a result,\\nphoton-ALP conversions would produce a significant spectral hardening for\\ncosmic TeV photons. This effect would be probed with the upcoming Cherenkov\\nTelescope Array detector. This possible detection would give a unique chance to\\nperform a tomography of the magnetized cosmic web with ALPs.\\n',\n",
       " '  Predicting the future state of a system has always been a natural motivation\\nfor science and practical applications. Such a topic, beyond its obvious\\ntechnical and societal relevance, is also interesting from a conceptual point\\nof view. This owes to the fact that forecasting lends itself to two equally\\nradical, yet opposite methodologies. A reductionist one, based on the first\\nprinciples, and the naive inductivist one, based only on data. This latter view\\nhas recently gained some attention in response to the availability of\\nunprecedented amounts of data and increasingly sophisticated algorithmic\\nanalytic techniques. The purpose of this note is to assess critically the role\\nof big data in reshaping the key aspects of forecasting and in particular the\\nclaim that bigger data leads to better predictions. Drawing on the\\nrepresentative example of weather forecasts we argue that this is not generally\\nthe case. We conclude by suggesting that a clever and context-dependent\\ncompromise between modelling and quantitative analysis stands out as the best\\nforecasting strategy, as anticipated nearly a century ago by Richardson and von\\nNeumann.\\n',\n",
       " '  We show that for an elliptic curve E defined over a number field K, the group\\nE(A) of points of E over the adele ring A of K is a topological group that can\\nbe analyzed in terms of the Galois representation associated to the torsion\\npoints of E. An explicit description of E(A) is given, and we prove that for K\\nof degree n, almost all elliptic curves over K have an adelic point group\\ntopologically isomorphic to a universal group depending on n. We also show that\\nthere exist infinitely many elliptic curves over K having a different adelic\\npoint group.\\n',\n",
       " '  Wireless backhaul communication has been recently realized with large\\nantennas operating in the millimeter wave (mmWave) frequency band and\\nimplementing highly directional beamforming. In this paper, we focus on the\\nalignment problem of narrow beams between fixed position network nodes in\\nmmWave backhaul systems that are subject to small displacements due to wind\\nflow or ground vibration. We consider nodes equipped with antenna arrays that\\nare capable of performing only analog processing and communicate through\\nwireless channels including a line-of-sight component. Aiming at minimizing the\\ntime needed to achieve beam alignment, we present an efficient method that\\ncapitalizes on the exchange of position information between the nodes to design\\ntheir beamforming and combining vectors. Some numerical results on the outage\\nprobability with the proposed beam alignment method offer useful preliminary\\ninsights on the impact of some system and operation parameters.\\n',\n",
       " '  Feature engineering has been the key to the success of many prediction\\nmodels. However, the process is non-trivial and often requires manual feature\\nengineering or exhaustive searching. DNNs are able to automatically learn\\nfeature interactions; however, they generate all the interactions implicitly,\\nand are not necessarily efficient in learning all types of cross features. In\\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\\nof a DNN model, and beyond that, it introduces a novel cross network that is\\nmore efficient in learning certain bounded-degree feature interactions. In\\nparticular, DCN explicitly applies feature crossing at each layer, requires no\\nmanual feature engineering, and adds negligible extra complexity to the DNN\\nmodel. Our experimental results have demonstrated its superiority over the\\nstate-of-art algorithms on the CTR prediction dataset and dense classification\\ndataset, in terms of both model accuracy and memory usage.\\n',\n",
       " '  We investigate the spin structure of a uni-axial chiral magnet near the\\ntransition temperatures in low fields perpendicular to the helical axis. We\\nfind a fan-type modulation structure where the clockwise and counterclockwise\\nwindings appear alternatively along the propagation direction of the modulation\\nstructure. This structure is often realized in a Yoshimori-type (non-chiral)\\nhelimagnet but it is rarely realized in a chiral helimagnet. To discuss\\nunderlying physics of this structure, we reconsider the phase diagram (phase\\nboundary and crossover lines) through the free energy and asymptotic behaviors\\nof isolated solitons. The fan structure appears slightly below the phase\\nboundary of the continuous transition of instability-type. In this region,\\nthere are no solutions containing any types of isolated solitons to the mean\\nfield equations.\\n',\n",
       " '  Several natural satellites of the giant planets have shown evidence of a\\nglobal internal ocean, coated by a thin, icy crust. This crust is probably\\nviscoelastic, which would alter its rotational response. This response would\\ntranslate into several rotational quantities, i.e. the obliquity, and the\\nlibrations at different frequencies, for which the crustal elasticity reacts\\ndifferently. This study aims at modelling the global response of the\\nviscoelastic crust. For that, I derive the time-dependency of the tensor of\\ninertia, which I combine with the time evolution of the rotational quantities,\\nthanks to an iterative algorithm. This algorithm combines numerical simulations\\nof the rotation with a digital filtering of the resulting tensor of inertia.\\nThe algorithm works very well in the elastic case, provided the problem is not\\nresonant. However, considering tidal dissipation adds different phase lags to\\nthe oscillating contributions, which challenge the convergence of the\\nalgorithm.\\n',\n",
       " '  A number of fundamental quantities in statistical signal processing and\\ninformation theory can be expressed as integral functions of two probability\\ndensity functions. Such quantities are called density functionals as they map\\ndensity functions onto the real line. For example, information divergence\\nfunctions measure the dissimilarity between two probability density functions\\nand are useful in a number of applications. Typically, estimating these\\nquantities requires complete knowledge of the underlying distribution followed\\nby multi-dimensional integration. Existing methods make parametric assumptions\\nabout the data distribution or use non-parametric density estimation followed\\nby high-dimensional integration. In this paper, we propose a new alternative.\\nWe introduce the concept of \"data-driven basis functions\" - functions of\\ndistributions whose value we can estimate given only samples from the\\nunderlying distributions without requiring distribution fitting or direct\\nintegration. We derive a new data-driven complete basis that is similar to the\\ndeterministic Bernstein polynomial basis and develop two methods for performing\\nbasis expansions of functionals of two distributions. We also show that the new\\nbasis set allows us to approximate functions of distributions as closely as\\ndesired. Finally, we evaluate the methodology by developing data driven\\nestimators for the Kullback-Leibler divergences and the Hellinger distance and\\nby constructing empirical estimates of tight bounds on the Bayes error rate.\\n',\n",
       " '  Let $E/\\\\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.\\nLet $p$ be a prime of ordinary reduction. We experimentally study conjecture\\n$4$ of B. Mazur and J. Tate in his article \"Refined Conjectures of the Birch\\nand Swinnerton-Dyer Type\". We report the computational evidence.\\n',\n",
       " '  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in\\nbase~$2$ (the Hamming weight of the binary expansion of $n$) and states the\\nfollowing: assume that $k$ is a positive integer and $1\\\\leq t<2^k-1$. Then\\n\\\\[\\\\Bigl \\\\lvert\\\\Bigl\\\\{(a,b)\\\\in\\\\bigl\\\\{0,\\\\ldots,2^k-2\\\\bigr\\\\}^2:a+b\\\\equiv t\\\\bmod\\n2^k-1, w(a)+w(b)<k\\\\Bigr\\\\}\\\\Bigr \\\\rvert\\\\leq 2^{k-1}.\\\\]\\nWe prove that the Tu--Deng Conjecture holds almost surely in the following\\nsense: the proportion of $t\\\\in[1,2^k-2]$ such that the above inequality holds\\napproaches $1$ as $k\\\\rightarrow\\\\infty$.\\nMoreover, we prove that the Tu--Deng Conjecture implies a conjecture due to\\nT.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.\\n',\n",
       " '  In this paper, we made an extension to the convergence analysis of the\\ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into\\nconsideration two popular regularization terms: the $\\\\ell_1$ and $\\\\ell_2$ norm\\nof the parameter vector $w$, and added it to the square loss function with\\ncoefficient $\\\\lambda/2$. We proved that when $\\\\lambda$ is small, the weight\\nvector $w$ converges to the optimal solution $\\\\hat{w}$ (with respect to the new\\nloss function) with probability $\\\\geq (1-\\\\varepsilon)(1-A_d)/2$ under random\\ninitiations in a sphere centered at the origin, where $\\\\varepsilon$ is a small\\nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams\\nand repeated simulations verified our theory.\\n',\n",
       " '  We further progress along the line of Ref. [Phys. Rev. {\\\\bf A 94}, 043614\\n(2016)] where a functional for Fermi systems with anomalously large $s$-wave\\nscattering length $a_s$ was proposed that has no free parameters. The\\nfunctional is designed to correctly reproduce the unitary limit in Fermi gases\\ntogether with the leading-order contributions in the s- and p-wave channels at\\nlow density. The functional is shown to be predictive up to densities\\n$\\\\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang\\nfunctional, valid for $\\\\rho < 10^{-6}$ fm$^{-3}$. The form of the functional\\nretained in this work is further motivated. It is shown that the new functional\\ncorresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all\\norders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One\\nconclusion from the present work is that, except in the extremely low--density\\nregime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with\\nrespect to the unitary limit. Starting from the functional, we introduce\\ndensity--dependent scales and show that scales associated to the bare\\ninteraction are strongly renormalized by medium effects. As a consequence, some\\nof the scales at play around saturation are dominated by the unitary gas\\nproperties and not directly to low-energy constants. For instance, we show that\\nthe scale in the s-wave channel around saturation is proportional to the\\nso-called Bertsch parameter $\\\\xi_0$ and becomes independent of $a_s$. We also\\npoint out that these scales are of the same order of magnitude than those\\nempirically obtained in the Skyrme energy density functional. We finally\\npropose a slight modification of the functional such that it becomes accurate\\nup to the saturation density $\\\\rho\\\\simeq 0.16$ fm$^{-3}$.\\n',\n",
       " '  Marker-based and marker-less optical skeletal motion-capture methods use an\\noutside-in arrangement of cameras placed around a scene, with viewpoints\\nconverging on the center. They often create discomfort by possibly needed\\nmarker suits, and their recording volume is severely restricted and often\\nconstrained to indoor scenes with controlled backgrounds. We therefore propose\\na new method for real-time, marker-less and egocentric motion capture which\\nestimates the full-body skeleton pose from a lightweight stereo pair of fisheye\\ncameras that are attached to a helmet or virtual-reality headset. It combines\\nthe strength of a new generative pose estimation framework for fisheye views\\nwith a ConvNet-based body-part detector trained on a new automatically\\nannotated and augmented dataset. Our inside-in method captures full-body motion\\nin general indoor and outdoor scenes, and also crowded scenes.\\n',\n",
       " '  Diffusion maps are an emerging data-driven technique for non-linear\\ndimensionality reduction, which are especially useful for the analysis of\\ncoherent structures and nonlinear embeddings of dynamical systems. However, the\\ncomputational complexity of the diffusion maps algorithm scales with the number\\nof observations. Thus, long time-series data presents a significant challenge\\nfor fast and efficient embedding. We propose integrating the Nyström method\\nwith diffusion maps in order to ease the computational demand. We achieve a\\nspeedup of roughly two to four times when approximating the dominant diffusion\\nmap components.\\n',\n",
       " \"  We present a set of effective outflow/open boundary conditions and an\\nassociated algorithm for simulating the dynamics of multiphase flows consisting\\nof $N$ ($N\\\\geqslant 2$) immiscible incompressible fluids in domains involving\\noutflows or open boundaries. These boundary conditions are devised based on the\\nproperties of energy stability and reduction consistency. The energy stability\\nproperty ensures that the contributions of these boundary conditions to the\\nenergy balance will not cause the total energy of the N-phase system to\\nincrease over time. Therefore, these open/outflow boundary conditions are very\\neffective in overcoming the backflow instability in multiphase systems. The\\nreduction consistency property ensures that if some fluid components are absent\\nfrom the N-phase system then these N-phase boundary conditions will reduce to\\nthose corresponding boundary conditions for the equivalent smaller system. Our\\nnumerical algorithm for the proposed boundary conditions together with the\\nN-phase governing equations involves only the solution of a set of de-coupled\\nindividual Helmholtz-type equations within each time step, and the resultant\\nlinear algebraic systems after discretization involve only constant and\\ntime-independent coefficient matrices which can be pre-computed. Therefore, the\\nalgorithm is computationally very efficient and attractive. We present\\nextensive numerical experiments for flow problems involving multiple fluid\\ncomponents and inflow/outflow boundaries to test the proposed method. In\\nparticular, we compare in detail the simulation results of a three-phase\\ncapillary wave problem with Prosperetti's exact physical solution and\\ndemonstrate that the method developed herein produces physically accurate\\nresults.\\n\",\n",
       " '  The recent detection of two faint and extended star clusters in the central\\nregions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,\\nraises the question of whether clusters with such low densities can survive the\\ntidal field of cold dark matter haloes with central density cusps. Using both\\nanalytic arguments and a suite of collisionless N-body simulations, I show that\\nthese clusters are extremely fragile and quickly disrupted in the presence of\\ncentral cusps $\\\\rho\\\\sim r^{-\\\\alpha}$ with $\\\\alpha\\\\gtrsim 0.2$. Furthermore, the\\nscenario in which the clusters where originally more massive and sank to the\\ncenter of the halo requires extreme fine tuning and does not naturally\\nreproduce the observed systems. In turn, these clusters are long lived in cored\\nhaloes, whose central regions are safe shelters for $\\\\alpha\\\\lesssim 0.2$. The\\nonly viable scenario for hosts that have preserved their primoridal cusp to the\\npresent time is that the clusters formed at rest at the bottom of the\\npotential, which is easily tested by measurement of the clusters proper\\nvelocity within the host. This offers means to readily probe the central\\ndensity profile of two dwarf galaxies as faint as $L_V\\\\sim5\\\\times 10^5 L_\\\\odot$\\nand $L_V\\\\sim6\\\\times10^4 L_\\\\odot$, in which stellar feedback is unlikely to be\\neffective.\\n',\n",
       " '  Fundamental relations between information and estimation have been\\nestablished in the literature for the continuous-time Gaussian and Poisson\\nchannels, in a long line of work starting from the classical representation\\ntheorems by Duncan and Kabanov respectively. In this work, we demonstrate that\\nsuch relations hold for a much larger family of continuous-time channels. We\\nintroduce the family of semi-martingale channels where the channel output is a\\nsemi-martingale stochastic process, and the channel input modulates the\\ncharacteristics of the semi-martingale. For these channels, which includes as a\\nspecial case the continuous time Gaussian and Poisson models, we establish new\\nrepresentations relating the mutual information between the channel input and\\noutput to an optimal causal filtering loss, thereby unifying and considerably\\nextending results from the Gaussian and Poisson settings. Extensions to the\\nsetting of mismatched estimation are also presented where the relative entropy\\nbetween the laws governing the output of the channel under two different input\\ndistributions is equal to the cumulative difference between the estimation loss\\nincurred by using the mismatched and optimal causal filters respectively. The\\nmain tool underlying these results is the Doob--Meyer decomposition of a class\\nof likelihood ratio sub-martingales. The results in this work can be viewed as\\nthe continuous-time analogues of recent generalizations for relations between\\ninformation and estimation for discrete-time Lévy channels.\\n',\n",
       " '  Galaxy cluster centring is a key issue for precision cosmology studies using\\ngalaxy surveys. Mis-identification of central galaxies causes systematics in\\nvarious studies such as cluster lensing, satellite kinematics, and galaxy\\nclustering. The red-sequence Matched-filter Probabilistic Percolation\\n(redMaPPer) estimates the probability that each member galaxy is central from\\nphotometric information rather than specifying one central galaxy. The\\nredMaPPer estimates can be used for calibrating the off-centring effect,\\nhowever, the centring algorithm has not previously been well-tested. We test\\nthe centring probabilities of redMaPPer cluster catalog using the projected\\ncross correlation between redMaPPer clusters with photometric red galaxies and\\ngalaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which\\nthe redMaPPer central galaxies (RMCGs) are not the brightest member galaxies\\n(BMEM) and both of them have spectroscopic redshift. This subsample represents\\nnearly 10% of the whole cluster sample. We find a clear difference in the\\ncross-correlation measurements between RMCGs and BMEMs, and the estimated\\ncentring probability is 74$\\\\pm$10% for RMCGs and 13$\\\\pm$4% for BMEMs in the\\nGaussian offset model and 78$\\\\pm$9% for RMCGs and 5$\\\\pm$5% for BMEMs in the NFW\\noffset model. These values are in agreement with the centring probability\\nvalues reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\\\\sigma$.\\nOur analysis provides a strong consistency test of the redMaPPer centring\\nprobabilities. Our results suggest that redMaPPer centring probabilities are\\nreliably estimated. We confirm that the brightest galaxy in the cluster is not\\nalways the central galaxy as has been shown in previous works.\\n',\n",
       " '  The goal of this article is to provide an useful criterion of positivity and\\nwell-posedness for a wide range of infinite dimensional semilinear abstract\\nCauchy problems. This criterion is based on some weak assumptions on the\\nnon-linear part of the semilinear problem and on the existence of a strongly\\ncontinuous semigroup generated by the differential operator. To illustrate a\\nlarge variety of applications, we exhibit the feasibility of this criterion\\nthrough three examples in mathematical biology: epidemiology, predator-prey\\ninteractions and oncology.\\n',\n",
       " '  The ongoing progress in quantum theory emphasizes the crucial role of the\\nvery basic principles of quantum theory. However, this is not properly followed\\nin teaching quantum mechanics on the graduate and undergraduate levels of\\nphysics studies. The existing textbooks typically avoid the axiomatic\\npresentation of the theory. We emphasize usefulness of the systematic,\\naxiomatic approach to the basics of quantum theory as well as its importance in\\nthe light of the modern scientific-research context.\\n',\n",
       " \"  Non-conding RNAs play a key role in the post-transcriptional regulation of\\nmRNA translation and turnover in eukaryotes. miRNAs, in particular, interact\\nwith their target RNAs through protein-mediated, sequence-specific binding,\\ngiving rise to extended and highly heterogeneous miRNA-RNA interaction\\nnetworks. Within such networks, competition to bind miRNAs can generate an\\neffective positive coupling between their targets. Competing endogenous RNAs\\n(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.\\nAlbeit potentially weak, ceRNA interactions can occur both dynamically,\\naffecting e.g. the regulatory clock, and at stationarity, in which case ceRNA\\nnetworks as a whole can be implicated in the composition of the cell's\\nproteome. Many features of ceRNA interactions, including the conditions under\\nwhich they become significant, can be unraveled by mathematical and in silico\\nmodels. We review the understanding of the ceRNA effect obtained within such\\nframeworks, focusing on the methods employed to quantify it, its role in the\\nprocessing of gene expression noise, and how network topology can determine its\\nreach.\\n\",\n",
       " \"  We show that in Grayson's model of higher algebraic $K$-theory using binary\\nacyclic complexes, the complexes of length two suffice to generate the whole\\ngroup. Moreover, we prove that the comparison map from Nenashev's model for\\n$K_1$ to Grayson's model for $K_1$ is an isomorphism. It follows that algebraic\\n$K$-theory of exact categories commutes with infinite products.\\n\",\n",
       " '  We study the min-cost seed selection problem in online social networks, where\\nthe goal is to select a set of seed nodes with the minimum total cost such that\\nthe expected number of influenced nodes in the network exceeds a predefined\\nthreshold. We propose several algorithms that outperform the previous studies\\nboth on the theoretical approximation ratios and on the experimental\\nperformance. Under the case where the nodes have heterogeneous costs, our\\nalgorithms are the first bi- criteria approximation algorithms with polynomial\\nrunning time and provable logarithmic performance bounds using a general\\ncontagion model. Under the case where the users have uniform costs, our\\nalgorithms achieve logarithmic approximation ratio and provable time complexity\\nwhich is smaller than that of existing algorithms in orders of magnitude. We\\nconduct extensive experiments using real social networks. The experimental\\nresults show that, our algorithms significantly outperform the existing\\nalgorithms both on the total cost and on the running time, and also scale well\\nto billion-scale networks.\\n',\n",
       " '  We propose a new splitting criterion for a meta-learning approach to\\nmulticlass classifier design that adaptively merges the classes into a\\ntree-structured hierarchy of increasingly difficult binary classification\\nproblems. The classification tree is constructed from empirical estimates of\\nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that\\nrank the binary subproblems in terms of difficulty of classification. The\\nproposed empirical estimates of the Bayes error rate are computed from the\\nminimal spanning tree (MST) of the samples from each pair of classes. Moreover,\\na meta-learning technique is presented for quantifying the one-vs-rest Bayes\\nerror rate for each individual class from a single MST on the entire dataset.\\nExtensive simulations on benchmark datasets show that the proposed hierarchical\\nmethod can often be learned much faster than competing methods, while achieving\\ncompetitive accuracy.\\n',\n",
       " '  Variational approaches for the calculation of vibrational wave functions and\\nenergies are a natural route to obtain highly accurate results with\\ncontrollable errors. However, the unfavorable scaling and the resulting high\\ncomputational cost of standard variational approaches limit their application\\nto small molecules with only few vibrational modes. Here, we demonstrate how\\nthe density matrix renormalization group (DMRG) can be exploited to optimize\\nvibrational wave functions (vDMRG) expressed as matrix product states. We study\\nthe convergence of these calculations with respect to the size of the local\\nbasis of each mode, the number of renormalized block states, and the number of\\nDMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for\\nsmall molecules that were intensively studied in the literature. We then\\nproceed to show that the complete fingerprint region of the sarcosyn-glycin\\ndipeptide can be calculated with vDMRG.\\n',\n",
       " '  In this work, we present a methodology that enables an agent to make\\nefficient use of its exploratory actions by autonomously identifying possible\\nobjectives in its environment and learning them in parallel. The identification\\nof objectives is achieved using an online and unsupervised adaptive clustering\\nalgorithm. The identified objectives are learned (at least partially) in\\nparallel using Q-learning. Using a simulated agent and environment, it is shown\\nthat the converged or partially converged value function weights resulting from\\noff-policy learning can be used to accumulate knowledge about multiple\\nobjectives without any additional exploration. We claim that the proposed\\napproach could be useful in scenarios where the objectives are initially\\nunknown or in real world scenarios where exploration is typically a time and\\nenergy intensive process. The implications and possible extensions of this work\\nare also briefly discussed.\\n',\n",
       " '  Recent studies on diffusion-based sampling methods have shown that Langevin\\nMonte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and\\nrigorous theoretical guarantees have been proven for both asymptotic and\\nfinite-time regimes. Algorithmically, LMC-based algorithms resemble the\\nwell-known gradient descent (GD) algorithm, where the GD recursion is perturbed\\nby an additive Gaussian noise whose variance has a particular form. Fractional\\nLangevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the\\nGaussian noise is replaced by a heavy-tailed {\\\\alpha}-stable noise. As opposed\\nto its Gaussian counterpart, these heavy-tailed perturbations can incur large\\njumps and it has been empirically demonstrated that the choice of\\n{\\\\alpha}-stable noise can provide several advantages in modern machine learning\\nproblems, both in optimization and sampling contexts. However, as opposed to\\nLMC, only asymptotic convergence properties of FLMC have been yet established.\\nIn this study, we analyze the non-asymptotic behavior of FLMC for non-convex\\noptimization and prove finite-time bounds for its expected suboptimality. Our\\nresults show that the weak-error of FLMC increases faster than LMC, which\\nsuggests using smaller step-sizes in FLMC. We finally extend our results to the\\ncase where the exact gradients are replaced by stochastic gradients and show\\nthat similar results hold in this setting as well.\\n',\n",
       " \"  We use automatic speech recognition to assess spoken English learner\\npronunciation based on the authentic intelligibility of the learners' spoken\\nresponses determined from support vector machine (SVM) classifier or deep\\nlearning neural network model predictions of transcription correctness. Using\\nnumeric features produced by PocketSphinx alignment mode and many recognition\\npasses searching for the substitution and deletion of each expected phoneme and\\ninsertion of unexpected phonemes in sequence, the SVM models achieve 82 percent\\nagreement with the accuracy of Amazon Mechanical Turk crowdworker\\ntranscriptions, up from 75 percent reported by multiple independent\\nresearchers. Using such features with SVM classifier probability prediction\\nmodels can help computer-aided pronunciation teaching (CAPT) systems provide\\nintelligibility remediation.\\n\",\n",
       " '  We consider bilinear optimal control problems, whose objective functionals do\\nnot depend on the controls. Hence, bang-bang solutions will appear. We\\ninvestigate sufficient second-order conditions for bang-bang controls, which\\nguarantee local quadratic growth of the objective functional in $L^1$. In\\naddition, we prove that for controls that are not bang-bang, no such growth can\\nbe expected. Finally, we study the finite-element discretization, and prove\\nerror estimates of bang-bang controls in $L^1$-norms.\\n',\n",
       " '  We carry out a comprehensive analysis of letter frequencies in contemporary\\nwritten Marathi. We determine sets of letters which statistically predominate\\nany large generic Marathi text, and use these sets to estimate the entropy of\\nMarathi.\\n',\n",
       " \"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,\\ncommunication, and storage capabilities of mobile and fixed devices in the\\nfield as well as those of computing and storage servers in remote datacenters\\nis envisioned. Mobile device clouds can be harnessed to enable innovative\\npervasive applications that rely on real-time, in-situ processing of sensor\\ndata collected in the field. To support concurrent mobile applications on the\\ndevice cloud, a robust and secure distributed computing framework, called\\nMaestro, is proposed. The key components of Maestro are (i) a task scheduling\\nmechanism that employs controlled task replication in addition to task\\nreallocation for robustness and (ii) Dedup for task deduplication among\\nconcurrent pervasive workflows. An architecture-based solution that relies on\\ntask categorization and authorized access to the categories of tasks is\\nproposed for different levels of protection. Experimental evaluation through\\nprototype testbed of Android- and Linux-based mobile devices as well as\\nsimulations is performed to demonstrate Maestro's capabilities.\\n\",\n",
       " '  Despite numerous studies the exact nature of the order parameter in\\nsuperconducting Sr2RuO4 remains unresolved. We have extended previous\\nsmall-angle neutron scattering studies of the vortex lattice in this material\\nto a wider field range, higher temperatures, and with the field applied close\\nto both the <100> and <110> basal plane directions. Measurements at high field\\nwere made possible by the use of both spin polarization and analysis to improve\\nthe signal-to-noise ratio. Rotating the field towards the basal plane causes a\\ndistortion of the square vortex lattice observed for H // <001>, and also a\\nsymmetry change to a distorted triangular symmetry for fields close to <100>.\\nThe vortex lattice distortion allows us to determine the intrinsic\\nsuperconducting anisotropy between the c-axis and the Ru-O basal plane,\\nyielding a value of ~60 at low temperature and low to intermediate fields. This\\ngreatly exceeds the upper critical field anisotropy of ~20 at low temperature,\\nreminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects\\non the unpaired quasiparticles in the vortex cores are observed, but a direct\\ndetection lies below the measurement sensitivity. The superconducting\\nanisotropy is found to be independent of temperature but increases for fields >\\n1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature\\ndependence of the scattered intensity provides further support for gap nodes or\\ndeep minima in the superconducting gap.\\n',\n",
       " \"  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$\\ndegenerate fundamental quarks with mass $m$ and a $\\\\theta$-parameter. For\\ngeneric $m$ and $\\\\theta$ the theory has a single gapped vacuum. However, as\\n$\\\\theta$ is varied through $\\\\theta=\\\\pi$ for large $m$ there is a first order\\ntransition. For $N_f=1$ the first order transition line ends at a point with a\\nmassless $\\\\eta'$ particle (for all $N$) and for $N_f>1$ the first order\\ntransition ends at $m=0$, where, depending on the value of $N_f$, the IR theory\\nhas free Nambu-Goldstone bosons, an interacting conformal field theory, or a\\nfree gauge theory. Even when the $4d$ bulk is smooth, domain walls and\\ninterfaces can have interesting phase transitions separating different $3d$\\nphases. These turn out to be the phases of the recently studied $3d$\\nChern-Simons matter theories, thus relating the dynamics of QCD$_4$ and\\nQCD$_3$, and, in particular, making contact with the recently discussed\\ndualities in 2+1 dimensions. For example, when the massless $4d$ theory has an\\n$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a\\n$3d$ massless $CP^{N_f-1}$ nonlinear $\\\\sigma$-model with a Wess-Zumino term, in\\nagreement with the conjectured dynamics in 2+1 dimensions.\\n\",\n",
       " '  Investigation of the autoignition delay of the butanol isomers has been\\nperformed at elevated pressures of 15 bar and 30 bar and low to intermediate\\ntemperatures of 680-860 K. The reactivity of the stoichiometric isomers of\\nbutanol, in terms of inverse ignition delay, was ranked as n-butanol >\\nsec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but\\nchanged to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For\\nthe temperature and pressure conditions in this study, no NTC or two-stage\\nignition behavior were observed. However, for both of the compressed pressures\\nstudied in this work, tert-butanol exhibited unique pre-ignition heat release\\ncharacteristics. As such, tert-butanol was further studied at two additional\\nequivalence ratios ($\\\\phi$ = 0.5 and 2.0) to help determine the cause of the\\nheat release.\\n',\n",
       " '  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method\\ncalled CLGrouping, for constructing tree-structured probabilistic graphical\\nmodels, a statistical framework that is commonly used for inferring\\nphylogenetic trees. While CLGrouping works correctly if there is a unique MST,\\nwe observe an indeterminacy in the method in the case that there are multiple\\nMSTs. In this work we remove this indeterminacy by introducing so-called\\nvertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely\\nrelated to the number of leaves in the MST. This motivates the problem of\\nfinding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We\\nprovide a polynomial time algorithm for the MLVRMST problem, and prove its\\ncorrectness for graphs whose edges are weighted with tree-additive distances.\\n',\n",
       " \"  Variational Bayesian neural nets combine the flexibility of deep learning\\nwith Bayesian uncertainty estimation. Unfortunately, there is a tradeoff\\nbetween cheap but simple variational families (e.g.~fully factorized) or\\nexpensive and complicated inference procedures. We show that natural gradient\\nascent with adaptive weight noise implicitly fits a variational posterior to\\nmaximize the evidence lower bound (ELBO). This insight allows us to train\\nfull-covariance, fully factorized, or matrix-variate Gaussian variational\\nposteriors using noisy versions of natural gradient, Adam, and K-FAC,\\nrespectively, making it possible to scale up to modern-size ConvNets. On\\nstandard regression benchmarks, our noisy K-FAC algorithm makes better\\npredictions and matches Hamiltonian Monte Carlo's predictive variances better\\nthan existing methods. Its improved uncertainty estimates lead to more\\nefficient exploration in active learning, and intrinsic motivation for\\nreinforcement learning.\\n\",\n",
       " '  We define rules for cellular automata played on quasiperiodic tilings of the\\nplane arising from the multigrid method in such a way that these cellular\\nautomata are isomorphic to Conway\\'s Game of Life. Although these tilings are\\nnonperiodic, determining the next state of each tile is a local computation,\\nrequiring only knowledge of the local structure of the tiling and the states of\\nfinitely many nearby tiles. As an example, we show a version of a \"glider\"\\nmoving through a region of a Penrose tiling. This constitutes a potential\\ntheoretical framework for a method of executing computations in\\nnon-periodically structured substrates such as quasicrystals.\\n',\n",
       " '  In the present work, we explore the existence, stability and dynamics of\\nsingle and multiple vortex ring states that can arise in Bose-Einstein\\ncondensates. Earlier works have illustrated the bifurcation of such states, in\\nthe vicinity of the linear limit, for isotropic or anisotropic\\nthree-dimensional harmonic traps. Here, we extend these states to the regime of\\nlarge chemical potentials, the so-called Thomas-Fermi limit, and explore their\\nproperties such as equilibrium radii and inter-ring distance, for multi-ring\\nstates, as well as their vibrational spectra and possible instabilities. In\\nthis limit, both the existence and stability characteristics can be partially\\ntraced to a particle picture that considers the rings as individual particles\\noscillating within the trap and interacting pairwise with one another. Finally,\\nwe examine some representative instability scenarios of the multi-ring dynamics\\nincluding breakup and reconnections, as well as the transient formation of\\nvortex lines.\\n',\n",
       " '  For a class of partially observed diffusions, sufficient conditions are given\\nfor the map from initial condition of the signal to filtering distribution to\\nbe contractive with respect to Wasserstein distances, with rate which has no\\ndependence on the dimension of the state-space and is stable under tensor\\nproducts of the model. The main assumptions are that the signal has affine\\ndrift and constant diffusion coefficient, and that the likelihood functions are\\nlog-concave. Contraction estimates are obtained from an $h$-process\\nrepresentation of the transition probabilities of the signal reweighted so as\\nto condition on the observations.\\n',\n",
       " '  We study the motion of an electron bubble in the zero temperature limit where\\nneither phonons nor rotons provide a significant contribution to the drag\\nexerted on an ion moving within the superfluid. By using the Gross-Clark model,\\nin which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled\\nto a Schrödinger equation for the electron wavefunction, we study how\\nvortex nucleation affects the measured drift velocity of the ion. We use\\nparameters that give realistic values of the ratio of the radius of the bubble\\nwith respect to the healing length in superfluid $^4$He at a pressure of one\\nbar. By performing fully 3D spatio-temporal simulations of the superfluid\\ncoupled to an electron, that is modelled within an adiabatic approximation and\\nmoving under the influence of an applied electric field, we are able to recover\\nthe key dynamics of the ion-vortex interactions that arise and the subsequent\\nion-vortex complexes that can form. Using the numerically computed drift\\nvelocity of the ion as a function of the applied electric field, we determine\\nthe vortex-nucleation limited mobility of the ion to recover values in\\nreasonable agreement with measured data.\\n',\n",
       " '  We present new JVLA multi-frequency measurements of a set of stars in\\ntransition from the post-AGB to the Planetary Nebula phase monitored in the\\nradio range over several years. Clear variability is found for five sources.\\nTheir light curves show increasing and decreasing patterns. New radio\\nobservations at high angular resolution are also presented for two sources.\\nAmong these is IRAS 18062+2410, whose radio structure is compared to\\nnear-infrared images available in the literature. With these new maps, we can\\nestimate inner and outer radii of 0.03$\"$ and 0.08$\"$ for the ionised shell, an\\nionised mass of $3.2\\\\times10^{-4}$ M$_\\\\odot$, and a density at the inner radius\\nof $7.7\\\\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with\\nthe new morphological constraints. The combination of multi-frequency data and,\\nwhere available, spectral-index maps leads to the detection of spectral indices\\nnot due to thermal emission, contrary to what one would expect in planetary\\nnebulae. Our results allow us to hypothesise the existence of a link between\\nradio variability and non-thermal emission mechanisms in the nebulae. This link\\nseems to hold for IRAS 22568+6141 and may generally hold for those nebulae\\nwhere the radio flux decreases over time.\\n',\n",
       " '  We develop an on-line monitoring procedure to detect a change in a large\\napproximate factor model. Our statistics are based on a well-known property of\\nthe $% \\\\left( r+1\\\\right) $-th eigenvalue of the sample covariance matrix of the\\ndata (having defined $r$ as the number of common factors): whilst under the\\nnull the $\\\\left( r+1\\\\right) $-th eigenvalue is bounded, under the alternative\\nof a change (either in the loadings, or in the number of factors itself) it\\nbecomes spiked. Given that the sample eigenvalue cannot be estimated\\nconsistently under the null, we regularise the problem by randomising the test\\nstatistic in conjunction with sample conditioning, obtaining a sequence of\\n\\\\textit{i.i.d.}, asymptotically chi-square statistics which are then employed\\nto build the monitoring scheme. Numerical evidence shows that our procedure\\nworks very well in finite samples, with a very small probability of false\\ndetections and tight detection times in presence of a genuine change-point.\\n',\n",
       " '  A susceptibility propagation that is constructed by combining a belief\\npropagation and a linear response method is used for approximate computation\\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\\npropagation by using the concept of a diagonal matching method that is based on\\nmean-field approaches to inverse Ising problems. The proposed susceptibility\\npropagation is robust for various network structures, and it is reduced to the\\nordinary susceptibility propagation and to the adaptive\\nThouless-Anderson-Palmer equation in special cases.\\n',\n",
       " '  This paper analyzes the downlink performance of ultra-dense networks with\\nelevated base stations (BSs). We consider a general dual-slope pathloss model\\nwith distance-dependent probability of line-of-sight (LOS) transmission between\\nBSs and receivers. Specifically, we consider the scenario where each link may\\nbe obstructed by randomly placed buildings. Using tools from stochastic\\ngeometry, we show that both coverage probability and area spectral efficiency\\ndecay to zero as the BS density grows large. Interestingly, we show that the BS\\nheight alone has a detrimental effect on the system performance even when the\\nstandard single-slope pathloss model is adopted.\\n',\n",
       " '  We demonstrate the first application of deep reinforcement learning to\\nautonomous driving. From randomly initialised parameters, our model is able to\\nlearn a policy for lane following in a handful of training episodes using a\\nsingle monocular image as input. We provide a general and easy to obtain\\nreward: the distance travelled by the vehicle without the safety driver taking\\ncontrol. We use a continuous, model-free deep reinforcement learning algorithm,\\nwith all exploration and optimisation performed on-vehicle. This demonstrates a\\nnew framework for autonomous driving which moves away from reliance on defined\\nlogical rules, mapping, and direct supervision. We discuss the challenges and\\nopportunities to scale this approach to a broader range of autonomous driving\\ntasks.\\n',\n",
       " '  Strong-coupling of monolayer metal dichalcogenide semiconductors with light\\noffers encouraging prospects for realistic exciton devices at room temperature.\\nHowever, the nature of this coupling depends extremely sensitively on the\\noptical confinement and the orientation of electronic dipoles and fields. Here,\\nwe show how plasmon strong coupling can be achieved in compact robust\\neasily-assembled gold nano-gap resonators at room temperature. We prove that\\nstrong coupling is impossible with monolayers due to the large exciton\\ncoherence size, but resolve clear anti-crossings for 8 layer devices with Rabi\\nsplittings exceeding 135 meV. We show that such structures improve on prospects\\nfor nonlinear exciton functionalities by at least 10^4, while retaining quantum\\nefficiencies above 50%.\\n',\n",
       " '  Positioning data offer a remarkable source of information to analyze crowds\\nurban dynamics. However, discovering urban activity patterns from the emergent\\nbehavior of crowds involves complex system modeling. An alternative approach is\\nto adopt computational techniques belonging to the emergent paradigm, which\\nenables self-organization of data and allows adaptive analysis. Specifically,\\nour approach is based on stigmergy. By using stigmergy each sample position is\\nassociated with a digital pheromone deposit, which progressively evaporates and\\naggregates with other deposits according to their spatiotemporal proximity.\\nBased on this principle, we exploit positioning data to identify high density\\nareas (hotspots) and characterize their activity over time. This\\ncharacterization allows the comparison of dynamics occurring in different days,\\nproviding a similarity measure exploitable by clustering techniques. Thus, we\\ncluster days according to their activity behavior, discovering unexpected urban\\nactivity patterns. As a case study, we analyze taxi traces in New York City\\nduring 2015.\\n',\n",
       " '  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with\\ntwo commuting multiplicative linear maps. The main purpose of this paper is to\\ndefine representations and a cohomology of BiHom-Lie colour algebras and to\\nstudy some key constructions and properties.\\nMoreover, we discuss $\\\\alpha^{k}\\\\beta^l$-generalized derivations,\\n$\\\\alpha^{k}\\\\beta^l$-quasi-derivations and $\\\\alpha^{k}\\\\beta^l$-quasi-centroid.\\nWe provide some properties and their relationships with BiHom-Jordan colour\\nalgebra.\\n',\n",
       " '  We consider the problem related to clustering of gamma-ray bursts (from\\n\"BATSE\" catalogue) through kernel principal component analysis in which our\\nproposed kernel outperforms results of other competent kernels in terms of\\nclustering accuracy and we obtain three physically interpretable groups of\\ngamma-ray bursts. The effectivity of the suggested kernel in combination with\\nkernel principal component analysis in revealing natural clusters in noisy and\\nnonlinear data while reducing the dimension of the data is also explored in two\\nsimulated data sets.\\n',\n",
       " '  Baker, Harman, and Pintz showed that a weak form of the Prime Number Theorem\\nholds in intervals of the form $[x-x^{0.525},x]$ for large $x$. In this paper,\\nwe extend a result of Maynard and Tao concerning small gaps between primes to\\nintervals of this length. More precisely, we prove that for any $\\\\delta\\\\in\\n[0.525,1]$ there exist positive integers $k,d$ such that for sufficiently large\\n$x$, the interval $[x-x^\\\\delta,x]$ contains $\\\\gg_{k} \\\\frac{x^\\\\delta}{(\\\\log\\nx)^k}$ pairs of consecutive primes differing by at most $d$. This confirms a\\nspeculation of Maynard that results on small gaps between primes can be refined\\nto the setting of short intervals of this length.\\n',\n",
       " '  Improved Phantom cell is a new scenario which has been introduced recently to\\nenhance the capacity of Heterogeneous Networks (HetNets). The main trait of\\nthis scenario is that, besides maximizing the total network capacity in both\\nindoor and outdoor environments, it claims to reduce the handover number\\ncompared to the conventional scenarios. In this paper, by a comprehensive\\nreview of the Improved Phantom cells structure, an appropriate algorithm will\\nbe introduced for the handover procedure of this scenario. To reduce the number\\nof handover in the proposed algorithm, various parameters such as the received\\nSignal to Interference plus Noise Ratio (SINR) at the user equipment (UE),\\nusers access conditions to the phantom cells, and users staying time in the\\ntarget cell based on its velocity, has been considered. Theoretical analyses\\nand simulation results show that applying the suggested algorithm the improved\\nphantom cell structure has a much better performance than conventional HetNets\\nin terms of the number of handover.\\n',\n",
       " '  We address the problem of localisation of objects as bounding boxes in images\\nwith weak labels. This weakly supervised object localisation problem has been\\ntackled in the past using discriminative models where each object class is\\nlocalised independently from other classes. We propose a novel framework based\\non Bayesian joint topic modelling. Our framework has three distinctive\\nadvantages over previous works: (1) All object classes and image backgrounds\\nare modelled jointly together in a single generative model so that \"explaining\\naway\" inference can resolve ambiguity and lead to better learning and\\nlocalisation. (2) The Bayesian formulation of the model enables easy\\nintegration of prior knowledge about object appearance to compensate for\\nlimited supervision. (3) Our model can be learned with a mixture of weakly\\nlabelled and unlabelled data, allowing the large volume of unlabelled images on\\nthe Internet to be exploited for learning. Extensive experiments on the\\nchallenging VOC dataset demonstrate that our approach outperforms the\\nstate-of-the-art competitors.\\n',\n",
       " '  All people have to make risky decisions in everyday life. And we do not know\\nhow true they are. But is it possible to mathematically assess the correctness\\nof our choice? This article discusses the model of decision making under risk\\non the example of project management. This is a game with two players, one of\\nwhich is Investor, and the other is the Project Manager. Each player makes a\\nrisky decision for himself, based on his past experience. With the help of a\\nmathematical model, the players form a level of confidence, depending on who\\nthe player accepts the strategy or does not accept. The project manager\\nassesses the costs and compares them with the level of confidence. An investor\\nevaluates past results. Also visit the case where the strategy of the player\\naccepts the part.\\n',\n",
       " \"  Modeling the interior of exoplanets is essential to go further than the\\nconclusions provided by mean density measurements. In addition to the still\\nlimited precision on the planets' fundamental parameters, models are limited by\\nthe existence of degeneracies on their compositions. Here we present a model of\\ninternal structure dedicated to the study of solid planets up to ~10 Earth\\nmasses, i.e. Super-Earths. When the measurement is available, the assumption\\nthat the bulk Fe/Si ratio of a planet is similar to that of its host star\\nallows us to significantly reduce the existing degeneracy and more precisely\\nconstrain the planet's composition. Based on our model, we provide an update of\\nthe mass-radius relationships used to provide a first estimate of a planet's\\ncomposition from density measurements. Our model is also applied to the cases\\nof two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently\\nupdated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are\\nfound to lie within the 10-37% and 10-33% ranges, respectively, allowing both\\nplanets to be compatible with an Earth-like composition. We also extend the\\nrecent study of Proxima Centauri b, and show that its radius may reach 1.94\\nEarth radii in the case of a 5 Earth masses planet, as there is a 96.7%\\nprobability that the real mass of Proxima Centauri b is below this value.\\n\",\n",
       " '  In this paper we discuss the characteristics and operation of Astro Space\\nCenter (ASC) software FX correlator that is an important component of\\nspace-ground interferometer for Radioastron project. This project performs\\njoint observations of compact radio sources using 10 meter space radio\\ntelescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm\\nwavelengths. In this paper we describe the main features of space-ground VLBI\\ndata processing of Radioastron project using ASC correlator. Quality of\\nimplemented fringe search procedure provides positive results without\\nsignificant losses in correlated amplitude. ASC Correlator has a computational\\npower close to real time operation. The correlator has a number of processing\\nmodes: \"Continuum\", \"Spectral Line\", \"Pulsars\", \"Giant Pulses\",\"Coherent\".\\nSpecial attention is paid to peculiarities of Radioastron space-ground VLBI\\ndata processing. The algorithms of time delay and delay rate calculation are\\nalso discussed, which is a matter of principle for data correlation of\\nspace-ground interferometers. During 5 years of Radioastron space radio\\ntelescope (SRT) successful operation, ASC correlator showed high potential of\\nsatisfying steady growing needs of current and future ground and space VLBI\\nscience. Results of ASC software correlator operation are demonstrated.\\n',\n",
       " \"  Schoof's classic algorithm allows point-counting for elliptic curves over\\nfinite fields in polynomial time. This algorithm was subsequently improved by\\nAtkin, using factorizations of modular polynomials, and by Elkies, using a\\ntheory of explicit isogenies. Moving to Jacobians of genus-2 curves, the\\ncurrent state of the art for point counting is a generalization of Schoof's\\nalgorithm. While we are currently missing the tools we need to generalize\\nElkies' methods to genus 2, recently Martindale and Milio have computed\\nanalogues of modular polynomials for genus-2 curves whose Jacobians have real\\nmultiplication by maximal orders of small discriminant. In this article, we\\nprove Atkin-style results for genus-2 Jacobians with real multiplication by\\nmaximal orders, with a view to using these new modular polynomials to improve\\nthe practicality of point-counting algorithms for these curves.\\n\",\n",
       " '  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It\\nis well-known that any ambiguous ideal in $L$ is locally free over\\n$\\\\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free\\nclass group of $\\\\mathcal{O}_KG$, where $\\\\mathcal{O}_K$ denotes the ring of\\nintegers of $K$. In this paper, we shall study the relationship among the\\nclasses arising from the ring of integers $\\\\mathcal{O}_L$ of $L$, the inverse\\ndifferent $\\\\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the\\ninverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is\\nabelian. They are naturally related because $A_{L/K}^2 =\\n\\\\mathfrak{D}_{L/K}^{-1} = \\\\mathcal{O}_L^*$, and $A_{L/K}$ is special because\\n$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of\\n$L/K$.\\n',\n",
       " '  Transformative AI technologies have the potential to reshape critical aspects\\nof society in the near future. However, in order to properly prepare policy\\ninitiatives for the arrival of such technologies accurate forecasts and\\ntimelines are necessary. A survey was administered to attendees of three AI\\nconferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).\\nThe survey included questions for estimating AI capabilities over the next\\ndecade, questions for forecasting five scenarios of transformative AI and\\nquestions concerning the impact of computational resources in AI research.\\nRespondents indicated a median of 21.5% of human tasks (i.e., all tasks that\\nhumans are currently paid to do) can be feasibly automated now, and that this\\nfigure would rise to 40% in 5 years and 60% in 10 years. Median forecasts\\nindicated a 50% probability of AI systems being capable of automating 90% of\\ncurrent human tasks in 25 years and 99% of current human tasks in 50 years. The\\nconference of attendance was found to have a statistically significant impact\\non all forecasts, with attendees of HLAI providing more optimistic timelines\\nwith less uncertainty. These findings suggest that AI experts expect major\\nadvances in AI technology to continue over the next decade to a degree that\\nwill likely have profound transformative impacts on society.\\n',\n",
       " '  Let $G,H$ be groups, $\\\\phi: G \\\\rightarrow H$ a group morphism, and $A$ a\\n$G$-graded algebra. The morphism $\\\\phi$ induces an $H$-grading on $A$, and on\\nany $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given\\nan injective $G$-graded $A$-module, we give bounds for its injective dimension\\nwhen seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give\\nan application of our results to the stability of dualizing complexes through\\nchange of grading.\\n',\n",
       " '  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts\\nas an approximate isometry when restricted to sparse vectors. Previous work has\\nshown it to be NP-hard to determine whether a matrix possess this property, but\\nonly in a narrow range of parameters. In this work, we show that it is NP-hard\\nto make this determination for any accuracy parameter, even when we restrict\\nourselves to instances which are either RIP or far from being RIP. This result\\nimplies that it is NP-hard to approximate the range of parameters for which a\\nmatrix possesses the Restricted Isometry Property with accuracy better than\\nsome constant. Ours is the first work to prove such a claim without any\\nadditional assumptions.\\n',\n",
       " '  We present a compact design for a velocity-map imaging spectrometer for\\nenergetic electrons and ions. The standard geometry by Eppink and Parker [A. T.\\nJ. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented\\nby just two extended electrodes so as to realize an additional einzel lens. In\\nthis way, for a maximum electrode voltage of 7 kV we experimentally demonstrate\\nimaging of electrons with energies up to 65 eV. Simulations show that energy\\nacceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are\\nachievable for electrode voltages of <20 kV when using diameters of the\\nposition-sensitive detector of 42 and 78 mm, respectively.\\n',\n",
       " '  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,\\nmimic a distributed storage of a set of $n$ data items on $m$ servers, in such\\na way that any batch of $k$ data items can be retrieved by reading at most some\\n$t$ symbols from each server. Combinatorial batch codes, are replication-based\\nbatch codes in which each server stores a subset of the data items.\\nIn this paper, we propose a generalization of combinatorial batch codes,\\ncalled multiset combinatorial batch codes (MCBC), in which $n$ data items are\\nstored in $m$ servers, such that any multiset request of $k$ items, where any\\nitem is requested at most $r$ times, can be retrieved by reading at most $t$\\nitems from each server. The setup of this new family of codes is motivated by\\nrecent work on codes which enable high availability and parallel reads in\\ndistributed storage systems. The main problem under this paradigm is to\\nminimize the number of items stored in the servers, given the values of\\n$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and\\nsufficient condition for the existence of MCBCs. Then, we present several\\nbounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we\\ndetermine the value of $N(n,k,m,1;r)$ for any $n\\\\geq\\n\\\\left\\\\lfloor\\\\frac{k-1}{r}\\\\right\\\\rfloor{m\\\\choose k-1}-(m-k+1)A(m,4,k-2)$, where\\n$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length\\n$m$, distance four and weight $k-2$. We also determine the exact value of\\n$N(n,k,m,1;r)$ when $r\\\\in\\\\{k,k-1\\\\}$ or $k=m$.\\n',\n",
       " '  The energy efficiency and power of a three-terminal thermoelectric nanodevice\\nare studied by considering elastic tunneling through a single quantum dot.\\nFacilitated by the three-terminal geometry, the nanodevice is able to generate\\nsimultaneously two electrical powers by utilizing only one temperature bias.\\nThese two electrical powers can add up constructively or destructively,\\ndepending on their signs. It is demonstrated that the constructive addition\\nleads to the enhancement of both energy efficiency and output power for various\\nsystem parameters. In fact, such enhancement, dubbed as thermoelectric\\ncooperative effect, can lead to maximum efficiency and power no less than when\\nonly one of the electrical power is harvested.\\n',\n",
       " '  In recent years, a number of methods for verifying DNNs have been developed.\\nBecause the approaches of the methods differ and have their own limitations, we\\nthink that a number of verification methods should be applied to a developed\\nDNN. To apply a number of methods to the DNN, it is necessary to translate\\neither the implementation of the DNN or the verification method so that one\\nruns in the same environment as the other. Since those translations are\\ntime-consuming, a utility tool, named DeepSaucer, which helps to retain and\\nreuse implementations of DNNs, verification methods, and their environments, is\\nproposed. In DeepSaucer, code snippets of loading DNNs, running verification\\nmethods, and creating their environments are retained and reused as software\\nassets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer\\nis confirmed by implementing it on the basis of Anaconda, which provides\\nvirtual environment for loading a DNN and running a verification method. In\\naddition, the effectiveness of DeepSaucer is demonstrated by usecase examples.\\n',\n",
       " '  During exploratory testing sessions the tester simultaneously learns, designs\\nand executes tests. The activity is iterative and utilizes the skills of the\\ntester and provides flexibility and creativity.Test charters are used as a\\nvehicle to support the testers during the testing. The aim of this study is to\\nsupport practitioners in the design of test charters through checklists. We\\naimed to identify factors allowing practitioners to critically reflect on their\\ndesigns and contents of test charters to support practitioners in making\\ninformed decisions of what to include in test charters. The factors and\\ncontents have been elicited through interviews. Overall, 30 factors and 35\\ncontent elements have been elicited.\\n',\n",
       " \"  We demonstrate the parallel and non-destructive readout of the hyperfine\\nstate for optically trapped $^{87}$Rb atoms. The scheme is based on\\nstate-selective fluorescence imaging and achieves detection fidelities $>$98%\\nwithin 10$\\\\,$ms, while keeping 99% of the atoms trapped. For the read-out of\\ndense arrays of neutral atoms in optical lattices, where the fluorescence\\nimages of neighboring atoms overlap, we apply a novel image analysis technique\\nusing Bayesian inference to determine the internal state of multiple atoms. Our\\nmethod is scalable to large neutral atom registers relevant for future quantum\\ninformation processing tasks requiring fast and non-destructive readout and can\\nalso be used for the simultaneous read-out of quantum information stored in\\ninternal qubit states and in the atoms' positions.\\n\",\n",
       " '  In this work we apply Amplitude Modulation Spectrum (AMS) features to the\\nsource localization problem. Our approach computes 36 bilateral features for 2s\\nlong signal segments and estimates the azimuthal directions of a sound source\\nthrough a binaurally trained classifier. This directional information of a\\nsound source could be e.g. used to steer the beamformer in a hearing aid to the\\nsource of interest in order to increase the SNR. We evaluated our approach on\\nthe development set of the IEEE-AASP Challenge on sound source localization and\\ntracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline\\napproach. Additionally, our approach is computationally less complex.\\n',\n",
       " '  We propose a novel class of dynamic shrinkage processes for Bayesian time\\nseries and regression analysis. Building upon a global-local framework of prior\\nconstruction, in which continuous scale mixtures of Gaussian distributions are\\nemployed for both desirable shrinkage properties and computational\\ntractability, we model dependence among the local scale parameters. The\\nresulting processes inherit the desirable shrinkage behavior of popular\\nglobal-local priors, such as the horseshoe prior, but provide additional\\nlocalized adaptivity, which is important for modeling time series data or\\nregression functions with local features. We construct a computationally\\nefficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture\\nrepresentation of the proposed process. Using dynamic shrinkage processes, we\\ndevelop a Bayesian trend filtering model that produces more accurate estimates\\nand tighter posterior credible intervals than competing methods, and apply the\\nmodel for irregular curve-fitting of minute-by-minute Twitter CPU usage data.\\nIn addition, we develop an adaptive time-varying parameter regression model to\\nassess the efficacy of the Fama-French five-factor asset pricing model with\\nmomentum added as a sixth factor. Our dynamic analysis of manufacturing and\\nhealthcare industry data shows that with the exception of the market risk, no\\nother risk factors are significant except for brief periods.\\n',\n",
       " '  The monitoring of the lifestyles may be performed based on a system for the\\nrecognition of Activities of Daily Living (ADL) and their environments,\\ncombining the results obtained with the user agenda. The system may be\\ndeveloped with the use of the off-the-shelf mobile devices commonly used,\\nbecause they have several types of sensors available, including motion,\\nmagnetic, acoustic, and location sensors. Data acquisition, data processing,\\ndata fusion, and artificial intelligence methods are applied in different\\nstages of the system developed, which recognizes the ADL with pattern\\nrecognition methods. The motion and magnetic sensors allow the recognition of\\nactivities with movement, but the acoustic sensors allow the recognition of the\\nenvironments. The fusion of the motion, magnetic and acoustic sensors allows\\nthe differentiation of other ADL. On the other hand, the location sensors\\nallows the recognition of ADL with large movement, and the combination of these\\nsensors with the other sensors increases the number of ADL recognized by the\\nsystem. This study consists on the comparison of different types of ANN for\\nchoosing the best methods for the recognition of several ADL, which they are\\nimplemented in a system for the recognition of ADL that combines the sensors\\ndata with the users agenda for the monitoring of the lifestyles. Conclusions\\npoint to the use of Deep Neural Networks (DNN) with normalized data for the\\nidentification of ADL with 85.89% of accuracy, the use of Feedforward neural\\nnetworks with non-normalized data for the identification of the environments\\nwith 86.50% of accuracy, and the use of DNN with normalized data for the\\nidentification of standing activities with 100% of accuracy, proving the\\nreliability of the framework presented in this study.\\n',\n",
       " \"  The extreme value index is a fundamental parameter in univariate Extreme\\nValue Theory (EVT). It captures the tail behavior of a distribution and is\\ncentral in the extrapolation beyond observed data. Among other semi-parametric\\nmethods (such as the popular Hill's estimator), the Block Maxima (BM) and\\nPeaks-Over-Threshold (POT) methods are widely used for assessing the extreme\\nvalue index and related normalizing constants. We provide asymptotic theory for\\nthe maximum likelihood estimators (MLE) based on the BM method. Our main result\\nis the asymptotic normality of the MLE with a non-trivial bias depending on the\\nextreme value index and on the so-called second order parameter. Our approach\\ncombines asymptotic expansions of the likelihood process and of the empirical\\nquantile process of block maxima. The results permit to complete the comparison\\nof most common semi-parametric estimators in EVT (MLE and probability weighted\\nmoment estimators based on the POT or BM methods) through their asymptotic\\nvariances, biases and optimal mean square errors.\\n\",\n",
       " '  The spread of opinions, memes, diseases, and \"alternative facts\" in a\\npopulation depends both on the details of the spreading process and on the\\nstructure of the social and communication networks on which they spread. In\\nthis paper, we explore how \\\\textit{anti-establishment} nodes (e.g.,\\n\\\\textit{hipsters}) influence the spreading dynamics of two competing products.\\nWe consider a model in which spreading follows a deterministic rule for\\nupdating node states (which describe which product has been adopted) in which\\nan adjustable fraction $p_{\\\\rm Hip}$ of the nodes in a network are hipsters,\\nwho choose to adopt the product that they believe is the less popular of the\\ntwo. The remaining nodes are conformists, who choose which product to adopt by\\nconsidering which products their immediate neighbors have adopted. We simulate\\nour model on both synthetic and real networks, and we show that the hipsters\\nhave a major effect on the final fraction of people who adopt each product:\\neven when only one of the two products exists at the beginning of the\\nsimulations, a very small fraction of hipsters in a network can still cause the\\nother product to eventually become the more popular one. To account for this\\nbehavior, we construct an approximation for the steady-state adoption fraction\\non $k$-regular trees in the limit of few hipsters. Additionally, our\\nsimulations demonstrate that a time delay $\\\\tau$ in the knowledge of the\\nproduct distribution in a population, as compared to immediate knowledge of\\nproduct adoption among nearest neighbors, can have a large effect on the final\\ndistribution of product adoptions. Our simple model and analysis may help shed\\nlight on the road to success for anti-establishment choices in elections, as\\nsuch success can arise rather generically in our model from a small number of\\nanti-establishment individuals and ordinary processes of social influence on\\nnormal individuals.\\n',\n",
       " '  We study the problem of testing identity against a given distribution with a\\nfocus on the high confidence regime. More precisely, given samples from an\\nunknown distribution $p$ over $n$ elements, an explicitly given distribution\\n$q$, and parameters $0< \\\\epsilon, \\\\delta < 1$, we wish to distinguish, {\\\\em\\nwith probability at least $1-\\\\delta$}, whether the distributions are identical\\nversus $\\\\varepsilon$-far in total variation distance. Most prior work focused\\non the case that $\\\\delta = \\\\Omega(1)$, for which the sample complexity of\\nidentity testing is known to be $\\\\Theta(\\\\sqrt{n}/\\\\epsilon^2)$. Given such an\\nalgorithm, one can achieve arbitrarily small values of $\\\\delta$ via black-box\\namplification, which multiplies the required number of samples by\\n$\\\\Theta(\\\\log(1/\\\\delta))$.\\nWe show that black-box amplification is suboptimal for any $\\\\delta = o(1)$,\\nand give a new identity tester that achieves the optimal sample complexity. Our\\nnew upper and lower bounds show that the optimal sample complexity of identity\\ntesting is \\\\[\\n\\\\Theta\\\\left( \\\\frac{1}{\\\\epsilon^2}\\\\left(\\\\sqrt{n \\\\log(1/\\\\delta)} +\\n\\\\log(1/\\\\delta) \\\\right)\\\\right) \\\\] for any $n, \\\\varepsilon$, and $\\\\delta$. For\\nthe special case of uniformity testing, where the given distribution is the\\nuniform distribution $U_n$ over the domain, our new tester is surprisingly\\nsimple: to test whether $p = U_n$ versus $d_{\\\\mathrm TV}(p, U_n) \\\\geq\\n\\\\varepsilon$, we simply threshold $d_{\\\\mathrm TV}(\\\\widehat{p}, U_n)$, where\\n$\\\\widehat{p}$ is the empirical probability distribution. The fact that this\\nsimple \"plug-in\" estimator is sample-optimal is surprising, even in the\\nconstant $\\\\delta$ case. Indeed, it was believed that such a tester would not\\nattain sublinear sample complexity even for constant values of $\\\\varepsilon$\\nand $\\\\delta$.\\n',\n",
       " '  Component-based design is a different way of constructing systems which\\noffers numerous benefits, in particular, decreasing the complexity of system\\ndesign. However, deploying components into a system is a challenging and\\nerror-prone task. Model checking is one of the reliable methods that\\nautomatically and systematically analyse the correctness of a given system. Its\\nbrute-force check of the state space significantly expands the level of\\nconfidence in the system. Nevertheless, model checking is limited by a critical\\nproblem so-called State Space Explosion (SSE). To benefit from model checking,\\nappropriate methods to reduce SSE, is required. In two last decades, a great\\nnumber of methods to mitigate the state space explosion have been proposed\\nwhich have many similarities, dissimilarities, and unclear concepts in some\\ncases. This research, firstly, aims at present a review and brief discussion of\\nthe methods of handling SSE problem and classify them based on their\\nsimilarities, principle and characteristics. Second, it investigates the\\nmethods for handling SSE problem in verifying Component-based system (CBS) and\\nprovides insight into CBS verification limitations that have not been addressed\\nyet. The analysis in this research has revealed the patterns, specific\\nfeatures, and gaps in the state-of-the-art methods. In addition, we identified\\nand discussed suitable methods to soften SSE problem in CBS and underlined the\\nkey challenges for future research efforts.\\n',\n",
       " '  Dust devils are likely the dominant source of dust for the martian\\natmosphere, but the amount and frequency of dust-lifting depend on the\\nstatistical distribution of dust devil parameters. Dust devils exhibit pressure\\nperturbations and, if they pass near a barometric sensor, they may register as\\na discernible dip in a pressure time-series. Leveraging this fact, several\\nsurveys using barometric sensors on landed spacecraft have revealed dust devil\\nstructures and occurrence rates. However powerful they are, though, such\\nsurveys suffer from non-trivial biases that skew the inferred dust devil\\nproperties. For example, such surveys are most sensitive to dust devils with\\nthe widest and deepest pressure profiles, but the recovered profiles will be\\ndistorted, broader and shallow than the actual profiles. In addition, such\\nsurveys often do not provide wind speed measurements alongside the pressure\\ntime series, and so the durations of the dust devil signals in the time series\\ncannot be directly converted to profile widths. Fortunately, simple statistical\\nand geometric considerations can de-bias these surveys, allowing conversion of\\nthe duration of dust devil signals into physical widths, given only a\\ndistribution of likely translation velocities, and the recovery of the\\nunderlying distributions of physical parameters. In this study, we develop a\\nscheme for de-biasing such surveys. Applying our model to an in-situ survey\\nusing data from the Phoenix lander suggests a larger dust flux and a dust devil\\noccurrence rate about ten times larger than previously inferred. Comparing our\\nresults to dust devil track surveys suggests only about one in five\\nlow-pressure cells lifts sufficient dust to leave a visible track.\\n',\n",
       " '  With the help of first principles calculation method based on the density\\nfunctional theory we have investigated the structural, elastic, mechanical\\nproperties and Debye temperature of Fe2ScM (M = P and As) compounds under\\npressure up to 60 GPa. The optical properties have been investigated under zero\\npressure. Our calculated optimized structural parameters of both the compounds\\nare in good agreement with the other theoretical results. The calculated\\nelastic constants show that Fe2ScM (M = P and As) compounds are mechanically\\nstable up to 60 GPa.\\n',\n",
       " \"  Lowpass envelope approximation of smooth continuous-variable signals are\\nintroduced in this work. Envelope approximations are necessary when a given\\nsignal has to be approximated always to a larger value (such as in TV white\\nspace protection regions). In this work, a near-optimal approximate algorithm\\nfor finding a signal's envelope, while minimizing a mean-squared cost function,\\nis detailed. The sparse (lowpass) signal approximation is obtained in the\\nlinear Fourier series basis. This approximate algorithm works by discretizing\\nthe envelope property from an infinite number of points to a large (but finite)\\nnumber of points. It is shown that this approximate algorithm is near-optimal\\nand can be solved by using efficient convex optimization programs available in\\nthe literature. Simulation results are provided towards the end to gain more\\ninsights into the analytical results presented.\\n\",\n",
       " '  We study the spectral properties of curl, a linear differential operator of\\nfirst order acting on differential forms of appropriate degree on an\\nodd-dimensional closed oriented Riemannian manifold. In three dimensions its\\neigenvalues are the electromagnetic oscillation frequencies in vacuum without\\nexternal sources. In general, the spectrum consists of the eigenvalue 0 with\\ninfinite multiplicity and further real discrete eigenvalues of finite\\nmultiplicity. We compute the Weyl asymptotics and study the zeta-function. We\\ngive a sharp lower eigenvalue bound for positively curved manifolds and analyze\\nthe equality case. Finally, we compute the spectrum for flat tori, round\\nspheres and 3-dimensional spherical space forms.\\n',\n",
       " '  This paper presents a topology optimization framework for structural problems\\nsubjected to transient loading. The mechanical model assumes a linear elastic\\nisotropic material, infinitesimal strains, and a dynamic response. The\\noptimization problem is solved using the gradient-based optimizer Method of\\nMoving Asymptotes (MMA) with time-dependent sensitivities provided via the\\nadjoint method. The stiffness of materials is interpolated using the Solid\\nIsotropic Material with Penalization (SIMP) method and the Heaviside Projection\\nMethod (HPM) is used to stabilize the problem numerically and improve the\\nmanufacturability of the topology-optimized designs. Both static and dynamic\\noptimization examples are considered here. The resulting optimized designs\\ndemonstrate the ability of topology optimization to tailor the transient\\nresponse of structures.\\n',\n",
       " \"  It is pointed out that the generalized Lambert series\\n$\\\\displaystyle\\\\sum_{n=1}^{\\\\infty}\\\\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by\\nKanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's\\nLost Notebook in a slightly more general form. We extend an important\\ntransformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by\\nremoving restrictions on the parameters $N$ and $h$ that they impose. From our\\nextension we deduce a beautiful new generalization of Ramanujan's famous\\nformula for odd zeta values which, for $N$ odd and $m>0$, gives a relation\\nbetween $\\\\zeta(2m+1)$ and $\\\\zeta(2Nm+1)$. A result complementary to the\\naforementioned generalization is obtained for any even $N$ and\\n$m\\\\in\\\\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded\\nas a formula for $\\\\zeta\\\\left(2m+1-\\\\frac{1}{N}\\\\right)$. Applications of these\\ntransformations include a generalization of the transformation for the\\nlogarithm of Dedekind eta-function $\\\\eta(z)$, Zudilin- and Rivoal-type results\\non transcendence of certain values, and a transcendence criterion for Euler's\\nconstant $\\\\gamma$.\\n\",\n",
       " \"  Motivated by applications in cancer genomics and following the work of\\nHajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, to\\nappear) introduced the minimum conflict-free row split (MCRS) problem: split\\neach row of a given binary matrix into a bitwise OR of a set of rows so that\\nthe resulting matrix corresponds to a perfect phylogeny and has the minimum\\npossible number of rows among all matrices with this property. Hajirasouliha\\nand Raphael also proposed the study of a similar problem, in which the task is\\nto minimize the number of distinct rows of the resulting matrix. Hujdurović\\net al. proved that both problems are NP-hard, gave a related characterization\\nof transitively orientable graphs, and proposed a polynomial-time heuristic\\nalgorithm for the MCRS problem based on coloring cocomparability graphs.\\nWe give new, more transparent formulations of the two problems, showing that\\nthe problems are equivalent to two optimization problems on branchings in a\\nderived directed acyclic graph. Building on these formulations, we obtain new\\nresults on the two problems, including: (i) a strengthening of the heuristic by\\nHujdurović et al. via a new min-max result in digraphs generalizing\\nDilworth's theorem, which may be of independent interest, (ii) APX-hardness\\nresults for both problems, (iii) approximation algorithms, and (iv)\\nexponential-time algorithms solving the two problems to optimality faster than\\nthe naïve brute-force approach. Our work relates to several well studied\\nnotions in combinatorial optimization: chain partitions in partially ordered\\nsets, laminar hypergraphs, and (classical and weighted) colorings of graphs.\\n\",\n",
       " '  Smart cities are a growing trend in many cities in Argentina. In particular,\\nthe so-called intermediate cities present a context and requirements different\\nfrom those of large cities with respect to smart cities. One aspect of\\nrelevance is to encourage the development of applications (generally for mobile\\ndevices) that enable citizens to take advantage of data and services normally\\nassociated with the city, for example, in the urban mobility domain. In this\\nwork, a platform is proposed for intermediate cities that provide \"high level\"\\nservices and that allow the construction of software applications that consume\\nthose services. Our platform-centric strategy focused aims to integrate systems\\nand heterogeneous data sources, and provide \"intelligent\" services to different\\napplications. Examples of these services include: construction of user\\nprofiles, recommending local events, and collaborative sensing based on data\\nmining techniques, among others. In this work, the design of this platform\\n(currently in progress) is described, and experiences of applications for urban\\nmobility are discussed, which are being migrated in the form of reusable\\nservices provided by the platform\\n',\n",
       " '  Bayesian estimation is increasingly popular for performing model based\\ninference to support policymaking. These data are often collected from surveys\\nunder informative sampling designs where subject inclusion probabilities are\\ndesigned to be correlated with the response variable of interest. Sampling\\nweights constructed from marginal inclusion probabilities are typically used to\\nform an exponentiated pseudo likelihood that adjusts the population likelihood\\nfor estimation on the sample due to ease-of-estimation. We propose an\\nalternative adjustment based on a Bayes rule construction that simultaneously\\nperforms weight smoothing and estimates the population model parameters in a\\nfully Bayesian construction. We formulate conditions on known marginal and\\npairwise inclusion probabilities that define a class of sampling designs where\\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\\nperformances between the two approaches on synthetic data, which reveals that\\nour fully Bayesian approach better estimates posterior uncertainty without a\\nrequirement to calibrate the normalization of the sampling weights. We\\ndemonstrate our method on an application concerning the National Health and\\nNutrition Examination Survey exploring the relationship between caffeine\\nconsumption and systolic blood pressure.\\n',\n",
       " '  Reaction-diffusion equations appear in biology and chemistry, and combine\\nlinear diffusion with different kind of reaction terms. Some of them are\\nremarkable from the mathematical point of view, since they admit families of\\ntravelling waves that describe the asymptotic behaviour of a larger class of\\nsolutions $0\\\\leq u(x,t)\\\\leq 1$ of the problem posed in the real line. We\\ninvestigate here the existence of waves with constant propagation speed, when\\nthe linear diffusion is replaced by the \"slow\" doubly nonlinear diffusion. In\\nthe present setting we consider bistable reaction terms, which present\\ninteresting differences w.r.t. the Fisher-KPP framework recently studied in\\n\\\\cite{AA-JLV:art}. We find different families of travelling waves that are\\nemployed to describe the wave propagation of more general solutions and to\\nstudy the stability/instability of the steady states, even when we extend the\\nstudy to several space dimensions. A similar study is performed in the critical\\ncase that we call \"pseudo-linear\", i.e., when the operator is still nonlinear\\nbut has homogeneity one. With respect to the classical model and the\\n\"pseudo-linear\" case, the travelling waves of the \"slow\" diffusion setting\\nexhibit free boundaries. \\\\\\\\ Finally, as a complement of \\\\cite{AA-JLV:art}, we\\nstudy the asymptotic behaviour of more general solutions in the presence of a\\n\"heterozygote superior\" reaction function and doubly nonlinear diffusion\\n(\"slow\" and \"pseudo-linear\").\\n',\n",
       " \"  Drivable free space information is vital for autonomous vehicles that have to\\nplan evasive maneuvers in real-time. In this paper, we present a new efficient\\nmethod for environmental free space detection with laser scanner based on 2D\\noccupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems\\n(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced\\ninverse sensor model tailored for high-resolution laser scanners for building\\nOGM. It compensates the unreflected beams and deals with the ray casting to\\ngrid cells accuracy and computational effort problems. Secondly, we introduce\\nthe 'vehicle on a circle for grid maps' map alignment algorithm that allows\\nbuilding more accurate local maps by avoiding the computationally expensive\\ninaccurate operations of image sub-pixel shifting and rotation. The resulted\\ngrid map is more convenient for ADAS features than existing methods, as it\\nallows using less memory sizes, and hence, results into a better real-time\\nperformance. Thirdly, we present an algorithm to detect what we call the\\n'in-sight edges'. These edges guarantee modeling the free space area with a\\nsingle polygon of a fixed number of vertices regardless the driving situation\\nand map complexity. The results from real world experiments show the\\neffectiveness of our approach.\\n\",\n",
       " \"  In this paper, the fundamental problem of distribution and proactive caching\\nof computing tasks in fog networks is studied under latency and reliability\\nconstraints. In the proposed scenario, computing can be executed either locally\\nat the user device or offloaded to an edge cloudlet. Moreover, cloudlets\\nexploit both their computing and storage capabilities by proactively caching\\npopular task computation results to minimize computing latency. To this end, a\\nclustering method to group spatially proximate user devices with mutual task\\npopularity interests and their serving cloudlets is proposed. Then, cloudlets\\ncan proactively cache the popular tasks' computations of their cluster members\\nto minimize computing latency. Additionally, the problem of distributing tasks\\nto cloudlets is formulated as a matching game in which a cost function of\\ncomputing delay is minimized under latency and reliability constraints.\\nSimulation results show that the proposed scheme guarantees reliable\\ncomputations with bounded latency and achieves up to 91% decrease in computing\\nlatency as compared to baseline schemes.\\n\",\n",
       " '  High-mass stars are expected to form from dense prestellar cores. Their\\nprecise formation conditions are widely discussed, including their virial\\ncondition, which results in slow collapse for super-virial cores with strong\\nsupport by turbulence or magnetic fields, or fast collapse for sub-virial\\nsources. To disentangle their formation processes, measurements of the\\ndeuterium fractions are frequently employed to approximately estimate the ages\\nof these cores and to obtain constraints on their dynamical evolution. We here\\npresent 3D magneto-hydrodynamical simulations including for the first time an\\naccurate non-equilibrium chemical network with 21 gas-phase species plus dust\\ngrains and 213 reactions. With this network we model the deuteration process in\\nfully depleted prestellar cores in great detail and determine its response to\\nvariations in the initial conditions. We explore the dependence on the initial\\ngas column density, the turbulent Mach number, the mass-to-magnetic flux ratio\\nand the distribution of the magnetic field, as well as the initial\\northo-to-para ratio of H2. We find excellent agreement with recent observations\\nof deuterium fractions in quiescent sources. Our results show that deuteration\\nis rather efficient, even when assuming a conservative ortho-to-para ratio of 3\\nand highly sub-virial initial conditions, leading to large deuterium fractions\\nalready within roughly a free-fall time. We discuss the implications of our\\nresults and give an outlook to relevant future investigations.\\n',\n",
       " '  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)\\nsimulations and among the most widely used computational kernels in science.\\nThe potential models atomistic attraction and repulsion with century old\\nprescribed parameters ($q=6, \\\\; p=12$, respectively), originally related by a\\nfactor of two for simplicity of calculations. We re-examine the value of the\\nrepulsion exponent through data driven uncertainty quantification. We perform\\nHierarchical Bayesian inference on MD simulations of argon using experimental\\ndata of the radial distribution function (RDF) for a range of thermodynamic\\nconditions, as well as dimer interaction energies from quantum mechanics\\nsimulations. The experimental data suggest a repulsion exponent ($p \\\\approx\\n6.5$), in contrast to the quantum simulations data that support values closer\\nto the original ($p=12$) exponent. Most notably, we find that predictions of\\nRDF, diffusion coefficient and density of argon are more accurate and robust in\\nproducing the correct argon phase around its triple point, when using the\\nvalues inferred from experimental data over those from quantum mechanics\\nsimulations. The present results suggest the need for data driven recalibration\\nof the LJ potential across MD simulations.\\n',\n",
       " '  Thompson sampling has emerged as an effective heuristic for a broad range of\\nonline decision problems. In its basic form, the algorithm requires computing\\nand sampling from a posterior distribution over models, which is tractable only\\nfor simple special cases. This paper develops ensemble sampling, which aims to\\napproximate Thompson sampling while maintaining tractability even in the face\\nof complex models such as neural networks. Ensemble sampling dramatically\\nexpands on the range of applications for which Thompson sampling is viable. We\\nestablish a theoretical basis that supports the approach and present\\ncomputational results that offer further insight.\\n',\n",
       " '  In recent years, Deep Learning has become the go-to solution for a broad\\nrange of applications, often outperforming state-of-the-art. However, it is\\nimportant, for both theoreticians and practitioners, to gain a deeper\\nunderstanding of the difficulties and limitations associated with common\\napproaches and algorithms. We describe four types of simple problems, for which\\nthe gradient-based algorithms commonly used in deep learning either fail or\\nsuffer from significant difficulties. We illustrate the failures through\\npractical experiments, and provide theoretical insights explaining their\\nsource, and how they might be remedied.\\n',\n",
       " '  Proxima Centauri is known as the closest star from the Sun. Recently, radial\\nvelocity observations revealed the existence of an Earth-mass planet around it.\\nWith an orbital period of ~11 days, the surface of Proxima Centauri b is\\ntemperate and might be habitable. We took a photometric monitoring campaign to\\nsearch for its transit, using the Bright Star Survey Telescope at the Zhongshan\\nStation in Antarctica. A transit-like signal appearing on 2016 September 8th,\\nis identified tentatively. Its midtime, $T_{C}=2,457,640.1990\\\\pm0.0017$ HJD, is\\nconsistent with the predicted ephemeris based on RV orbit in a 1$\\\\sigma$\\nconfidence interval. Time-correlated noise is pronounced in the light curve of\\nProxima Centauri, affecting detection of transits. We develop a technique, in a\\nGaussian process framework, to gauge the statistical significance of potential\\ntransit detection. The tentative transit signal reported here, has a confidence\\nlevel of $2.5\\\\sigma$. Further detection of its periodic signals is necessary to\\nconfirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima\\nCentauri in next Polar night at Dome A in Antarctica, taking the advantage of\\ncontinuous darkness. \\\\citet{Kipping17} reported two tentative transit-like\\nsignals of Proxima Centauri b, observed by the Microvariability and Oscillation\\nof Stars space Telescope in 2014 and 2015, respectively. The midtransit time of\\nour detection is 138 minutes later than that predicted by their transit\\nephemeris. If all the signals are real transits, the misalignment of the epochs\\nplausibly suggests transit timing variations of Proxima Centauri b induced by\\nan outer planet in this system.\\n',\n",
       " '  In this paper, we propose an optimization-based sparse learning approach to\\nidentify the set of most influential reactions in a chemical reaction network.\\nThis reduced set of reactions is then employed to construct a reduced chemical\\nreaction mechanism, which is relevant to chemical interaction network modeling.\\nThe problem of identifying influential reactions is first formulated as a\\nmixed-integer quadratic program, and then a relaxation method is leveraged to\\nreduce the computational complexity of our approach. Qualitative and\\nquantitative validation of the sparse encoding approach demonstrates that the\\nmodel captures important network structural properties with moderate\\ncomputational load.\\n',\n",
       " '  Using density-functional theory calculations, we analyze the optical\\nabsorption properties of lead (Pb)-free metal halide perovskites\\n(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or\\nmonovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent\\nmetal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is\\nnot Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions\\nbecause of their indirect bandgap nature. Among the nine possible types of\\nPb-free metal halide double perovskites, six have direct bandgaps. Of these six\\ntypes, four show inversion symmetry-induced parity-forbidden or weak\\ntransitions between band edges, making them not ideal for thin-film solar cell\\napplication. Only one type of Pb-free double perovskite shows optical\\nabsorption and electronic properties suitable for solar cell applications,\\nnamely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide\\nimportant insights for designing new metal halide perovskites and double\\nperovskites for optoelectronic applications.\\n',\n",
       " '  MicroRNAs play important roles in many biological processes. Their aberrant\\nexpression can have oncogenic or tumor suppressor function directly\\nparticipating to carcinogenesis, malignant transformation, invasiveness and\\nmetastasis. Indeed, miRNA profiles can distinguish not only between normal and\\ncancerous tissue but they can also successfully classify different subtypes of\\na particular cancer. Here, we focus on a particular class of transcripts\\nencoding polycistronic miRNA genes that yields multiple miRNA components. We\\ndescribe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully\\nredesigned release of the MMRA computational pipeline (MiRNA Master Regulator\\nAnalysis), developed to search for clustered miRNAs potentially driving cancer\\nmolecular subtyping. Genomically clustered miRNAs are frequently co-expressed\\nto target different components of pro-tumorigenic signalling pathways. By\\napplying ClustMMRA to breast cancer patient data, we identified key miRNA\\nclusters driving the phenotype of different tumor subgroups. The pipeline was\\napplied to two independent breast cancer datasets, providing statistically\\nconcordant results between the two analysis. We validated in cell lines the\\nmiR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative\\nsubtype phenotype through its control of proliferation and EMT.\\n',\n",
       " '  We present a simultaneous localization and mapping (SLAM) algorithm that is\\nbased on radio signals and the association of specular multipath components\\n(MPCs) with geometric features. Especially in indoor scenarios, robust\\nlocalization from radio signals is challenging due to diffuse multipath\\npropagation, unknown MPC-feature association, and limited visibility of\\nfeatures. In our approach, specular reflections at flat surfaces are described\\nin terms of virtual anchors (VAs) that are mirror images of the physical\\nanchors (PAs). The positions of these VAs and possibly also of the PAs are\\nunknown. We develop a Bayesian model of the SLAM problem including the unknown\\nMPC-VA/PA association. We represent this model by a factor graph, which enables\\nthe use of the belief propagation (BP) scheme for efficient marginalization of\\nthe joint posterior distribution. The resulting BP-based SLAM algorithm detects\\nthe VAs associated with the PAs and estimates jointly the time-varying position\\nof the mobile agent and the positions of the VAs and possibly also of the PAs,\\nthereby leveraging the MPCs in the radio signal for improved accuracy and\\nrobustness of agent localization. A core aspect of the algorithm is BP-based\\nprobabilistic MPC-VA/PA association. Moreover, for improved initialization of\\nnew VA positions, the states of unobserved potential VAs are modeled as a\\nrandom finite set and propagated in time by means of a \"zero-measurement\"\\nprobability hypothesis density filter. The proposed BP-based SLAM algorithm has\\na low computational complexity and scales well in all relevant system\\nparameters. Experimental results using both synthetically generated\\nmeasurements and real ultra-wideband radio signals demonstrate the excellent\\nperformance of the algorithm in challenging indoor environments.\\n',\n",
       " '  This survey is about old and new results about the modular representation\\ntheory of finite reductive groups with a strong emphasis on local methods. This\\nincludes subpairs, Brauer\\'s Main Theorems, fusion, Rickard equivalences. In the\\ndefining characteristic we describe the relation between $p$-local subgroups\\nand parabolic subgroups, then give classical consequences on simple modules and\\nblocks, including the Alperin weight conjecture in that case. In the\\nnon-defining characteristics, we sketch a picture of the local methods\\npioneered by Fong-Srinivasan in the determination of blocks and their ordinary\\ncharacters. This includes the relationship with Lusztig\\'s twisted induction and\\nthe determination of defect groups. We conclude with a survey of the results\\nand methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks\\nthat preserve defect groups and the local structures.\\nThe text grew out of the course and talks given by the author in July and\\nSeptember 2016 during the program \"Local representation theory and simple\\ngroups\" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume\\npublished by EMS.\\n',\n",
       " '  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is\\npresented in this paper where the motion planning to realize \"human-like\"\\nautonomous navigation and manipulation tasks is studied. Firstly, an improved\\nMaxiMin NSGA-II algorithm, which optimizes five objective functions to solve\\nthe problems of singularity, redundancy, and coupling between mobile base and\\nmanipulator simultaneously, is proposed to design the optimal pose to\\nmanipulate the target object. Then, in order to link the initial pose and that\\noptimal pose, an off-line motion planning algorithm is designed. In detail, an\\nefficient direct-connect bidirectional RRT and gradient descent algorithm is\\nproposed to reduce the sampled nodes largely, and a geometric optimization\\nmethod is proposed for path pruning. Besides, head forward behaviors are\\nrealized by calculating the reasonable orientations and assigning them to the\\nmobile base to improve the quality of human-robot interaction. Thirdly, the\\nextension to on-line planning is done by introducing real-time sensing,\\ncollision-test and control cycles to update robotic motion in dynamic\\nenvironments. Fourthly, an EEs\\' via-point-based multi-objective genetic\\nalgorithm is proposed to design the \"human-like\" via-poses by optimizing four\\nobjective functions. Finally, numerous simulations are presented to validate\\nthe effectiveness of proposed algorithms.\\n',\n",
       " '  This article discusses a framework to support the design and end-to-end\\nplanning of fixed millimeter-wave networks. Compared to traditional techniques,\\nthe framework allows an organization to quickly plan a deployment in a\\ncost-effective way. We start by using LiDAR data---basically, a 3D point cloud\\ncaptured from a city---to estimate potential sites to deploy antennas and\\nwhether there is line-of-sight between them. With that data on hand, we use\\ncombinatorial optimization techniques to determine the optimal set of locations\\nand how they should communicate with each other, to satisfy engineering (e.g.,\\nlatency, polarity), design (e.g., reliability) and financial (e.g., total cost\\nof operation) constraints. The primary goal is to connect as many people as\\npossible to the network. Our methodology can be used for strategic planning\\nwhen an organization is in the process of deciding whether to adopt a\\nmillimeter-wave technology or choosing between locations, or for operational\\nplanning when conducting a detailed design of the actual network to be deployed\\nin a selected location.\\n',\n",
       " \"  The amount of ultraviolet irradiation and ablation experienced by a planet\\ndepends strongly on the temperature of its host star. Of the thousands of\\nextra-solar planets now known, only four giant planets have been found that\\ntransit hot, A-type stars (temperatures of 7300-10,000K), and none are known to\\ntransit even hotter B-type stars. WASP-33 is an A-type star with a temperature\\nof ~7430K, which hosts the hottest known transiting planet; the planet is\\nitself as hot as a red dwarf star of type M. The planet displays a large heat\\ndifferential between its day-side and night-side, and is highly inflated,\\ntraits that have been linked to high insolation. However, even at the\\ntemperature of WASP-33b's day-side, its atmosphere likely resembles the\\nmolecule-dominated atmospheres of other planets, and at the level of\\nultraviolet irradiation it experiences, its atmosphere is unlikely to be\\nsignificantly ablated over the lifetime of its star. Here we report\\nobservations of the bright star HD 195689, which reveal a close-in (orbital\\nperiod ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star\\nis at the dividing line between stars of type A and B, and we measure the\\nKELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar\\ntype K4. The molecules in K stars are entirely dissociated, and thus the\\nprimary sources of opacity in the day-side atmosphere of KELT-9b are likely\\natomic metals. Furthermore, KELT-9b receives ~700 times more extreme\\nultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,\\nleading to a predicted range of mass-loss rates that could leave the planet\\nlargely stripped of its envelope during the main-sequence lifetime of the host\\nstar.\\n\",\n",
       " \"  Online video services, messaging systems, games and social media services are\\ntremendously popular among young people and children in many countries. Most of\\nthe digital services offered on the internet are advertising funded, which\\nmakes advertising ubiquitous in children's everyday life. To understand the\\nimpact of advertising-based digital services on children, we study the\\ncollective behavior of users of YouTube for kids channels and present the\\ndemographics of a large number of users. We collected data from 12,848 videos\\nfrom 17 channels in US and UK and 24 channels in Brazil. The channels in\\nEnglish have been viewed more than 37 billion times. We also collected more\\nthan 14 million comments made by users. Based on a combination of text-analysis\\nand face recognition tools, we show the presence of racial and gender biases in\\nour large sample of users. We also identify children actively using YouTube,\\nalthough the minimum age for using the service is 13 years in most countries.\\nWe provide comparisons of user behavior among the three countries, which\\nrepresent large user populations in the global North and the global South.\\n\",\n",
       " '  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks\\nsatisfies cdh-descent. We apply this descent result to prove that if X is a\\nNoetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.\\nK_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).\\nOur descent and vanishing results apply more generally to certain Artin stacks\\nwhose stabilizers are extensions of finite group schemes by group schemes of\\nmultiplicative type.\\n',\n",
       " '  Screened modified gravity (SMG) is a kind of scalar-tensor theory with\\nscreening mechanisms, which can suppress the fifth force in dense regions and\\nallow theories to evade the solar system and laboratory tests. In this paper,\\nwe investigate how the screening mechanisms in SMG affect the gravitational\\nradiation damping effects, calculate in detail the rate of the energy loss due\\nto the emission of tensor and scalar gravitational radiations, and derive their\\ncontributions to the change in the orbital period of the binary system. We find\\nthat the scalar radiation depends on the screened parameters and the\\npropagation speed of scalar waves, and the scalar dipole radiation dominates\\nthe orbital decay of the binary system. For strongly self-gravitating bodies,\\nall effects of scalar sector are strongly suppressed by the screening\\nmechanisms in SMG. By comparing our results to observations of binary system\\nPSR J1738+0333, we place the stringent constraints on the screening mechanisms\\nin SMG. As an application of these results, we focus on three specific models\\nof SMG (chameleon, symmetron, and dilaton), and derive the constraints on the\\nmodel parameters, respectively.\\n',\n",
       " '  Selective weed treatment is a critical step in autonomous crop management as\\nrelated to crop health and yield. However, a key challenge is reliable, and\\naccurate weed detection to minimize damage to surrounding plants. In this\\npaper, we present an approach for dense semantic weed classification with\\nmultispectral images collected by a micro aerial vehicle (MAV). We use the\\nrecently developed encoder-decoder cascaded Convolutional Neural Network (CNN),\\nSegnet, that infers dense semantic classes while allowing any number of input\\nimage channels and class balancing with our sugar beet and weed datasets. To\\nobtain training datasets, we established an experimental field with varying\\nherbicide levels resulting in field plots containing only either crop or weed,\\nenabling us to use the Normalized Difference Vegetation Index (NDVI) as a\\ndistinguishable feature for automatic ground truth generation. We train 6\\nmodels with different numbers of input channels and condition (fine-tune) it to\\nachieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification\\nmetrics. For model deployment, an embedded GPU system (Jetson TX2) is tested\\nfor MAV integration. Dataset used in this paper is released to support the\\ncommunity and future work.\\n',\n",
       " '  Let $G:=\\\\widehat{SL_2}$ denote the affine Kac-Moody group associated to\\n$SL_2$ and $\\\\bar{\\\\mathcal{X}}$ the associated affine Grassmannian. We determine\\nan inductive formula for the Schubert basis structure constants in the\\ntorus-equivariant Grothendieck group of $\\\\bar{\\\\mathcal{X}}$. In the case of\\nordinary (non-equivariant) $K$-theory we find an explicit closed form for the\\nstructure constants. We also determine an inductive formula for the structure\\nconstants in the torus-equivariant cohomology ring, and use this formula to\\nfind closed forms for some of the structure constants.\\n',\n",
       " '  The existing measurement theory interprets the variance as the dispersion of\\nmeasured value, which is actually contrary to a general mathematical knowledge\\nthat the variance of a constant is 0. This paper will fully demonstrate that\\nthe variance in measurement theory is actually the evaluation of probability\\ninterval of an error instead of the dispersion of a measured value, point out\\nthe key point of mistake in the existing interpretation, and fully interpret a\\nseries of changes in conceptual logic and processing method brought about by\\nthis new concept.\\n',\n",
       " '  Researchers are often interested in analyzing conditional treatment effects.\\nOne variant of this is \"causal moderation,\" which implies that intervention\\nupon a third (moderator) variable would alter the treatment effect. This study\\npresents a generalized, non-parametric framework for estimating causal\\nmoderation effects given randomized treatments and non-randomized moderators\\nthat achieves a number of goals. First, it highlights how conventional\\napproaches do not constitute unbiased or consistent estimators of causal\\nmoderation effects. Second, it offers researchers a simple, transparent\\napproach for estimating causal moderation effects and lays out the assumptions\\nunder which this can be performed consistently and/or without bias. Third, as\\npart of the estimation process, it allows researchers to implement their\\npreferred method of covariate adjustment, including parametric and\\nnon-parametric methods, or alternative identification strategies of their\\nchoosing. Fourth, it provides a set-up whereby sensitivity analysis designed\\nfor the average-treatment-effect context can be extended to the moderation\\ncontext. An original application is also presented.\\n',\n",
       " '  This paper deals with some simple results about spherical functions of type\\n$\\\\delta$, namely new integral formulas, new results about behavior at infinity\\nand some facts about the related $C_\\\\sigma$ functions.\\n',\n",
       " '  In this study, we determine all modular curves $X_0(N)$ that admit infinitely\\nmany cubic points.\\n',\n",
       " \"  In the framework of multi-body dynamics, successive encounters with a third\\nbody, even if well outside of its sphere of influence, can noticeably alter the\\ntrajectory of a spacecraft. Examples of these effects have already been\\nexploited by past missions such as SMART-1, as well as are proposed to benefit\\nfuture missions to Jupiter, Saturn or Neptune, and disposal strategies from\\nEarth's High Eccentric or Libration Point Orbits. This paper revises three\\ntotally different descriptions of the effects of the third body gravitational\\nperturbation. These are the averaged dynamics of the classical third body\\nperturbing function, the Opik's close encounter theory and the Keplerian map\\napproach. The first two techniques have respectively been applied to the cases\\nof a spacecraft either always remaining very far or occasionally experiencing\\nextremely close approaches to the third body. However, the paper also seeks\\nsolutions for trajectories that undergo one or more close approaches at\\ndistances in the order of the sphere of influence of the third body. The paper\\nattempts to gain insight into the accuracy of these different perturbative\\ntechniques into each of these scenarios, as compared with the motion in the\\nCircular Restricted Three Body Problem.\\n\",\n",
       " '  Capsule Networks envision an innovative point of view about the\\nrepresentation of the objects in the brain and preserve the hierarchical\\nspatial relationships between them. This type of networks exhibits a huge\\npotential for several Machine Learning tasks like image classification, while\\noutperforming Convolutional Neural Networks (CNNs). A large body of work has\\nexplored adversarial examples for CNNs, but their efficacy to Capsule Networks\\nis not well explored. In our work, we study the vulnerabilities in Capsule\\nNetworks to adversarial attacks. These perturbations, added to the test inputs,\\nare small and imperceptible to humans, but fool the network to mis-predict. We\\npropose a greedy algorithm to automatically generate targeted imperceptible\\nadversarial examples in a black-box attack scenario. We show that this kind of\\nattacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),\\nmislead Capsule Networks. Moreover, we apply the same kind of adversarial\\nattacks to a 9-layer CNN and analyze the outcome, compared to the Capsule\\nNetworks to study their differences / commonalities.\\n',\n",
       " '  The effects of MHD boundary layer flow of non-linear thermal radiation with\\nconvective heat transfer and non-uniform heat source/sink in presence of\\nthermophortic velocity and chemical reaction investigated in this study.\\nSuitable similarity transformation are used to solve the partial ordinary\\ndifferential equation of considered governing flow. Runge-Kutta fourth fifth\\norder Fehlberg method with shooting techniques are used to solved\\nnon-dimensional governing equations. The variation of different parameters such\\nas thermophoretic parameter, chemical reaction parameter, non- uniform heat\\nsource/sink parameters are studied on velocity, temperature and concentration\\nprofiles, and are described by suitable graphs and tables. The obtained results\\nare in very well agreement with previous results.\\n',\n",
       " '  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily\\npractice and academic research stay discreet compared to resting-state BOLD.\\nHowever, by giving direct access to cerebral blood flow maps, rs-fASL leads to\\nsignificant clinical subject scaled application as CBF can be considered as a\\nbiomarker in common neuropathology. Our work here focuses on the link between\\noverall quality of rs-fASL and duration of acquisition. To this end, we\\nconsider subject self-Default Mode Network (DMN), and assess DMN quality\\ndepletion compared to a gold standard DMN depending on the duration of\\nacquisition.\\n',\n",
       " \"  We propose a novel end-to-end neural network architecture that, once trained,\\ndirectly outputs a probabilistic clustering of a batch of input examples in one\\npass. It estimates a distribution over the number of clusters $k$, and for each\\n$1 \\\\leq k \\\\leq k_\\\\mathrm{max}$, a distribution over the individual cluster\\nassignment for each data point. The network is trained in advance in a\\nsupervised fashion on separate data to learn grouping by any perceptual\\nsimilarity criterion based on pairwise labels (same/different group). It can\\nthen be applied to different data containing different groups. We demonstrate\\npromising performance on high-dimensional data like images (COIL-100) and\\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\\ndifference to deep metric learning, semi-supervise clustering and other related\\napproaches while having the advantage of performing learnable clustering fully\\nend-to-end.\\n\",\n",
       " '  The collective magnetic excitations in the spin-orbit Mott insulator\\n(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\\\\,0.01,\\\\,0.04,\\\\, 0.1$) were investigated by\\nmeans of resonant inelastic x-ray scattering. We report significant magnon\\nenergy gaps at both the crystallographic and antiferromagnetic zone centers at\\nall doping levels, along with a remarkably pronounced momentum-dependent\\nlifetime broadening. The spin-wave gap is accounted for by a significant\\nanisotropy in the interactions between $J_\\\\text{eff}=1/2$ isospins, thus\\nmarking the departure of Sr$_2$IrO$_4$ from the essentially isotropic\\nHeisenberg model appropriate for the superconducting cuprates.\\n',\n",
       " '  We report the proximity induced anomalous transport behavior in a Nb\\nBi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of\\nBi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm\\nthick Niobium superconductor using DC magnetron sputtering by shadow masking\\ntechnique. The magnetotransport (MR) measurements have been performed\\nsimultaneously on the TI sample with and without Nb top layer in the\\ntemperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI\\nregion shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous\\nlinear change in resistance is observed in the field range of negative 4T to\\npositive 4T at which Nb is superconducting. At 0 T field, the temperature\\ndependence of resistance on the Nb covered region revealed a superconducting\\ntransition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of\\nzero resistance states due to the additional resistance from superconductor\\n(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed\\nan enhancement in resistance for positive field and prominent fall in\\nresistance for negative field direction. This indicates the directional\\ndependent scattering of the Cooper pairs on the surface of the TI due to the\\nsuperposition of spin singlet and triplet states in the superconductor and TI\\nrespectively.\\n',\n",
       " '  Networked control systems (NCS) have attracted considerable attention in\\nrecent years. While the stabilizability and optimal control of NCS for a given\\ncommunication system has already been studied extensively, the design of the\\ncommunication system for NCS has recently seen an increase in more thorough\\ninvestigation. In this paper, we address an optimal scheduling problem for a\\nset of NCS sharing a dedicated communication channel, providing performance\\nbounds and asymptotic stability. We derive a suboptimal scheduling policy with\\ndynamic state-based priorities calculated at the sensors, which are then used\\nfor stateless priority queuing in the network, making it both scalable and\\nefficient to implement on routers or multi-layer switches. These properties are\\nbeneficial towards leveraging existing IP networks for control, which will be a\\ncrucial factor for the proliferation of wide-area NCS applications. By allowing\\nfor an arbitrary number of concurrent transmissions, we are able to investigate\\nthe relationship between available bandwidth, transmission rate, and delay. To\\ndemonstrate the feasibility of our approach, we provide a proof-of-concept\\nimplementation of the priority scheduler using real networking hardware.\\n',\n",
       " \"  Given a property of representations satisfying a basic stability condition,\\nRamakrishna developed a variant of Mazur's Galois deformation theory for\\nrepresentations with that property. We introduce an axiomatic definition of\\npseudorepresentations with such a property. Among other things, we show that\\npseudorepresentations with a property enjoy a good deformation theory,\\ngeneralizing Ramakrishna's theory to pseudorepresentations.\\n\",\n",
       " '  A novel adaptive local surface refinement technique based on Locally Refined\\nNon-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model\\ncomplex geometries exactly and are the rational extension of LR B-splines. The\\nlocal representation of the parameter space overcomes the drawback of\\nnon-existent local refinement in standard NURBS-based isogeometric analysis.\\nFor a convenient embedding into general finite element code, the Bézier\\nextraction operator for LR NURBS is formulated. An automatic remeshing\\ntechnique is presented that allows adaptive local refinement and coarsening of\\nLR NURBS. In this work, LR NURBS are applied to contact computations of 3D\\nsolids and membranes. For solids, LR NURBS-enriched finite elements are used to\\ndiscretize the contact surfaces with LR NURBS finite elements, while the rest\\nof the body is discretized by linear Lagrange finite elements. For membranes,\\nthe entire surface is discretized by LR NURBS. Various numerical examples are\\nshown, and they demonstrate the benefit of using LR NURBS: Compared to uniform\\nrefinement, LR NURBS can achieve high accuracy at lower computational cost.\\n',\n",
       " '  This paper introduces a general method to approximate the convolution of an\\narbitrary program with a Gaussian kernel. This process has the effect of\\nsmoothing out a program. Our compiler framework models intermediate values in\\nthe program as random variables, by using mean and variance statistics. Our\\napproach breaks the input program into parts and relates the statistics of the\\ndifferent parts, under the smoothing process. We give several approximations\\nthat can be used for the different parts of the program. These include the\\napproximation of Dorn et al., a novel adaptive Gaussian approximation, Monte\\nCarlo sampling, and compactly supported kernels. Our adaptive Gaussian\\napproximation is accurate up to the second order in the standard deviation of\\nthe smoothing kernel, and mathematically smooth. We show how to construct a\\ncompiler that applies chosen approximations to given parts of the input\\nprogram. Because each expression can have multiple approximation choices, we\\nuse a genetic search to automatically select the best approximations. We apply\\nthis framework to the problem of automatically bandlimiting procedural shader\\nprograms. We evaluate our method on a variety of complex shaders, including\\nshaders with parallax mapping, animation, and spatially varying statistics. The\\nresulting smoothed shader programs outperform previous approaches both\\nnumerically, and aesthetically, due to the smoothing properties of our\\napproximations.\\n',\n",
       " '  Large-scale computational experiments, often running over weeks and over\\nlarge datasets, are used extensively in fields such as epidemiology,\\nmeteorology, computational biology, and healthcare to understand phenomena, and\\ndesign high-stakes policies affecting everyday health and economy. For\\ninstance, the OpenMalaria framework is a computationally-intensive simulation\\nused by various non-governmental and governmental agencies to understand\\nmalarial disease spread and effectiveness of intervention strategies, and\\nsubsequently design healthcare policies. Given that such shared results form\\nthe basis of inferences drawn, technological solutions designed, and day-to-day\\npolicies drafted, it is essential that the computations are validated and\\ntrusted. In particular, in a multi-agent environment involving several\\nindependent computing agents, a notion of trust in results generated by peers\\nis critical in facilitating transparency, accountability, and collaboration.\\nUsing a novel combination of distributed validation of atomic computation\\nblocks and a blockchain-based immutable audits mechanism, this work proposes a\\nuniversal framework for distributed trust in computations. In particular we\\naddress the scalaibility problem by reducing the storage and communication\\ncosts using a lossy compression scheme. This framework guarantees not only\\nverifiability of final results, but also the validity of local computations,\\nand its cost-benefit tradeoffs are studied using a synthetic example of\\ntraining a neural network.\\n',\n",
       " '  We provide an overview of several non-linear activation functions in a neural\\nnetwork architecture that have proven successful in many machine learning\\napplications. We conduct an empirical analysis on the effectiveness of using\\nthese function on the MNIST classification task, with the aim of clarifying\\nwhich functions produce the best results overall. Based on this first set of\\nresults, we examine the effects of building deeper architectures with an\\nincreasing number of hidden layers. We also survey the impact of using, on the\\nsame task, different initialisation schemes for the weights of our neural\\nnetwork. Using these sets of experiments as a base, we conclude by providing a\\noptimal neural network architecture that yields impressive results in accuracy\\non the MNIST classification task.\\n',\n",
       " '  Advancements in deep learning over the years have attracted research into how\\ndeep artificial neural networks can be used in robotic systems. This research\\nsurvey will present a summarization of the current research with a specific\\nfocus on the gains and obstacles for deep learning to be applied to mobile\\nrobotics.\\n',\n",
       " \"  Our desire and fascination with intelligent machines dates back to the\\nantiquity's mythical automaton Talos, Aristotle's mode of mechanical thought\\n(syllogism) and Heron of Alexandria's mechanical machines and automata.\\nHowever, the quest for Artificial General Intelligence (AGI) is troubled with\\nrepeated failures of strategies and approaches throughout the history. This\\ndecade has seen a shift in interest towards bio-inspired software and hardware,\\nwith the assumption that such mimicry entails intelligence. Though these steps\\nare fruitful in certain directions and have advanced automation, their singular\\ndesign focus renders them highly inefficient in achieving AGI. Which set of\\nrequirements have to be met in the design of AGI? What are the limits in the\\ndesign of the artificial? Here, a careful examination of computation in\\nbiological systems hints that evolutionary tinkering of contextual processing\\nof information enabled by a hierarchical architecture is the key to build AGI.\\n\",\n",
       " '  Conventional crystalline magnets are characterized by symmetry breaking and\\nnormal modes of excitation called magnons with quantized angular momentum\\n$\\\\hbar$. Neutron scattering correspondingly features extra magnetic Bragg\\ndiffraction at low temperatures and dispersive inelastic scattering associated\\nwith single magnon creation and annihilation. Exceptions are anticipated in\\nso-called quantum spin liquids as exemplified by the one-dimensional spin-1/2\\nchain which has no magnetic order and where magnons accordingly fractionalize\\ninto spinons with angular momentum $\\\\hbar/2$. This is spectacularly revealed by\\na continuum of inelastic neutron scattering associated with two-spinon\\nprocesses and the absence of magnetic Bragg diffraction. Here, we report\\nevidence for these same key features of a quantum spin liquid in the\\nthree-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific\\nheat and neutron scattering measurements, Monte Carlo simulations, and analytic\\napproximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is\\nan almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on\\na pyrochlore lattice with weak connectivity and frustrated interactions.\\nMagnetic Bragg diffraction is absent and 90\\\\% of the spectral weight forms a\\ncontinuum of magnetic scattering not dissimilar to that of the spin-1/2 chain\\nbut with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb\\nphase. The residual entropy and diffuse elastic scattering points to an exotic\\nstate of matter driven by frustration, quantum fluctuations and weak exchange\\ndisorder.\\n',\n",
       " '  We introduce the new version of SimProp, a Monte Carlo code for simulating\\nthe propagation of ultra-high energy cosmic rays in intergalactic space. This\\nversion, SimProp v2r4, together with an overall improvement of the code\\ncapabilities with a substantial reduction in the computation time, also\\ncomputes secondary cosmogenic particles such as electron-positron pairs and\\ngamma rays produced during the propagation of ultra-high energy cosmic rays. As\\nrecently pointed out by several authors, the flux of this secondary radiation\\nand its products, within reach of the current observatories, provides useful\\ninformation about models of ultra-high energy cosmic ray sources which would be\\nhard to discriminate otherwise.\\n',\n",
       " \"  Given a collection of data points, non-negative matrix factorization (NMF)\\nsuggests to express them as convex combinations of a small set of `archetypes'\\nwith non-negative entries. This decomposition is unique only if the true\\narchetypes are non-negative and sufficiently sparse (or the weights are\\nsufficiently sparse), a regime that is captured by the separability condition\\nand its generalizations.\\nIn this paper, we study an approach to NMF that can be traced back to the\\nwork of Cutler and Breiman (1994) and does not require the data to be\\nseparable, while providing a generally unique decomposition. We optimize the\\ntrade-off between two objectives: we minimize the distance of the data points\\nfrom the convex envelope of the archetypes (which can be interpreted as an\\nempirical risk), while minimizing the distance of the archetypes from the\\nconvex envelope of the data (which can be interpreted as a data-dependent\\nregularization). The archetypal analysis method of (Cutler, Breiman, 1994) is\\nrecovered as the limiting case in which the last term is given infinite weight.\\nWe introduce a `uniqueness condition' on the data which is necessary for\\nexactly recovering the archetypes from noiseless data. We prove that, under\\nuniqueness (plus additional regularity conditions on the geometry of the\\narchetypes), our estimator is robust. While our approach requires solving a\\nnon-convex optimization problem, we find that standard optimization methods\\nsucceed in finding good solutions both for real and synthetic data.\\n\",\n",
       " \"  Muon reconstruction in the Daya Bay water pools would serve to verify the\\nsimulated muon fluxes and offer the possibility of studying cosmic muons in\\ngeneral. This reconstruction is, however, complicated by many optical obstacles\\nand the small coverage of photomultiplier tubes (PMTs) as compared to other\\nlarge water Cherenkov detectors. The PMTs' timing information is useful only in\\nthe case of direct, unreflected Cherenkov light. This requires PMTs to be added\\nand removed as an hypothesized muon trajectory is iteratively improved, to\\naccount for the changing effects of obstacles and direction of light.\\nTherefore, muon reconstruction in the Daya Bay water pools does not lend itself\\nto a general fitting procedure employing smoothly varying functions with\\ncontinuous derivatives. Here, an algorithm is described which overcomes these\\ncomplications. It employs the method of Least Mean Squares to determine an\\nhypothesized trajectory from the PMTs' charge-weighted positions. This\\ninitially hypothesized trajectory is then iteratively refined using the PMTs'\\ntiming information. Reconstructions with simulated data reproduce the simulated\\ntrajectory to within about 5 degrees in direction and about 45 cm in position\\nat the pool surface, with a bias that tends to pull tracks away from the\\nvertical by about 3 degrees.\\n\",\n",
       " '  We present a method to improve the accuracy of a foot-mounted,\\nzero-velocity-aided inertial navigation system (INS) by varying estimator\\nparameters based on a real-time classification of motion type. We train a\\nsupport vector machine (SVM) classifier using inertial data recorded by a\\nsingle foot-mounted sensor to differentiate between six motion types (walking,\\njogging, running, sprinting, crouch-walking, and ladder-climbing) and report\\nmean test classification accuracy of over 90% on a dataset with five different\\nsubjects. From these motion types, we select two of the most common (walking\\nand running), and describe a method to compute optimal zero-velocity detection\\nparameters tailored to both a specific user and motion type by maximizing the\\ndetector F-score. By combining the motion classifier with a set of optimal\\ndetection parameters, we show how we can reduce INS position error during mixed\\nwalking and running motion. We evaluate our adaptive system on a total of 5.9\\nkm of indoor pedestrian navigation performed by five different subjects moving\\nalong a 130 m path with surveyed ground truth markers.\\n',\n",
       " '  The primary function of memory allocators is to allocate and deallocate\\nchunks of memory primarily through the malloc API. Many memory allocators also\\nimplement other API extensions, such as deriving the size of an allocated\\nobject from the object\\'s pointer, or calculating the base address of an\\nallocation from an interior pointer. In this paper, we propose a general\\npurpose extended allocator API built around these common extensions. We argue\\nthat such extended APIs have many applications and demonstrate several use\\ncases, such as (manual) memory error detection, meta data storage, typed\\npointers and compact data-structures. Because most existing allocators were not\\ndesigned for the extended API, traditional implementations are expensive or not\\npossible.\\nRecently, the LowFat allocator for heap and stack objects has been developed.\\nThe LowFat allocator is an implementation of the idea of low-fat pointers,\\nwhere object bounds information (size and base) are encoded into the native\\nmachine pointer representation itself. The \"killer app\" for low-fat pointers is\\nautomated bounds check instrumentation for program hardening and bug detection.\\nHowever, the LowFat allocator can also be used to implement highly optimized\\nversion of the extended allocator API, which makes the new applications (listed\\nabove) possible. In this paper, we implement and evaluate several applications\\nbased efficient memory allocator API extensions using low-fat pointers. We also\\nextend the LowFat allocator to cover global objects for the first time.\\n',\n",
       " '  We extend a data-based model-free multifractal method of exoplanet detection\\nto probe exoplanetary atmospheres. Whereas the transmission spectrum is studied\\nduring the primary eclipse, we analyze the emission spectrum during the\\nsecondary eclipse, thereby probing the atmospheric limb. In addition to the\\nspectral structure of exoplanet atmospheres, the approach provides information\\nto study phenomena such as atmospheric flows, tidal-locking behavior, and the\\ndayside-nightside redistribution of energy. The approach is demonstrated using\\nSpitzer data for exoplanet HD189733b. The central advantage of the method is\\nthe lack of model assumptions in the detection and observational schemes.\\n',\n",
       " '  We formulate and analyze a novel hypothesis testing problem for inferring the\\nedge structure of an infection graph. In our model, a disease spreads over a\\nnetwork via contagion or random infection, where the random variables governing\\nthe rates of contracting the disease from neighbors or random infection are\\nindependent exponential random variables with unknown rate parameters. A subset\\nof nodes is also censored uniformly at random. Given the statuses of nodes in\\nthe network, the goal is to determine the underlying graph. We present a\\nprocedure based on permutation testing, and we derive sufficient conditions for\\nthe validity of our test in terms of automorphism groups of the graphs\\ncorresponding to the null and alternative hypotheses. Further, the test is\\nvalid more generally for infection processes satisfying a basic symmetry\\ncondition. Our test is easy to compute and does not involve estimating unknown\\nparameters governing the process. We also derive risk bounds for our\\npermutation test in a variety of settings, and motivate our test statistic in\\nterms of approximate equivalence to likelihood ratio testing and maximin tests.\\nWe conclude with an application to real data from an HIV infection network.\\n',\n",
       " '  This work is motivated by a particular problem of a modern paper\\nmanufacturing industry, in which maximum efficiency of the fiber-filler\\nrecovery process is desired. A lot of unwanted materials along with valuable\\nfibers and fillers come out as a by-product of the paper manufacturing process\\nand mostly goes as waste. The job of an efficient Krofta supracell is to\\nseparate the unwanted materials from the valuable ones so that fibers and\\nfillers can be collected from the waste materials and reused in the\\nmanufacturing process. The efficiency of Krofta depends on several crucial\\nprocess parameters and monitoring them is a difficult proposition. To solve\\nthis problem, we propose a novel hybridization of regression trees (RT) and\\nartificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of\\nlow recovery percentage of the supracell. This model is used to achieve the\\ngoal of improving supracell efficiency, viz., gain in percentage recovery. In\\naddition, theoretical results for the universal consistency of the proposed\\nmodel are given with the optimal value of a vital model parameter. Experimental\\nfindings show that the proposed hybrid RT-ANN model achieves higher accuracy in\\npredicting Krofta recovery percentage than other conventional regression models\\nfor solving the Krofta efficiency problem. This work will help the paper\\nmanufacturing company to become environmentally friendly with minimal\\necological damage and improved waste recovery.\\n',\n",
       " '  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)\\ndetect images of the atmospheric showers generated by gamma rays and cosmic\\nrays as they are absorbed by the atmosphere. The much more frequent cosmic-ray\\nevents form the main background when looking for gamma-ray sources, and\\ntherefore IACT sensitivity is significantly driven by the capability to\\ndistinguish between these two types of events. Supervised learning algorithms,\\nlike random forests and boosted decision trees, have been shown to effectively\\nclassify IACT events. In this contribution we present results from exploratory\\nwork using deep learning as an event classification method for the Cherenkov\\nTelescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an\\ninternational project for a next-generation ground-based gamma-ray observatory,\\naiming to improve on the sensitivity of current-generation experiments by an\\norder of magnitude and provide energy coverage from 20 GeV to more than 300\\nTeV.\\n',\n",
       " '  Transition metal dichalcogenides (TMDs) are emerging as promising\\ntwo-dimensional (2d) semiconductors for optoelectronic and flexible devices.\\nHowever, a microscopic explanation of their photophysics -- of pivotal\\nimportance for the understanding and optimization of device operation -- is\\nstill lacking. Here we use femtosecond transient absorption spectroscopy, with\\npump pulse tunability and broadband probing, to monitor the relaxation dynamics\\nof single-layer MoS2 over the entire visible range, upon photoexcitation of\\ndifferent excitonic transitions. We find that, irrespective of excitation\\nphoton energy, the transient absorption spectrum shows the simultaneous\\nbleaching of all excitonic transitions and corresponding red-shifted\\nphotoinduced absorption bands. First-principle modeling of the ultrafast\\noptical response reveals that a transient bandgap renormalization, caused by\\nthe presence of photo-excited carriers, is primarily responsible for the\\nobserved features. Our results demonstrate the strong impact of many-body\\neffects in the transient optical response of TMDs even in the\\nlow-excitation-density regime.\\n',\n",
       " '  In a classical regression model, it is usually assumed that the explanatory\\nvariables are independent of each other and error terms are normally\\ndistributed. But when these assumptions are not met, situations like the error\\nterms are not independent or they are not identically distributed or both of\\nthese, LSE will not be robust. Hence, quantile regression has been used to\\ncomplement this deficiency of classical regression analysis and to improve the\\nleast square estimation (LSE). In this study, we consider preliminary test and\\nshrinkage estimation strategies for quantile regression models with\\nindependently and non-identically distributed (i.ni.d.) errors. A Monte Carlo\\nsimulation study is conducted to assess the relative performance of the\\nestimators. Also, we numerically compare their performance with Ridge, Lasso,\\nElastic Net penalty estimation strategies. A real data example is presented to\\nillustrate the usefulness of the suggested methods. Finally, we obtain the\\nasymptotic results of suggested estimators\\n',\n",
       " '  We present a clustering-based language model using word embeddings for text\\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\\nholds true for word embeddings whose training is done by observing word\\nco-occurrences. We argue that clustering with word embeddings in the metric\\nspace should yield feature representations in a higher semantic space\\nappropriate for text regression. Also, by representing features in terms of\\nhistograms, our approach can naturally address documents of varying lengths. An\\nempirical evaluation using the Common Core Standards corpus reveals that the\\nfeatures formed on our clustering-based language model significantly improve\\nthe previously known results for the same corpus in readability prediction. We\\nalso evaluate the task of sentence matching based on semantic relatedness using\\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\\nperformance.\\n',\n",
       " \"  We study special circle bundles over two elementary moduli spaces of\\nmeromorphic quadratic differentials with real periods denoted by $\\\\mathcal\\nQ_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$. The space\\n$\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ is the moduli space of meromorphic quadratic\\ndifferentials on the Riemann sphere with one pole of order 7 with real periods;\\nit appears naturally in the study of a neighbourhood of the Witten's cycle\\n$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic\\ndifferentials of $\\\\mathcal M_{g,n}$. The space $\\\\mathcal Q^{\\\\mathbb\\nR}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the\\nRiemann sphere with two poles of order at most 3 with real periods; it appears\\nin description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the\\ncombinatorial model. The application of the formalism of the Bergman\\ntau-function to the combinatorial model (with the goal of computing\\nanalytically Poincare dual cycles to certain combinations of tautological\\nclasses) requires the study of special sections of circle bundles over\\n$\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$; in the\\ncase of the space $\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ a section of this circle\\nbundle is given by the argument of the modular discriminant. We study the\\nspaces $\\\\mathcal Q_0^{\\\\mathbb R}(-7)$ and $\\\\mathcal Q^{\\\\mathbb R}_0([-3]^2)$,\\nalso called the spaces of Boutroux curves, in detail, together with\\ncorresponding circle bundles.\\n\",\n",
       " \"  We report a method to control the positions of ellipsoidal magnets in flowing\\nchannels of rectangular or circular cross section at low Reynolds number.A\\nstatic uniform magnetic field is used to pin the particle orientation, and the\\nparticles move with translational drift velocities resulting from hydrodynamic\\ninteractions with the channel walls which can be described using Blake's image\\ntensor.Building on his insights, we are able to present a far-field theory\\npredicting the particle motion in rectangular channels, and validate the\\naccuracy of the theory by comparing to numerical solutions using the boundary\\nelement method.We find that, by changing the direction of the applied magnetic\\nfield, the motion can be controlled so that particles move either to a curved\\nfocusing region or to the channel walls.We also use simulations to show that\\nthe particles are focused to a single line in a circular channel.Our results\\nsuggest ways to focus and segregate magnetic particles in lab-on-a-chip\\ndevices.\\n\",\n",
       " \"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\\\\chi$ be a\\nDirichlet character of conductor coprime with $N,$ and let\\n$\\\\mathfrak{L}(f\\\\otimes \\\\chi, s)$ denote either $\\\\log L(f\\\\otimes \\\\chi, s)$ or\\n$(L'/L)(f\\\\otimes \\\\chi, s).$ In this article we study the distribution of the\\nvalues of $\\\\mathfrak{L}$ when either $\\\\chi$ or $f$ vary. First, for a\\nquasi-character $\\\\psi\\\\colon \\\\mathbb{C} \\\\to \\\\mathbb{C}^\\\\times$ we find the limit\\nfor the average $\\\\mathrm{Avg}\\\\_\\\\chi \\\\psi(L(f\\\\otimes\\\\chi, s)),$ when $f$ is\\nfixed and $\\\\chi$ varies through the set of characters with prime conductor that\\ntends to infinity. Second, we prove an equidistribution result for the values\\nof $\\\\mathfrak{L}(f\\\\otimes \\\\chi,s)$ by establishing analytic properties of the\\nabove limit function. Third, we study the limit of the harmonic average\\n$\\\\mathrm{Avg}^h\\\\_f \\\\psi(L(f, s)),$ when $f$ runs through the set of primitive\\ncusp forms of given weight $k$ and level $N\\\\to \\\\infty.$ Most of the results are\\nobtained conditionally on the Generalized Riemann Hypothesis for\\n$L(f\\\\otimes\\\\chi, s).$\\n\",\n",
       " '  Hierarchical graph clustering is a common technique to reveal the multi-scale\\nstructure of complex networks. We propose a novel metric for assessing the\\nquality of a hierarchical clustering. This metric reflects the ability to\\nreconstruct the graph from the dendrogram, which encodes the hierarchy. The\\noptimal representation of the graph defines a class of reducible linkages\\nleading to regular dendrograms by greedy agglomerative clustering.\\n',\n",
       " '  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced\\nlong-ranged dynamical heterogeneities as dynamical arrest is approached. Here\\nwe focus on the most direct approach to study these heterogeneities: we\\nidentify clusters of slow particles and determine their size, $N_c$, and their\\nradius of gyration, $R_G$. We show that $N_c\\\\propto R_G^{d_f}$, providing\\ndirect evidence that the most immobile particles arrange in fractal objects\\nwith a fractal dimension, $d_f$, that is observed to increase with packing\\nfraction $\\\\phi$. The cluster size distribution obeys scaling, approaching an\\nalgebraic decay in the limit of structural arrest, i.e., $\\\\phi\\\\to\\\\phi_c$.\\nAlternatively, dynamical heterogeneities are analyzed via the four-point\\nstructure factor $S_4(q,t)$ and the dynamical susceptibility $\\\\chi_4(t)$.\\n$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,\\n$0.6\\\\leq\\\\phi\\\\leq 0.805$, and to become increasingly long-ranged as\\n$\\\\phi\\\\to\\\\phi_c$. Finite size scaling of $\\\\chi_4(t)$ provides a consistency\\ncheck for the previously analyzed divergences of $\\\\chi_4(t)\\\\propto\\n(\\\\phi-\\\\phi_c)^{-\\\\gamma_{\\\\chi}}$ and the correlation length $\\\\xi\\\\propto\\n(\\\\phi-\\\\phi_c)^{-\\\\gamma_{\\\\xi}}$. We check the robustness of our results with\\nrespect to our definition of mobility. The divergences and the scaling for\\n$\\\\phi\\\\to\\\\phi_c$ suggest a non-equilibrium glass transition which seems\\nqualitatively independent of the coefficient of restitution.\\n',\n",
       " '  We study the \\\\emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm\\nintroduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured\\nconvex-concave saddle point problems consisting of the sum of a smooth convex\\nfunction, a finite collection of nonsmooth convex functions and bilinear terms.\\nWe introduce the notion of pointwise quadratic supportability, which is a\\nrelaxation of a standard strong convexity assumption and allows us to show that\\nthe primal sequence is R-linearly convergent to an optimal solution and the\\nprimal-dual sequence is globally Q-linearly convergent. We illustrate the\\nproposed method on total variation denoising problems and on locally adaptive\\nestimation in signal/image deconvolution and denoising with multiresolution\\nstatistical constraints.\\n',\n",
       " '  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their\\nstability against dissociating agents. In this paper, a comparative study of\\nstability between native CMs and CMs cross-linked with genipin (CMs-GP) as a\\nfunction of pH is described. Stability to temperature and ethanol were\\ninvestigated in the pH range 2.0-7.0. The size and the charge\\n($\\\\zeta$-potential) of the particles were determined by dynamic light\\nscattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH\\n3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.\\nThe isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability\\nagainst heat and ethanol was observed for CMs-GP at pH 2, where visible\\ncoagulation was determined only after 800 s at 140 $^\\\\circ$C or 87.5% (v/v) of\\nethanol. These results confirmed the hypothesis that cross-linking by GP\\nincreased the stability of CMs.\\n',\n",
       " '  We investigate a construction of an integral residuated lattice starting from\\nan integral residuated lattice and two sets with an injective mapping from one\\nset into the second one. The resulting algebra has a shape of a Chinese cascade\\nkite, therefore, we call this algebra simply a kite. We describe subdirectly\\nirreducible kites and we classify them. We show that the variety of integral\\nresiduated lattices generated by kites is generated by all finite-dimensional\\nkites. In particular, we describe some homomorphisms among kites.\\n',\n",
       " '  Nowadays, online video platforms mostly recommend related videos by analyzing\\nuser-driven data such as viewing patterns, rather than the content of the\\nvideos. However, content is more important than any other element when videos\\naim to deliver knowledge. Therefore, we have developed a web application which\\nrecommends related TED lecture videos to the users, considering the content of\\nthe videos from the transcripts. TED Talk Recommender constructs a network for\\nrecommending videos that are similar content-wise and providing a user\\ninterface.\\n',\n",
       " '  Exploration of asteroids and small-bodies can provide valuable insight into\\nthe origins of the solar system, into the origins of Earth and the origins of\\nthe building blocks of life. However, the low-gravity and unknown surface\\nconditions of asteroids presents a daunting challenge for surface exploration,\\nmanipulation and for resource processing. This has resulted in the loss of\\nseveral landers or shortened missions. Fundamental studies are required to\\nobtain better readings of the material surface properties and physical models\\nof these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat\\ncentrifuge laboratory that spins at up to 4 rpm to simulate the milligravity\\nconditions of sub 1 km asteroids. Such a laboratory will help to de-risk\\ndevelopment and testing of landing and resource processing technology for\\nasteroids. Inside the laboratory are crushed meteorites, the remains of\\nasteroids. The laboratory is equipped with cameras and actuators to perform a\\nseries of science experiments to better understand material properties and\\nasteroid surface physics. These results will help to improve our physics models\\nof asteroids. The CubeSat has been designed to be low-cost and contains 3-axis\\nmagnetorquers and a single reaction-wheel to induce spin. In our work, we first\\nanalyze how the attitude control system will de-tumble the spacecraft after\\ndeployment. Further analysis has been conducted to analyze the impact and\\nstability of the attitude control system to shifting mass (crushed meteorites)\\ninside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be\\nthe first in a series of low-cost CubeSat centrifuges that will be launched\\nsetting the stage for a larger, permanent, on-orbit centrifuge laboratory for\\nexperiments in planetary science, life sciences and manufacturing.\\n',\n",
       " '  In automatic speech processing systems, speaker diarization is a crucial\\nfront-end component to separate segments from different speakers. Inspired by\\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\\ntriplet loss-based architectures have been successfully used for this problem.\\nHowever, existing work utilizes conventional i-vectors as the input\\nrepresentation and builds simple fully connected networks for metric learning,\\nthus not fully leveraging the modeling power of DNN architectures. This paper\\ninvestigates the importance of learning effective representations from the\\nsequences directly in metric learning pipelines for speaker diarization. More\\nspecifically, we propose to employ attention models to learn embeddings and the\\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\\nCALLHOME conversational speech corpus. The diarization results demonstrate\\nthat, besides providing a unified model, the proposed approach achieves\\nimproved performance when compared against existing approaches.\\n',\n",
       " '  Predicting when rupture occurs or cracks progress is a major challenge in\\nnumerous elds of industrial, societal and geophysical importance. It remains\\nlargely unsolved: Stress enhancement at cracks and defects, indeed, makes the\\nmacroscale dynamics extremely sensitive to the microscale material disorder.\\nThis results in giant statistical uctuations and non-trivial behaviors upon\\nupscaling dicult to assess via the continuum approaches of engineering. These\\nissues are examined here. We will see: How linear elastic fracture mechanics\\nsidetracks the diculty by reducing the problem to that of the propagation of a\\nsingle crack in an eective material free of defects, How slow cracks sometimes\\ndisplay jerky dynamics, with sudden violent events incompatible with the\\nprevious approach, and how some paradigms of statistical physics can explain\\nit, How abnormally fast cracks sometimes emerge due to the formation of\\nmicrocracks at very small scales.\\n',\n",
       " '  This paper investigates the multiplicative spread spectrum watermarking\\nmethod for the image. The information bit is spreaded into middle-frequency\\nDiscrete Cosine Transform (DCT) coefficients of each block of an image using a\\ngenerated pseudo-random sequence. Unlike the conventional signal modeling, we\\nsuppose that both signal and noise are distributed with Laplacian distribution\\nbecause the sample loss of digital media can be better modeled with this\\ndistribution than the Gaussian one. We derive the optimum decoder for the\\nproposed embedding method thanks to the maximum likelihood decoding scheme. We\\nalso analyze our watermarking system in the presence of noise and provide\\nanalytical evaluations and several simulations. The results show that it has\\nthe suitable performance and transparency required for watermarking\\napplications.\\n',\n",
       " '  We present a representation learning algorithm that learns a low-dimensional\\nlatent dynamical system from high-dimensional \\\\textit{sequential} raw data,\\ne.g., video. The framework builds upon recent advances in amortized inference\\nmethods that use both an inference network and a refinement procedure to output\\nsamples from a variational distribution given an observation sequence, and\\ntakes advantage of the duality between control and inference to approximately\\nsolve the intractable inference problem using the path integral control\\napproach. The learned dynamical model can be used to predict and plan the\\nfuture states; we also present the efficient planning method that exploits the\\nlearned low-dimensional latent dynamics. Numerical experiments show that the\\nproposed path-integral control based variational inference method leads to\\ntighter lower bounds in statistical model learning of sequential data. The\\nsupplementary video: this https URL\\n',\n",
       " \"  Consider a social network where only a few nodes (agents) have meaningful\\ninteractions in the sense that the conditional dependency graph over node\\nattribute variables (behaviors) is sparse. A company that can only observe the\\ninteractions between its own customers will generally not be able to accurately\\nestimate its customers' dependency subgraph: it is blinded to any external\\ninteractions of its customers and this blindness creates false edges in its\\nsubgraph. In this paper we address the semiblind scenario where the company has\\naccess to a noisy summary of the complementary subgraph connecting external\\nagents, e.g., provided by a consolidator. The proposed framework applies to\\nother applications as well, including field estimation from a network of awake\\nand sleeping sensors and privacy-constrained information sharing over social\\nsubnetworks. We propose a penalized likelihood approach in the context of a\\ngraph signal obeying a Gaussian graphical models (GGM). We use a convex-concave\\niterative optimization algorithm to maximize the penalized likelihood.\\n\",\n",
       " '  In this paper, we present a novel structure, Semi-AutoEncoder, based on\\nAutoEncoder. We generalize it into a hybrid collaborative filtering model for\\nrating prediction as well as personalized top-n recommendations. Experimental\\nresults on two real-world datasets demonstrate its state-of-the-art\\nperformances.\\n',\n",
       " '  We show how a characteristic length scale imprinted in the galaxy two-point\\ncorrelation function, dubbed the \"linear point\", can serve as a comoving\\ncosmological standard ruler. In contrast to the Baryon Acoustic Oscillation\\npeak location, this scale is constant in redshift and is unaffected by\\nnon-linear effects to within $0.5$ percent precision. We measure the location\\nof the linear point in the galaxy correlation function of the LOWZ and CMASS\\nsamples from the Twelfth Data Release (DR12) of the Baryon Oscillation\\nSpectroscopic Survey (BOSS) collaboration. We combine our linear-point\\nmeasurement with cosmic-microwave-background constraints from the Planck\\nsatellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying\\non a model-template or reconstruction method. We find $D_V(0.32)=1264\\\\pm 28$\\nMpc and $D_V(0.57)=2056\\\\pm 22$ Mpc respectively, consistent with the quoted\\nvalues from the BOSS collaboration. This remarkable result suggests that all\\nthe distance information contained in the baryon acoustic oscillations can be\\nconveniently compressed into the single length associated with the linear\\npoint.\\n',\n",
       " '  Starting from isentropic compressible Navier-Stokes equations with growth\\nterm in the continuity equation, we rigorously justify that performing an\\nincompressible limit one arrives to the two-phase free boundary fluid system.\\n',\n",
       " \"  We study a stochastic primal-dual method for constrained optimization over\\nRiemannian manifolds with bounded sectional curvature. We prove non-asymptotic\\nconvergence to the optimal objective value. More precisely, for the class of\\nhyperbolic manifolds, we establish a convergence rate that is related to the\\nsectional curvature lower bound. To prove a convergence rate in terms of\\nsectional curvature for the elliptic manifolds, we leverage Toponogov's\\ncomparison theorem. In addition, we provide convergence analysis for the\\nasymptotically elliptic manifolds, where the sectional curvature at each given\\npoint on manifold is locally bounded from below by the distance function. We\\ndemonstrate the performance of the primal-dual algorithm on the sphere for the\\nnon-negative principle component analysis (PCA). In particular, under the\\nnon-negativity constraint on the principle component and for the symmetric\\nspiked covariance model, we empirically show that the primal-dual approach\\noutperforms the spectral method. We also examine the performance of the\\nprimal-dual method for the anchored synchronization from partial noisy\\nmeasurements of relative rotations on the Lie group SO(3). Lastly, we show that\\nthe primal-dual algorithm can be applied to the weighted MAX-CUT problem under\\nconstraints on the admissible cut. Specifically, we propose different\\napproximation algorithms for the weighted MAX-CUT problem based on optimizing a\\nfunction on the manifold of direct products of the unit spheres as well as the\\nmanifold of direct products of the rotation groups.\\n\",\n",
       " '  We investigate a new sampling scheme aimed at improving the performance of\\nparticle filters whenever (a) there is a significant mismatch between the\\nassumed model dynamics and the actual system, or (b) the posterior probability\\ntends to concentrate in relatively small regions of the state space. The\\nproposed scheme pushes some particles towards specific regions where the\\nlikelihood is expected to be high, an operation known as nudging in the\\ngeophysics literature. We re-interpret nudging in a form applicable to any\\nparticle filtering scheme, as it does not involve any changes in the rest of\\nthe algorithm. Since the particles are modified, but the importance weights do\\nnot account for this modification, the use of nudging leads to additional bias\\nin the resulting estimators. However, we prove analytically that nudged\\nparticle filters can still attain asymptotic convergence with the same error\\nrates as conventional particle methods. Simple analysis also yields an\\nalternative interpretation of the nudging operation that explains its\\nrobustness to model errors. Finally, we show numerical results that illustrate\\nthe improvements that can be attained using the proposed scheme. In particular,\\nwe present nonlinear tracking examples with synthetic data and a model\\ninference example using real-world financial data.\\n',\n",
       " '  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the\\nnoncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet\\nFermi surface are identified, including a small pocket (frequency 40 T) which\\nis three dimensional and anisotropic. From the temperature dependence of the\\namplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\\\\pm$\\n0.1) $m_e$. Further analysis showed a non-trivial $\\\\pi$-Berry phase is\\nassociated with the 40 T pocket, which strongly supports the presence of\\ntopological states in bulk BiPd and may result in topological superconductivity\\ndue to the proximity coupling to other bands.\\n',\n",
       " '  Statistical inference can be computationally prohibitive in\\nultrahigh-dimensional linear models. Correlation-based variable screening, in\\nwhich one leverages marginal correlations for removal of irrelevant variables\\nfrom the model prior to statistical inference, can be used to overcome this\\nchallenge. Prior works on correlation-based variable screening either impose\\nstrong statistical priors on the linear model or assume specific post-screening\\ninference methods. This paper first extends the analysis of correlation-based\\nvariable screening to arbitrary linear models and post-screening inference\\ntechniques. In particular, ($i$) it shows that a condition---termed the\\nscreening condition---is sufficient for successful correlation-based screening\\nof linear models, and ($ii$) it provides insights into the dependence of\\nmarginal correlation-based screening on different problem parameters. Numerical\\nexperiments confirm that these insights are not mere artifacts of analysis;\\nrather, they are reflective of the challenges associated with marginal\\ncorrelation-based variable screening. Second, the paper explicitly derives the\\nscreening condition for two families of linear models, namely, sub-Gaussian\\nlinear models and arbitrary (random or deterministic) linear models. In the\\nprocess, it establishes that---under appropriate conditions---it is possible to\\nreduce the dimension of an ultrahigh-dimensional, arbitrary linear model to\\nalmost the sample size even when the number of active variables scales almost\\nlinearly with the sample size.\\n',\n",
       " \"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy\\ncurrency of life. Chemiosmosis, a proton centric mechanism, advocates that\\nComplex V harnesses a transmembrane potential (TMP) for ATP synthesis. This\\nperception of cellular respiration requires oxygen to stay tethered at Complex\\nIV (an association inhibited by cyanide) and diffusible reactive oxygen species\\n(DROS) are considered wasteful and toxic products. With new mechanistic\\ninsights on heme and flavin enzymes, an oxygen or DROS centric explanation\\n(called murburn concept) was recently proposed for mOxPhos. In the new\\nmechanism, TMP is not directly harnessed, protons are a rate limiting reactant\\nand DROS within matrix serve as the chemical coupling agents that directly link\\nNADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites\\nand solvent accessible DROS channels in respiratory proteins, which validate\\nthe oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.\\nSince cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is\\nlethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study\\nalso provides comprehensive arguments against Mitchell's and Boyer's\\nexplanations and extensive support for murburn concept based holistic\\nperspectives for mOxPhos.\\n\",\n",
       " '  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling\\nStormer for special JB-algebras, we prove that a synaptic algebra is norm\\ncomplete (i.e., Banach) if and only if it is isomorphic to the self-adjoint\\npart of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic\\nalgebra that are equivalent to the condition that it is isomorphic to the\\nself-adjoint part of an AW*-algebra. Moreover, we study some relationships\\nbetween synaptic algebras and so-called generalized Hermitian algebras.\\n',\n",
       " \"  High-pressure neutron powder diffraction, muon-spin rotation and\\nmagnetization studies of the structural, magnetic and the superconducting\\nproperties of the Ce-underdoped superconducting (SC) electron-doped cuprate\\nsystem T'-Pr_1.3-xLa_0.7Ce_xCuO_4 with x = 0.1 are reported. A strong reduction\\nof the lattice constants a and c is observed under pressure. However, no\\nindication of any pressure induced phase transition from T' to T structure is\\nobserved up to the maximum applied pressure of p = 11 GPa. Large and non-linear\\nincrease of the short-range magnetic order temperature T_so in\\nT'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1) was observed under pressure.\\nSimultaneously pressure causes a non-linear decrease of the SC transition\\ntemperature T_c. All these experiments establish the short-range magnetic order\\nas an intrinsic and a new competing phase in SC T'-Pr_1.2La_0.7Ce_0.1CuO_4. The\\nobserved pressure effects may be interpreted in terms of the improved nesting\\nconditions through the reduction of the in-plane and out-of-plane lattice\\nconstants upon hydrostatic pressure.\\n\",\n",
       " '  This paper presents a fixturing strategy for regrasping that does not require\\na physical fixture. To regrasp an object in a gripper, a robot pushes the\\nobject against external contact/s in the environment such that the external\\ncontact keeps the object stationary while the fingers slide over the object. We\\ncall this manipulation technique fixtureless fixturing. Exploiting the\\nmechanics of pushing, we characterize a convex polyhedral set of pushes that\\nresults in fixtureless fixturing. These pushes are robust against uncertainty\\nin the object inertia, grasping force, and the friction at the contacts. We\\npropose a sampling-based planner that uses the sets of robust pushes to rapidly\\nbuild a tree of reachable grasps. A path in this tree is a pushing strategy,\\npossibly involving pushes from different sides, to regrasp the object. We\\ndemonstrate the experimental validity and robustness of the proposed\\nmanipulation technique with different regrasp examples on a manipulation\\nplatform. Such a fast and flexible regrasp planner facilitates versatile and\\nflexible automation solutions.\\n',\n",
       " '  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard\\nunsupervised Latent Dirichlet Allocation (LDA) algorithm, to address\\nmulti-label learning tasks. Previous work has shown it to perform in par with\\nother state-of-the-art multi-label methods. Nonetheless, with increasing label\\nsets sizes LLDA encounters scalability issues. In this work, we introduce\\nSubset LLDA, a simple variant of the standard LLDA algorithm, that not only can\\neffectively scale up to problems with hundreds of thousands of labels but also\\nimproves over the LLDA state-of-the-art. We conduct extensive experiments on\\neight data sets, with label sets sizes ranging from hundreds to hundreds of\\nthousands, comparing our proposed algorithm with the previously proposed LLDA\\nalgorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme\\nmulti-label classification. The results show a steady advantage of our method\\nover the other LLDA algorithms and competitive results compared to the extreme\\nmulti-label classification algorithms.\\n',\n",
       " '  Multimedia Forensics allows to determine whether videos or images have been\\ncaptured with the same device, and thus, eventually, by the same person.\\nCurrently, the most promising technology to achieve this task, exploits the\\nunique traces left by the camera sensor into the visual content. Anyway, image\\nand video source identification are still treated separately from one another.\\nThis approach is limited and anachronistic if we consider that most of the\\nvisual media are today acquired using smartphones, that capture both images and\\nvideos. In this paper we overcome this limitation by exploring a new approach\\nthat allows to synergistically exploit images and videos to study the device\\nfrom which they both come. Indeed, we prove it is possible to identify the\\nsource of a digital video by exploiting a reference sensor pattern noise\\ngenerated from still images taken by the same device of the query video. The\\nproposed method provides comparable or even better performance, when compared\\nto the current video identification strategies, where a reference pattern is\\nestimated from video frames. We also show how this strategy can be effective\\neven in case of in-camera digitally stabilized videos, where a non-stabilized\\nreference is not available, by solving some state-of-the-art limitations. We\\nexplore a possible direct application of this result, that is social media\\nprofile linking, i.e. discovering relationships between two or more social\\nmedia profiles by comparing the visual contents - images or videos - shared\\ntherein.\\n',\n",
       " '  We present an informal review of recent work on the asymptotics of\\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\\nhave more data? The results we review show that ABC can perform well in terms\\nof point estimation, but standard implementations will over-estimate the\\nuncertainty about the parameters. If we use the regression correction of\\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\\ntheoretical results also have practical implications for how to implement ABC.\\n',\n",
       " '  We consider the problem of learning sparse polymatrix games from observations\\nof strategic interactions. We show that a polynomial time method based on\\n$\\\\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash\\nequilibria are the $\\\\epsilon$-Nash equilibria of the game from which the data\\nwas generated (true game), in $\\\\mathcal{O}(m^4 d^4 \\\\log (pd))$ samples of\\nstrategy profiles --- where $m$ is the maximum number of pure strategies of a\\nplayer, $p$ is the number of players, and $d$ is the maximum degree of the game\\ngraph. Under slightly more stringent separability conditions on the payoff\\nmatrices of the true game, we show that our method learns a game with the exact\\nsame Nash equilibria as the true game. We also show that $\\\\Omega(d \\\\log (pm))$\\nsamples are necessary for any method to consistently recover a game, with the\\nsame Nash-equilibria as the true game, from observations of strategic\\ninteractions. We verify our theoretical results through simulation experiments.\\n',\n",
       " '  The KdV equation can be derived in the shallow water limit of the Euler\\nequations. Over the last few decades, this equation has been extended to\\ninclude higher order effects. Although this equation has only one conservation\\nlaw, exact periodic and solitonic solutions exist. Khare and Saxena\\n\\\\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact\\nsolutions by combining known ones for several fundamental equations (e.g.,\\nKorteweg - de Vries, Nonlinear Schrödinger). Here we find that this\\nconstruction can be repeated for higher order, non-integrable extensions of\\nthese equations. Contrary to many statements in the literature, there seems to\\nbe no correlation between integrability and the number of nonlinear one\\nvariable wave solutions.\\n',\n",
       " '  This paper proposes a new actor-critic-style algorithm called Dual\\nActor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian\\ndual form of the Bellman optimality equation, which can be viewed as a\\ntwo-player game between the actor and a critic-like function, which is named as\\ndual critic. Compared to its actor-critic relatives, Dual-AC has the desired\\nproperty that the actor and dual critic are updated cooperatively to optimize\\nthe same objective function, providing a more transparent way for learning the\\ncritic that is directly related to the objective function of the actor. We then\\nprovide a concrete algorithm that can effectively solve the minimax\\noptimization problem, using techniques of multi-step bootstrapping, path\\nregularization, and stochastic dual ascent algorithm. We demonstrate that the\\nproposed algorithm achieves the state-of-the-art performances across several\\nbenchmarks.\\n',\n",
       " '  Counting dominating sets in a graph $G$ is closely related to the\\nneighborhood complex of $G$. We exploit this relation to prove that the number\\nof dominating sets $d(G)$ of a graph is determined by the number of complete\\nbipartite subgraphs of its complement. More precisely, we state the following.\\nLet $G$ be a simple graph of order $n$ such that its complement has exactly\\n$a(G)$ subgraphs isomorphic to $K_{2p,2q}$ and exactly $b(G)$ subgraphs\\nisomorphic to $K_{2p+1,2q+1}$. Then $d(G) = 2^n -1 + 2[a(G)-b(G)]$. We also\\nshow some new relations between the domination polynomial and the neighborhood\\npolynomial of a graph.\\n',\n",
       " '  High signal to noise ratio (SNR) consistency of model selection criteria in\\nlinear regression models has attracted a lot of attention recently. However,\\nmost of the existing literature on high SNR consistency deals with model order\\nselection. Further, the limited literature available on the high SNR\\nconsistency of subset selection procedures (SSPs) is applicable to linear\\nregression with full rank measurement matrices only. Hence, the performance of\\nSSPs used in underdetermined linear models (a.k.a compressive sensing (CS)\\nalgorithms) at high SNR is largely unknown. This paper fills this gap by\\nderiving necessary and sufficient conditions for the high SNR consistency of\\npopular CS algorithms like $l_0$-minimization, basis pursuit de-noising or\\nLASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions\\nanalytically establish the high SNR inconsistency of CS algorithms when used\\nwith the tuning parameters discussed in literature. Novel tuning parameters\\nwith SNR adaptations are developed using the sufficient conditions and the\\nchoice of SNR adaptations are discussed analytically using convergence rate\\nanalysis. CS algorithms with the proposed tuning parameters are numerically\\nshown to be high SNR consistent and outperform existing tuning parameters in\\nthe moderate to high SNR regime.\\n',\n",
       " \"  Reduction of communication and efficient partitioning are key issues for\\nachieving scalability in hierarchical $N$-Body algorithms like FMM. In the\\npresent work, we propose four independent strategies to improve partitioning\\nand reduce communication. First of all, we show that the conventional wisdom of\\nusing space-filling curve partitioning may not work well for boundary integral\\nproblems, which constitute about 50% of FMM's application user base. We propose\\nan alternative method which modifies orthogonal recursive bisection to solve\\nthe cell-partition misalignment that has kept it from scaling previously.\\nSecondly, we optimize the granularity of communication to find the optimal\\nbalance between a bulk-synchronous collective communication of the local\\nessential tree and an RDMA per task per cell. Finally, we take the dynamic\\nsparse data exchange proposed by Hoefler et al. and extend it to a hierarchical\\nsparse data exchange, which is demonstrated at scale to be faster than the MPI\\nlibrary's MPI_Alltoallv that is commonly used.\\n\",\n",
       " '  In this paper, we focus on fully automatic traffic surveillance camera\\ncalibration, which we use for speed measurement of passing vehicles. We improve\\nover a recent state-of-the-art camera calibration method for traffic\\nsurveillance based on two detected vanishing points. More importantly, we\\npropose a novel automatic scene scale inference method. The method is based on\\nmatching bounding boxes of rendered 3D models of vehicles with detected\\nbounding boxes in the image. The proposed method can be used from arbitrary\\nviewpoints, since it has no constraints on camera placement. We evaluate our\\nmethod on the recent comprehensive dataset for speed measurement BrnoCompSpeed.\\nExperiments show that our automatic camera calibration method by detection of\\ntwo vanishing points reduces error by 50% (mean distance ratio error reduced\\nfrom 0.18 to 0.09) compared to the previous state-of-the-art method. We also\\nshow that our scene scale inference method is more precise, outperforming both\\nstate-of-the-art automatic calibration method for speed measurement (error\\nreduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error\\nreduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results\\nof the proposed automatic camera calibration method on video sequences obtained\\nfrom real surveillance cameras in various places, and under different lighting\\nconditions (night, dawn, day).\\n',\n",
       " '  The success of autonomous systems will depend upon their ability to safely\\nnavigate human-centric environments. This motivates the need for a real-time,\\nprobabilistic forecasting algorithm for pedestrians, cyclists, and other agents\\nsince these predictions will form a necessary step in assessing the risk of any\\naction. This paper presents a novel approach to probabilistic forecasting for\\npedestrians based on weighted sums of ordinary differential equations that are\\nlearned from historical trajectory information within a fixed scene. The\\nresulting algorithm is embarrassingly parallel and is able to work at real-time\\nspeeds using a naive Python implementation. The quality of predicted locations\\nof agents generated by the proposed algorithm is validated on a variety of\\nexamples and considerably higher than existing state of the art approaches over\\nlong time horizons.\\n',\n",
       " '  This paper presents a distance-based discriminative framework for learning\\nwith probability distributions. Instead of using kernel mean embeddings or\\ngeneralized radial basis kernels, we introduce embeddings based on\\ndissimilarity of distributions to some reference distributions denoted as\\ntemplates. Our framework extends the theory of similarity of Balcan et al.\\n(2008) to the population distribution case and we show that, for some learning\\nproblems, some dissimilarity on distribution achieves low-error linear decision\\nfunctions with high probability. Our key result is to prove that the theory\\nalso holds for empirical distributions. Algorithmically, the proposed approach\\nconsists in computing a mapping based on pairwise dissimilarity where learning\\na linear decision function is amenable. Our experimental results show that the\\nWasserstein distance embedding performs better than kernel mean embeddings and\\ncomputing Wasserstein distance is far more tractable than estimating pairwise\\nKullback-Leibler divergence of empirical distributions.\\n',\n",
       " '  Molecular reflections on usual wall surfaces can be statistically described\\nby the Maxwell diffuse reflection model, which has been successfully applied in\\nthe DSBGK simulations. We develop the DSBGK algorithm to implement the\\nCercignani-Lampis-Lord (CLL) reflection model, which is widely applied to\\npolished surfaces and used particularly in modeling space shuttles to predict\\nthe heat and force loads exerted by the high-speed flows around the surfaces.\\nWe also extend the DSBGK method to simulate gas mixtures and high contrast of\\nnumber densities of different components can be handled at a cost of memory\\nusage much lower than that needed by the DSMC simulations because the average\\nnumbers of simulated molecules of different components per cell can be equal in\\nthe DSBGK simulations.\\n',\n",
       " '  Let $f(a,b,c,d)=\\\\sqrt{a^2+b^2}+\\\\sqrt{c^2+d^2}-\\\\sqrt{(a+c)^2+(b+d)^2}$, let\\n$(a,b,c,d)$ stand for $a,b,c,d\\\\in\\\\mathbb Z_{\\\\geq 0}$ such that $ad-bc=1$.\\nDefine \\\\begin{equation} \\\\label{eq_main} F(s) = \\\\sum_{(a,b,c,d)} f(a,b,c,d)^s.\\n\\\\end{equation} In other words, we consider the sum of the powers of the\\ntriangle inequality defects for the lattice parallelograms (in the first\\nquadrant) of area one.\\nWe prove that $F(s)$ converges when $s>1/2$ and diverges at $s=1/2$. We also\\nprove $$\\\\sum\\\\limits_{\\\\substack{(a,b,c,d),\\\\\\\\ 1\\\\leq a\\\\leq b, 1\\\\leq c\\\\leq d}}\\n\\\\frac{1}{(a+b)^2(c+d)^2(a+b+c+d)^2} = 1/24,$$ and show a general method to\\nobtain such formulae. The method comes from the consideration of the tropical\\nanalogue of the caustic curves, whose moduli give a complete set of continuous\\ninvariants on the space of convex domains.\\n',\n",
       " '  We consider the task of generating draws from a Markov jump process (MJP)\\nbetween two time points at which the process is known. Resulting draws are\\ntypically termed bridges and the generation of such bridges plays a key role in\\nsimulation-based inference algorithms for MJPs. The problem is challenging due\\nto the intractability of the conditioned process, necessitating the use of\\ncomputationally intensive methods such as weighted resampling or Markov chain\\nMonte Carlo. An efficient implementation of such schemes requires an\\napproximation of the intractable conditioned hazard/propensity function that is\\nboth cheap and accurate. In this paper, we review some existing approaches to\\nthis problem before outlining our novel contribution. Essentially, we leverage\\nthe tractability of a Gaussian approximation of the MJP and suggest a\\ncomputationally efficient implementation of the resulting conditioned hazard\\napproximation. We compare and contrast our approach with existing methods using\\nthree examples.\\n',\n",
       " '  Using holography, we model experiments in which a 2+1D strange metal is\\npumped by a laser pulse into a highly excited state, after which the time\\nevolution of the optical conductivity is probed. We consider a finite-density\\nstate with mildly broken translation invariance and excite it by oscillating\\nelectric field pulses. At zero density, the optical conductivity would assume\\nits thermalized value immediately after the pumping has ended. At finite\\ndensity, pulses with significant DC components give rise to slow exponential\\nrelaxation, governed by a vector quasinormal mode. In contrast, for\\nhigh-frequency pulses the amplitude of the quasinormal mode is strongly\\nsuppressed, so that the optical conductivity assumes its thermalized value\\neffectively instantaneously. This surprising prediction may provide a stimulus\\nfor taking up the challenge to realize these experiments in the laboratory.\\nSuch experiments would test a crucial open question faced by applied\\nholography: Are its predictions artefacts of the large $N$ limit or do they\\nenjoy sufficient UV independence to hold at least qualitatively in real-world\\nsystems?\\n',\n",
       " '  In this paper, we study random subsampling of Gaussian process regression,\\none of the simplest approximation baselines, from a theoretical perspective.\\nAlthough subsampling discards a large part of training data, we show provable\\nguarantees on the accuracy of the predictive mean/variance and its\\ngeneralization ability. For analysis, we consider embedding kernel matrices\\ninto graphons, which encapsulate the difference of the sample size and enables\\nus to evaluate the approximation and generalization errors in a unified manner.\\nThe experimental results show that the subsampling approximation achieves a\\nbetter trade-off regarding accuracy and runtime than the Nyström and random\\nFourier expansion methods.\\n',\n",
       " '  Both hybrid automata and action languages are formalisms for describing the\\nevolution of dynamic systems. This paper establishes a formal relationship\\nbetween them. We show how to succinctly represent hybrid automata in an action\\nlanguage which in turn is defined as a high-level notation for answer set\\nprogramming modulo theories (ASPMT) --- an extension of answer set programs to\\nthe first-order level similar to the way satisfiability modulo theories (SMT)\\nextends propositional satisfiability (SAT). We first show how to represent\\nlinear hybrid automata with convex invariants by an action language modulo\\ntheories. A further translation into SMT allows for computing them using SMT\\nsolvers that support arithmetic over reals. Next, we extend the representation\\nto the general class of non-linear hybrid automata allowing even non-convex\\ninvariants. We represent them by an action language modulo ODE (Ordinary\\nDifferential Equations), which can be compiled into satisfiability modulo ODE.\\nWe developed a prototype system cplus2aspmt based on these translations, which\\nallows for a succinct representation of hybrid transition systems that can be\\ncomputed effectively by the state-of-the-art SMT solver dReal.\\n',\n",
       " '  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice\\nBoltzmann (LB) method is developed for solid-liquid phase change heat transfer\\nin metal foams under local thermal non-equilibrium (LTNE) condition. The\\nenthalpy-based MRT-LB method consists of three different MRT-LB models: one for\\nflow field based on the generalized non-Darcy model, and the other two for\\nphase change material (PCM) and metal foam temperature fields described by the\\nLTNE model. The moving solid-liquid phase interface is implicitly tracked\\nthrough the liquid fraction, which is simultaneously obtained when the energy\\nequations of PCM and metal foam are solved. The present method has several\\ndistinctive features. First, as compared with previous studies, the present\\nmethod avoids the iteration procedure, thus it retains the inherent merits of\\nthe standard LB method and is superior over the iteration method in terms of\\naccuracy and computational efficiency. Second, a volumetric LB scheme instead\\nof the bounce-back scheme is employed to realize the no-slip velocity condition\\nin the interface and solid phase regions, which is consistent with the actual\\nsituation. Last but not least, the MRT collision model is employed, and with\\nadditional degrees of freedom, it has the ability to reduce the numerical\\ndiffusion across phase interface induced by solid-liquid phase change.\\nNumerical tests demonstrate that the present method can be served as an\\naccurate and efficient numerical tool for studying metal foam enhanced\\nsolid-liquid phase change heat transfer in latent heat storage. Finally,\\ncomparisons and discussions are made to offer useful information for practical\\napplications of the present method.\\n',\n",
       " '  Barchan dunes are crescentic shape dunes with horns pointing downstream. The\\npresent paper reports the formation of subaqueous barchan dunes from initially\\nconical heaps in a rectangular channel. Because the most unique feature of a\\nbarchan dune is its horns, we associate the timescale for the appearance of\\nhorns to the formation of a barchan dune. A granular heap initially conical was\\nplaced on the bottom wall of a closed conduit and it was entrained by a water\\nflow in turbulent regime. After a certain time, horns appear and grow, until an\\nequilibrium length is reached. Our results show the existence of the timescales\\n$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,\\nrespectively, where $t_c$ is a characteristic time that scales with the grains\\ndiameter, gravity acceleration, densities of the fluid and grains, and shear\\nand threshold velocities.\\n',\n",
       " '  Isotonic regression is a standard problem in shape-constrained estimation\\nwhere the goal is to estimate an unknown nondecreasing regression function $f$\\nfrom independent pairs $(x_i, y_i)$ where $\\\\mathbb{E}[y_i]=f(x_i), i=1, \\\\ldots\\nn$. While this problem is well understood both statistically and\\ncomputationally, much less is known about its uncoupled counterpart where one\\nis given only the unordered sets $\\\\{x_1, \\\\ldots, x_n\\\\}$ and $\\\\{y_1, \\\\ldots,\\ny_n\\\\}$. In this work, we leverage tools from optimal transport theory to derive\\nminimax rates under weak moments conditions on $y_i$ and to give an efficient\\nalgorithm achieving optimal rates. Both upper and lower bounds employ\\nmoment-matching arguments that are also pertinent to learning mixtures of\\ndistributions and deconvolution.\\n',\n",
       " '  This paper considers a time-inconsistent stopping problem in which the\\ninconsistency arises from non-constant time preference rates. We show that the\\nsmooth pasting principle, the main approach that has been used to construct\\nexplicit solutions for conventional time-consistent optimal stopping problems,\\nmay fail under time-inconsistency. Specifically, we prove that the smooth\\npasting principle solves a time-inconsistent problem within the intra-personal\\ngame theoretic framework if and only if a certain inequality on the model\\nprimitives is satisfied. We show that the violation of this inequality can\\nhappen even for very simple non-exponential discount functions. Moreover, we\\ndemonstrate that the stopping problem does not admit any intra-personal\\nequilibrium whenever the smooth pasting principle fails. The \"negative\" results\\nin this paper caution blindly extending the classical approaches for\\ntime-consistent stopping problems to their time-inconsistent counterparts.\\n',\n",
       " '  The Internet of Things (IoT) demands authentication systems which can provide\\nboth security and usability. Recent research utilizes the rich sensing\\ncapabilities of smart devices to build security schemes operating without human\\ninteraction, such as zero-interaction pairing (ZIP) and zero-interaction\\nauthentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and\\nreported promising results. However, those schemes were often evaluated under\\nconditions which do not reflect realistic IoT scenarios. In addition, drawing\\nany comparison among the existing schemes is impossible due to the lack of a\\ncommon public dataset and unavailability of scheme implementations.\\nIn this paper, we address these challenges by conducting the first\\nlarge-scale comparative study of ZIP and ZIA schemes, carried out under\\nrealistic conditions. We collect and release the most comprehensive dataset in\\nthe domain to date, containing over 4250 hours of audio recordings and 1\\nbillion sensor readings from three different scenarios, and evaluate five\\nstate-of-the-art schemes based on these data. Our study reveals that the\\neffectiveness of the existing proposals is highly dependent on the scenario\\nthey are used in. In particular, we show that these schemes are subject to\\nerror rates between 0.6% and 52.8%.\\n',\n",
       " '  The increasing number of protein-based metamaterials demands reliable and\\nefficient methods to study the physicochemical properties they may display. In\\nthis regard, we develop a simulation strategy based on Molecular Dynamics (MD)\\nthat addresses the geometric degrees of freedom of an auxetic two-dimensional\\nprotein crystal. This model consists of a network of impenetrable rigid squares\\nlinked through massless rigid rods, thus featuring a large number of both\\nholonomic and nonholonomic constraints. Our MD methodology is optimized to\\nstudy highly constrained systems and allows for the simulation of long-time\\ndynamics with reasonably large timesteps. The data extracted from the\\nsimulations shows a persistent motional interdependence among the protein\\nsubunits in the crystal. We characterize the dynamical correlations featured by\\nthese subunits and identify two regimes characterized by their locality or\\nnonlocality, depending on the geometric parameters of the crystal. From the\\nsame data, we also calculate the Poisson\\\\rq{}s (longitudinal to axial strain)\\nratio of the crystal, and learn that, due to holonomic constraints (rigidness\\nof the rod links), the crystal remains auxetic even after significant changes\\nin the original geometry. The nonholonomic ones (collisions between subunits)\\nincrease the number of inhomogeneous deformations of the crystal, thus driving\\nit away from an isotropic response. Our work provides the first simulation of\\nthe dynamics of protein crystals and offers insights into promising mechanical\\nproperties afforded by these materials.\\n',\n",
       " '  Recent advances in the field of network representation learning are mostly\\nattributed to the application of the skip-gram model in the context of graphs.\\nState-of-the-art analogues of skip-gram model in graphs define a notion of\\nneighbourhood and aim to find the vector representation for a node, which\\nmaximizes the likelihood of preserving this neighborhood.\\nIn this paper, we take a drastic departure from the existing notion of\\nneighbourhood of a node by utilizing the idea of coreness. More specifically,\\nwe utilize the well-established idea that nodes with similar core numbers play\\nequivalent roles in the network and hence induce a novel and an organic notion\\nof neighbourhood. Based on this idea, we propose core2vec, a new algorithmic\\nframework for learning low dimensional continuous feature mapping for a node.\\nConsequently, the nodes having similar core numbers are relatively closer in\\nthe vector space that we learn.\\nWe further demonstrate the effectiveness of core2vec by comparing word\\nsimilarity scores obtained by our method where the node representations are\\ndrawn from standard word association graphs against scores computed by other\\nstate-of-the-art network representation techniques like node2vec, DeepWalk and\\nLINE. Our results always outperform these existing methods\\n',\n",
       " '  Numerous studies have been carried out to measure wind pressures around\\ncircular cylinders since the early 20th century due to its engineering\\nsignificance. Consequently, a large amount of wind pressure data sets have\\naccumulated, which presents an excellent opportunity for using machine learning\\n(ML) techniques to train models to predict wind pressures around circular\\ncylinders. Wind pressures around smooth circular cylinders are a function of\\nmainly the Reynolds number (Re), turbulence intensity (Ti) of the incident\\nwind, and circumferential angle of the cylinder. Considering these three\\nparameters as the inputs, this study trained two ML models to predict mean and\\nfluctuating pressures respectively. Three machine learning algorithms including\\ndecision tree regressor, random forest, and gradient boosting regression trees\\n(GBRT) were tested. The GBRT models exhibited the best performance for\\npredicting both mean and fluctuating pressures, and they are capable of making\\naccurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to\\n15%. It is believed that the GBRT models provide very efficient and economical\\nalternative to traditional wind tunnel tests and computational fluid dynamic\\nsimulations for determining wind pressures around smooth circular cylinders\\nwithin the studied Re and Ti range.\\n',\n",
       " '  We study the problem of constructing a (near) uniform random proper\\n$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum\\ndegree $\\\\Delta$. (Proper in that no edge is mono-colored and simple in that two\\nedges have maximum intersection of size one). We show that if $q\\\\geq\\n\\\\max\\\\{C_k\\\\log n,500k^3\\\\Delta^{1/(k-1)}\\\\}$ then the Glauber Dynamics will become\\nclose to uniform in $O(n\\\\log n)$ time, given a random (improper) start. This\\nimproves on the results in Frieze and Melsted [5].\\n',\n",
       " '  We begin by introducing the main ideas of the paper under discussion. We\\ndiscuss some interesting issues regarding adaptive component-wise credible\\nintervals. We then briefly touch upon the concepts of self-similarity and\\nexcessive bias restriction. This is then followed by some comments on the\\nextensive simulation study carried out in the paper.\\n',\n",
       " '  Time series shapelets are discriminative sub-sequences and their similarity\\nto time series can be used for time series classification. Initial shapelet\\nextraction algorithms searched shapelets by complete enumeration of all\\npossible data sub-sequences. Research on shapelets for univariate time series\\nproposed a mechanism called shapelet learning which parameterizes the shapelets\\nand learns them jointly with a prediction model in an optimization procedure.\\nTrivial extension of this method to multivariate time series does not yield\\nvery good results due to the presence of noisy channels which lead to\\noverfitting. In this paper we propose a shapelet learning scheme for\\nmultivariate time series in which we introduce channel masks to discount noisy\\nchannels and serve as an implicit regularization.\\n',\n",
       " '  In this work, we present an experimental study of spin mediated enhanced\\nnegative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The\\nresistance measurement shows a reduction of ~2.5% for the bilayer specimen as\\ncompared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane\\napplied magnetic field of 3T. In the Ni80Fe20-only film, the negative\\nmagnetoresistance behavior is attributed to anisotropic magnetoresistance. We\\npropose that spin polarization due to spin-Hall effect is the underlying cause\\nof the enhanced negative magnetoresistance observed in the bilayer. Silicon has\\nweak spin orbit coupling so spin Hall magnetoresistance measurement is not\\nfeasible. We use V2{\\\\omega} and V3{\\\\omega} measurement as a function of\\nmagnetic field and angular rotation of magnetic field in direction normal to\\nelectric current to elucidate the spin-Hall effect. The angular rotation of\\nmagnetic field shows a sinusoidal behavior for both V2{\\\\omega} and V3{\\\\omega},\\nwhich is attributed to the spin phonon interactions resulting from the\\nspin-Hall effect mediated spin polarization. We propose that the spin\\npolarization leads to a decrease in hole-phonon scattering resulting in\\nenhanced negative magnetoresistance.\\n',\n",
       " '  Recent work on the representation of functions on sets has considered the use\\nof summation in a latent space to enforce permutation invariance. In\\nparticular, it has been conjectured that the dimension of this latent space may\\nremain fixed as the cardinality of the sets under consideration increases.\\nHowever, we demonstrate that the analysis leading to this conjecture requires\\nmappings which are highly discontinuous and argue that this is only of limited\\npractical use. Motivated by this observation, we prove that an implementation\\nof this model via continuous mappings (as provided by e.g. neural networks or\\nGaussian processes) actually imposes a constraint on the dimensionality of the\\nlatent space. Practical universal function representation for set inputs can\\nonly be achieved with a latent dimension at least the size of the maximum\\nnumber of input elements.\\n',\n",
       " '  Measurement error in observational datasets can lead to systematic bias in\\ninferences based on these datasets. As studies based on observational data are\\nincreasingly used to inform decisions with real-world impact, it is critical\\nthat we develop a robust set of techniques for analyzing and adjusting for\\nthese biases. In this paper we present a method for estimating the distribution\\nof an outcome given a binary exposure that is subject to underreporting. Our\\nmethod is based on a missing data view of the measurement error problem, where\\nthe true exposure is treated as a latent variable that is marginalized out of a\\njoint model. We prove three different conditions under which the outcome\\ndistribution can still be identified from data containing only error-prone\\nobservations of the exposure. We demonstrate this method on synthetic data and\\nanalyze its sensitivity to near violations of the identifiability conditions.\\nFinally, we use this method to estimate the effects of maternal smoking and\\nopioid use during pregnancy on childhood obesity, two import problems from\\npublic health. Using the proposed method, we estimate these effects using only\\nsubject-reported drug use data and substantially refine the range of estimates\\ngenerated by a sensitivity analysis-based approach. Further, the estimates\\nproduced by our method are consistent with existing literature on both the\\neffects of maternal smoking and the rate at which subjects underreport smoking.\\n',\n",
       " \"  An extremely simple, description of Karmarkar's algorithm with very few\\ntechnical terms is given.\\n\",\n",
       " '  We consider a wireless sensor network that uses inductive near-field coupling\\nfor wireless powering or communication, or for both. The severely limited range\\nof an inductively coupled source-destination pair can be improved using\\nresonant relay devices, which are purely passive in nature. Utilization of such\\nmagneto-inductive relays has only been studied for regular network topologies,\\nallowing simplified assumptions on the mutual antenna couplings. In this work\\nwe present an analysis of magneto-inductive passive relaying in arbitrarily\\narranged networks. We find that the resulting channel has characteristics\\nsimilar to multipath fading: the channel power gain is governed by a\\nnon-coherent sum of phasors, resulting in increased frequency selectivity. We\\npropose and study two strategies to increase the channel power gain of random\\nrelay networks: i) deactivation of individual relays by open-circuit switching\\nand ii) frequency tuning. The presented results show that both methods improve\\nthe utilization of available passive relays, leading to reliable and\\nsignificant performance gains.\\n',\n",
       " '  Transfer operators such as the Perron--Frobenius or Koopman operator play an\\nimportant role in the global analysis of complex dynamical systems. The\\neigenfunctions of these operators can be used to detect metastable sets, to\\nproject the dynamics onto the dominant slow processes, or to separate\\nsuperimposed signals. We extend transfer operator theory to reproducing kernel\\nHilbert spaces and show that these operators are related to Hilbert space\\nrepresentations of conditional distributions, known as conditional mean\\nembeddings in the machine learning community. Moreover, numerical methods to\\ncompute empirical estimates of these embeddings are akin to data-driven methods\\nfor the approximation of transfer operators such as extended dynamic mode\\ndecomposition and its variants. One main benefit of the presented kernel-based\\napproaches is that these methods can be applied to any domain where a\\nsimilarity measure given by a kernel is available. We illustrate the results\\nwith the aid of guiding examples and highlight potential applications in\\nmolecular dynamics as well as video and text data analysis.\\n',\n",
       " '  In this paper we consider the three-dimensional Schrödinger operator with\\na $\\\\delta$-interaction of strength $\\\\alpha > 0$ supported on an unbounded\\nsurface parametrized by the mapping $\\\\mathbb{R}^2\\\\ni x\\\\mapsto (x,\\\\beta f(x))$,\\nwhere $\\\\beta \\\\in [0,\\\\infty)$ and $f\\\\colon \\\\mathbb{R}^2\\\\rightarrow\\\\mathbb{R}$,\\n$f\\\\not\\\\equiv 0$, is a $C^2$-smooth, compactly supported function. The surface\\nsupporting the interaction can be viewed as a local deformation of the plane.\\nIt is known that the essential spectrum of this Schrödinger operator\\ncoincides with $[-\\\\frac14\\\\alpha^2,+\\\\infty)$. We prove that for all sufficiently\\nsmall $\\\\beta > 0$ its discrete spectrum is non-empty and consists of a unique\\nsimple eigenvalue. Moreover, we obtain an asymptotic expansion of this\\neigenvalue in the limit $\\\\beta \\\\rightarrow 0+$. In particular, this eigenvalue\\ntends to $-\\\\frac14\\\\alpha^2$ exponentially fast as $\\\\beta\\\\rightarrow 0+$.\\n',\n",
       " '  In this paper, a comparative study was conducted between complex networks\\nrepresenting origin and destination survey data. Similarities were found\\nbetween the characteristics of the networks of Brazilian cities with networks\\nof foreign cities. Power laws were found in the distributions of edge weights\\nand this scale - free behavior can occur due to the economic characteristics of\\nthe cities.\\n',\n",
       " \"  Tension-network (`tensegrity') robots encounter many control challenges as\\narticulated soft robots, due to the structures' high-dimensional nonlinear\\ndynamics. Control approaches have been developed which use the inverse\\nkinematics of tensegrity structures, either for open-loop control or as\\nequilibrium inputs for closed-loop controllers. However, current formulations\\nof the tensegrity inverse kinematics problem are limited in robotics\\napplications: first, they can lead to higher than needed cable tensions, and\\nsecond, may lack solutions when applied to robots with high node-to-cable\\nratios. This work provides progress in both directions. To address the first\\nlimitation, the objective function for the inverse kinematics optimization\\nproblem is modified to produce cable tensions as low or lower than before, thus\\nreducing the load on the robots' motors. For the second, a reformulation of the\\nstatic equilibrium constraint is proposed, which produces solutions independent\\nof the number of nodes within each rigid body. Simulation results using the\\nsecond reformulation on a specific tensegrity spine robot show reasonable\\nopen-loop control results, whereas the previous formulation could not produce\\nany solution.\\n\",\n",
       " \"  The statistical behaviour of the smallest eigenvalue has important\\nimplications for systems which can be modeled using a Wishart-Laguerre\\nensemble, the regular one or the fixed trace one. For example, the density of\\nthe smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role\\nin characterizing multiple channel telecommunication systems. Similarly, in the\\nquantum entanglement problem, the smallest eigenvalue of the fixed trace\\nensemble carries information regarding the nature of entanglement.\\nFor real Wishart-Laguerre matrices, there exists an elegant recurrence scheme\\nsuggested by Edelman to directly obtain the exact expression for the smallest\\neigenvalue density. In the case of complex Wishart-Laguerre matrices, for\\nfinding exact and explicit expressions for the smallest eigenvalue density,\\nexisting results based on determinants become impractical when the determinants\\ninvolve large-size matrices. In this work, we derive a recurrence scheme for\\nthe complex case which is analogous to that of Edelman's for the real case.\\nThis is used to obtain exact results for the smallest eigenvalue density for\\nboth the regular, and the fixed trace complex Wishart-Laguerre ensembles. We\\nvalidate our analytical results using Monte Carlo simulations. We also study\\nscaled Wishart-Laguerre ensemble and investigate its efficacy in approximating\\nthe fixed-trace ensemble. Eventually, we apply our result for the fixed-trace\\nensemble to investigate the behaviour of the smallest eigenvalue in the\\nparadigmatic system of coupled kicked tops.\\n\",\n",
       " '  Dam breach models are commonly used to predict outflow hydrographs of\\npotentially failing dams and are key ingredients for evaluating flood risk. In\\nthis paper a new dam breach modeling framework is introduced that shall improve\\nthe reliability of hydrograph predictions of homogeneous earthen embankment\\ndams. Striving for a small number of parameters, the simplified physics-based\\nmodel describes the processes of failing embankment dams by breach enlargement,\\ndriven by progressive surface erosion. Therein the erosion rate of dam material\\nis modeled by empirical sediment transport formulations. Embedding the model\\ninto a Bayesian multilevel framework allows for quantitative analysis of\\ndifferent categories of uncertainties. To this end, data available in\\nliterature of observed peak discharge and final breach width of historical dam\\nfailures was used to perform model inversion by applying Markov Chain Monte\\nCarlo simulation. Prior knowledge is mainly based on non-informative\\ndistribution functions. The resulting posterior distribution shows that the\\nmain source of uncertainty is a correlated subset of parameters, consisting of\\nthe residual error term and the epistemic term quantifying the breach erosion\\nrate. The prediction intervals of peak discharge and final breach width are\\ncongruent with values known from literature. To finally predict the outflow\\nhydrograph for real case applications, an alternative residual model was\\nformulated that assumes perfect data and a perfect model. The fully\\nprobabilistic fashion of hydrograph prediction has the potential to improve the\\nadequate risk management of downstream flooding.\\n',\n",
       " '  We study the near-infrared properties of 690 Mira candidates in the central\\nregion of the Large Magellanic Cloud, based on time-series observations at\\nJHKs. We use densely-sampled I-band observations from the OGLE project to\\ngenerate template light curves in the near infrared and derive robust mean\\nmagnitudes at those wavelengths. We obtain near-infrared Period-Luminosity\\nrelations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We\\nstudy the Period-Luminosity-Color relations and the color excesses of\\nCarbon-rich Miras, which show evidence for a substantially different reddening\\nlaw.\\n',\n",
       " '  There is an inherent need for autonomous cars, drones, and other robots to\\nhave a notion of how their environment behaves and to anticipate changes in the\\nnear future. In this work, we focus on anticipating future appearance given the\\ncurrent frame of a video. Existing work focuses on either predicting the future\\nappearance as the next frame of a video, or predicting future motion as optical\\nflow or motion trajectories starting from a single video frame. This work\\nstretches the ability of CNNs (Convolutional Neural Networks) to predict an\\nanticipation of appearance at an arbitrarily given future time, not necessarily\\nthe next video frame. We condition our predicted future appearance on a\\ncontinuous time variable that allows us to anticipate future frames at a given\\ntemporal distance, directly from the input video frame. We show that CNNs can\\nlearn an intrinsic representation of typical appearance changes over time and\\nsuccessfully generate realistic predictions at a deliberate time difference in\\nthe near future.\\n',\n",
       " '  We consider the problem of dynamic spectrum access for network utility\\nmaximization in multichannel wireless networks. The shared bandwidth is divided\\ninto K orthogonal channels. In the beginning of each time slot, each user\\nselects a channel and transmits a packet with a certain transmission\\nprobability. After each time slot, each user that has transmitted a packet\\nreceives a local observation indicating whether its packet was successfully\\ndelivered or not (i.e., ACK signal). The objective is a multi-user strategy for\\naccessing the spectrum that maximizes a certain network utility in a\\ndistributed manner without online coordination or message exchanges between\\nusers. Obtaining an optimal solution for the spectrum access problem is\\ncomputationally expensive in general due to the large state space and partial\\nobservability of the states. To tackle this problem, we develop a novel\\ndistributed dynamic spectrum access algorithm based on deep multi-user\\nreinforcement leaning. Specifically, at each time slot, each user maps its\\ncurrent state to spectrum access actions based on a trained deep-Q network used\\nto maximize the objective function. Game theoretic analysis of the system\\ndynamics is developed for establishing design principles for the implementation\\nof the algorithm. Experimental results demonstrate strong performance of the\\nalgorithm.\\n',\n",
       " '  We design a new myopic strategy for a wide class of sequential design of\\nexperiment (DOE) problems, where the goal is to collect data in order to to\\nfulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling\\n(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for\\nmulti-armed bandits and leverages the flexibility of probabilistic programming\\nand approximate Bayesian inference to address a broad set of problems.\\nEmpirically, this general-purpose strategy is competitive with more specialised\\nmethods in a wide array of DOE tasks, and more importantly, enables addressing\\ncomplex DOE goals where no existing method seems applicable. On the theoretical\\nside, we leverage ideas from adaptive submodularity and reinforcement learning\\nto derive conditions under which MPS achieves sublinear regret against natural\\nbenchmark policies.\\n',\n",
       " '  Deep convolutional neural networks have liberated its extraordinary power on\\nvarious tasks. However, it is still very challenging to deploy state-of-the-art\\nmodels into real-world applications due to their high computational complexity.\\nHow can we design a compact and effective network without massive experiments\\nand expert knowledge? In this paper, we propose a simple and effective\\nframework to learn and prune deep models in an end-to-end manner. In our\\nframework, a new type of parameter -- scaling factor is first introduced to\\nscale the outputs of specific structures, such as neurons, groups or residual\\nblocks. Then we add sparsity regularizations on these factors, and solve this\\noptimization problem by a modified stochastic Accelerated Proximal Gradient\\n(APG) method. By forcing some of the factors to zero, we can safely remove the\\ncorresponding structures, thus prune the unimportant parts of a CNN. Comparing\\nwith other structure selection methods that may need thousands of trials or\\niterative fine-tuning, our method is trained fully end-to-end in one training\\npass without bells and whistles. We evaluate our method, Sparse Structure\\nSelection with several state-of-the-art CNNs, and demonstrate very promising\\nresults with adaptive depth and width selection.\\n',\n",
       " '  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both\\nwith and without a significant mean field are obtained. Exact bounds are\\nderived for the total energy which conform with the Kolmogorov phenomenology of\\nturbulence. Best fits to numerical data show the same functional dependences as\\nthe inequalities obtained from optimum theory.\\n',\n",
       " '  Using a projection-based decoupling of the Fokker-Planck equation, control\\nstrategies that allow to speed up the convergence to the stationary\\ndistribution are investigated. By means of an operator theoretic framework for\\na bilinear control system, two different feedback control laws are proposed.\\nProjected Riccati and Lyapunov equations are derived and properties of the\\nassociated solutions are given. The well-posedness of the closed loop systems\\nis shown and local and global stabilization results, respectively, are\\nobtained. An essential tool in the construction of the controls is the choice\\nof appropriate control shape functions. Results for a two dimensional double\\nwell potential illustrate the theoretical findings in a numerical setup.\\n',\n",
       " '  We offer a generalization of a formula of Popov involving the Von Mangoldt\\nfunction. Some commentary on its relation to other results in analytic number\\ntheory is mentioned as well as an analogue involving the m$\\\\ddot{o}$bius\\nfunction.\\n',\n",
       " '  In this paper, we show that any compact manifold that carries a\\nSL(n;R)-foliation is fibered on the circle S^1.\\n',\n",
       " '  Given the importance of crystal symmetry for the emergence of topological\\nquantum states, we have studied, as exemplified in NbNiTe2, the interplay of\\ncrystal symmetry, atomic displacements (lattice vibration), band degeneracy,\\nand band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an\\ninversion center arising from two glide planes and one mirror plane with a\\n2-fold rotation and screw axis - a full gap opening exists between two band\\nmanifolds near the Fermi energy. Upon atomic displacements by optical phonons,\\nthe symmetry lowers to space group 28 (Pma2), eliminating one glide plane along\\nc, the associated rotation and screw axis, and the inversion center. As a\\nresult, twenty Weyl points emerge, including four type-II Weyl points in the\\nG-X direction at the boundary between a pair of adjacent electron and hole\\nbands. Thus, optical phonons may offer control of the transition to a Weyl\\nfermion state.\\n',\n",
       " \"  Several social, medical, engineering and biological challenges rely on\\ndiscovering the functionality of networks from their structure and node\\nmetadata, when it is available. For example, in chemoinformatics one might want\\nto detect whether a molecule is toxic based on structure and atomic types, or\\ndiscover the research field of a scientific collaboration network. Existing\\ntechniques rely on counting or measuring structural patterns that are known to\\nshow large variations from network to network, such as the number of triangles,\\nor the assortativity of node metadata. We introduce the concept of multi-hop\\nassortativity, that captures the similarity of the nodes situated at the\\nextremities of a randomly selected path of a given length. We show that\\nmulti-hop assortativity unifies various existing concepts and offers a\\nversatile family of 'fingerprints' to characterize networks. These fingerprints\\nallow in turn to recover the functionalities of a network, with the help of the\\nmachine learning toolbox. Our method is evaluated empirically on established\\nsocial and chemoinformatic network benchmarks. Results reveal that our\\nassortativity based features are competitive providing highly accurate results\\noften outperforming state of the art methods for the network classification\\ntask.\\n\",\n",
       " '  This paper is concerned with the online estimation of a nonlinear dynamic\\nsystem from a series of noisy measurements. The focus is on cases wherein\\noutliers are present in-between normal noises. We assume that the outliers\\nfollow an unknown generating mechanism which deviates from that of normal\\nnoises, and then model the outliers using a Bayesian nonparametric model called\\nDirichlet process mixture (DPM). A sequential particle-based algorithm is\\nderived for posterior inference for the outlier model as well as the state of\\nthe system to be estimated. The resulting algorithm is termed DPM based robust\\nPF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to\\n\"speak for itself\" to determine the complexity and structure of the outlier\\nmodel. Simulation results show that it performs remarkably better than two\\nstate-of-the-art methods especially when outliers appear frequently along time.\\n',\n",
       " '  Computed tomography (CT) examinations are commonly used to predict lung\\nnodule malignancy in patients, which are shown to improve noninvasive early\\ndiagnosis of lung cancer. It remains challenging for computational approaches\\nto achieve performance comparable to experienced radiologists. Here we present\\nNoduleX, a systematic approach to predict lung nodule malignancy from CT data,\\nbased on deep learning convolutional neural networks (CNN). For training and\\nvalidation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.\\nAll nodules were identified and classified by four experienced thoracic\\nradiologists who participated in the LIDC project. NoduleX achieves high\\naccuracy for nodule malignancy classification, with an AUC of ~0.99. This is\\ncommensurate with the analysis of the dataset by experienced radiologists. Our\\napproach, NoduleX, provides an effective framework for highly accurate nodule\\nmalignancy prediction with the model trained on a large patient population. Our\\nresults are replicable with software available at\\nthis http URL.\\n',\n",
       " '  The turbulent Rayleigh--Taylor system in a rotating reference frame is\\ninvestigated by direct numerical simulations within the Oberbeck-Boussinesq\\napproximation. On the basis of theoretical arguments, supported by our\\nsimulations, we show that the Rossby number decreases in time, and therefore\\nthe Coriolis force becomes more important as the system evolves and produces\\nmany effects on Rayleigh--Taylor turbulence. We find that rotation reduces the\\nintensity of turbulent velocity fluctuations and therefore the growth rate of\\nthe temperature mixing layer. Moreover, in presence of rotation the conversion\\nof potential energy into turbulent kinetic energy is found to be less effective\\nand the efficiency of the heat transfer is reduced. Finally, during the\\nevolution of the mixing layer we observe the development of a\\ncyclone-anticyclone asymmetry.\\n',\n",
       " '  In the animal world, the competition between individuals belonging to\\ndifferent species for a resource often requires the cooperation of several\\nindividuals in groups. This paper proposes a generalization of the Hawk-Dove\\nGame for an arbitrary number of agents: the N-person Hawk-Dove Game. In this\\nmodel, doves exemplify the cooperative behavior without intraspecies conflict,\\nwhile hawks represent the aggressive behavior. In the absence of hawks, doves\\nshare the resource equally and avoid conflict, but having hawks around lead to\\ndoves escaping without fighting. Conversely, hawks fight for the resource at\\nthe cost of getting injured. Nevertheless, if doves are present in sufficient\\nnumber to expel the hawks, they can aggregate to protect the resource, and thus\\navoid being plundered by hawks. We derive and numerically solve an exact\\nequation for the evolution of the system in both finite and infinite well-mixed\\npopulations, finding the conditions for stable coexistence between both\\nspecies. Furthermore, by varying the different parameters, we found a scenario\\nof bifurcations that leads the system from dominating hawks and coexistence to\\nbi-stability, multiple interior equilibria and dominating doves.\\n',\n",
       " \"  With approximately half of the world's population at risk of contracting\\ndengue, this mosquito-borne disease is of global concern. International\\ntravellers significantly contribute to dengue's rapid and large-scale spread by\\nimporting the disease from endemic into non-endemic countries. To prevent\\nfuture outbreaks and dengue from establishing in non-endemic countries,\\nknowledge about the arrival time and location of infected travellers is\\ncrucial. We propose a network model that predicts the monthly number of dengue\\ninfected air passengers arriving at any given airport. We consider\\ninternational air travel volumes, monthly dengue incidence rates and temporal\\ninfection dynamics. Our findings shed light onto dengue importation routes and\\nreveal country-specific reporting rates that have been until now largely\\nunknown.\\n\",\n",
       " '  The best summary of a long video differs among different people due to its\\nhighly subjective nature. Even for the same person, the best summary may change\\nwith time or mood. In this paper, we introduce the task of generating\\ncustomized video summaries through simple text. First, we train a deep\\narchitecture to effectively learn semantic embeddings of video frames by\\nleveraging the abundance of image-caption data via a progressive and residual\\nmanner. Given a user-specific text description, our algorithm is able to select\\nsemantically relevant video segments and produce a temporally aligned video\\nsummary. In order to evaluate our textually customized video summaries, we\\nconduct experimental comparison with baseline methods that utilize ground-truth\\ninformation. Despite the challenging baselines, our method still manages to\\nshow comparable or even exceeding performance. We also show that our method is\\nable to generate semantically diverse video summaries by only utilizing the\\nlearned visual embeddings.\\n',\n",
       " '  Recently, heavily doped semiconductors are emerging as an alternate for low\\nloss plasmonic materials. InN, belonging to the group III nitrides, possesses\\nthe unique property of surface electron accumulation (SEA) which provides two\\ndimensional electron gas (2DEG) system. In this report, we demonstrated the\\nsurface plasmon properties of InN nanoparticles originating from SEA using the\\nreal space mapping of the surface plasmon fields for the first time. The SEA is\\nconfirmed by Raman studies which are further corroborated by photoluminescence\\nand photoemission spectroscopic studies. The frequency of 2DEG corresponding to\\nSEA is found to be in the THz region. The periodic fringes are observed in the\\nnear-field scanning optical microscopic images of InN nanostructures. The\\nobserved fringes are attributed to the interference of propagated and back\\nreflected surface plasmon polaritons (SPPs). The observation of SPPs is solely\\nattributed to the 2DEG corresponding to the SEA of InN. In addition, resonance\\nkind of behavior with the enhancement of the near-field intensity is observed\\nin the near-field images of InN nanostructures. Observation of SPPs indicates\\nthat InN with SEA can be a promising THz plasmonic material for the light\\nconfinement.\\n',\n",
       " '  It is shown that using beam splitters with non-equal wave vectors results in\\na new recoil diagram which is qualitatively different from the well-known\\ndiagram associated with the Mach-Zehnder atom interferometer. We predict a new\\nasymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses\\na Raman beam splitter. The main feature is that the phase of AMZAI contains a\\nquantum part proportional to the recoil frequency. A response sensitive only to\\nthe quantum phase was found. A new technique to measure the recoil frequency\\nand fine structure constant is proposed and studied outside of the Raman-Nath\\napproximation.\\n',\n",
       " '  In this paper, we consider a partial information two-person zero-sum\\nstochastic differential game problem where the system is governed by a backward\\nstochastic differential equation driven by Teugels martingales associated with\\na Lévy process and an independent Brownian motion. One sufficient (a\\nverification theorem) and one necessary conditions for the existence of optimal\\ncontrols are proved. To illustrate the general results, a linear quadratic\\nstochastic differential game problem is discussed.\\n',\n",
       " \"  In recent years, research has been done on applying Recurrent Neural Networks\\n(RNNs) as recommender systems. Results have been promising, especially in the\\nsession-based setting where RNNs have been shown to outperform state-of-the-art\\nmodels. In many of these experiments, the RNN could potentially improve the\\nrecommendations by utilizing information about the user's past sessions, in\\naddition to its own interactions in the current session. A problem for\\nsession-based recommendation, is how to produce accurate recommendations at the\\nstart of a session, before the system has learned much about the user's current\\ninterests. We propose a novel approach that extends a RNN recommender to be\\nable to process the user's recent sessions, in order to improve\\nrecommendations. This is done by using a second RNN to learn from recent\\nsessions, and predict the user's interest in the current session. By feeding\\nthis information to the original RNN, it is able to improve its\\nrecommendations. Our experiments on two different datasets show that the\\nproposed approach can significantly improve recommendations throughout the\\nsessions, compared to a single RNN working only on the current session. The\\nproposed model especially improves recommendations at the start of sessions,\\nand is therefore able to deal with the cold start problem within sessions.\\n\",\n",
       " '  Shock wave interactions with defects, such as pores, are known to play a key\\nrole in the chemical initiation of energetic materials. The shock response of\\nhexanitrostilbene is studied through a combination of large scale reactive\\nmolecular dynamics and mesoscale hydrodynamic simulations. In order to extend\\nour simulation capability at the mesoscale to include weak shock conditions (<\\n6 GPa), atomistic simulations of pore collapse are used to define a strain rate\\ndependent strength model. Comparing these simulation methods allows us to\\nimpose physically-reasonable constraints on the mesoscale model parameters. In\\ndoing so, we have been able to study shock waves interacting with pores as a\\nfunction of this viscoplastic material response. We find that the pore collapse\\nbehavior of weak shocks is characteristically different to that of strong\\nshocks.\\n',\n",
       " '  We propose factor models for the cross-section of daily cryptoasset returns\\nand provide source code for data downloads, computing risk factors and\\nbacktesting them out-of-sample. In \"cryptoassets\" we include all\\ncryptocurrencies and a host of various other digital assets (coins and tokens)\\nfor which exchange market data is available. Based on our empirical analysis,\\nwe identify the leading factor that appears to strongly contribute into daily\\ncryptoasset returns. Our results suggest that cross-sectional statistical\\narbitrage trading may be possible for cryptoassets subject to efficient\\nexecutions and shorting.\\n',\n",
       " '  Many signals on Cartesian product graphs appear in the real world, such as\\ndigital images, sensor observation time series, and movie ratings on Netflix.\\nThese signals are \"multi-dimensional\" and have directional characteristics\\nalong each factor graph. However, the existing graph Fourier transform does not\\ndistinguish these directions, and assigns 1-D spectra to signals on product\\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\\nmain result is a multi-dimensional graph Fourier transform that solves such\\nproblems associated with the conventional GFT. Using algebraic properties of\\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\\nthe conventional GFT into the multi-dimensional frequency domain, of which each\\ndimension represents a directional frequency along each factor graph. Thus, the\\nmulti-dimensional graph Fourier transform enables directional frequency\\nanalysis, in addition to frequency analysis with the conventional GFT.\\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\\nfilterings and stationarities that utilize dimensional information of graph\\nsignals, which are also discussed in this study. The proposed methods are\\napplicable to a wide variety of data that can be regarded as signals on\\nCartesian product graphs. This study also notes that multivariate graph signals\\ncan be regarded as 2-D univariate graph signals. This correspondence provides\\nnatural definitions of the multivariate graph Fourier transform and the\\nmultivariate stationarity based on their 2-D univariate versions.\\n',\n",
       " '  The double exponential formula was introduced for calculating definite\\nintegrals with singular point oscillation functions and Fourier-integrals. The\\ndouble exponential transformation is not only useful for numerical computations\\nbut it is also used in different methods of Sinc theory. In this paper we use\\ndouble exponential transformation for calculating particular improper\\nintegrals. By improving integral estimates having singular final points. By\\ncomparison between double exponential transformations and single exponential\\ntransformations it is proved that the error margin of double exponential\\ntransformations is smaller. Finally Fourier-integral and double exponential\\ntransformations are discussed.\\n',\n",
       " '  Strain engineering has attracted great attention, particularly for epitaxial\\nfilms grown on a different substrate. Residual strains of SiC have been widely\\nemployed to form ultra-high frequency and high Q factor resonators. However, to\\ndate the highest residual strain of SiC was reported to be limited to\\napproximately 0.6%. Large strains induced into SiC could lead to several\\ninteresting physical phenomena, as well as significant improvement of resonant\\nfrequencies. We report an unprecedented nano strain-amplifier structure with an\\nultra-high residual strain up to 8% utilizing the natural residual stress\\nbetween epitaxial 3C SiC and Si. In addition, the applied strain can be tuned\\nby changing the dimensions of the amplifier structure. The possibility of\\nintroducing such a controllable and ultra-high strain will open the door to\\ninvestigating the physics of SiC in large strain regimes, and the development\\nof ultra sensitive mechanical sensors.\\n',\n",
       " '  A complex system can be represented and analyzed as a network, where nodes\\nrepresent the units of the network and edges represent connections between\\nthose units. For example, a brain network represents neurons as nodes and axons\\nbetween neurons as edges. In many networks, some nodes have a\\ndisproportionately high number of edges. These nodes also have many edges\\nbetween each other, and are referred to as the rich club. In many different\\nnetworks, the nodes of this club are assumed to support global network\\nintegration. However, another set of nodes potentially exhibits a connectivity\\nstructure that is more advantageous to global network integration. Here, in a\\nmyriad of different biological and man-made networks, we discover the diverse\\nclub--a set of nodes that have edges diversely distributed across the network.\\nThe diverse club exhibits, to a greater extent than the rich club, properties\\nconsistent with an integrative network function--these nodes are more highly\\ninterconnected and their edges are more critical for efficient global\\nintegration. Moreover, we present a generative evolutionary network model that\\nproduces networks with a diverse club but not a rich club, thus demonstrating\\nthat these two clubs potentially evolved via distinct selection pressures.\\nGiven the variety of different networks that we analyzed--the c. elegans, the\\nmacaque brain, the human brain, the United States power grid, and global air\\ntraffic--the diverse club appears to be ubiquitous in complex networks. These\\nresults warrant the distinction and analysis of two critical clubs of nodes in\\nall complex systems.\\n',\n",
       " \"  Neural network based generative models with discriminative components are a\\npowerful approach for semi-supervised learning. However, these techniques a)\\ncannot account for model uncertainty in the estimation of the model's\\ndiscriminative component and b) lack flexibility to capture complex stochastic\\npatterns in the label generation process. To avoid these problems, we first\\npropose to use a discriminative component with stochastic inputs for increased\\nnoise flexibility. We show how an efficient Gibbs sampling procedure can\\nmarginalize the stochastic inputs when inferring missing labels in this model.\\nFollowing this, we extend the discriminative component to be fully Bayesian and\\nproduce estimates of uncertainty in its parameter values. This opens the door\\nfor semi-supervised Bayesian active learning.\\n\",\n",
       " '  Detection of interactions between treatment effects and patient descriptors\\nin clinical trials is critical for optimizing the drug development process. The\\nincreasing volume of data accumulated in clinical trials provides a unique\\nopportunity to discover new biomarkers and further the goal of personalized\\nmedicine, but it also requires innovative robust biomarker detection methods\\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\\nof novel univariate statistical tests, based on the theory of random walks,\\nwhich are able to capture non-linear and non-monotonic covariate-treatment\\ninteractions. We also propose a novel combined test, which leverages the power\\nof all of our proposed univariate tests into a single general-case tool. We\\npresent results for both synthetic trials as well as real-world clinical\\ntrials, where we compare our method with state-of-the-art techniques and\\ndemonstrate the utility and robustness of our approach.\\n',\n",
       " '  The limitations in performance of the present RICH system in the LHCb\\nexperiment are given by the natural chromatic dispersion of the gaseous\\nCherenkov radiator, the aberrations of the optical system and the pixel size of\\nthe photon detectors. Moreover, the overall PID performance can be affected by\\nhigh detector occupancy as the pattern recognition becomes more difficult with\\nhigh particle multiplicities. This paper shows a way to improve performance by\\nsystematically addressing each of the previously mentioned limitations. These\\nideas are applied in the present and future upgrade phases of the LHCb\\nexperiment. Although applied to specific circumstances, they are used as a\\nparadigm on what is achievable in the development and realisation of high\\nprecision RICH detectors.\\n',\n",
       " '  Given a klt singularity $x\\\\in (X, D)$, we show that a quasi-monomial\\nvaluation $v$ with a finitely generated associated graded ring is the minimizer\\nof the normalized volume function $\\\\widehat{\\\\rm vol}_{(X,D),x}$, if and only if\\n$v$ induces a degeneration to a K-semistable log Fano cone singularity.\\nMoreover, such a minimizer is unique among all quasi-monomial valuations up to\\nrescaling. As a consequence, we prove that for a klt singularity $x\\\\in X$ on\\nthe Gromov-Hausdorff limit of Kähler-Einstein Fano manifolds, the\\nintermediate K-semistable cone associated to its metric tangent cone is\\nuniquely determined by the algebraic structure of $x\\\\in X$, hence confirming a\\nconjecture by Donaldson-Sun.\\n',\n",
       " '  Condensed-matter analogs of the Higgs boson in particle physics allow\\ninsights into its behavior in different symmetries and dimensionalities.\\nEvidence for the Higgs mode has been reported in a number of different\\nsettings, including ultracold atomic gases, disordered superconductors, and\\ndimerized quantum magnets. However, decay processes of the Higgs mode (which\\nare eminently important in particle physics) have not yet been studied in\\ncondensed matter due to the lack of a suitable material system coupled to a\\ndirect experimental probe. A quantitative understanding of these processes is\\nparticularly important for low-dimensional systems where the Higgs mode decays\\nrapidly and has remained elusive to most experimental probes. Here, we discover\\nand study the Higgs mode in a two-dimensional antiferromagnet using\\nspin-polarized inelastic neutron scattering. Our spin-wave spectra of\\nCa$_2$RuO$_4$ directly reveal a well-defined, dispersive Higgs mode, which\\nquickly decays into transverse Goldstone modes at the antiferromagnetic\\nordering wavevector. Through a complete mapping of the transverse modes in the\\nreciprocal space, we uniquely specify the minimal model Hamiltonian and\\ndescribe the decay process. We thus establish a novel condensed matter platform\\nfor research on the dynamics of the Higgs mode.\\n',\n",
       " '  Well-known for its simplicity and effectiveness in classification, AdaBoost,\\nhowever, suffers from overfitting when class-conditional distributions have\\nsignificant overlap. Moreover, it is very sensitive to noise that appears in\\nthe labels. This article tackles the above limitations simultaneously via\\noptimizing a modified loss function (i.e., the conditional risk). The proposed\\napproach has the following two advantages. (1) It is able to directly take into\\naccount label uncertainty with an associated label confidence. (2) It\\nintroduces a \"trustworthiness\" measure on training samples via the Bayesian\\nrisk rule, and hence the resulting classifier tends to have finite sample\\nperformance that is superior to that of the original AdaBoost when there is a\\nlarge overlap between class conditional distributions. Theoretical properties\\nof the proposed method are investigated. Extensive experimental results using\\nsynthetic data and real-world data sets from UCI machine learning repository\\nare provided. The empirical study shows the high competitiveness of the\\nproposed method in predication accuracy and robustness when compared with the\\noriginal AdaBoost and several existing robust AdaBoost algorithms.\\n',\n",
       " '  We study the problem of sparsity constrained $M$-estimation with arbitrary\\ncorruptions to both {\\\\em explanatory and response} variables in the\\nhigh-dimensional regime, where the number of variables $d$ is larger than the\\nsample size $n$. Our main contribution is a highly efficient gradient-based\\noptimization algorithm that we call Trimmed Hard Thresholding -- a robust\\nvariant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient\\ncomputations. Our algorithm can deal with a wide class of sparsity constrained\\n$M$-estimation problems, and we can tolerate a nearly dimension independent\\nfraction of arbitrarily corrupted samples. More specifically, when the\\ncorrupted fraction satisfies $\\\\epsilon \\\\lesssim {1} /\\\\left({\\\\sqrt{k} \\\\log\\n(nd)}\\\\right)$, where $k$ is the sparsity of the parameter, we obtain accurate\\nestimation and model selection guarantees with optimal sample complexity.\\nFurthermore, we extend our algorithm to sparse Gaussian graphical model\\n(precision matrix) estimation via a neighborhood selection approach. We\\ndemonstrate the effectiveness of robust estimation in sparse linear, logistic\\nregression, and sparse precision matrix estimation on synthetic and real-world\\nUS equities data.\\n',\n",
       " \"  We present a communication- and data-sensitive formulation of ADER-DG for\\nhyperbolic differential equation systems. Sensitive here has multiple flavours:\\nFirst, the formulation reduces the persistent memory footprint. This reduces\\npressure on the memory subsystem. Second, the formulation realises the\\nunderlying predictor-corrector scheme with single-touch semantics, i.e., each\\ndegree of freedom is read on average only once per time step from the main\\nmemory. This reduces communication through the memory controllers. Third, the\\nformulation breaks up the tight coupling of the explicit time stepping's\\nalgorithmic steps to mesh traversals. This averages out data access peaks.\\nDifferent operations and algorithmic steps are ran on different grid entities.\\nFinally, the formulation hides distributed memory data transfer behind the\\ncomputation aligned with the mesh traversal. This reduces pressure on the\\nmachine interconnects. All techniques applied by our formulation are elaborated\\nby means of a rigorous task formalism. They break up ADER-DG's tight causal\\ncoupling of compute steps and can be generalised to other predictor-corrector\\nschemes.\\n\",\n",
       " '  This paper outlines a methodology for Bayesian multimodel uncertainty\\nquantification (UQ) and propagation and presents an investigation into the\\neffect of prior probabilities on the resulting uncertainties. The UQ\\nmethodology is adapted from the information-theoretic method previously\\npresented by the authors (Zhang and Shields, 2018) to a fully Bayesian\\nconstruction that enables greater flexibility in quantifying uncertainty in\\nprobability model form. Being Bayesian in nature and rooted in UQ from small\\ndatasets, prior probabilities in both probability model form and model\\nparameters are shown to have a significant impact on quantified uncertainties\\nand, consequently, on the uncertainties propagated through a physics-based\\nmodel. These effects are specifically investigated for a simplified plate\\nbuckling problem with uncertainties in material properties derived from a small\\nnumber of experiments using noninformative priors and priors derived from past\\nstudies of varying appropriateness. It is illustrated that prior probabilities\\ncan have a significant impact on multimodel UQ for small datasets and\\ninappropriate (but seemingly reasonable) priors may even have lingering effects\\nthat bias probabilities even for large datasets. When applied to uncertainty\\npropagation, this may result in probability bounds on response quantities that\\ndo not include the true probabilities.\\n',\n",
       " '  Failing to distinguish between a sheepdog and a skyscraper should be worse\\nand penalized more than failing to distinguish between a sheepdog and a poodle;\\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\\nclassification/recognition via neural networks seldom view a sheepdog as more\\nsimilar to a poodle than to a skyscraper. We define a metric that, inter alia,\\ncan penalize failure to distinguish between a sheepdog and a skyscraper more\\nthan failure to distinguish between a sheepdog and a poodle. Unlike previously\\nemployed possibilities, this metric is based on an ultrametric tree associated\\nwith any given tree organization into a semantically meaningful hierarchy of a\\nclassifier\\'s classes.\\n',\n",
       " '  Achieving the goals in the title (and others) relies on a cardinality-wise\\nscanning of the ideals of the poset. Specifically, the relevant numbers\\nattached to the k+1 element ideals are inferred from the corresponding numbers\\nof the k-element (order) ideals. Crucial in all of this is a compressed\\nrepresentation (using wildcards) of the ideal lattice. The whole scheme invites\\ndistributed computation.\\n',\n",
       " '  We present a scalable, black box, perception-in-the-loop technique to find\\nadversarial examples for deep neural network classifiers. Black box means that\\nour procedure only has input-output access to the classifier, and not to the\\ninternal structure, parameters, or intermediate confidence values.\\nPerception-in-the-loop means that the notion of proximity between inputs can be\\ndirectly queried from human participants rather than an arbitrarily chosen\\nmetric. Our technique is based on covariance matrix adaptation evolution\\nstrategy (CMA-ES), a black box optimization approach. CMA-ES explores the\\nsearch space iteratively in a black box manner, by generating populations of\\ncandidates according to a distribution, choosing the best candidates according\\nto a cost function, and updating the posterior distribution to favor the best\\ncandidates. We run CMA-ES using human participants to provide the fitness\\nfunction, using the insight that the choice of best candidates in CMA-ES can be\\nnaturally modeled as a perception task: pick the top $k$ inputs perceptually\\nclosest to a fixed input. We empirically demonstrate that finding adversarial\\nexamples is feasible using small populations and few iterations. We compare the\\nperformance of CMA-ES on the MNIST benchmark with other black-box approaches\\nusing $L_p$ norms as a cost function, and show that it performs favorably both\\nin terms of success in finding adversarial examples and in minimizing the\\ndistance between the original and the adversarial input. In experiments on the\\nMNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find\\nperceptually similar adversarial inputs with a small number of iterations and\\nsmall population sizes when using perception-in-the-loop. Finally, we show that\\nnetworks trained specifically to be robust against $L_\\\\infty$ norm can still be\\nsusceptible to perceptually similar adversarial examples.\\n',\n",
       " '  This paper presents a novel generative model to synthesize fluid simulations\\nfrom a set of reduced parameters. A convolutional neural network is trained on\\na collection of discrete, parameterizable fluid simulation velocity fields. Due\\nto the capability of deep learning architectures to learn representative\\nfeatures of the data, our generative model is able to accurately approximate\\nthe training data set, while providing plausible interpolated in-betweens. The\\nproposed generative model is optimized for fluids by a novel loss function that\\nguarantees divergence-free velocity fields at all times. In addition, we\\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\\nadvance simulations in time by integrating in the latent space with a second\\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\\napplications such as fast construction of simulations, interpolation of fluids\\nwith different parameters, time re-sampling, latent space simulations, and\\ncompression of fluid simulation data. Reconstructed velocity fields are\\ngenerated up to 700x faster than traditional CPU solvers, while achieving\\ncompression rates of over 1300x.\\n',\n",
       " '  The two-stage least-squares (2SLS) estimator is known to be biased when its\\nfirst-stage fit is poor. I show that better first-stage prediction can\\nalleviate this bias. In a two-stage linear regression model with Normal noise,\\nI consider shrinkage in the estimation of the first-stage instrumental variable\\ncoefficients. For at least four instrumental variables and a single endogenous\\nregressor, I establish that the standard 2SLS estimator is dominated with\\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\\nin a first-stage high-dimensional Normal-means problem followed by a\\ncontrol-function approach in the second stage. It preserves invariances of the\\nstructural instrumental variable equations.\\n',\n",
       " '  An unsupervised learning classification model is described. It achieves\\nclassification error probability competitive with that of popular supervised\\nlearning classifiers such as SVM or kNN. The model is based on the incremental\\nexecution of small step shift and rotation operations upon selected\\ndiscriminative hyperplanes at the arrival of input samples. When applied, in\\nconjunction with a selected feature extractor, to a subset of the ImageNet\\ndataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by\\nmerely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both\\nusing same feature extractor. This result may also be contrasted with popular\\nunsupervised learning schemes such as k-Means which is shown to be practically\\nuseless on same dataset.\\n',\n",
       " '  We investigate the predictability of several range-based stock volatility\\nestimators, and compare them to the standard close-to-close estimator which is\\nmost commonly acknowledged as the volatility. The patterns of volatility\\nchanges are analyzed using LSTM recurrent neural networks, which are a state of\\nthe art method of sequence learning. We implement the analysis on all current\\nconstituents of the Dow Jones Industrial Average index, and report averaged\\nevaluation results. We find that changes in the values of range-based\\nestimators are more predictable than that of the estimator using daily closing\\nvalues only.\\n',\n",
       " \"  Correlated random walks (CRW) have been used for a long time as a null model\\nfor animal's random search movement in two dimensions (2D). An increasing\\nnumber of studies focus on animals' movement in three dimensions (3D), but the\\nkey properties of CRW, such as the way the mean squared displacement is related\\nto the path length, are well known only in 1D and 2D. In this paper I derive\\nsuch properties for 3D CRW, in a consistent way with the expression of these\\nproperties in 2D. This should allow 3D CRW to act as a null model when\\nanalyzing actual 3D movements similarly to what is done in 2D\\n\",\n",
       " '  This paper presents a novel context-based approach for pedestrian motion\\nprediction in crowded, urban intersections, with the additional flexibility of\\nprediction in similar, but new, environments. Previously, Chen et. al. combined\\nMarkovian-based and clustering-based approaches to learn motion primitives in a\\ngrid-based world and subsequently predict pedestrian trajectories by modeling\\nthe transition between learned primitives as a Gaussian Process (GP). This work\\nextends that prior approach by incorporating semantic features from the\\nenvironment (relative distance to curbside and status of pedestrian traffic\\nlights) in the GP formulation for more accurate predictions of pedestrian\\ntrajectories over the same timescale. We evaluate the new approach on\\nreal-world data collected using one of the vehicles in the MIT Mobility On\\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\\nto quantify the span of predicted set of trajectories, such that a lower AUC\\ncorresponds to a higher level of confidence in the future direction of\\npedestrian motion.\\n',\n",
       " '  We present E NERGY N ET , a new framework for analyzing and building\\nartificial neural network architectures. Our approach adaptively learns the\\nstructure of the networks in an unsupervised manner. The methodology is based\\nupon the theoretical guarantees of the energy function of restricted Boltzmann\\nmachines (RBM) of infinite number of nodes. We present experimental results to\\nshow that the final network adapts to the complexity of a given problem.\\n',\n",
       " '  Finding the dense regions of a graph and relations among them is a\\nfundamental problem in network analysis. Core and truss decompositions reveal\\ndense subgraphs with hierarchical relations. The incremental nature of\\nalgorithms for computing these decompositions and the need for global\\ninformation at each step of the algorithm hinders scalable parallelization and\\napproximations since the densest regions are not revealed until the end. In a\\nprevious work, Lu et al. proposed to iteratively compute the $h$-indices of\\nneighbor vertex degrees to obtain the core numbers and prove that the\\nconvergence is obtained after a finite number of iterations. This work\\ngeneralizes the iterative $h$-index computation for truss decomposition as well\\nas nucleus decomposition which leverages higher-order structures to generalize\\ncore and truss decompositions. In addition, we prove convergence bounds on the\\nnumber of iterations. We present a framework of local algorithms to obtain the\\ncore, truss, and nucleus decompositions. Our algorithms are local, parallel,\\noffer high scalability, and enable approximations to explore time and quality\\ntrade-offs. Our shared-memory implementation verifies the efficiency,\\nscalability, and effectiveness of our local algorithms on real-world networks.\\n',\n",
       " '  We propose a robust gesture-based communication pipeline for divers to\\ninstruct an Autonomous Underwater Vehicle (AUV) to assist them in performing\\nhigh-risk tasks and helping in case of emergency. A gesture communication\\nlanguage (CADDIAN) is developed, based on consolidated and standardized diver\\ngestures, including an alphabet, syntax and semantics, ensuring a logical\\nconsistency. A hierarchical classification approach is introduced for hand\\ngesture recognition based on stereo imagery and multi-descriptor aggregation to\\nspecifically cope with underwater image artifacts, e.g. light backscatter or\\ncolor attenuation. Once the classification task is finished, a syntax check is\\nperformed to filter out invalid command sequences sent by the diver or\\ngenerated by errors in the classifier. Throughout this process, the diver\\nreceives constant feedback from an underwater tablet to acknowledge or abort\\nthe mission at any time. The objective is to prevent the AUV from executing\\nunnecessary, infeasible or potentially harmful motions. Experimental results\\nunder different environmental conditions in archaeological exploration and\\nbridge inspection applications show that the system performs well in the field.\\n',\n",
       " '  Unique among alkali-doped $\\\\textit {A}$$_3$C$_{60}$ fullerene compounds, the\\nA15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying\\nunder hydrostatic pressure with highest transition temperatures at $T_\\\\textrm\\n{C}$$^\\\\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that\\nthese two compounds under pressure represent the optimal materials of the\\n$\\\\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated\\nsuperconductivity is mediated through Coulombic interactions with charges on\\nthe alkalis. A derivation of the interlayer Coulombic pairing model of\\nhigh-$T_\\\\textrm {C}$ superconductivity employing non-planar geometry is\\nintroduced, generalizing the picture of two interacting layers to an\\ninteraction between charge reservoirs located on the C$_{60}$ and alkali ions.\\nThe optimal transition temperature follows the algebraic expression, $T_\\\\textrm\\n{C0}$ = (12.474 nm$^2$ K)/$\\\\ell$${\\\\zeta}$, where $\\\\ell$ relates to the mean\\nspacing between interacting surface charges on the C$_{60}$ and ${\\\\zeta}$ is\\nthe average radial distance between the C$_{60}$ surface and the neighboring Cs\\nions. Values of $T_\\\\textrm {C0}$ for the measured cation stoichiometries of\\nCs$_{3-\\\\textrm{x}}$C$_{60}$ with x $\\\\approx$ 0 are found to be 38.19 and 36.88\\nK for the A15 and fcc forms, respectively, with the dichotomy in transition\\ntemperature reflecting the larger ${\\\\zeta}$ and structural disorder in the fcc\\nform. In the A15 form, modeled interacting charges and Coulomb potential\\ne$^2$/${\\\\zeta}$ are shown to agree quantitatively with findings from\\nnuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,\\nsuppression of $T_\\\\textrm {C}$$^\\\\textrm {meas}$ below $T_\\\\textrm {C0}$ is\\nascribed to native structural disorder. Phononic effects in conjunction with\\nCoulombic pairing are discussed.\\n',\n",
       " '  Improving the performance of superconducting qubits and resonators generally\\nresults from a combination of materials and fabrication process improvements\\nand design modifications that reduce device sensitivity to residual losses. One\\ninstance of this approach is to use trenching into the device substrate in\\ncombination with superconductors and dielectrics with low intrinsic losses to\\nimprove quality factors and coherence times. Here we demonstrate titanium\\nnitride coplanar waveguide resonators with mean quality factors exceeding two\\nmillion and controlled trenching reaching 2.2 $\\\\mu$m into the silicon\\nsubstrate. Additionally, we measure sets of resonators with a range of sizes\\nand trench depths and compare these results with finite-element simulations to\\ndemonstrate quantitative agreement with a model of interface dielectric loss.\\nWe then apply this analysis to determine the extent to which trenching can\\nimprove resonator performance.\\n',\n",
       " '  This paper will detail changes in the operational paradigm of the Fermi\\nNational Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to\\nupgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions\\nfor High Energy Physics (HEP) experiments were extracted at ~18 keV vertically\\ndownward into a 90 degree bending magnet and accelerated through a\\nCockcroft-Walton accelerating column to 750 keV. Following the upgrade in the\\nfall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35\\nkeV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This\\nchange in extraction energy as well as the orientation of the ion source\\nrequired not only a redesign of the ion source, but an updated understanding of\\nits operation at these new values. Discussed in detail are the changes to the\\nion source timing, arc discharge current, hydrogen gas pressure, and cesium\\ndelivery system that were needed to maintain consistent operation at >99%\\nuptime for HEP, with an increased ion source lifetime of over 9 months.\\n',\n",
       " \"  We consider the withdrawal of a ball from a fluid reservoir to understand the\\nlongevity of the connection between that ball and the fluid it breaks away\\nfrom, at intermediate Reynolds numbers. Scaling arguments based on the\\nprocesses observed as the ball interacts with the fluid surface were applied to\\nthe `pinch-off time', when the ball breaks its connection with the fluid from\\nwhich it has been withdrawn, measured experimentally. At the lowest Reynolds\\nnumbers tested, pinch-off occurs in a `surface seal' close to the reservoir\\nsurface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'\\nclose to the ball. Our scaling analysis shows that the connection between ball\\nand fluid is controlled by the fluid film draining from the ball as it\\ncontinues to be winched away from the fluid reservoir. The draining flow itself\\ndepends on the amount of fluid coating the ball on exit from the reservoir. We\\nconsider the possibilities that this coating was created through: a surface\\ntension driven Landau Levitch Derjaguin wetting of the surface; a\\nvisco-inertial quick coating; or alternatively through the inertia of the fluid\\nmoving with the ball through the reservoir. We show that although the pinch-off\\nmechanism is controlled by viscosity, the coating mechanism is governed by a\\ndifferent length and timescale, dictated by the inertial added mass of the ball\\nwhen submersed.\\n\",\n",
       " '  We disentangle all the individual degrees of freedom in the quantum impurity\\nproblem to deconstruct the Kondo singlet, both in real and energy space, by\\nstudying the contribution of each individual free electron eigenstate. This is\\na problem of two spins coupled to a bath, where the bath is formed by the\\nremaining conduction electrons. Being a mixed state, we resort to the\\n\"concurrence\" to quantify entanglement. We identify \"projected natural\\norbitals\" that allow us to individualize a single-particle electronic wave\\nfunction that is responsible of more than $90\\\\%$ of the impurity screening. In\\nthe weak coupling regime, the impurity is entangled to an electron at the Fermi\\nlevel, while in the strong coupling regime, the impurity counterintuitively\\nentangles mostly with the high energy electrons and disentangles completely\\nfrom the low-energy states carving a \"hole\" around the Fermi level. This\\nenables one to use concurrence as a pseudo order parameter to compute the\\ncharacteristic \"size\" of the Kondo cloud, beyond which electrons are are weakly\\ncorrelated to the impurity and are dominated by the physics of the boundary.\\n',\n",
       " '  Motivated by the recently proposed parallel orbital-updating approach in real\\nspace method, we propose a parallel orbital-updating based plane-wave basis\\nmethod for electronic structure calculations, for solving the corresponding\\neigenvalue problems. In addition, we propose two new modified parallel\\norbital-updating methods. Compared to the traditional plane-wave methods, our\\nmethods allow for two-level parallelization, which is particularly interesting\\nfor large scale parallelization. Numerical experiments show that these new\\nmethods are more reliable and efficient for large scale calculations on modern\\nsupercomputers\\n',\n",
       " '  The particular type of four-kink multi-solitons (or quadrons) adiabatic\\ndynamics of the sine-Gordon equation in a model with two identical point\\nattracting impurities has been studied. This model can be used for describing\\nmagnetization localized waves in multilayer ferromagnet. The quadrons structure\\nand properties has been numerically investigated. The cases of both large and\\nsmall distances between impurities has been viewed. The dependence of the\\nlocalized in impurity region nonlinear high-amplitude waves frequencies on the\\ndistance between the impurities has been found. For an analytical description\\nof two bound localized on impurities nonlinear waves dynamics, using\\nperturbation theory, the system of differential equations for harmonic\\noscillators with elastic link has been found. The analytical model\\nqualitatively describes the results of the sine-Gordon equation numerical\\nsimulation.\\n',\n",
       " \"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the\\ncosmic microwave background (CMB) differ at the $\\\\sim$3-$\\\\sigma$ level,\\nindicating a potential issue with the standard $\\\\Lambda$CDM cosmology.\\nInterpreting this tension correctly requires a model comparison calculation\\ndepending on not only the traditional `$n$-$\\\\sigma$' mismatch but also the\\ntails of the likelihoods. Determining the form of the tails of the local $H_0$\\nlikelihood is impossible with the standard Gaussian least-squares\\napproximation, as it requires using non-Gaussian distributions to faithfully\\nrepresent anchor likelihoods and model outliers in the Cepheid and supernova\\n(SN) populations, and simultaneous fitting of the full distance-ladder dataset\\nto correctly propagate uncertainties. We have developed a Bayesian hierarchical\\nmodel that describes the full distance ladder, from nearby geometric anchors\\nthrough Cepheids to Hubble-Flow SNe. This model does not rely on any\\ndistributions being Gaussian, allowing outliers to be modeled and obviating the\\nneed for arbitrary data cuts. Sampling from the $\\\\sim$3000-parameter joint\\nposterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\\\\pm$ 1.67)\\n${\\\\rm km\\\\,s^{-1}\\\\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.\\n(2016) data, and ($73.15 \\\\pm 1.78$) ${\\\\rm km\\\\,s^{-1}\\\\,Mpc^{-1}}$ with SN\\noutliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the\\ndistance-ladder likelihood allows us to apply Bayesian model comparison to\\nassess the evidence for deviation from $\\\\Lambda$CDM. We set up this comparison\\nto yield a lower limit on the odds of the underlying model being $\\\\Lambda$CDM\\ngiven the distance-ladder and Planck XIII (2016) CMB data. The odds against\\n$\\\\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers\\nare cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)\\nlikelihood is used.\\n\",\n",
       " '  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms\\nfor uncoordinated spectrum access. The number of users is assumed to be unknown\\nto each user. A stochastic setting is first considered, where the rewards on a\\nchannel are the same for each user. In contrast to prior work, it is assumed\\nthat the number of users can possibly exceed the number of channels, and that\\nrewards can be non-zero even under collisions. The proposed algorithm consists\\nof an estimation phase and an allocation phase. It is shown that if every user\\nadopts the algorithm, the system wide regret is constant with time with high\\nprobability. The regret guarantees hold for any number of users and channels,\\nin particular, even when the number of users is less than the number of\\nchannels. Next, an adversarial multi-user MAB framework is considered, where\\nthe rewards on the channels are user-dependent. It is assumed that the number\\nof users is less than the number of channels, and that the users receive zero\\nreward on collision. The proposed algorithm combines the Exp3.P algorithm\\ndeveloped in prior work for single user adversarial bandits with a collision\\nresolution mechanism to achieve sub-linear regret. It is shown that if every\\nuser employs the proposed algorithm, the system wide regret is of the order\\n$O(T^\\\\frac{3}{4})$ over a horizon of time $T$. The algorithms in both\\nstochastic and adversarial scenarios are extended to the dynamic case where the\\nnumber of users in the system evolves over time and are shown to lead to\\nsub-linear regret.\\n',\n",
       " '  In this paper, we analyze the effects of contact models on contact-implicit\\ntrajectory optimization for manipulation. We consider three different\\napproaches: (1) a contact model that is based on complementarity constraints,\\n(2) a smooth contact model, and our proposed method (3) a variable smooth\\ncontact model. We compare these models in simulation in terms of physical\\naccuracy, quality of motions, and computation time. In each case, the\\noptimization process is initialized by setting all torque variables to zero,\\nnamely, without a meaningful initial guess. For simulations, we consider a\\npushing task with varying complexity for a 7 degrees-of-freedom robot arm. Our\\nresults demonstrate that the optimization based on the proposed variable smooth\\ncontact model provides a good trade-off between the physical fidelity and\\nquality of motions at the cost of increased computation time.\\n',\n",
       " '  The challenge of assigning importance to individual neurons in a network is\\nof interest when interpreting deep learning models. In recent work, Dhamdhere\\net al. proposed Total Conductance, a \"natural refinement of Integrated\\nGradients\" for attributing importance to internal neurons. Unfortunately, the\\nauthors found that calculating conductance in tensorflow required the addition\\nof several custom gradient operators and did not scale well. In this work, we\\nshow that the formula for Total Conductance is mathematically equivalent to\\nPath Integrated Gradients computed on a hidden layer in the network. We provide\\na scalable implementation of Total Conductance using standard tensorflow\\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\\napproach that is applicable to calculating internal neuron importance. We find\\nthat DeepLIFT produces strong empirical results and is faster to compute, but\\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\\nmay not always be preferred in practice. Colab notebook reproducing results:\\nthis http URL\\n',\n",
       " \"  An elementary rheory of concatenation is introduced and used to establish\\nmutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory,\\nthe quantifier-free part of Kirby's finitary set theory, and Adjunctive Set\\nTheory, with or without extensionality.\\n\",\n",
       " '  JavaBIP allows the coordination of software components by clearly separating\\nthe functional and coordination aspects of the system behavior. JavaBIP\\nimplements the principles of the BIP component framework rooted in rigorous\\noperational semantics. Recent work both on BIP and JavaBIP allows the\\ncoordination of static components defined prior to system deployment, i.e., the\\narchitecture of the coordinated system is fixed in terms of its component\\ninstances. Nevertheless, modern systems, often make use of components that can\\nregister and deregister dynamically during system execution. In this paper, we\\npresent an extension of JavaBIP that can handle this type of dynamicity. We use\\nfirst-order interaction logic to define synchronization constraints based on\\ncomponent types. Additionally, we use directed graphs with edge coloring to\\nmodel dependencies among components that determine the validity of an online\\nsystem. We present the software architecture of our implementation, provide and\\ndiscuss performance evaluation results.\\n',\n",
       " '  In rapid release development processes, patches that fix critical issues, or\\nimplement high-value features are often promoted directly from the development\\nchannel to a stabilization channel, potentially skipping one or more\\nstabilization channels. This practice is called patch uplift. Patch uplift is\\nrisky, because patches that are rushed through the stabilization phase can end\\nup introducing regressions in the code. This paper examines patch uplift\\noperations at Mozilla, with the aim to identify the characteristics of uplifted\\npatches that introduce regressions. Through statistical and manual analyses, we\\nquantitatively and qualitatively investigate the reasons behind patch uplift\\ndecisions and the characteristics of uplifted patches that introduced\\nregressions. Additionally, we interviewed three Mozilla release managers to\\nunderstand organizational factors that affect patch uplift decisions and\\noutcomes. Results show that most patches are uplifted because of a wrong\\nfunctionality or a crash. Uplifted patches that lead to faults tend to have\\nlarger patch size, and most of the faults are due to semantic or memory errors\\nin the patches. Also, release managers are more inclined to accept patch uplift\\nrequests that concern certain specific components, and-or that are submitted by\\ncertain specific developers.\\n',\n",
       " '  Examining games from a fresh perspective we present the idea of game-inspired\\nand game-based algorithms, dubbed \"gamorithms\".\\n',\n",
       " '  We establish the convergence rates and asymptotic distributions of the common\\nbreak change-point estimators, obtained by least squares and maximum likelihood\\nin panel data models and compare their asymptotic variances. Our model\\nassumptions accommodate a variety of commonly encountered probability\\ndistributions and, in particular, models of particular interest in econometrics\\nbeyond the commonly analyzed Gaussian model, including the zero-inflated\\nPoisson model for count data, and the probit and tobit models. We also provide\\nnovel results for time dependent data in the signal-plus-noise model, with\\nemphasis on a wide array of noise processes, including Gaussian process,\\nMA$(\\\\infty)$ and $m$-dependent processes. The obtained results show that\\nmaximum likelihood estimation requires a stronger signal-to-noise model\\nidentifiability condition compared to its least squares counterpart. Finally,\\nsince there are three different asymptotic regimes that depend on the behavior\\nof the norm difference of the model parameters before and after the change\\npoint, which cannot be realistically assumed to be known, we develop a novel\\ndata driven adaptive procedure that provides valid confidence intervals for the\\ncommon break, without requiring a priori knowledge of the asymptotic regime the\\nproblem falls in.\\n',\n",
       " '  The study of relays with the scope of energy-harvesting (EH) looks\\ninteresting as a means of enabling sustainable, wireless communication without\\nthe need to recharge or replace the battery driving the relays. However,\\nreliability of such communication systems becomes an important design challenge\\nwhen such relays scavenge energy from the information bearing RF signals\\nreceived from the source, using the technique of simultaneous wireless\\ninformation and power transfer (SWIPT). To this aim, this work studies\\nbidirectional communication in a decode-and-forward (DF) relay assisted\\ncooperative wireless network in presence of co-channel interference (CCI). In\\norder to quantify the reliability of the bidirectional communication systems, a\\nclosed form expression for the outage probability of the system is derived for\\nboth power splitting (PS) and time switching (TS) mode of operation of the\\nrelay. Simulation results are used to validate the accuracy of our analytical\\nresults and illustrate the dependence of the outage probability on various\\nsystem parameters, like PS factor, TS factor, and distance of the relay from\\nboth the users. Results of performance comparison between PS relaying (PSR) and\\nTS relaying (TSR) schemes are also presented. Besides, simulation results are\\nalso used to illustrate the spectral-efficiency and the energy-efficiency of\\nthe proposed system. The results show that, both in terms of spectral\\nefficiency and the energy-efficiency, the two-way communication system in\\npresence of moderate CCI power, performs better than the similar system without\\nCCI. Additionally, it is also found that PSR is superior to TSR protocol in\\nterms of peak energy-efficiency.\\n',\n",
       " '  Generative Adversarial Networks (GANs) were intuitively and attractively\\nexplained under the perspective of game theory, wherein two involving parties\\nare a discriminator and a generator. In this game, the task of the\\ndiscriminator is to discriminate the real and generated (i.e., fake) data,\\nwhilst the task of the generator is to generate the fake data that maximally\\nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs,\\nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows\\na connection between the general loss of a classification problem regarding a\\nconvex loss function and a f-divergence between the true and fake data\\ndistributions. Mathematically, we proposed a setting for the classification\\nproblem of the true and fake data, wherein we can prove that the general loss\\nof this classification problem is exactly the negative f-divergence for a\\ncertain convex function f. This allows us to interpret the problem of learning\\nthe generator for dismissing the f-divergence between the true and fake data\\ndistributions as that of maximizing the general loss which is equivalent to the\\nmin-max problem in GAN if the Logistic loss is used in the classification\\nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows\\nus to employ any convex loss function for the discriminator. Second, it\\nsuggests that rather than limiting ourselves in NN-based discriminators, we can\\nalternatively utilize other powerful families. Bearing this viewpoint, we then\\npropose using the kernel-based family for discriminators. This family has two\\nappealing features: i) a powerful capacity in classifying non-linear nature\\ndata and ii) being convex in the feature space. Using the convexity of this\\nfamily, we can further develop Fenchel duality to equivalently transform the\\nmax-min problem to the max-max dual problem.\\n',\n",
       " '  This paper is concerned with the computation of representation matrices for\\nthe action of Frobenius to the cohomology groups of algebraic varieties.\\nSpecifically we shall give an algorithm to compute the matrices for arbitrary\\nalgebraic varieties with defining equations over perfect fields of positive\\ncharacteristic, and estimate its complexity. Moreover, we propose a specific\\nefficient method, which works for complete intersections.\\n',\n",
       " '  We present possible explanations of pulsations in early B-type main sequence\\nstars which arise purely from the excitation of gravity modes. There are three\\nstars with this type of oscillations detected from the BRITE light curves:\\n$\\\\kappa$ Cen, a Car, $\\\\kappa$ Vel. We show that by changing metallicity or the\\nopacity profile it is possible in some models to dump pressure modes keeping\\ngravity modes unstable. Other possible scenario involves pulsations of a lower\\nmass companion.\\n',\n",
       " '  Recently introduced composition operator for credal sets is an analogy of\\nsuch operators in probability, possibility, evidence and valuation-based\\nsystems theories. It was designed to construct multidimensional models (in the\\nframework of credal sets) from a system of low- dimensional credal sets. In\\nthis paper we study its potential from the computational point of view\\nutilizing methods of polyhedral geometry.\\n',\n",
       " '  Internet-of-Things end-nodes demand low power processing platforms\\ncharacterized by heterogeneous dedicated units, controlled by a processor core\\nrunning concurrent control threads. Such architecture scheme fits one of the\\nmain target application domain of the RISC-V instruction set. We present an\\nopen-source processing core compliant with RISC-V on the software side and with\\nthe popular Pulpino processor platform on the hardware side, while supporting\\ninterleaved multi-threading for IoT applications. The latter feature is a novel\\ncontribution in this application domain. We report details about the\\nmicroarchitecture design along with performance data.\\n',\n",
       " '  During the last two decades, Genetic Programming (GP) has been largely used\\nto tackle optimization, classification, and automatic features selection\\nrelated tasks. The widespread use of GP is mainly due to its flexible and\\ncomprehensible tree-type structure. Similarly, research is also gaining\\nmomentum in the field of Image Processing (IP) because of its promising results\\nover wide areas of applications ranging from medical IP to multispectral\\nimaging. IP is mainly involved in applications such as computer vision, pattern\\nrecognition, image compression, storage and transmission, and medical\\ndiagnostics. This prevailing nature of images and their associated algorithm\\ni.e complexities gave an impetus to the exploration of GP. GP has thus been\\nused in different ways for IP since its inception. Many interesting GP\\ntechniques have been developed and employed in the field of IP. To give the\\nresearch community an extensive view of these techniques, this paper presents\\nthe diverse applications of GP in IP and provides useful resources for further\\nresearch. Also, comparison of different parameters used in ten different\\napplications of IP are summarized in tabular form. Moreover, analysis of\\ndifferent parameters used in IP related tasks is carried-out to save the time\\nneeded in future for evaluating the parameters of GP. As more advancement is\\nmade in GP methodologies, its success in solving complex tasks not only related\\nto IP but also in other fields will increase. Additionally, guidelines are\\nprovided for applying GP in IP related tasks, pros and cons of GP techniques\\nare discussed, and some future directions are also set.\\n',\n",
       " '  The control of dynamical, networked systems continues to receive much\\nattention across the engineering and scientific research fields. Of particular\\ninterest is the proper way to determine which nodes of the network should\\nreceive external control inputs in order to effectively and efficiently control\\nportions of the network. Published methods to accomplish this task either find\\na minimal set of driver nodes to guarantee controllability or a larger set of\\ndriver nodes which optimizes some control metric. Here, we investigate the\\ncontrol of lattice systems which provides analytical insight into the\\nrelationship between network structure and controllability. First we derive a\\nclosed form expression for the individual elements of the controllability\\nGramian of infinite lattice systems. Second, we focus on nearest neighbor\\nlattices for which the distance between nodes appears in the expression for the\\ncontrollability Gramian. We show that common control energy metrics scale\\nexponentially with respect to the maximum distance between a driver node and a\\ntarget node.\\n',\n",
       " '  We construct constant mean curvature surfaces in euclidean space with genus\\nzero and n ends asymptotic to Delaunay surfaces using the DPW method.\\n',\n",
       " '  We discuss various universality aspects of numerical computations using\\nstandard algorithms. These aspects include empirical observations and rigorous\\nresults. We also make various speculations about computation in a broader\\nsense.\\n',\n",
       " '  The relativistic jets created by some active galactic nuclei are important\\nagents of AGN feedback. In spite of this, our understanding of what produces\\nthese jets is still incomplete. X-ray observations, which can probe the\\nprocesses operating in the central regions in immediate vicinity of the\\nsupermassive black hole, the presumed jet launching point, are potentially\\nparticularly valuable in illuminating the jet formation process. Here, we\\npresent the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in\\na joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our\\nspectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and\\nconfirms the presence of ionized reflection in the source. From the average\\nspectrum we detect that the accretion disk is mildly recessed with an inner\\nradius of $R_\\\\mathrm{in}=4-180\\\\,R_\\\\mathrm{g}$. However, no significant\\nevolution of the inner radius is seen during the three months covered by our\\nNuSTAR campaign. This lack of variation could mean that the jet formation in\\nthis radio-loud quasar differs from what is observed in broad-line radio\\ngalaxies.\\n',\n",
       " '  A new synthesis scheme is proposed to effectively generate a random vector\\nwith prescribed joint density that induces a (latent) Gaussian tree structure.\\nThe quality of synthesis is measured by total variation distance between the\\nsynthesized and desired statistics. The proposed layered and successive\\nencoding scheme relies on the learned structure of tree to use minimal number\\nof common random variables to synthesize the desired density. We characterize\\nthe achievable rate region for the rate tuples of multi-layer latent Gaussian\\ntree, through which the number of bits needed to simulate such Gaussian joint\\ndensity are determined. The random sources used in our algorithm are the latent\\nvariables at the top layer of tree, the additive independent Gaussian noises,\\nand the Bernoulli sign inputs that capture the ambiguity of correlation signs\\nbetween the variables.\\n',\n",
       " '  We present a machine learning based information retrieval system for\\nastronomical observatories that tries to address user defined queries related\\nto an instrument. In the modern instrumentation scenario where heterogeneous\\nsystems and talents are simultaneously at work, the ability to supply with the\\nright information helps speeding up the detector maintenance operations.\\nEnhancing the detector uptime leads to increased coincidence observation and\\nimproves the likelihood for the detection of astrophysical signals. Besides,\\nsuch efforts will efficiently disseminate technical knowledge to a wider\\naudience and will help the ongoing efforts to build upcoming detectors like the\\nLIGO-India etc even at the design phase to foresee possible challenges. The\\nproposed method analyses existing documented efforts at the site to\\nintelligently group together related information to a query and to present it\\non-line to the user. The user in response can further go into interesting links\\nand find already developed solutions or probable ways to address the present\\nsituation optimally. A web application that incorporates the above idea has\\nbeen implemented and tested for LIGO Livingston, LIGO Hanford and Virgo\\nobservatories.\\n',\n",
       " \"  In today's databases, previous query answers rarely benefit answering future\\nqueries. For the first time, to the best of our knowledge, we change this\\nparadigm in an approximate query processing (AQP) context. We make the\\nfollowing observation: the answer to each query reveals some degree of\\nknowledge about the answer to another query because their answers stem from the\\nsame underlying distribution that has produced the entire dataset. Exploiting\\nand refining this knowledge should allow us to answer queries more\\nanalytically, rather than by reading enormous amounts of raw data. Also,\\nprocessing more queries should continuously enhance our knowledge of the\\nunderlying distribution, and hence lead to increasingly faster response times\\nfor future queries.\\nWe call this novel idea---learning from past query answers---Database\\nLearning. We exploit the principle of maximum entropy to produce answers, which\\nare in expectation guaranteed to be more accurate than existing sample-based\\napproximations. Empowered by this idea, we build a query engine on top of Spark\\nSQL, called Verdict. We conduct extensive experiments on real-world query\\ntraces from a large customer of a major database vendor. Our results\\ndemonstrate that Verdict supports 73.7% of these queries, speeding them up by\\nup to 23.0x for the same accuracy level compared to existing AQP systems.\\n\",\n",
       " '  Beam search is a desirable choice of test-time decoding algorithm for neural\\nsequence models because it potentially avoids search errors made by simpler\\ngreedy methods. However, typical cross entropy training procedures for these\\nmodels do not directly consider the behaviour of the final decoding method. As\\na result, for cross-entropy trained models, beam decoding can sometimes yield\\nreduced test performance when compared with greedy decoding. In order to train\\nmodels that can more effectively make use of beam search, we propose a new\\ntraining procedure that focuses on the final loss metric (e.g. Hamming loss)\\nevaluated on the output of beam search. While well-defined, this \"direct loss\"\\nobjective is itself discontinuous and thus difficult to optimize. Hence, in our\\napproach, we form a sub-differentiable surrogate objective by introducing a\\nnovel continuous approximation of the beam search decoding procedure. In\\nexperiments, we show that optimizing this new training objective yields\\nsubstantially better results on two sequence tasks (Named Entity Recognition\\nand CCG Supertagging) when compared with both cross entropy trained greedy\\ndecoding and cross entropy trained beam decoding baselines.\\n',\n",
       " '  In 1997 B. Weiss introduced the notion of measurably entire functions and\\nproved that they exist on every arbitrary free C- action defined on standard\\nprobability space. In the same paper he asked about the minimal possible growth\\nof measurably entire functions. In this work we show that for every arbitrary\\nfree C- action defined on a standard probability space there exists a\\nmeasurably entire function whose growth does not exceed exp (exp[log^p |z|])\\nfor any p > 3. This complements a recent result by Buhovski, Glücksam,\\nLogunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot\\ngrow slower than exp (exp[log^p |z|]) for any p < 2.\\n',\n",
       " '  In this paper, we show how to construct graph theoretical models of\\nn-dimensional continuous objects and manifolds. These models retain topological\\nproperties of their continuous counterparts. An LCL collection of n-cells in\\nEuclidean space is introduced and investigated. If an LCL collection of n-cells\\nis a cover of a continuous n-dimensional manifold then the intersection graph\\nof this cover is a digital closed n-dimensional manifold with the same topology\\nas its continuous counterpart. As an example, we prove that the digital model\\nof a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2\\npoints, the digital model of a continuous projective plane is a digital\\nprojective plane with at least eleven points, the digital model of a continuous\\nKlein bottle is the digital Klein bottle with at least sixteen points, the\\ndigital model of a continuous torus is the digital torus with at least sixteen\\npoints and the digital model of a continuous Moebius band is the digital\\nMoebius band with at least twelve points.\\n',\n",
       " '  Let $f(x,y)=ax^2+bxy+cy^2$ be a binary quadratic form with integer\\ncoefficients. For a prime $p$ not dividing the discriminant of $f$, we say $f$\\nis completely $p$-primitive if for any non-zero integer $N$, the diophantine\\nequation $f(x,y)=N$ has always an integer solution $(x,y)=(m,n)$ with\\n$(m,n,p)=1$ whenever it has an integer solution. In this article, we study\\nvarious properties of completely $p$-primitive binary quadratic forms. In\\nparticular, we give a necessary and sufficient condition for a definite binary\\nquadratic form $f$ to be completely $p$-primitive.\\n',\n",
       " '  The numerical availability of statistical inference methods for a modern and\\nrobust analysis of longitudinal- and multivariate data in factorial experiments\\nis an essential element in research and education. While existing approaches\\nthat rely on specific distributional assumptions of the data (multivariate\\nnormality and/or characteristic covariance matrices) are implemented in\\nstatistical software packages, there is a need for user-friendly software that\\ncan be used for the analysis of data that do not fulfill the aforementioned\\nassumptions and provide accurate p-value and confidence interval estimates.\\nTherefore, newly developed statistical methods for the analysis of repeated\\nmeasures designs and multivariate data that neither assume multivariate\\nnormality nor specific covariance matrices have been implemented in the freely\\navailable R-package MANOVA.RM. The package is equipped with a graphical user\\ninterface for plausible applications in academia and other educational purpose.\\nSeveral motivating examples illustrate the application of the methods.\\n',\n",
       " '  Phaseless super-resolution is the problem of recovering an unknown signal\\nfrom measurements of the magnitudes of the low frequency Fourier transform of\\nthe signal. This problem arises in applications where measuring the phase, and\\nmaking high-frequency measurements, are either too costly or altogether\\ninfeasible. The problem is especially challenging because it combines the\\ndifficult problems of phase retrieval and classical super-resolution\\n',\n",
       " \"  This paper is the first chapter of three of the author's undergraduate\\nthesis. We study the random matrix ensemble of covariance matrices arising from\\nrandom $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and\\n$N$ white vertices, for $d_b \\\\gg \\\\log^4 N$. We simultaneously prove that the\\nGreen's functions of these covariance matrices and the adjacency matrices of\\nthe underlying graphs agree with the corresponding limiting law (e.g.\\nMarchenko-Pastur law for covariance matrices) down to the optimal scale. This\\nis an improvement from the previously known mesoscopic results. We obtain\\neigenvector delocalization for the covariance matrix ensemble as consequence,\\nas well as a weak rigidity estimate.\\n\",\n",
       " '  Photoelectron yields of extruded scintillation counters with titanium dioxide\\ncoating and embedded wavelength shifting fibers read out by silicon\\nphotomultipliers have been measured at the Fermilab Test Beam Facility using\\n120\\\\,GeV protons. The yields were measured as a function of transverse,\\nlongitudinal, and angular positions for a variety of scintillator compositions\\nand reflective coating mixtures, fiber diameters, and photosensor sizes. Timing\\nperformance was also studied. These studies were carried out by the Cosmic Ray\\nVeto Group of the Mu2e collaboration as part of their R\\\\&D program.\\n',\n",
       " '  We report a precise measurement of hyperfine structure in the $ \\\\rm\\n{3\\\\,S_{1/2}} $ state of the odd isotope of Li, namely $ \\\\rm {^7Li} $. The state\\nis excited from the ground $ \\\\rm {2\\\\,S_{1/2}} $ state (which has the same\\nparity) using two single-photon transitions via the intermediate $ \\\\rm\\n{2\\\\,P_{3/2}} $ state. The value of the hyperfine constant we measure is $ A =\\n93.095(52)$ MHz, which resolves two discrepant values reported in the\\nliterature measured using other techniques. Our value is also consistent with\\ntheoretical calculations.\\n',\n",
       " '  We study a dynamical system induced by the Artin reciprocity map for a global\\nfield. We translate the conjugacy of such dynamical systems into various\\narithmetical properties that are equivalent to field isomorphism, relating it\\nto anabelian geometry.\\n',\n",
       " '  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that\\nallows to distinguish smooth real algebraic surfaces up to birational\\ndiffeomorphism. As an application, we construct infinite families of smooth\\nrational real algebraic surfaces with trivial homology groups, whose real loci\\nare diffeomorphic to $\\\\mathbb{R}^2$, but which are pairwise not birationally\\ndiffeomorphic. There are thus infinitely many non-trivial models of the\\neuclidean plane, contrary to the compact case.\\n',\n",
       " \"  Effective communication is required for teams of robots to solve\\nsophisticated collaborative tasks. In practice it is typical for both the\\nencoding and semantics of communication to be manually defined by an expert;\\nthis is true regardless of whether the behaviors themselves are bespoke,\\noptimization based, or learned. We present an agent architecture and training\\nmethodology using neural networks to learn task-oriented communication\\nsemantics based on the example of a communication-unaware expert policy. A\\nperimeter defense game illustrates the system's ability to handle dynamically\\nchanging numbers of agents and its graceful degradation in performance as\\ncommunication constraints are tightened or the expert's observability\\nassumptions are broken.\\n\",\n",
       " \"  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a\\nplanetesimal born in another planetary system. This interloper exhibits a\\nvariable colour within a range that is broadly consistent with local small\\nbodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited\\nKuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an\\naxial ratio exceeding 5:1. Rotation period estimates are inconsistent and\\nvaried, with reported values between 6.9 and 8.3 hours. Here we analyse all\\navailable optical photometry reported to date. No single rotation period can\\nexplain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be\\nin an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or\\ntumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135\\nand 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the\\navailable data are insufficient to uniquely constrain the true frequencies and\\nshape. Assuming a body that responds to NPA rotation in a similar manner to\\nSolar System asteroids and comets, the timescale to damp 1I/'Oumuamua's\\ntumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling\\nwithin its parent planetary system, and will remain tumbling well after it has\\nleft ours.\\n\",\n",
       " '  In the context of orientable circuits and subcomplexes of these as\\nrepresenting certain singular spaces, we consider characteristic class formulas\\ngeneralizing those classical results as seen for the Riemann-Hurwitz formula\\nfor regulating the topology of branched covering maps and that for monoidal\\ntransformations which include the standard blowing-up process. Here the results\\nare presented as cap product pairings, which will be elements of a suitable\\nhomology theory, rather than characteristic numbers as would be the case when\\ntaking Kronecker products once Poincaré duality is defined. We further\\nconsider possible applications and examples including branched covering maps,\\nsingular varieties involving virtual tangent bundles, the\\nChern-Schwartz-MacPherson class, the homology L-class, generalized signature,\\nand the cohomology signature class.\\n',\n",
       " '  Purpose: Basic surgical skills of suturing and knot tying are an essential\\npart of medical training. Having an automated system for surgical skills\\nassessment could help save experts time and improve training efficiency. There\\nhave been some recent attempts at automated surgical skills assessment using\\neither video analysis or acceleration data. In this paper, we present a novel\\napproach for automated assessment of OSATS based surgical skills and provide an\\nanalysis of different features on multi-modal data (video and accelerometer\\ndata). Methods: We conduct the largest study, to the best of our knowledge, for\\nbasic surgical skills assessment on a dataset that contained video and\\naccelerometer data for suturing and knot-tying tasks. We introduce \"entropy\\nbased\" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy\\n(XApEn), which quantify the amount of predictability and regularity of\\nfluctuations in time-series data. The proposed features are compared to\\nexisting methods of Sequential Motion Texture (SMT), Discrete Cosine Transform\\n(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.\\nResults: We report average performance of different features across all\\napplicable OSATS criteria for suturing and knot tying tasks. Our analysis shows\\nthat the proposed entropy based features out-perform previous state-of-the-art\\nmethods using video data. For accelerometer data, our method performs better\\nfor suturing only. We also show that fusion of video and acceleration features\\ncan improve overall performance with the proposed entropy features achieving\\nhighest accuracy. Conclusions: Automated surgical skills assessment can be\\nachieved with high accuracy using the proposed entropy features. Such a system\\ncan significantly improve the efficiency of surgical training in medical\\nschools and teaching hospitals.\\n',\n",
       " '  Many complex systems share two characteristics: 1) they are stochastic in\\nnature, and 2) they are characterized by a large number of factors. At the same\\ntime, various natural complex systems appear to have two types of intertwined\\nconstituents that exhibit counteracting effects on their equilibrium. In this\\nstudy, we employ these few characteristics to lay the groundwork for analyzing\\nsuch complex systems. The equilibrium point of these systems is generally\\nstudied either through the kinetic notion of equilibrium or its energetic\\nnotion, but not both. We postulate that these systems attempt to regulate the\\nstate vector of their constituents such that both the kinetic and the energetic\\nnotions of equilibrium are met. Based on this postulate, we prove: 1) the\\nexistence of a point such that the kinetic notion of equilibrium is met for the\\nless abundant constituents and, at the same time, the state vector of more\\nabundant entities is regulated to minimize the energetic notion of equilibrium;\\n2) the effect of unboundedly increasing less (more) abundant constituents\\nstabilizes (destabilizes) the system; and 3) the (unrestricted) equilibrium of\\nthe system is the point at which the number of stabilizing and destabilizing\\nentities increase unboundedly with the same rate.\\n',\n",
       " '  Even- and odd-frequency superconductivity coexist due to broken time-reversal\\nsymmetry under magnetic field. In order to describe this mixing, we extend the\\nlinearized Eliashberg equation for the spin and charge fluctuation mechanism in\\nstrongly correlated electron systems. We apply this extended Eliashberg\\nequation to the odd-frequency superconductivity on a quasi-one-dimensional\\nisosceles triangular lattice under in-plane magnetic field and examine the\\neffect of the even-frequency component.\\n',\n",
       " '  Let X be an irreducible smooth projective curve, of genus at least two, over\\nan algebraically closed field k. Let $\\\\mathcal{M}^d_G$ denote the moduli stack\\nof principal G-bundles over X of fixed topological type $d \\\\in \\\\pi_1(G)$, where\\nG is any almost simple affine algebraic group over k. We prove that the\\nuniversal bundle over $X \\\\times \\\\mathcal{M}^d_G$ is stable with respect to any\\npolarization on $X \\\\times \\\\mathcal{M}^d_G$. A similar result is proved for the\\nPoincaré adjoint bundle over $X \\\\times M_G^{d, rs}$, where $M_G^{d, rs}$ is\\nthe coarse moduli space of regularly stable principal G-bundles over X of fixed\\ntopological type d.\\n',\n",
       " '  With $\\\\Fq$ the finite field of $q$ elements, we investigate the following\\nquestion. If $\\\\gamma$ generates $\\\\Fqn$ over $\\\\Fq$ and $\\\\beta$ is a non-zero\\nelement of $\\\\Fqn$, is there always an $a \\\\in \\\\Fq$ such that $\\\\beta(\\\\gamma + a)$\\nis a primitive element? We resolve this case when $n=3$, thereby proving a\\nconjecture by Cohen. We also improve substantially on what is known when $n=4$.\\n',\n",
       " '  In this paper we exhibit Morse geodesics, often called \"hyperbolic\\ndirections\", in infinite unbounded torsion groups. The groups studied are\\nlacunary hyperbolic groups and constructed using graded small cancellation\\nconditions. In all previously known examples, Morse geodesics were found in\\ngroups which also contained Morse elements, infinite order elements whose\\ncyclic subgroup gives a Morse quasi-geodesic. Our result presents the first\\nexample of a group which contains Morse geodesics but no Morse elements. In\\nfact, we show that there is an isometrically embedded $7$-regular tree inside\\nsuch groups where every infinite, simple path is a Morse geodesic.\\n',\n",
       " '  We study the ultimate bounds on the estimation of temperature for an\\ninteracting quantum system. We consider two coupled bosonic modes that are\\nassumed to be thermal and using quantum estimation theory establish the role\\nthe Hamiltonian parameters play in thermometry. We show that in the case of a\\nconserved particle number the interaction between the modes leads to a decrease\\nin the overall sensitivity to temperature, while interestingly, if particle\\nexchange is allowed with the thermal bath the converse is true. We explain this\\ndichotomy by examining the energy spectra. Finally, we devise experimentally\\nimplementable thermometry schemes that rely only on locally accessible\\ninformation from the total system, showing that almost Heisenberg limited\\nprecision can still be achieved, and we address the (im)possibility for\\nmultiparameter estimation in the system.\\n',\n",
       " \"  We show that publishing results using the statistical significance\\nfilter---publishing only when the p-value is less than 0.05---leads to a\\nvicious cycle of overoptimistic expectation of the replicability of results.\\nFirst, we show analytically that when true statistical power is relatively low,\\ncomputing power based on statistically significant results will lead to\\noverestimates of power. Then, we present a case study using 10 experimental\\ncomparisons drawn from a recently published meta-analysis in psycholinguistics\\n(Jäger et al., 2017). We show that the statistically significant results\\nyield an illusion of replicability. This illusion holds even if the researcher\\ndoesn't conduct any formal power analysis but just uses statistical\\nsignificance to informally assess robustness (i.e., replicability) of results.\\n\",\n",
       " '  The center-of-mass motion of a single optically levitated nanoparticle\\nresembles three uncoupled harmonic oscillators. We show how a suitable\\nmodulation of the optical trapping potential can give rise to a coupling\\nbetween two of these oscillators, such that their dynamics are governed by a\\nclassical equation of motion that resembles the Schrödinger equation for a\\ntwo-level system. Based on experimental data, we illustrate the dynamics of\\nthis parametrically coupled system both in the frequency and in the time\\ndomain. We discuss the limitations and differences of the mechanical analogue\\nin comparison to a true quantum mechanical system.\\n',\n",
       " '  We introduce new techniques to the analysis of neural spatiotemporal dynamics\\nvia applying $\\\\epsilon$-machine reconstruction to electroencephalography (EEG)\\nmicrostate sequences. Microstates are short duration quasi-stable states of the\\ndynamically changing electrical field topographies recorded via an array of\\nelectrodes from the human scalp, and cluster into four canonical classes. The\\nsequence of microstates observed under particular conditions can be considered\\nan information source with unknown underlying structure. $\\\\epsilon$-machines\\nare discrete dynamical system automata with state-dependent probabilities on\\ndifferent future observations (in this case the next measured EEG microstate).\\nThey artificially reproduce underlying structure in an optimally predictive\\nmanner as generative models exhibiting dynamics emulating the behaviour of the\\nsource. Here we present experiments using both simulations and empirical data\\nsupporting the value of associating these discrete dynamical systems with\\nmental states (e.g. mind-wandering, focused attention, etc.) and with clinical\\npopulations. The neurodynamics of mental states and clinical populations can\\nthen be further characterized by properties of these dynamical systems,\\nincluding: i) statistical complexity (determined by the number of states of the\\ncorresponding $\\\\epsilon$-automaton); ii) entropy rate; iii) characteristic\\nsequence patterning (syntax, probabilistic grammars); iv) duration, persistence\\nand stability of dynamical patterns; and v) algebraic measures such as\\nKrohn-Rhodes complexity or holonomy length of the decompositions of these. The\\npotential applications include the characterization of mental states in\\nneurodynamic terms for mental health diagnostics, well-being interventions,\\nhuman-machine interface, and others on both subject-specific and\\ngroup/population-level.\\n',\n",
       " '  We describe a 20-year survey carried out by the Lick-Carnegie Exoplanet\\nSurvey Team (LCES), using precision radial velocities from HIRES on the Keck-I\\ntelescope to find and characterize extrasolar planetary systems orbiting nearby\\nF, G, K, and M dwarf stars. We provide here 60,949 precision radial velocities\\nfor 1,624 stars contained in that survey. We tabulate a list of 357 significant\\nperiodic signals that are of constant period and phase, and not coincident in\\nperiod and/or phase with stellar activity indices. These signals are thus\\nstrongly suggestive of barycentric reflex motion of the star induced by one or\\nmore candidate exoplanets in Keplerian motion about the host star. Of these\\nsignals, 225 have already been published as planet claims, 60 are classified as\\nsignificant unpublished planet candidates that await photometric follow-up to\\nrule out activity-related causes, and 54 are also unpublished, but are\\nclassified as \"significant\" signals that require confirmation by additional\\ndata before rising to classification as planet candidates. Of particular\\ninterest is our detection of a candidate planet with a minimum mass of 3.9\\nEarth masses and an orbital period of 9.9 days orbiting Lalande 21185, the\\nfourth-closest main sequence star to the Sun. For each of our exoplanetary\\ncandidate signals, we provide the period and semi-amplitude of the Keplerian\\norbital fit, and a likelihood ratio estimate of its statistical significance.\\nWe also tabulate 18 Keplerian-like signals that we classify as likely arising\\nfrom stellar activity.\\n',\n",
       " '  Artificial intelligence methods have often been applied to perform specific\\nfunctions or tasks in the cyber-defense realm. However, as adversary methods\\nbecome more complex and difficult to divine, piecemeal efforts to understand\\ncyber-attacks, and malware-based attacks in particular, are not providing\\nsufficient means for malware analysts to understand the past, present and\\nfuture characteristics of malware.\\nIn this paper, we present the Malware Analysis and Attributed using Genetic\\nInformation (MAAGI) system. The underlying idea behind the MAAGI system is that\\nthere are strong similarities between malware behavior and biological organism\\nbehavior, and applying biologically inspired methods to corpora of malware can\\nhelp analysts better understand the ecosystem of malware attacks. Due to the\\nsophistication of the malware and the analysis, the MAAGI system relies heavily\\non artificial intelligence techniques to provide this capability. It has\\nalready yielded promising results over its development life, and will hopefully\\ninspire more integration between the artificial intelligence and cyber--defense\\ncommunities.\\n',\n",
       " '  Let a and b be algebraic numbers such that exactly one of a and b is an\\nalgebraic integer, and let f_t(z):=z^2+t be a family of polynomials\\nparametrized by t. We prove that the set of all algebraic numbers t for which\\nthere exist positive integers m and n such that f_t^m(a)=f_t^n(b) has bounded\\nWeil height. This is a special case of a more general result supporting a new\\nbounded height conjecture in dynamics. Our results fit into the general setting\\nof the principle of unlikely intersections in arithmetic dynamics.\\n',\n",
       " '  This note establishes the input-to-state stability (ISS) property for a\\nclamped-free damped string with respect to distributed and boundary\\ndisturbances. While efficient methods for establishing ISS properties for\\ndistributed parameter systems with respect to distributed disturbances have\\nbeen developed during the last decades, establishing ISS properties with\\nrespect to boundary disturbances remains challenging. One of the well-known\\nmethods for well-posedness analysis of systems with boundary inputs is to use\\nan adequate lifting operator, which transfers the boundary disturbance to a\\ndistributed one. However, the resulting distributed disturbance involves time\\nderivatives of the boundary perturbation. Thus, the subsequent ISS estimate\\ndepends on its amplitude, and may not be expressed in the strict form of ISS\\nproperties. To solve this problem, we show for a clamped-free damped string\\nequation that the projection of the original system trajectories in an adequate\\nRiesz basis can be used to establish the desired ISS property.\\n',\n",
       " \"  The problem of reliable communication over the multiple-access channel (MAC)\\nwith states is investigated. We propose a new coding scheme for this problem\\nwhich uses quasi-group codes (QGC). We derive a new computable single-letter\\ncharacterization of the achievable rate region. As an example, we investigate\\nthe problem of doubly-dirty MAC with modulo-$4$ addition. It is shown that the\\nsum-rate $R_1+R_2=1$ bits per channel use is achievable using the new scheme.\\nWhereas, the natural extension of the Gel'fand-Pinsker scheme, sum-rates\\ngreater than $0.32$ are not achievable.\\n\",\n",
       " '  We present a new paradigm for understanding optical absorption and hot\\nelectron dynamics experiments in graphene. Our analysis pivots on assigning\\nproper importance to phonon assisted indirect processes and bleaching of direct\\nprocesses. We show indirect processes figure in the excess absorption in the UV\\nregion. Experiments which were thought to indicate ultrafast relaxation of\\nelectrons and holes, reaching a thermal distribution from an extremely\\nnon-thermal one in under 5-10 fs, instead are explained by the nascent electron\\nand hole distributions produced by indirect transitions. These need no\\nrelaxation or ad-hoc energy removal to agree with the observed emission spectra\\nand fast pulsed absorption spectra. The fast emission following pulsed\\nabsorption is dominated by phonon assisted processes, which vastly outnumber\\ndirect ones and are always available, connecting any electron with any hole any\\ntime. Calculations are given, including explicitly calculating the magnitude of\\nindirect processes, supporting these views.\\n',\n",
       " '  We propose a probabilistic model for interpreting gene expression levels that\\nare observed through single-cell RNA sequencing. In the model, each cell has a\\nlow-dimensional latent representation. Additional latent variables account for\\ntechnical effects that may erroneously set some observations of gene expression\\nlevels to zero. Conditional distributions are specified by neural networks,\\ngiving the proposed model enough flexibility to fit the data well. We use\\nvariational inference and stochastic optimization to approximate the posterior\\ndistribution. The inference procedure scales to over one million cells, whereas\\ncompeting algorithms do not. Even for smaller datasets, for several tasks, the\\nproposed procedure outperforms state-of-the-art methods like ZIFA and\\nZINB-WaVE. We also extend our framework to take into account batch effects and\\nother confounding factors and propose a natural Bayesian hypothesis framework\\nfor differential expression that outperforms tradition DESeq2.\\n',\n",
       " \"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered\\nby radioactive beta decay of hundreds of $r$-process nuclides. We derive, using\\nFermi's theory of beta decay, an analytic estimate of the nuclear heating rate.\\nWe show that the heating rate evolves as a power law ranging between $t^{-6/5}$\\nto $t^{-4/3}$. The overall magnitude of the heating rate is determined by the\\nmean values of nuclear quantities, e.g., the nuclear matrix elements of beta\\ndecay. These values are specified by using nuclear experimental data. We\\ndiscuss the role of higher order beta transitions and the robustness of the\\npower law. The robust and simple form of the heating rate suggests that\\nobservations of the late-time bolometric light curve $\\\\propto t^{-\\\\frac{4}{3}}$\\nwould be a direct evidence of a $r$-process driven macronova. Such observations\\ncould also enable us to estimate the total amount of $r$-process nuclei\\nproduced in the merger.\\n\",\n",
       " '  The calculation of caloric properties such as heat capacity, Joule-Thomson\\ncoefficients and the speed of sound by classical force-field-based molecular\\nsimulation methodology has received scant attention in the literature,\\nparticularly for systems composed of complex molecules whose force fields (FFs)\\nare characterized by a combination of intramolecular and intermolecular terms\\n(referred to herein as \"flexible FFs\"). The calculation of a thermodynamic\\nproperty for a system whose molecules are described by such a FF involves the\\ncalculation of the residual property prior to its addition to the corresponding\\nideal-gas (IG) property, the latter of which is separately calculated, either\\nusing thermochemical compilations or nowadays accurate quantum mechanical\\ncalculations. Although the simulation of a volumetric residual property\\nproceeds by simply replacing the intermolecular FF in the rigid molecule case\\nby the total (intramolecular plus intermolecular) FF, this is not the case for\\na caloric property. We discuss the methodology required in performing such\\ncalculations, and focus on the example of the molar heat capacity at constant\\npressure, $c_P$, one of the most important caloric properties. We also consider\\nthree approximations for the calculation procedure, and illustrate their\\nconsequences for the examples of the relatively simple molecule 2-propanol,\\n${\\\\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\\\\rm HO(CH_2)_2NH_2}$, an\\nimportant fluid used in carbon capture.\\n',\n",
       " '  We study well-posedness of a velocity-vorticity formulation of the\\nNavier--Stokes equations, supplemented with no-slip velocity boundary\\nconditions, a no-penetration vorticity boundary condition, along with a natural\\nvorticity boundary condition depending on a pressure functional. In the\\nstationary case we prove existence and uniqueness of a suitable weak solution\\nto the system under a small data condition. The topic of the paper is driven by\\nrecent developments of vorticity based numerical methods for the Navier--Stokes\\nequations.\\n',\n",
       " '  Generative Adversarial Networks (GANs) represent a promising class of\\ngenerative networks that combine neural networks with game theory. From\\ngenerating realistic images and videos to assisting musical creation, GANs are\\ntransforming many fields of arts and sciences. However, their application to\\nhealthcare has not been fully realized, more specifically in generating\\nelectronic health records (EHR) data. In this paper, we propose a framework for\\nexploring the value of GANs in the context of continuous laboratory time series\\ndata. We devise an unsupervised evaluation method that measures the predictive\\npower of synthetic laboratory test time series. Further, we show that when it\\ncomes to predicting the impact of drug exposure on laboratory test data,\\nincorporating representation learning of the training cohorts prior to training\\nGAN models is beneficial.\\n',\n",
       " '  A database of minima and transition states corresponds to a network where the\\nminima represent nodes and the transition states correspond to edges between\\nthe pairs of minima they connect via steepest-descent paths. Here we construct\\nnetworks for small clusters bound by the Morse potential for a selection of\\nphysically relevant parameters, in two and three dimensions. The properties of\\nthese unweighted and undirected networks are analysed to examine two features:\\nwhether they are small-world, where the shortest path between nodes involves\\nonly a small number or edges; and whether they are scale-free, having a degree\\ndistribution that follows a power law. Small-world character is present, but\\nstatistical tests show that a power law is not a good fit, so the networks are\\nnot scale-free. These results for clusters are compared with the corresponding\\nproperties for the molecular and atomic structural glass formers\\northo-terphenyl and binary Lennard-Jones. These glassy systems do not show\\nsmall-world properties, suggesting that such behaviour is linked to the\\nstructure-seeking landscapes of the Morse clusters.\\n',\n",
       " '  Asynchronous distributed machine learning solutions have proven very\\neffective so far, but always assuming perfectly functioning workers. In\\npractice, some of the workers can however exhibit Byzantine behavior, caused by\\nhardware failures, software bugs, corrupt data, or even malicious attacks. We\\nintroduce \\\\emph{Kardam}, the first distributed asynchronous stochastic gradient\\ndescent (SGD) algorithm that copes with Byzantine workers. Kardam consists of\\ntwo complementary components: a filtering and a dampening component. The first\\nis scalar-based and ensures resilience against $\\\\frac{1}{3}$ Byzantine workers.\\nEssentially, this filter leverages the Lipschitzness of cost functions and acts\\nas a self-stabilizer against Byzantine workers that would attempt to corrupt\\nthe progress of SGD. The dampening component bounds the convergence rate by\\nadjusting to stale information through a generic gradient weighting scheme. We\\nprove that Kardam guarantees almost sure convergence in the presence of\\nasynchrony and Byzantine behavior, and we derive its convergence rate. We\\nevaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead\\nwith respect to non Byzantine-resilient solutions. We empirically show that\\nKardam does not introduce additional noise to the learning procedure but does\\ninduce a slowdown (the cost of Byzantine resilience) that we both theoretically\\nand empirically show to be less than $f/n$, where $f$ is the number of\\nByzantine failures tolerated and $n$ the total number of workers.\\nInterestingly, we also empirically observe that the dampening component is\\ninteresting in its own right for it enables to build an SGD algorithm that\\noutperforms alternative staleness-aware asynchronous competitors in\\nenvironments with honest workers.\\n',\n",
       " '  This paper describes an English audio and textual dataset of debating\\nspeeches, a unique resource for the growing research field of computational\\nargumentation and debating technologies. We detail the process of speech\\nrecording by professional debaters, the transcription of the speeches with an\\nAutomatic Speech Recognition (ASR) system, their consequent automatic\\nprocessing to produce a text that is more \"NLP-friendly\", and in parallel --\\nthe manual transcription of the speeches in order to produce gold-standard\\n\"reference\" transcripts. We release 60 speeches on various controversial\\ntopics, each in five formats corresponding to the different stages in the\\nproduction of the data. The intention is to allow utilizing this resource for\\nmultiple research purposes, be it the addition of in-domain training data for a\\ndebate-specific ASR system, or applying argumentation mining on either noisy or\\nclean debate transcripts. We intend to make further releases of this data in\\nthe future.\\n',\n",
       " '  Deep learning has been demonstrated to achieve excellent results for image\\nclassification and object detection. However, the impact of deep learning on\\nvideo analysis (e.g. action detection and recognition) has been limited due to\\ncomplexity of video data and lack of annotations. Previous convolutional neural\\nnetworks (CNN) based video action detection approaches usually consist of two\\nmajor steps: frame-level action proposal detection and association of proposals\\nacross frames. Also, these methods employ two-stream CNN framework to handle\\nspatial and temporal feature separately. In this paper, we propose an\\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\\naction detection in videos. The proposed architecture is a unified network that\\nis able to recognize and localize action based on 3D convolution features. A\\nvideo is first divided into equal length clips and for each clip a set of tube\\nproposals are generated next based on 3D Convolutional Network (ConvNet)\\nfeatures. Finally, the tube proposals of different clips are linked together\\nemploying network flow and spatio-temporal action detection is performed using\\nthese linked video proposals. Extensive experiments on several video datasets\\ndemonstrate the superior performance of T-CNN for classifying and localizing\\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.\\n',\n",
       " '  We develop the theoretical foundations of a network distance that has\\nrecently been applied to various subfields of topological data analysis, namely\\npersistent homology and hierarchical clustering. While this network distance\\nhas previously appeared in the context of finite networks, we extend the\\nsetting to that of compact networks. The main challenge in this new setting is\\nthe lack of an easy notion of sampling from compact networks; we solve this\\nproblem in the process of obtaining our results. The generality of our setting\\nmeans that we automatically establish results for exotic objects such as\\ndirected metric spaces and Finsler manifolds. We identify readily computable\\nnetwork invariants and establish their quantitative stability under this\\nnetwork distance. We also discuss the computational complexity involved in\\nprecisely computing this distance, and develop easily-computable lower bounds\\nby using the identified invariants. By constructing a wide range of explicit\\nexamples, we show that these lower bounds are effective in distinguishing\\nbetween networks. Finally, we provide a simple algorithm that computes a lower\\nbound on the distance between two networks in polynomial time and illustrate\\nour metric and invariant constructions on a database of random networks and a\\ndatabase of simulated hippocampal networks.\\n',\n",
       " '  Using the Panama Papers, we show that the beginning of media reporting on\\nexpropriations and property confiscations in a country increases the\\nprobability that offshore entities are incorporated by agents from the same\\ncountry in the same month. This result is robust to the use of country-year\\nfixed effects and the exclusion of tax havens. Further analysis shows that the\\neffect is driven by countries with non-corrupt and effective governments, which\\nsupports the notion that offshore entities are incorporated when reasonably\\nwell-intended and well-functioning governments become more serious about\\nfighting organized crime by confiscating proceeds of crime.\\n',\n",
       " '  Mammography screening for early detection of breast lesions currently suffers\\nfrom high amounts of false positive findings, which result in unnecessary\\ninvasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many\\nof these false-positive findings prior to biopsy. Current approaches estimate\\ntissue properties by means of quantitative parameters taken from generative,\\nbiophysical models fit to the q-space encoded signal under certain assumptions\\nregarding noise and spatial homogeneity. This process is prone to fitting\\ninstability and partial information loss due to model simplicity. We reveal\\nunexplored potentials of the signal by integrating all data processing\\ncomponents into a convolutional neural network (CNN) architecture that is\\ndesigned to propagate clinical target information down to the raw input images.\\nThis approach enables simultaneous and target-specific optimization of image\\nnormalization, signal exploitation, global representation learning and\\nclassification. Using a multicentric data set of 222 patients, we demonstrate\\nthat our approach significantly improves clinical decision making with respect\\nto the current state of the art.\\n',\n",
       " '  Small depth networks arise in a variety of network related applications,\\noften in the form of maximum flow and maximum weighted matching. Recent works\\nhave generalized such methods to include costs arising from concave functions.\\nIn this paper we give an algorithm that takes a depth $D$ network and strictly\\nincreasing concave weight functions of flows on the edges and computes a $(1 -\\n\\\\epsilon)$-approximation to the maximum weight flow in time $mD \\\\epsilon^{-1}$\\ntimes an overhead that is logarithmic in the various numerical parameters\\nrelated to the magnitudes of gradients and capacities.\\nOur approach is based on extending the scaling algorithm for approximate\\nmaximum weighted matchings by [Duan-Pettie JACM`14] to the setting of small\\ndepth networks, and then generalizing it to concave functions. In this more\\nrestricted setting of linear weights in the range $[w_{\\\\min}, w_{\\\\max}]$, it\\nproduces a $(1 - \\\\epsilon)$-approximation in time $O(mD \\\\epsilon^{-1} \\\\log(\\nw_{\\\\max} /w_{\\\\min}))$. The algorithm combines a variety of tools and provides a\\nunified approach towards several problems involving small depth networks.\\n',\n",
       " '  Knowledge-intensive companies that adopt Agile Software Development (ASD)\\nrelay on efficient implementation of Knowledge Management (KM) strategies to\\npromotes different Knowledge Processes (KPs) to gain competitive advantage.\\nThis study aims to explore how companies that adopt ASD implement KM strategies\\nutilizing practices that promote the KPs in the different organizational\\nlayers. Through a systematic literature review, we analyzed 32 primary studies,\\nselected by automated search and snowballing in the extant literature. To\\nanalyze the data, we applied narrative synthesis. Most of the identified KM\\npractices implement personalization strategies (81 %), supported by\\ncodification (19 %). Our review shows that the primary studies do not report KM\\npractices in the strategic layer and two of them in the product portfolio\\nlayer; on the other hand, in the project layer, the studies report 33 practices\\nthat implement personalization strategy, and seven practices that implement\\ncodification. KM strategies in ASD promote mainly the knowledge transfer\\nprocess with practices that stimulate social interaction to share tacit\\nknowledge in the project layer. As a result of using informal communication, a\\nsignificant amount of knowledge can be lost or not properly transferred to\\nother individuals and, instead of propagating the knowledge, it remains inside\\na few individuals minds.\\n',\n",
       " '  Recent 60Fe results have suggested that the estimated distances of supernovae\\nin the last few million years should be reduced from 100 pc to 50 pc. Two\\nevents or series of events are suggested, one about 2.7 million years to 1.7\\nmillion years ago, and another may at 6.5 to 8.7 million years ago. We ask what\\neffects such supernovae are expected to have on the terrestrial atmosphere and\\nbiota. Assuming that the Local Bubble was formed before the event being\\nconsidered, and that the supernova and the Earth were both inside a weak,\\ndisordered magnetic field at that time, TeV-PeV cosmic rays at Earth will\\nincrease by a factor of a few hundred. Tropospheric ionization will increase\\nproportionately, and the overall muon radiation load on terrestrial organisms\\nwill increase by a factor of 150. All return to pre-burst levels within 10kyr.\\nIn the case of an ordered magnetic field, effects depend strongly on the field\\norientation. The upper bound in this case is with a largely coherent field\\naligned along the line of sight to the supernova, in which case TeV-PeV cosmic\\nray flux increases are 10^4; in the case of a transverse field they are below\\ncurrent levels. We suggest a substantial increase in the extended effects of\\nsupernovae on Earth and in the lethal distance estimate; more work is\\nneeded.This paper is an explicit followup to Thomas et al. (2016). We also here\\nprovide more detail on the computational procedures used in both works.\\n',\n",
       " '  The surface tension of flowing soap films is measured with respect to the\\nfilm thickness and the concentration of soap solution. We perform this\\nmeasurement by measuring the curvature of the nylon wires that bound the soap\\nfilm channel and use the measured curvature to parametrize the relation between\\nthe surface tension and the tension of the wire. We find the surface tension of\\nour soap films increases when the film is relatively thin or made of soap\\nsolution of low concentration, otherwise it approaches an asymptotic value 30\\nmN/m. A simple adsorption model with only two parameters describes our\\nobservations reasonably well. With our measurements, we are also able to\\nmeasure Gibbs elasticity for our soap film.\\n',\n",
       " '  Indoor localization based on Visible Light Communication (VLC) has been in\\nfavor with both the academia and industry for years. In this paper, we present\\na prototyping photodiode-based VLC system towards large-scale localization.\\nSpecially, we give in-depth analysis of the design constraints and\\nconsiderations for large-scale indoor localization research. After that we\\nidentify the key enablers for such systems: 1) distributed architecture, 2)\\none-way communication, and 3) random multiple access. Accordingly, we propose\\nPlugo -- a photodiode-based VLC system conforming to the aforementioned\\ncriteria. We present a compact design of the VLC-compatible LED bulbs featuring\\nplug-and-go use-cases. The basic framed slotted Additive Links On-line Hawaii\\nArea (ALOHA) is exploited to achieve random multiple access over the shared\\noptical medium. We show its effectiveness in beacon broadcasting by\\nexperiments, and further demonstrate its scalability to large-scale scenarios\\nthrough simulations. Finally, preliminary localization experiments are\\nconducted using fingerprinting-based methods in a customized testbed, achieving\\nan average accuracy of 0.14 m along with a 90-percentile accuracy of 0.33 m.\\n',\n",
       " '  We show that the output of a (residual) convolutional neural network (CNN)\\nwith an appropriate prior over the weights and biases is a Gaussian process\\n(GP) in the limit of infinitely many convolutional filters, extending similar\\nresults for dense networks. For a CNN, the equivalent kernel can be computed\\nexactly and, unlike \"deep kernels\", has very few parameters: only the\\nhyperparameters of the original CNN. Further, we show that this kernel has two\\nproperties that allow it to be computed efficiently; the cost of evaluating the\\nkernel for a pair of images is similar to a single forward pass through the\\noriginal CNN with only one filter per layer. The kernel equivalent to a\\n32-layer ResNet obtains 0.84% classification error on MNIST, a new record for\\nGPs with a comparable number of parameters.\\n',\n",
       " '  Detecting attacks in control systems is an important aspect of designing\\nsecure and resilient control systems. Recently, a dynamic watermarking approach\\nwas proposed for detecting malicious sensor attacks for SISO LTI systems with\\npartial state observations and MIMO LTI systems with a full rank input matrix\\nand full state observations; however, these previous approaches cannot be\\napplied to general LTI systems that are MIMO and have partial state\\nobservations. This paper designs a dynamic watermarking approach for detecting\\nmalicious sensor attacks for general LTI systems, and we provide a new set of\\nasymptotic and statistical tests. We prove these tests can detect attacks that\\nfollow a specified attack model (more general than replay attacks), and we also\\nshow that these tests simplify to existing tests when the system is SISO or has\\nfull rank input matrix and full state observations. The benefit of our approach\\nis demonstrated with a simulation analysis of detecting sensor attacks in\\nautonomous vehicles. Our approach can distinguish between sensor attacks and\\nwind disturbance (through an internal model principle framework), whereas\\nimproperly designed tests cannot distinguish between sensor attacks and wind\\ndisturbance.\\n',\n",
       " '  For people with visual impairments, tactile graphics are an important means\\nto learn and explore information. However, raised line tactile graphics created\\nwith traditional materials such as embossing are static. While available\\nrefreshable displays can dynamically change the content, they are still too\\nexpensive for many users, and are limited in size. These factors limit\\nwide-spread adoption and the representation of large graphics or data sets. In\\nthis paper, we present FluxMaker, an inexpensive scalable system that renders\\ndynamic information on top of static tactile graphics with movable tactile\\nmarkers. These dynamic tactile markers can be easily reconfigured and used to\\nannotate static raised line tactile graphics, including maps, graphs, and\\ndiagrams. We developed a hardware prototype that actuates magnetic tactile\\nmarkers driven by low-cost and scalable electromagnetic coil arrays, which can\\nbe fabricated with standard printed circuit board manufacturing. We evaluate\\nour prototype with six participants with visual impairments and found positive\\nresults across four application areas: location finding or navigating on\\ntactile maps, data analysis, and physicalization, feature identification for\\ntactile graphics, and drawing support. The user study confirms advantages in\\napplication domains such as education and data exploration.\\n',\n",
       " '  Very often features come with their own vectorial descriptions which provide\\ndetailed information about their properties. We refer to these vectorial\\ndescriptions as feature side-information. In the standard learning scenario,\\ninput is represented as a vector of features and the feature side-information\\nis most often ignored or used only for feature selection prior to model\\nfitting. We believe that feature side-information which carries information\\nabout features intrinsic property will help improve model prediction if used in\\na proper way during learning process. In this paper, we propose a framework\\nthat allows for the incorporation of the feature side-information during the\\nlearning of very general model families to improve the prediction performance.\\nWe control the structures of the learned models so that they reflect features\\nsimilarities as these are defined on the basis of the side-information. We\\nperform experiments on a number of benchmark datasets which show significant\\npredictive performance gains, over a number of baselines, as a result of the\\nexploitation of the side-information.\\n',\n",
       " '  We give a short proof of the $L^{1}$ criterion for Beurling generalized\\nintegers to have a positive asymptotic density. We actually prove the existence\\nof density under a weaker hypothesis. We also discuss related sufficient\\nconditions for the estimate $m(x)=\\\\sum_{n_{k}\\\\leq x} \\\\mu(n_k)/n_k=o(1)$, with\\n$\\\\mu$ the Beurling analog of the Moebius function.\\n',\n",
       " '  Social media users often make explicit predictions about upcoming events.\\nSuch statements vary in the degree of certainty the author expresses toward the\\noutcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\"\\nor \"No way Leonardo wins!\". Can popular beliefs on social media predict who\\nwill win? To answer this question, we build a corpus of tweets annotated for\\nveridicality on which we train a log-linear classifier that detects positive\\nveridicality with high precision. We then forecast uncertain outcomes using the\\nwisdom of crowds, by aggregating users\\' explicit predictions. Our method for\\nforecasting winners is fully automated, relying only on a set of contenders as\\ninput. It requires no training data of past outcomes and outperforms sentiment\\nand tweet volume baselines on a broad range of contest prediction tasks. We\\nfurther demonstrate how our approach can be used to measure the reliability of\\nindividual accounts\\' predictions and retrospectively identify surprise\\noutcomes.\\n',\n",
       " '  Applications involving autonomous navigation and planning of mobile agents\\ncan benefit greatly by employing online Simultaneous Localization and Mapping\\n(SLAM) techniques, however, their proper implementation still warrants an\\nefficient amalgamation with any offline path planning method that may be used\\nfor the particular application. In this paper, such a case of amalgamation is\\nconsidered for a LiDAR-based indoor mapping system which presents itself as a\\n2D coverage path planning problem implemented along with online SLAM. This\\npaper shows how classic offline Coverage Path Planning (CPP) can be altered for\\nuse with online SLAM by proposing two modifications: (i) performing convex\\ndecomposition of the polygonal coverage area to allow for an arbitrary choice\\nof an initial point while still tracing the shortest coverage path and (ii)\\nusing a new approach to stitch together the different cells within the\\npolygonal area to form a continuous coverage path. Furthermore, an alteration\\nto the SLAM operation to suit the coverage path planning strategy is also made\\nthat evaluates navigation errors in terms of an area coverage cost function.\\nThe implementation results show how the combination of the two modified offline\\nand online planning strategies allow for an improvement in the total area\\ncoverage by the mapping system - the modification thus presents an approach for\\nmodifying offline and online navigation strategies for robust operation.\\n',\n",
       " '  We apply a method that combines the tight-binding approximation and the\\nLowdin down-folding procedure to evaluate the electronic band structure of the\\nnewly discovered pressure-induced superconductor CrAs. By integrating out all\\nlow-lying arsenic degrees of freedom, we derive an effective Hamiltonian model\\ndescribing the Cr d bands near the Fermi level. We calculate and make\\npredictions for the energy spectra, the Fermi surface, the density of states\\nand transport and magnetic properties of this compound. Our results are\\nconsistent with local-density approximation calculations as well as they show\\ngood agreement with available experimental data for resistivity and Cr magnetic\\nmoment.\\n',\n",
       " '  In this lecture note, we describe high dynamic range (HDR) imaging systems;\\nsuch systems are able to represent luminances of much larger brightness and,\\ntypically, also a larger range of colors than conventional standard dynamic\\nrange (SDR) imaging systems. The larger luminance range greatly improve the\\noverall quality of visual content, making it appears much more realistic and\\nappealing to observers. HDR is one of the key technologies of the future\\nimaging pipeline, which will change the way the digital visual content is\\nrepresented and manipulated today.\\n',\n",
       " '  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use\\nthis classification to build a pro-$p$ analogue of the curve complex and\\nestablish its basic properties. We conclude with some statements concerning\\nseparability properties of the mapping class group.\\n',\n",
       " '  Sports data analysis is becoming increasingly large-scale, diversified, and\\nshared, but difficulty persists in rapidly accessing the most crucial\\ninformation. Previous surveys have focused on the methodologies of sports video\\nanalysis from the spatiotemporal viewpoint instead of a content-based\\nviewpoint, and few of these studies have considered semantics. This study\\ndevelops a deeper interpretation of content-aware sports video analysis by\\nexamining the insight offered by research into the structure of content under\\ndifferent scenarios. On the basis of this insight, we provide an overview of\\nthe themes particularly relevant to the research on content-aware systems for\\nbroadcast sports. Specifically, we focus on the video content analysis\\ntechniques applied in sportscasts over the past decade from the perspectives of\\nfundamentals and general review, a content hierarchical model, and trends and\\nchallenges. Content-aware analysis methods are discussed with respect to\\nobject-, event-, and context-oriented groups. In each group, the gap between\\nsensation and content excitement must be bridged using proper strategies. In\\nthis regard, a content-aware approach is required to determine user demands.\\nFinally, the paper summarizes the future trends and challenges for sports video\\nanalysis. We believe that our findings can advance the field of research on\\ncontent-aware video analysis for broadcast sports.\\n',\n",
       " '  In kernel methods, temporal information on the data is commonly included by\\nusing time-delayed embeddings as inputs. Recently, an alternative formulation\\nwas proposed by defining a gamma-filter explicitly in a reproducing kernel\\nHilbert space, giving rise to a complex model where multiple kernels operate on\\ndifferent temporal combinations of the input signal. In the original\\nformulation, the kernels are then simply combined to obtain a single kernel\\nmatrix (for instance by averaging), which provides computational benefits but\\ndiscards important information on the temporal structure of the signal.\\nInspired by works on multiple kernel learning, we overcome this drawback by\\nconsidering the different kernels separately. We propose an efficient strategy\\nto adaptively combine and select these kernels during the training phase. The\\nresulting batch and online algorithms automatically learn to process highly\\nnonlinear temporal information extracted from the input signal, which is\\nimplicitly encoded in the kernel values. We evaluate our proposal on several\\nartificial and real tasks, showing that it can outperform classical approaches\\nboth in batch and online settings.\\n',\n",
       " '  We consider the problem of sequential learning from categorical observations\\nbounded in [0,1]. We establish an ordering between the Dirichlet posterior over\\ncategorical outcomes and a Gaussian posterior under observations with N(0,1)\\nnoise. We establish that, conditioned upon identical data with at least two\\nobservations, the posterior mean of the categorical distribution will always\\nsecond-order stochastically dominate the posterior mean of the Gaussian\\ndistribution. These results provide a useful tool for the analysis of\\nsequential learning under categorical outcomes.\\n',\n",
       " '  We develop a new approach to learn the parameters of regression models with\\nhidden variables. In a nutshell, we estimate the gradient of the regression\\nfunction at a set of random points, and cluster the estimated gradients. The\\ncenters of the clusters are used as estimates for the parameters of hidden\\nunits. We justify this approach by studying a toy model, whereby the regression\\nfunction is a linear combination of sigmoids. We prove that indeed the\\nestimated gradients concentrate around the parameter vectors of the hidden\\nunits, and provide non-asymptotic bounds on the number of required samples. To\\nthe best of our knowledge, no comparable guarantees have been proven for linear\\ncombinations of sigmoids.\\n',\n",
       " '  The intricate interplay between optically dark and bright excitons governs\\nthe light-matter interaction in transition metal dichalcogenide monolayers. We\\nhave performed a detailed investigation of the \"spin-forbidden\" dark excitons\\nin WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field\\nBz. In agreement with the theoretical predictions deduced from group theory\\nanalysis, magneto-photoluminescence experiments reveal a zero field splitting\\n$\\\\delta=0.6 \\\\pm 0.1$ meV between two dark exciton states. The low energy state\\nbeing strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state\\nis partially coupled to light with z polarization (\"grey\" exciton). The first\\ndetermination of the dark neutral exciton lifetime $\\\\tau_D$ in a transition\\nmetal dichalcogenide monolayer is obtained by time-resolved photoluminescence.\\nWe measure $\\\\tau_D \\\\sim 110 \\\\pm 10$ ps for the grey exciton state, i.e. two\\norders of magnitude longer than the radiative lifetime of the bright neutral\\nexciton at T=12 K.\\n',\n",
       " '  In this article, we develop a notion of Quillen bifibration which combines\\nthe two notions of Grothendieck bifibration and of Quillen model structure. In\\nparticular, given a bifibration $p:\\\\mathcal E\\\\to\\\\mathcal B$, we describe when a\\nfamily of model structures on the fibers $\\\\mathcal E_A$ and on the basis\\ncategory $\\\\mathcal B$ combines into a model structure on the total category\\n$\\\\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and\\nweak equivalences. Using this Grothendieck construction for model structures,\\nwe revisit the traditional definition of Reedy model structures, and possible\\ngeneralizations, and exhibit their bifibrational nature.\\n',\n",
       " '  In this paper we define canonical sine and cosine transform, convolution\\noperations, prove convolution theorems in space of integrable functions on real\\nspace. Further, obtain some results require to construct the spaces of\\nintegrable Boehmians then extend this canonical sine and canonical cosine\\ntransforms to space of integrable Boehmians and obtain their properties.\\n',\n",
       " '  Compressed sensing (CS) is a sampling theory that allows reconstruction of\\nsparse (or compressible) signals from an incomplete number of measurements,\\nusing of a sensing mechanism implemented by an appropriate projection matrix.\\nThe CS theory is based on random Gaussian projection matrices, which satisfy\\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\\nprojections are more suitable for hardware implementation. In this paper, we\\npresent a deep learning approach to obtain very sparse ternary projections for\\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\\nexperimental results on real images demonstrate the effectiveness of the\\nproposed approach compared to state-of-the-art methods, with significant\\nadvantage in terms of complexity.\\n',\n",
       " '  Predictive models for music are studied by researchers of algorithmic\\ncomposition, the cognitive sciences and machine learning. They serve as base\\nmodels for composition, can simulate human prediction and provide a\\nmultidisciplinary application domain for learning algorithms. A particularly\\nwell established and constantly advanced subtask is the prediction of\\nmonophonic melodies. As melodies typically involve non-Markovian dependencies\\ntheir prediction requires a capable learning algorithm. In this thesis, I apply\\nthe recent feature discovery and learning method PULSE to the realm of symbolic\\nmusic modeling. PULSE is comprised of a feature generating operation and\\nL1-regularized optimization. These are used to iteratively expand and cull the\\nfeature set, effectively exploring feature spaces that are too large for common\\nfeature selection approaches. I design a general Python framework for PULSE,\\npropose task-optimized feature generating operations and various\\nmusic-theoretically motivated features that are evaluated on a standard corpus\\nof monophonic folk and chorale melodies. The proposed method significantly\\noutperforms comparable state-of-the-art models. I further discuss the free\\nparameters of the learning algorithm and analyze the feature composition of the\\nlearned models. The models learned by PULSE afford an easy inspection and are\\nmusicologically interpreted for the first time.\\n',\n",
       " '  Many real-world data sets, especially in biology, are produced by highly\\nmultivariate and nonlinear complex dynamical systems. In this paper, we focus\\non brain imaging data, including both calcium imaging and functional MRI data.\\nStandard vector-autoregressive models are limited by their linearity\\nassumptions, while nonlinear general-purpose, large-scale temporal models, such\\nas LSTM networks, typically require large amounts of training data, not always\\nreadily available in biological applications; furthermore, such models have\\nlimited interpretability. We introduce here a novel approach for learning a\\nnonlinear differential equation model aimed at capturing brain dynamics.\\nSpecifically, we propose a variable-projection optimization approach to\\nestimate the parameters of the multivariate (coupled) van der Pol oscillator,\\nand demonstrate that such a model can accurately represent nonlinear dynamics\\nof the brain data. Furthermore, in order to improve the predictive accuracy\\nwhen forecasting future brain-activity time series, we use this analytical\\nmodel as an unlimited source of simulated data for pretraining LSTM; such\\nmodel-specific data augmentation approach consistently improves LSTM\\nperformance on both calcium and fMRI imaging data.\\n',\n",
       " '  In this research, we propose a deep learning based approach for speeding up\\nthe topology optimization methods. The problem we seek to solve is the layout\\nproblem. The main novelty of this work is to state the problem as an image\\nsegmentation task. We leverage the power of deep learning methods as the\\nefficient pixel-wise image labeling technique to perform the topology\\noptimization. We introduce convolutional encoder-decoder architecture and the\\noverall approach of solving the above-described problem with high performance.\\nThe conducted experiments demonstrate the significant acceleration of the\\noptimization process. The proposed approach has excellent generalization\\nproperties. We demonstrate the ability of the application of the proposed model\\nto other problems. The successful results, as well as the drawbacks of the\\ncurrent method, are discussed.\\n',\n",
       " '  We present and evaluate a technique for computing path-sensitive interference\\nconditions during abstract interpretation of concurrent programs. In lieu of\\nfixed point computation, we use prime event structures to compactly represent\\ncausal dependence and interference between sequences of transformers. Our main\\ncontribution is an unfolding algorithm that uses a new notion of independence\\nto avoid redundant transformer application, thread-local fixed points to reduce\\nthe size of the unfolding, and a novel cutoff criterion based on subsumption to\\nguarantee termination of the analysis. Our experiments show that the abstract\\nunfolding produces an order of magnitude fewer false alarms than a mature\\nabstract interpreter, while being several orders of magnitude faster than\\nsolver-based tools that have the same precision.\\n',\n",
       " '  The next generation of cosmological surveys will operate over unprecedented\\nscales, and will therefore provide exciting new opportunities for testing\\ngeneral relativity. The standard method for modelling the structures that these\\nsurveys will observe is to use cosmological perturbation theory for linear\\nstructures on horizon-sized scales, and Newtonian gravity for non-linear\\nstructures on much smaller scales. We propose a two-parameter formalism that\\ngeneralizes this approach, thereby allowing interactions between large and\\nsmall scales to be studied in a self-consistent and well-defined way. This uses\\nboth post-Newtonian gravity and cosmological perturbation theory, and can be\\nused to model realistic cosmological scenarios including matter, radiation and\\na cosmological constant. We find that the resulting field equations can be\\nwritten as a hierarchical set of perturbation equations. At leading-order,\\nthese equations allow us to recover a standard set of Friedmann equations, as\\nwell as a Newton-Poisson equation for the inhomogeneous part of the Newtonian\\nenergy density in an expanding background. For the perturbations in the\\nlarge-scale cosmology, however, we find that the field equations are sourced by\\nboth non-linear and mode-mixing terms, due to the existence of small-scale\\nstructures. These extra terms should be expected to give rise to new\\ngravitational effects, through the mixing of gravitational modes on small and\\nlarge scales - effects that are beyond the scope of standard linear\\ncosmological perturbation theory. We expect our formalism to be useful for\\naccurately modelling gravitational physics in universes that contain non-linear\\nstructures, and for investigating the effects of non-linear gravity in the era\\nof ultra-large-scale surveys.\\n',\n",
       " '  Playing the game of heads or tails in zero gravity demonstrates that there\\nexists a contextual \"measurement\" in classical mechanics. When the coin is\\nflipped, its orientation is a continuous variable. However, the \"measurement\"\\nthat occurs when the coin is caught by clapping two hands together gives a\\ndiscrete value (heads or tails) that depends on the context (orientation of the\\nhands). It is then shown that there is a strong analogy with the spin\\nmeasurement of the Stern-Gerlach experiment, and in particular with Stern and\\nGerlach\\'s sequential measurements. Finally, we clarify the analogy by recalling\\nhow the de Broglie-Bohm interpretation simply explains the spin \"measurement\".\\n',\n",
       " '  The variational autoencoder (VAE) is a popular model for density estimation\\nand representation learning. Canonically, the variational principle suggests to\\nprefer an expressive inference model so that the variational approximation is\\naccurate. However, it is often overlooked that an overly-expressive inference\\nmodel can be detrimental to the test set performance of both the amortized\\nposterior approximator and, more importantly, the generative density estimator.\\nIn this paper, we leverage the fact that VAEs rely on amortized inference and\\npropose techniques for amortized inference regularization (AIR) that control\\nthe smoothness of the inference model. We demonstrate that, by applying AIR, it\\nis possible to improve VAE generalization on both inference and generative\\nperformance. Our paper challenges the belief that amortized inference is simply\\na mechanism for approximating maximum likelihood training and illustrates that\\nregularization of the amortization family provides a new direction for\\nunderstanding and improving generalization in VAEs.\\n',\n",
       " \"  Synchronization on multiplex networks have attracted increasing attention in\\nthe past few years. We investigate collective behaviors of Kuramoto oscillators\\non single layer and duplex spacial networks with total cost restriction, which\\nwas introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,\\nStanley H. E. and Jr A. J., {\\\\it Phys. Rev. Lett.} 104, 018701 (2010)] and\\ntermed as the Li network afterwards. In the Li network model, with the increase\\nof its spacial exponent, the network's structure will vary from the random type\\nto the small-world one, and finally to the regular lattice.We first explore how\\nthe spacial exponent influences the synchronizability of Kuramoto oscillators\\non single layer Li networks and find that the closer the Li network is to a\\nregular lattice, the more difficult for it to evolve into synchronization. Then\\nwe investigate synchronizability of duplex Li networks and find that the\\nexistence of inter-layer interaction can greatly enhance inter-layer and global\\nsynchronizability. When the inter-layer coupling strength is larger than a\\ncertain critical value, whatever the intra-layer coupling strength is, the\\ninter-layer synchronization will always occur. Furthermore, on single layer Li\\nnetworks, nodes with larger degrees more easily reach global synchronization,\\nwhile on duplex Li networks, this phenomenon becomes much less obvious.\\nFinally, we study the impact of inter-link density on global synchronization\\nand obtain that sparse inter-links can lead to the emergence of global\\nsynchronization for duplex Li networks just as dense inter-links do. In a word,\\ninter-layer interaction plays a vital role in determining synchronizability for\\nduplex spacial networks with total cost constraint.\\n\",\n",
       " '  We continue to investigate binary sequence $(f_u)$ over $\\\\{0,1\\\\}$ defined by\\n$(-1)^{f_u}=\\\\left(\\\\frac{(u^w-u^{wp})/p}{p}\\\\right)$ for integers $u\\\\ge 0$, where\\n$\\\\left(\\\\frac{\\\\cdot}{p}\\\\right)$ is the Legendre symbol and we restrict\\n$\\\\left(\\\\frac{0}{p}\\\\right)=1$. In an earlier work, the linear complexity of\\n$(f_u)$ was determined for $w=p-1$ under the assumption of $2^{p-1}\\\\not\\\\equiv 1\\n\\\\pmod {p^2}$. In this work, we give possible values on the linear complexity of\\n$(f_u)$ for all $1\\\\le w<p-1$ under the same conditions. We also state that the\\ncase of larger $w(\\\\geq p)$ can be reduced to that of $0\\\\leq w\\\\leq p-1$.\\n',\n",
       " '  A central question in science of science concerns how time affects citations.\\nDespite the long-standing interests and its broad impact, we lack systematic\\nanswers to this simple yet fundamental question. By reviewing and classifying\\nprior studies for the past 50 years, we find a significant lack of consensus in\\nthe literature, primarily due to the coexistence of retrospective and\\nprospective approaches to measuring citation age distributions. These two\\napproaches have been pursued in parallel, lacking any known connections between\\nthe two. Here we developed a new theoretical framework that not only allows us\\nto connect the two approaches through precise mathematical relationships, it\\nalso helps us reconcile the interplay between temporal decay of citations and\\nthe growth of science, helping us uncover new functional forms characterizing\\ncitation age distributions. We find retrospective distribution follows a\\nlognormal distribution with exponential cutoff, while prospective distribution\\nis governed by the interplay between a lognormal distribution and the growth in\\nthe number of references. Most interestingly, the two approaches can be\\nconnected once rescaled by the growth of publications and citations. We further\\nvalidate our framework using both large-scale citation datasets and analytical\\nmodels capturing citation dynamics. Together this paper presents a\\ncomprehensive analysis of the time dimension of science, representing a new\\nempirical and theoretical basis for all future studies in this area.\\n',\n",
       " '  Excited states of a single donor in bulk silicon have previously been studied\\nextensively based on effective mass theory. However, a proper theoretical\\ndescription of the excited states of a donor cluster is still scarce. Here we\\nstudy the excitations of lines of defects within a single-valley spherical band\\napproximation, thus mapping the problem to a scaled hydrogen atom array. A\\nseries of detailed full configuration-interaction and time-dependent hybrid\\ndensity-functional theory calculations have been performed to understand linear\\nclusters of up to 10 donors. Our studies illustrate the generic features of\\ntheir excited states, addressing the competition between formation of\\ninter-donor ionic states and intra-donor atomic excited states. At short\\ninter-donor distances, excited states of donor molecules are dominant, at\\nintermediate distances ionic states play an important role, and at long\\ndistances the intra-donor excitations are predominant as expected. The\\ncalculations presented here emphasise the importance of correlations between\\ndonor electrons, and are thus complementary to other recent approaches that\\ninclude effective mass anisotropy and multi-valley effects. The exchange\\nsplittings between relevant excited states have also been estimated for a donor\\npair and for a three-donor arrays; the splittings are much larger than those in\\nthe ground state in the range of donor separations between 10 and 20 nm. This\\nestablishes a solid theoretical basis for the use of excited-state exchange\\ninteractions for controllable quantum gate operations in silicon.\\n',\n",
       " \"  We present a statistical study on the [C I]($^{3} \\\\rm P_{1} \\\\rightarrow {\\\\rm\\n^3 P}_{0}$), [C I] ($^{3} \\\\rm P_{2} \\\\rightarrow {\\\\rm ^3 P}_{1}$) lines\\n(hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0)\\nline for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore\\nthe correlations between the luminosities of CO (1$-$0) and [C I] lines, and\\nfind that $L'_\\\\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_\\n\\\\mathrm{[CI](1-0)}$ and $L'_\\\\mathrm{[CI](2-1)}$, suggesting that [C I] lines\\ncan trace total molecular gas mass at least for (U)LIRGs. We also investigate\\nthe dependence of $L'_\\\\mathrm{[CI](1-0)}$/$L'_\\\\mathrm{CO(1-0)}$,\\n$L'_\\\\mathrm{[CI](2-1)}$/$L'_\\\\mathrm{CO(1-0)}$ and\\n$L'_\\\\mathrm{[CI](2-1)}$/$L'_\\\\mathrm{[CI](1-0)}$ on the far-infrared color of\\n60-to-100 $\\\\mu$m, and find non-correlation, a weak correlation and a modest\\ncorrelation, respectively. Under the assumption that these two carbon\\ntransitions are optically thin, we further calculate the [C I] line excitation\\ntemperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass\\nconversion factors for our sample. The resulting $\\\\mathrm{H_2}$ masses using\\nthese [C I]-based conversion factors roughly agree with those derived from\\n$L'_\\\\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.\\n\",\n",
       " '  We study large-scale kernel methods for acoustic modeling in speech\\nrecognition and compare their performance to deep neural networks (DNNs). We\\nperform experiments on four speech recognition datasets, including the TIMIT\\nand Broadcast News benchmark tasks, and compare these two types of models on\\nframe-level performance metrics (accuracy, cross-entropy), as well as on\\nrecognition metrics (word/character error rate). In order to scale kernel\\nmethods to these large datasets, we use the random Fourier feature method of\\nRahimi and Recht (2007). We propose two novel techniques for improving the\\nperformance of kernel acoustic models. First, in order to reduce the number of\\nrandom features required by kernel models, we propose a simple but effective\\nmethod for feature selection. The method is able to explore a large number of\\nnon-linear features while maintaining a compact model more efficiently than\\nexisting approaches. Second, we present a number of frame-level metrics which\\ncorrelate very strongly with recognition performance when computed on the\\nheldout set; we take advantage of these correlations by monitoring these\\nmetrics during training in order to decide when to stop learning. This\\ntechnique can noticeably improve the recognition performance of both DNN and\\nkernel models, while narrowing the gap between them. Additionally, we show that\\nthe linear bottleneck method of Sainath et al. (2013) improves the performance\\nof our kernel models significantly, in addition to speeding up training and\\nmaking the models more compact. Together, these three methods dramatically\\nimprove the performance of kernel acoustic models, making their performance\\ncomparable to DNNs on the tasks we explored.\\n',\n",
       " '  We determine the composition factors of the tensor product $S(E)\\\\otimes S(E)$\\nof two copies of the symmetric algebra of the natural module $E$ of a general\\nlinear group over an algebraically closed field of positive characteristic. Our\\nmain result may be regarded as a substantial generalisation of the tensor\\nproduct theorem of Krop and Sullivan, on composition factors of $S(E)$. We\\nearlier answered the question of which polynomially injective modules are\\ninfinitesimally injective in terms of the \"divisibility index\". We are now able\\nto give an explicit description of the divisibility index for polynomial\\nmodules for general linear groups of degree at most $3$.\\n',\n",
       " '  Current understanding of how contractility emerges in disordered actomyosin\\nnetworks of non-muscle cells is still largely based on the intuition derived\\nfrom earlier works on muscle contractility. This view, however, largely\\noverlooks the free energy gain following passive cross-linker binding, which,\\neven in the absence of active fluctuations, provides a thermodynamic drive\\ntowards highly overlapping filamentous states. In this work, we shed light on\\nthis phenomenon, showing that passive cross-linkers, when considered in the\\ncontext of two anti-parallel filaments, generate noticeable contractile forces.\\nHowever, as binding free energy of cross-linkers is increased, a sharp onset of\\nkinetic arrest follows, greatly diminishing effectiveness of this contractility\\nmechanism, allowing the network to contract only with weakly resisting tensions\\nat its boundary. We have carried out stochastic simulations elucidating this\\nmechanism, followed by a mean-field treatment that predicts how contractile\\nforces asymptotically scale at small and large binding energies, respectively.\\nFurthermore, when considering an active contractile filament pair, based on\\nnon-muscle myosin II, we found that the non-processive nature of these motors\\nleads to highly inefficient force generation, due to recoil slippage of the\\noverlap during periods when the motor is dissociated. However, we discovered\\nthat passive cross-linkers can serve as a structural ratchet during these\\nunbound motor time spans, resulting in vast force amplification. Our results\\nshed light on the non-equilibrium effects of transiently binding proteins in\\nbiological active matter, as observed in the non-muscle actin cytoskeleton,\\nshowing that highly efficient contractile force dipoles result from synergy of\\npassive cross-linker and active motor dynamics, via a ratcheting mechanism on a\\nfunneled energy landscape.\\n',\n",
       " '  This work encompasses Rate-Splitting (RS), providing significant benefits in\\nmulti-user settings in the context of huge degrees of freedom promised by\\nmassive Multiple-Input Multiple-Output (MIMO). However, the requirement of\\nmassive MIMO for cost-efficient implementation makes them more prone to\\nhardware imperfections such as phase noise (PN). As a result, we focus on a\\nrealistic broadcast channel with a large number of antennas and hampered by the\\nunavoidable PN. Moreover, we employ the RS transmission strategy, and we show\\nits robustness against PN, since the sum-rate does not saturate at high\\nsignal-to-noise ratio (SNR). Although, the analytical results are obtained by\\nmeans of the deterministic equivalent analysis, they coincide with simulation\\nresults even for finite system dimensions.\\n',\n",
       " '  We prove a general width duality theorem for combinatorial structures with\\nwell-defined notions of cohesion and separation. These might be graphs and\\nmatroids, but can be much more general or quite different. The theorem asserts\\na duality between the existence of high cohesiveness somewhere local and a\\nglobal overall tree structure.\\nWe describe cohesive substructures in a unified way in the format of tangles:\\nas orientations of low-order separations satisfying certain consistency axioms.\\nThese axioms can be expressed without reference to the underlying structure,\\nsuch as a graph or matroid, but just in terms of the poset of the separations\\nthemselves. This makes it possible to identify tangles, and apply our\\ntangle-tree duality theorem, in very diverse settings.\\nOur result implies all the classical duality theorems for width parameters in\\ngraph minor theory, such as path-width, tree-width, branch-width or rank-width.\\nIt yields new, tangle-type, duality theorems for tree-width and path-width. It\\nimplies the existence of width parameters dual to cohesive substructures such\\nas $k$-blocks, edge-tangles, or given subsets of tangles, for which no width\\nduality theorems were previously known.\\nAbstract separation systems can be found also in structures quite unlike\\ngraphs and matroids. For example, our theorem can be applied to image analysis\\nby capturing the regions of an image as tangles of separations defined as\\nnatural partitions of its set of pixels. It can be applied in big data contexts\\nby capturing clusters as tangles. It can be applied in the social sciences,\\ne.g. by capturing as tangles the few typical mindsets of individuals found by a\\nsurvey. It could also be applied in pure mathematics, e.g. to separations of\\ncompact manifolds.\\n',\n",
       " '  Network pruning is aimed at imposing sparsity in a neural network\\narchitecture by increasing the portion of zero-valued weights for reducing its\\nsize regarding energy-efficiency consideration and increasing evaluation speed.\\nIn most of the conducted research efforts, the sparsity is enforced for network\\npruning without any attention to the internal network characteristics such as\\nunbalanced outputs of the neurons or more specifically the distribution of the\\nweights and outputs of the neurons. That may cause severe accuracy drop due to\\nuncontrolled sparsity. In this work, we propose an attention mechanism that\\nsimultaneously controls the sparsity intensity and supervised network pruning\\nby keeping important information bottlenecks of the network to be active. On\\nCIFAR-10, the proposed method outperforms the best baseline method by 6% and\\nreduced the accuracy drop by 2.6x at the same level of sparsity.\\n',\n",
       " '  Graph models are widely used to analyse diffusion processes embedded in\\nsocial contacts and to develop applications. A range of graph models are\\navailable to replicate the underlying social structures and dynamics\\nrealistically. However, most of the current graph models can only consider\\nconcurrent interactions among individuals in the co-located interaction\\nnetworks. However, they do not account for indirect interactions that can\\ntransmit spreading items to individuals who visit the same locations at\\ndifferent times but within a certain time limit. The diffusion phenomena\\noccurring through direct and indirect interactions is called same place\\ndifferent time (SPDT) diffusion. This paper introduces a model to synthesize\\nco-located interaction graphs capturing both direct interactions, where\\nindividuals meet at a location, and indirect interactions, where individuals\\nvisit the same location at different times within a set timeframe. We analyze\\n60 million location updates made by 2 million users from a social networking\\napplication to characterize the graph properties, including the space-time\\ncorrelations and its time evolving characteristics, such as bursty or ongoing\\nbehaviors. The generated synthetic graph reproduces diffusion dynamics of a\\nrealistic contact graph, and reduces the prediction error by up to 82% when\\ncompare to other contact graph models demonstrating its potential for\\nforecasting epidemic spread.\\n',\n",
       " '  Training neural networks involves finding minima of a high-dimensional\\nnon-convex loss function. Knowledge of the structure of this energy landscape\\nis sparse. Relaxing from linear interpolations, we construct continuous paths\\nbetween minima of recent neural network architectures on CIFAR10 and CIFAR100.\\nSurprisingly, the paths are essentially flat in both the training and test\\nlandscapes. This implies that neural networks have enough capacity for\\nstructural changes, or that these changes are small between minima. Also, each\\nminimum has at least one vanishing Hessian eigenvalue in addition to those\\nresulting from trivial invariance.\\n',\n",
       " '  We study the homogenization process for families of strongly nonlinear\\nelliptic systems with the homogeneous Dirichlet boundary conditions. The growth\\nand the coercivity of the elliptic operator is assumed to be indicated by a\\ngeneral inhomogeneous anisotropic $N-$function, which may be possibly also\\ndependent on the spatial variable, i.e., the homogenization process will change\\nthe characteristic function spaces at each step. Such a problem is well known\\nand there exists many positive results for the function satisfying $\\\\Delta_2$\\nand $\\\\nabla_2$ conditions an being in addition Hölder continuous with\\nrespect to the spatial variable. We shall show that cases these conditions can\\nbe neglected and will deal with a rather general problem in general function\\nspace setting.\\n',\n",
       " \"  A polynomial $p\\\\in\\\\mathbb{R}[z_1,\\\\dots,z_n]$ is real stable if it has no\\nroots in the upper-half complex plane. Gurvits's permanent inequality gives a\\nlower bound on the coefficient of the $z_1z_2\\\\dots z_n$ monomial of a real\\nstable polynomial $p$ with nonnegative coefficients. This fundamental\\ninequality has been used to attack several counting and optimization problems.\\nHere, we study a more general question: Given a stable multilinear polynomial\\n$p$ with nonnegative coefficients and a set of monomials $S$, we show that if\\nthe polynomial obtained by summing up all monomials in $S$ is real stable, then\\nwe can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.\\nWe also prove generalizations of this theorem to (real stable) polynomials that\\nare not multilinear. We use our theorem to give a new proof of Schrijver's\\ninequality on the number of perfect matchings of a regular bipartite graph,\\ngeneralize a recent result of Nikolov and Singh, and give deterministic\\npolynomial time approximation algorithms for several counting problems.\\n\",\n",
       " '  This volume contains the proceedings of the Fifth International Workshop on\\nVerification and Program Transformation (VPT 2017). The workshop took place in\\nUppsala, Sweden, on April 29th, 2017, affiliated with the European Joint\\nConferences on Theory and Practice of Software (ETAPS). The aim of the VPT\\nworkshop series is to provide a forum where people from the areas of program\\ntransformation and program verification can fruitfully exchange ideas and gain\\na deeper understanding of the interactions between those two fields. Seven\\npapers were presented at the workshop. Additionally, three invited talks were\\ngiven by Javier Esparza (Technische Universität München, Germany), Manuel\\nHermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov\\n(Linux Verification Center, ISPRAS, Moscow, Russia).\\n',\n",
       " '  This chapter presents an H-infinity filtering framework for cloud-aided\\nsemiactive suspension system with time-varying delays. In this system, road\\nprofile information is downloaded from a cloud database to facilitate onboard\\nestimation of suspension states. Time-varying data transmission delays are\\nconsidered and assumed to be bounded. A quarter-car linear suspension model is\\nused and an H-infinity filter is designed with both onboard sensor measurements\\nand delayed road profile information from the cloud. The filter design\\nprocedure is designed based on linear matrix inequalities (LMIs). Numerical\\nsimulation results are reported that illustrates the fusion of cloud-based and\\non-board information that can be achieved in Vehicleto- Cloud-to-Vehicle\\n(V2C2V) implementation.\\n',\n",
       " '  In this work we introduce a time- and memory-efficient method for structured\\nprediction that couples neuron decisions across both space at time. We show\\nthat we are able to perform exact and efficient inference on a densely\\nconnected spatio-temporal graph by capitalizing on recent advances on deep\\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\\nalongside contemporary deep networks for video understanding. We experiment\\nwith multiple connectivity patterns in the temporal domain, and present\\nempirical improvements over strong baselines on the tasks of both semantic and\\ninstance segmentation of videos.\\n',\n",
       " \"  This paper introduces a new approach to Large-Eddy Simulation (LES) where\\nsubgrid-scale (SGS) dissipation is applied proportionally to the degree of\\nlocal spectral broadening, hence mitigated or deactivated in regions dominated\\nby large-scale and/or laminar vortical motion. The proposed Coherent vorticity\\npreserving (CvP) LES methodology is based on the evaluation of the ratio of the\\ntest-filtered to resolved (or grid-filtered) enstrophy $\\\\sigma$. Values of\\n$\\\\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying\\nlocal deactivation of the SGS dissipation. The intensity of the SGS dissipation\\nis progressively increased for $\\\\sigma < 1$ which corresponds to a small-scale\\nspectral broadening. The SGS dissipation is then fully activated in developed\\nturbulence characterized by $\\\\sigma \\\\le \\\\sigma_{eq}$, where the value\\n$\\\\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach\\ncan be applied to any eddy-viscosity model, is algorithmically simple and\\ncomputationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates\\nthat the CvP methodology improves the performance of traditional, non-dynamic\\ndissipative SGS models, capturing the peak of total turbulent kinetic energy\\ndissipation during transition. Similar accuracy is obtained by adopting\\nGermano's dynamic procedure albeit at more than twice the computational\\noverhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to\\npredict accurately the experimentally observed growth rate using coarse\\nresolutions. The ability of the CvP methodology to dynamically sort the\\ncoherent, large-scale motion from the smaller, broadband scales during\\ntransition is demonstrated via flow visualizations. LES of compressible channel\\nare carried out and show a good match with a reference DNS.\\n\",\n",
       " '  We consider the problem of inference in a causal generative model where the\\nset of available observations differs between data instances. We show how\\ncombining samples drawn from the graphical model with an appropriate masking\\nfunction makes it possible to train a single neural network to approximate all\\nthe corresponding conditional marginal distributions and thus amortize the cost\\nof inference. We further demonstrate that the efficiency of importance sampling\\nmay be improved by basing proposals on the output of the neural network. We\\nalso outline how the same network can be used to generate samples from an\\napproximate joint posterior via a chain decomposition of the graph.\\n',\n",
       " '  The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard\\nproblem with numerous applications arising in artificial intelligence. As an\\nefficient tool for heuristic design, the backbone has been applied to\\nheuristics design for many NP-hard problems. In this paper, we investigated the\\ncomputational complexity for retrieving the backbone in weighted MAX-SAT and\\ndeveloped a new algorithm for solving this problem. We showed that it is\\nintractable to retrieve the full backbone under the assumption that . Moreover,\\nit is intractable to retrieve a fixed fraction of the backbone as well. And\\nthen we presented a backbone guided local search (BGLS) with Walksat operator\\nfor weighted MAX-SAT. BGLS consists of two phases: the first phase samples the\\nbackbone information from local optima and the backbone phase conducts local\\nsearch under the guideline of backbone. Extensive experimental results on the\\nbenchmark showed that BGLS outperforms the existing heuristics in both solution\\nquality and runtime.\\n',\n",
       " '  We show that in the presence of magnetic field, two superconducting phases\\nwith the center-of-mass momentum of Cooper pair parallel to the magnetic field\\nare induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,\\nat small magnetic field, the center-of-mass momentum is induced due to the\\nenergy-spectrum distortion and no unpairing region with vanishing singlet\\ncorrelation appears. We refer to this superconducting state as the drift-BCS\\nstate. By further increasing the magnetic field, the superconducting state\\nfalls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the\\nunpairing regions. The observed abrupt enhancement of the center-of-mass\\nmomenta and suppression on the order parameters during the crossover indicate\\nthe first-order phase transition. Enhanced Pauli limit and hence enlarged\\nmagnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the\\nspin-flip terms of the spin-orbit coupling, are revealed. We also address the\\ntriplet correlations induced by the spin-orbit coupling, and show that the\\nCooper-pair spin polarizations, generated by the magnetic field and\\ncenter-of-mass momentum with the triplet correlations, exhibit totally\\ndifferent magnetic-field dependences between the drift-BCS and\\nFulde-Ferrell-Larkin-Ovchinnikov states.\\n',\n",
       " \"  We have recently established some integral inequalities for convex functions\\nvia the Hermite-Hadamard's inequalities. In continuation here, we also\\nestablish some interesting new integral inequalities for convex functions via\\nthe Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful\\napplications involving special means are also included.\\n\",\n",
       " '  The support vector machine (SVM) is a powerful and widely used classification\\nalgorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide\\nrigorous mathematical proof for new insights into the behavior of SVM. These\\ninsights provide perhaps unexpected relationships between SVM and two other\\nlinear classifiers: the mean difference and the maximal data piling direction.\\nFor example, we show that in many cases SVM can be viewed as a cropped version\\nof these classifiers. By carefully exploring these connections we show how SVM\\ntuning behavior is affected by characteristics including: balanced vs.\\nunbalanced classes, low vs. high dimension, separable vs. non-separable data.\\nThese results provide further insights into tuning SVM via cross-validation by\\nexplaining observed pathological behavior and motivating improved\\ncross-validation methodology. Finally, we also provide new results on the\\ngeometry of complete data piling directions in high dimensional space.\\n',\n",
       " '  Data analytics and data science play a significant role in nowadays society.\\nIn the context of Smart Grids (SG), the collection of vast amounts of data has\\nseen the emergence of a plethora of data analysis approaches. In this paper, we\\nconduct a Systematic Mapping Study (SMS) aimed at getting insights about\\ndifferent facets of SG data analysis: application sub-domains (e.g., power load\\ncontrol), aspects covered (e.g., forecasting), used techniques (e.g.,\\nclustering), tool-support, research methods (e.g., experiments/simulations),\\nreplicability/reproducibility of research. The final goal is to provide a view\\nof the current status of research. Overall, we found that each sub-domain has\\nits peculiarities in terms of techniques, approaches and research methodologies\\napplied. Simulations and experiments play a crucial role in many areas. The\\nreplicability of studies is limited concerning the provided implemented\\nalgorithms, and to a lower extent due to the usage of private datasets.\\n',\n",
       " \"  In this paper, we present a new task that investigates how people interact\\nwith and make judgments about towers of blocks. In Experiment~1, participants\\nin the lab solved a series of problems in which they had to re-configure three\\nblocks from an initial to a final configuration. We recorded whether they used\\none hand or two hands to do so. In Experiment~2, we asked participants online\\nto judge whether they think the person in the lab used one or two hands. The\\nresults revealed a close correspondence between participants' actions in the\\nlab, and the mental simulations of participants online. To explain\\nparticipants' actions and mental simulations, we develop a model that plans\\nover a symbolic representation of the situation, executes the plan using a\\ngeometric solver, and checks the plan's feasibility by taking into account the\\nphysical constraints of the scene. Our model explains participants' actions and\\njudgments to a high degree of quantitative accuracy.\\n\",\n",
       " '  Recent progress in applying complex network theory to problems faced in\\nquantum information and computation has resulted in a beneficial crossover\\nbetween two fields. Complex network methods have successfully been used to\\ncharacterize quantum walk and transport models, entangled communication\\nnetworks, graph-theoretic models of emergent space-time and in detecting\\nmesoscale structure in quantum systems. Information physics is setting the\\nstage for a theory of complex and networked systems with quantum\\ninformation-inspired methods appearing in complex network science, including\\ninformation-theoretic distance and correlation measures for network\\ncharacterization. Novel quantum induced effects have been predicted in random\\ngraphs---where edges represent entangled links---and quantum computer\\nalgorithms have recently been proposed to offer super-polynomial enhancement\\nfor several network and graph theoretic problems. Here we review the results at\\nthe cutting edge, pinpointing the similarities and reconciling the differences\\nfound in the series of results at the intersection of these two fields.\\n',\n",
       " '  Under suitable conditions, a substitution tiling gives rise to a Smale space,\\nfrom which three equivalence relations can be constructed, namely the stable,\\nunstable, and asymptotic equivalence relations. We denote with $S$, $U$, and\\n$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article\\nwe show that the $K$-theories of $S$ and $U$ can be computed from the\\ncohomology and homology of a single cochain complex with connecting maps for\\ntilings of the line and of the plane. Moreover, we provide formulas to compute\\nthe $K$-theory for these three $C^*$-algebras. Furthermore, we show that the\\n$K$-theory groups for tilings of dimension 1 are always torsion free. For\\ntilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.\\n',\n",
       " \"  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain\\nrandom planar maps converge to $\\\\gamma$-Liouville quantum gravity\\n($\\\\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized\\nmatings of correlated continuum random trees, and $\\\\gamma$ ranges from $0$ to\\n$2$ as one varies the correlation parameter. We also show that the associated\\nspace-filling path on the embedded map converges to space-filling\\nSLE$_{\\\\kappa}$ for $\\\\kappa =16/\\\\gamma^2$ (in the annealed sense) and that\\nsimple random walk on the embedded map converges to Brownian motion (in the\\nquenched sense). Our arguments also yield analogous statements for the Smith\\n(square tiling) embedding of the mated-CRT map.\\nThis work constitutes the first proof that a discrete conformal embedding of\\na random planar map converges to LQG. Many more such statements have been\\nconjectured. Since the mated-CRT map can be viewed as a coarse-grained\\napproximation to other random planar maps (the UIPT, tree-weighted maps,\\nbipolar-oriented maps, etc.), our results indicate a potential approach for\\nproving that embeddings of these maps converge to LQG as well.\\nTo prove the main result, we establish several (independently interesting)\\ntheorems about LQG surfaces decorated by space-filling SLE. There is a natural\\nway to use the SLE curve to divide the plane into `cells' corresponding to\\nvertices of the mated-CRT map. We study the law of the shape of the\\norigin-containing cell, in particular proving moments for the ratio of its\\nsquared diameter to its area. We also give bounds on the degree of the\\norigin-containing cell and establish a form of ergodicity for the entire\\nconfiguration. Ultimately, we use these properties to show (using a general\\ntheorem proved in a separate paper) that random walk on these cells converges\\nto a time change of Brownian motion, which in turn leads to the Tutte embedding\\nresult.\\n\",\n",
       " '  Muroga [M52] showed how to express the Shannon channel capacity of a discrete\\nchannel with noise [S49] as an explicit function of the transition\\nprobabilities. His method accommodates channels with any finite number of input\\nsymbols, any finite number of output symbols and any transition probability\\nmatrix. Silverman [S55] carried out Muroga\\'s method in the special case of a\\nbinary channel (and went on to analyse \"cascades\" of several such binary\\nchannels).\\nThis article is a note on the resulting formula for the capacity C(a, c) of a\\nsingle binary channel. We aim to clarify some of the arguments and correct a\\nsmall error. In service of this aim, we first formulate several of Shannon\\'s\\ndefinitions and proofs in terms of discrete measure-theoretic probability\\ntheory. We provide an alternate proof to Silverman\\'s, of the feasibility of the\\noptimal input distribution for a binary channel. For convenience, we also\\nexpress C(a, c) in a single expression explicitly dependent on a and c only,\\nwhich Silverman stopped short of doing.\\n',\n",
       " '  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,\\nwhich is finitely generated on both sides. We study Gorenstein homological\\nproperties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$\\nis Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein\\nprojective $T_R(M)$-modules in terms of $R$-modules.\\n',\n",
       " '  Regression based methods are not performing as well as detection based\\nmethods for human pose estimation. A central problem is that the structural\\ninformation in the pose is not well exploited in the previous regression\\nmethods. In this work, we propose a structure-aware regression approach. It\\nadopts a reparameterized pose representation using bones instead of joints. It\\nexploits the joint connection structure to define a compositional loss function\\nthat encodes the long range interactions in the pose. It is simple, effective,\\nand general for both 2D and 3D pose estimation in a unified setting.\\nComprehensive evaluation validates the effectiveness of our approach. It\\nsignificantly advances the state-of-the-art on Human3.6M and is competitive\\nwith state-of-the-art results on MPII.\\n',\n",
       " '  We consider systems with memory represented by stochastic functional\\ndifferential equations. Substantially, these are stochastic differential\\nequations with coefficients depending on the past history of the process\\nitself. Such coefficients are hence defined on a functional space. Models with\\nmemory appear in many applications ranging from biology to finance. Here we\\nconsider the results of some evaluations based on these models (e.g. the prices\\nof some financial products) and the risks connected to the choice of these\\nmodels. In particular we focus on the impact of the initial condition on the\\nevaluations. This problem is known as the analysis of sensitivity to the\\ninitial condition and, in the terminology of finance, it is referred to as the\\nDelta. In this work the initial condition is represented by the relevant past\\nhistory of the stochastic functional differential equation. This naturally\\nleads to the redesign of the definition of Delta. We suggest to define it as a\\nfunctional directional derivative, this is a natural choice. For this we study\\na representation formula which allows for its computation without requiring\\nthat the evaluation functional is differentiable. This feature is particularly\\nrelevant for applications. Our formula is achieved by studying an appropriate\\nrelationship between Malliavin derivative and functional directional\\nderivative. For this we introduce the technique of {\\\\it randomisation of the\\ninitial condition}.\\n',\n",
       " '  We discuss the concept of inner function in reproducing kernel Hilbert spaces\\nwith an orthogonal basis of monomials and examine connections between inner\\nfunctions and optimal polynomial approximants to $1/f$, where $f$ is a function\\nin the space. We revisit some classical examples from this perspective, and\\nshow how a construction of Shapiro and Shields can be modified to produce inner\\nfunctions.\\n',\n",
       " '  Data center networks are an important infrastructure in various applications\\nof modern information technologies. Note that each data center always has a\\nfinite lifetime, thus once a data center fails, then it will lose all its\\nstorage files and useful information. For this, it is necessary to replicate\\nand copy each important file into other data centers such that this file can\\nincrease its lifetime of staying in a data center network. In this paper, we\\ndescribe a large-scale data center network with a file d-threshold policy,\\nwhich is to replicate each important file into at most d-1 other data centers\\nsuch that this file can maintain in the data center network under a given level\\nof data security in the long-term. To this end, we develop three relevant\\nMarkov processes to propose two effective methods for assessing the file\\nlifetime and data security. By using the RG-factorizations, we show that the\\ntwo methods are used to be able to more effectively evaluate the file lifetime\\nof large-scale data center networks. We hope the methodology and results given\\nin this paper are applicable in the file lifetime study of more general data\\ncenter networks with replication mechanism.\\n',\n",
       " '  In the setting of high-dimensional linear regression models, we propose two\\nframeworks for constructing pointwise and group confidence sets for penalized\\nestimators which incorporate prior knowledge about the organization of the\\nnon-zero coefficients. This is done by desparsifying the estimator as in van de\\nGeer et al. [18] and van de Geer and Stucky [17], then using an appropriate\\nestimator for the precision matrix $\\\\Theta$. In order to estimate the precision\\nmatrix a corresponding structured matrix norm penalty has to be introduced.\\nAfter normalization the result is an asymptotic pivot.\\nThe asymptotic behavior is studied and simulations are added to study the\\ndifferences between the two schemes.\\n',\n",
       " '  Finding patterns in data and being able to retrieve information from those\\npatterns is an important task in Information retrieval. Complex search\\nrequirements which are not fulfilled by simple string matching and require\\nexploring certain patterns in data demand a better query engine that can\\nsupport searching via structured queries. In this article, we built a\\nstructured query engine which supports searching data through structured\\nqueries on the lines of ElasticSearch. We will show how we achieved real time\\nindexing and retrieving of data through a RESTful API and how complex queries\\ncan be created and processed using efficient data structures we created for\\nstoring the data in structured way. Finally, we will conclude with an example\\nof movie recommendation system built on top of this query engine.\\n',\n",
       " '  As a natural extension of compressive sensing and the requirement of some\\npractical problems, Phaseless Compressed Sensing (PCS) has been introduced and\\nstudied recently. Many theoretical results have been obtained for PCS with the\\naid of its convex relaxation. Motivated by successful applications of nonconvex\\nrelaxed methods for solving compressive sensing, in this paper, we try to\\ninvestigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the\\nreal context by the corresponding $\\\\ell_p$-minimization with $p\\\\in (0,1)$. We\\nshow that there exists a constant $p^\\\\ast\\\\in (0,1]$ such that for any fixed\\n$p\\\\in(0, p^\\\\ast)$, every optimal solution to the $\\\\ell_p$-minimization also\\nsolves the concerned problem; and derive an expression of such a constant\\n$p^\\\\ast$ by making use of the known data and the sparsity level of the\\nconcerned problem. These provide a theoretical basis for solving this class of\\nproblems via the corresponding $\\\\ell_p$-minimization.\\n',\n",
       " '  We present an exhaustive census of Lyman alpha (Ly$\\\\alpha$) emission in the\\ngeneral galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber\\nSystem (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of\\n625 galaxies homogeneously distributed in the range\\n$7.6<\\\\log{\\\\mbox{M$_*$/M$_{\\\\odot}$}}<10.6$. Our sample is selected from the\\n3D-HST/CANDELS survey, which provides the complementary data to estimate\\nLy$\\\\alpha$ equivalent widths ($W_{Ly\\\\alpha}$) and escape fractions ($f_{esc}$)\\nfor our galaxies. We find both quantities to anti-correlate with M$_*$,\\nstar-formation rate (SFR), UV luminosity, and UV slope ($\\\\beta$). We then model\\nthe $W_{Ly\\\\alpha}$ distribution as a function of M$_{UV}$ and $\\\\beta$ using a\\nBayesian approach. Based on our model and matching the properties of typical\\nLyman break galaxy (LBG) selections, we conclude that the $W_{Ly\\\\alpha}$\\ndistribution in such samples is heavily dependent on the limiting M$_{UV}$ of\\nthe survey. Regarding narrowband surveys, we find their $W_{Ly\\\\alpha}$\\nselections to bias samples toward low M$_*$, while their line-flux limitations\\npreferentially leave out low-SFR galaxies. We can also use our model to predict\\nthe fraction of Ly$\\\\alpha$-emitting LBGs at $4\\\\leqslant z\\\\leqslant 7$. We show\\nthat reported drops in the Ly$\\\\alpha$ fraction at $z\\\\geqslant6$, usually\\nattributed to the rapidly increasing neutral gas fraction of the universe, can\\nalso be explained by survey M$_{UV}$ incompleteness. This result does not\\ndismiss reionization occurring at $z\\\\sim7$, but highlights that current data is\\nnot inconsistent with this process taking place at $z>7$.\\n',\n",
       " '  Building on insights of Jovanovic (1982) and subsequent authors, we develop a\\ncomprehensive theory of optimal timing of decisions based around continuation\\nvalue functions and operators that act on them. Optimality results are provided\\nunder general settings, with bounded or unbounded reward functions. This\\napproach has several intrinsic advantages that we exploit in developing the\\ntheory. One is that continuation value functions are smoother than value\\nfunctions, allowing for sharper analysis of optimal policies and more efficient\\ncomputation. Another is that, for a range of problems, the continuation value\\nfunction exists in a lower dimensional space than the value function,\\nmitigating the curse of dimensionality. In one typical experiment, this reduces\\nthe computation time from over a week to less than three minutes.\\n',\n",
       " '  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This\\narticle describes how pristine was defined based on expectations of Bennu and\\non a realistic understanding of what is achievable with a constrained schedule\\nand budget, and how that definition flowed to requirements and implementation.\\nTo return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was\\nmaintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the\\nsampler head through precision cleaning, control of materials, and vigilance.\\nContamination is further characterized via witness material exposed to the\\nspacecraft assembly and testing environment as well as in space. This\\ncharacterization provided knowledge of the expected background and will be used\\nin conjunction with archived spacecraft components for comparison with the\\nsamples when they are delivered to Earth for analysis. Most of all, the\\ncleanliness of the OSIRIS-REx spacecraft was achieved through communication\\namong scientists, engineers, managers, and technicians.\\n',\n",
       " \"  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)\\nwith a constant step-size and the so called Polyak-Ruppert (PR) averaging of\\niterates. LSAs are widely applied in machine learning and reinforcement\\nlearning (RL), where the aim is to compute an appropriate $\\\\theta_{*} \\\\in\\n\\\\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$\\nupdates per iteration. In this paper, we are motivated by the problem (in RL)\\nof policy evaluation from experience replay using the \\\\emph{temporal\\ndifference} (TD) class of learning algorithms that are also LSAs. For LSAs with\\na constant step-size, and PR averaging, we provide bounds for the mean squared\\nerror (MSE) after $t$ iterations. We assume that data is \\\\iid with finite\\nvariance (underlying distribution being $P$) and that the expected dynamics is\\nHurwitz. For a given LSA with PR averaging, and data distribution $P$\\nsatisfying the said assumptions, we show that there exists a range of constant\\nstep-sizes such that its MSE decays as $O(\\\\frac{1}{t})$.\\nWe examine the conditions under which a constant step-size can be chosen\\nuniformly for a class of data distributions $\\\\mathcal{P}$, and show that not\\nall data distributions `admit' such a uniform constant step-size. We also\\nsuggest a heuristic step-size tuning algorithm to choose a constant step-size\\nof a given LSA for a given data distribution $P$. We compare our results with\\nrelated work and also discuss the implication of our results in the context of\\nTD algorithms that are LSAs.\\n\",\n",
       " '  We study the fundamental tradeoffs between statistical accuracy and\\ncomputational tractability in the analysis of high dimensional heterogeneous\\ndata. As examples, we study sparse Gaussian mixture model, mixture of sparse\\nlinear regressions, and sparse phase retrieval model. For these models, we\\nexploit an oracle-based computational model to establish conjecture-free\\ncomputationally feasible minimax lower bounds, which quantify the minimum\\nsignal strength required for the existence of any algorithm that is both\\ncomputationally tractable and statistically accurate. Our analysis shows that\\nthere exist significant gaps between computationally feasible minimax risks and\\nclassical ones. These gaps quantify the statistical price we must pay to\\nachieve computational tractability in the presence of data heterogeneity. Our\\nresults cover the problems of detection, estimation, support recovery, and\\nclustering, and moreover, resolve several conjectures of Azizyan et al. (2013,\\n2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our\\nresults reveal a new but counter-intuitive phenomenon in heterogeneous data\\nanalysis that more data might lead to less computation complexity.\\n',\n",
       " '  Audio-visual speech recognition (AVSR) system is thought to be one of the\\nmost promising solutions for robust speech recognition, especially in noisy\\nenvironment. In this paper, we propose a novel multimodal attention based\\nmethod for audio-visual speech recognition which could automatically learn the\\nfused representation from both modalities based on their importance. Our method\\nis realized using state-of-the-art sequence-to-sequence (Seq2seq)\\narchitectures. Experimental results show that relative improvements from 2% up\\nto 36% over the auditory modality alone are obtained depending on the different\\nsignal-to-noise-ratio (SNR). Compared to the traditional feature concatenation\\nmethods, our proposed approach can achieve better recognition performance under\\nboth clean and noisy conditions. We believe modality attention based end-to-end\\nmethod can be easily generalized to other multimodal tasks with correlated\\ninformation.\\n',\n",
       " '  The goal of this survey article is to explain and elucidate the affine\\nstructure of recent models appearing in the rough volatility literature, and\\nshow how it leads to exponential-affine transform formulas.\\n',\n",
       " '  White dwarf stars have been used as flux standards for decades, thanks to\\ntheir staid simplicity. We have empirically tested their photometric stability\\nby analyzing the light curves of 398 high-probability candidates and\\nspectroscopically confirmed white dwarfs observed during the original Kepler\\nmission and later with K2 Campaigns 0-8. We find that the vast majority (>97\\nper cent) of non-pulsating and apparently isolated white dwarfs are stable to\\nbetter than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,\\nconfirming that these stellar remnants are useful flux standards. From the\\ncases that do exhibit significant variability, we caution that binarity,\\nmagnetism, and pulsations are three important attributes to rule out when\\nestablishing white dwarfs as flux standards, especially those hotter than\\n30,000 K.\\n',\n",
       " '  Most existing approaches address multi-view subspace clustering problem by\\nconstructing the affinity matrix on each view separately and afterwards propose\\nhow to extend spectral clustering algorithm to handle multi-view data. This\\npaper presents an approach to multi-view subspace clustering that learns a\\njoint subspace representation by constructing affinity matrix shared among all\\nviews. Relying on the importance of both low-rank and sparsity constraints in\\nthe construction of the affinity matrix, we introduce the objective that\\nbalances between the agreement across different views, while at the same time\\nencourages sparsity and low-rankness of the solution. Related low-rank and\\nsparsity constrained optimization problem is for each view solved using the\\nalternating direction method of multipliers. Furthermore, we extend our\\napproach to cluster data drawn from nonlinear subspaces by solving the\\ncorresponding problem in a reproducing kernel Hilbert space. The proposed\\nalgorithm outperforms state-of-the-art multi-view subspace clustering\\nalgorithms on one synthetic and four real-world datasets.\\n',\n",
       " \"  The P300 event-related potential (ERP), evoked in scalp-recorded\\nelectroencephalography (EEG) by external stimuli, has proven to be a reliable\\nresponse for controlling a BCI. The P300 component of an event related\\npotential is thus widely used in brain-computer interfaces to translate the\\nsubjects' intent by mere thoughts into commands to control artificial devices.\\nThe main challenge in the classification of P300 trials in\\nelectroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of\\nthe P300 response. To overcome the low SNR of individual trials, it is common\\npractice to average together many consecutive trials, which effectively\\ndiminishes the random noise. Unfortunately, when more repeated trials are\\nrequired for applications such as the P300 speller, the communication rate is\\ngreatly reduced. This has resulted in a need for better methods to improve\\nsingle-trial classification accuracy of P300 response. In this work, we use\\nPrincipal Component Analysis (PCA) as a preprocessing method and use Linear\\nDiscriminant Analysis (LDA)and neural networks for classification. The results\\nshow that a combination of PCA with these methods provided as high as 13\\\\%\\naccuracy gain for single-trial classification while using only 3 to 4 principal\\ncomponents.\\n\",\n",
       " '  The purpose of this paper is to study stable representations of partially\\nordered sets (posets) and compare it to the well known theory for quivers. In\\nparticular, we prove that every indecomposable representation of a poset of\\nfinite type is stable with respect to some weight and construct that weight\\nexplicitly in terms of the dimension vector. We show that if a poset is\\nprimitive then Coxeter transformations preserve stable representations. When\\nthe base field is the field of complex numbers we establish the connection\\nbetween the polystable representations and the unitary $\\\\chi$-representations\\nof posets. This connection explains the similarity of the results obtained in\\nthe series of papers.\\n',\n",
       " \"  Segmentation in dynamic outdoor environments can be difficult when the\\nillumination levels and other aspects of the scene cannot be controlled.\\nSpecifically in orchard and vineyard automation contexts, a background material\\nis often used to shield a camera's field of view from other rows of crops. In\\nthis paper, we describe a method that uses superpixels to determine low texture\\nregions of the image that correspond to the background material, and then show\\nhow this information can be integrated with the color distribution of the image\\nto compute optimal segmentation parameters to segment objects of interest.\\nQuantitative and qualitative experiments demonstrate the suitability of this\\napproach for dynamic outdoor environments, specifically for tree reconstruction\\nand apple flower detection applications.\\n\",\n",
       " '  Stochastic bandit algorithms can be used for challenging non-convex\\noptimization problems. Hyperparameter tuning of neural networks is particularly\\nchallenging, necessitating new approaches. To this end, we present a method\\nthat adaptively partitions the combined space of hyperparameters, context, and\\ntraining resources (e.g., total number of training iterations). By adaptively\\npartitioning the space, the algorithm is able to focus on the portions of the\\nhyperparameter search space that are most relevant in a practical way. By\\nincluding the resources in the combined space, the method tends to use fewer\\ntraining resources overall. Our experiments show that this method can surpass\\nstate-of-the-art methods in tuning neural networks on benchmark datasets. In\\nsome cases, our implementations can achieve the same levels of accuracy on\\nbenchmark datasets as existing state-of-the-art approaches while saving over\\n50% of our computational resources (e.g. time, training iterations).\\n',\n",
       " '  Dynamic security analysis is an important problem of power systems on\\nensuring safe operation and stable power supply even when certain faults occur.\\nNo matter such faults are caused by vulnerabilities of system components,\\nphysical attacks, or cyber-attacks that are more related to cyber-security,\\nthey eventually affect the physical stability of a power system. Examples of\\nthe loss of physical stability include the Northeast blackout of 2003 in North\\nAmerica and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid\\nnature, that is, nonlinear continuous dynamics integrated with discrete\\nswitching, and the high degree of freedom property of power system dynamics\\nmake it challenging to conduct the dynamic security analysis. In this paper, we\\nuse the hybrid automaton model to describe the dynamics of a power system and\\nmainly deal with the index-1 differential-algebraic equation models regarding\\nthe continuous dynamics in different discrete states. The analysis problem is\\nformulated as a reachability problem of the associated hybrid model. A\\nsampling-based algorithm is then proposed by integrating modeling and\\nrandomized simulation of the hybrid dynamics to search for a feasible execution\\nconnecting an initial state of the post-fault system and a target set in the\\ndesired operation mode. The proposed method enables the use of existing power\\nsystem simulators for the synthesis of discrete switching and control\\nstrategies through randomized simulation. The effectiveness and performance of\\nthe proposed approach are demonstrated with an application to the dynamic\\nsecurity analysis of the New England 39-bus benchmark power system exhibiting\\nhybrid dynamics. In addition to evaluating the dynamic security, the proposed\\nmethod searches for a feasible strategy to ensure the dynamic security of the\\nsystem in face of disruptions.\\n',\n",
       " '  Second order conic programming (SOCP) has been used to model various\\napplications in power systems, such as operation and expansion planning. In\\nthis paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model\\nfor the distribution system expansion planning problem that considers\\nuncertainty and also captures the nonlinear AC power flow. To avoid costly\\ninvestment plans due to some extreme scenarios, we further present a\\nchance-constrained variant that could lead to cost-effective solutions. To\\naddress the computational challenge, we extend the basic Benders decomposition\\nmethod and develop a bilinear variant to compute stochastic and\\nchance-constrained MISOCP formulations. A set of numerical experiments is\\nperformed to illustrate the performance of our models and computational\\nmethods. In particular, results show that our Benders decomposition algorithms\\ndrastically outperform a professional MISOCP solver in handling stochastic\\nscenarios by orders of magnitude.\\n',\n",
       " '  We present the multi-hop extensions of the recently proposed energy-efficient\\ntime synchronization scheme for wireless sensor networks, which is based on the\\nasynchronous source clock frequency recovery and reversed two-way message\\nexchanges. We consider two hierarchical extensions based on packet relaying and\\ntime-translating gateways, respectively, and analyze their performance with\\nrespect to the number of layers and the delay variations through simulations.\\nThe simulation results demonstrate that the time synchronization performance of\\nthe packet relaying, which has lower complexity, is close to that of\\ntime-translating gateways.\\n',\n",
       " '  Instructional labs are widely seen as a unique, albeit expensive, way to\\nteach scientific content. We measured the effectiveness of introductory lab\\ncourses at achieving this educational goal across nine different lab courses at\\nthree very different institutions. These institutions and courses encompassed a\\nbroad range of student populations and instructional styles. The nine courses\\nstudied had two key things in common: the labs aimed to reinforce the content\\npresented in lectures, and the labs were optional. By comparing the performance\\nof students who did and did not take the labs (with careful normalization for\\nselection effects), we found universally and precisely no added value to\\nlearning from taking the labs as measured by course exam performance. This work\\nshould motivate institutions and departments to reexamine the goals and conduct\\nof their lab courses, given their resource-intensive nature. We show why these\\nresults make sense when looking at the comparative mental processes of students\\ninvolved in research and instructional labs, and offer alternative goals and\\ninstructional approaches that would make lab courses more educationally\\nvaluable.\\n',\n",
       " '  The formation of pattern in biological systems may be modeled by a set of\\nreaction-diffusion equations. A diffusion-type coupling operator biologically\\nsignificant in neuroscience is a difference of Gaussian functions (Mexican Hat\\noperator) used as a spatial-convolution kernel. We are interested in the\\ndifference among behaviors of \\\\emph{stochastic} neural field equations, namely\\nspace-time stochastic differential-integral equations, and similar\\ndeterministic ones. We explore, quantitatively, how the parameters of our model\\nthat measure the shape of the coupling kernel, coupling strength, and aspects\\nof the spatially-smoothed space-time noise, control the pattern in the\\nresulting evolving random field. We find that a spatial pattern that is damped\\nin time in a deterministic system may be sustained and amplified by\\nstochasticity, most strikingly at an optimal spatio-temporal noise level. In\\naddition, we find that spatially-smoothed noise alone causes pattern formation\\neven without spatial coupling.\\n',\n",
       " \"  Named-entity recognition (NER) aims at identifying entities of interest in a\\ntext. Artificial neural networks (ANNs) have recently been shown to outperform\\nexisting NER systems. However, ANNs remain challenging to use for non-expert\\nusers. In this paper, we present NeuroNER, an easy-to-use named-entity\\nrecognition tool based on ANNs. Users can annotate entities using a graphical\\nweb-based user interface (BRAT): the annotations are then used to train an ANN,\\nwhich in turn predict entities' locations and categories in new texts. NeuroNER\\nmakes this annotation-training-prediction flow smooth and accessible to anyone.\\n\",\n",
       " \"  Blocking objects (blockages) between a transmitter and receiver cause\\nwireless communication links to transition from line-of-sight (LOS) to\\nnon-line-of-sight (NLOS) propagation, which can greatly reduce the received\\npower, particularly at higher frequencies such as millimeter wave (mmWave). We\\nconsider a cellular network in which a mobile user attempts to connect to two\\nor more base stations (BSs) simultaneously, to increase the probability of at\\nleast one LOS link, which is a form of macrodiversity. We develop a framework\\nfor determining the LOS probability as a function of the number of BSs, when\\ntaking into account the correlation between blockages: for example, a single\\nblockage close to the device -- including the user's own body -- could block\\nmultiple BSs. We consider the impact of the size of blocking objects on the\\nsystem reliability probability and show that macrodiversity gains are higher\\nwhen the blocking objects are small. We also show that the BS density must\\nscale as the square of the blockage density to maintain a given level of\\nreliability.\\n\",\n",
       " '  We present an introduction to a novel model of an individual and group\\nopinion dynamics, taking into account different ways in which different sources\\nof information are filtered due to cognitive biases. The agent based model,\\nusing Bayesian updating of the individual belief distribution, is based on the\\nrecent psychology work by Dan Kahan. Open nature of the model allows to study\\nthe effects of both static and time-dependent biases and information processing\\nfilters. In particular, the paper compares the effects of two important\\npsychological mechanisms: the confirmation bias and the politically motivated\\nreasoning. Depending on the effectiveness of the information filtering (agent\\nbias), the agents confronted with an objective information source may either\\nreach a consensus based on the truth, or remain divided despite the evidence.\\nIn general, the model might provide an understanding into the increasingly\\npolarized modern societies, especially as it allows mixing of different types\\nof filters: psychological, social, and algorithmic.\\n',\n",
       " '  Convolutional Neural Networks (CNNs) are commonly thought to recognise\\nobjects by learning increasingly complex representations of object shapes. Some\\nrecent studies suggest a more important role of image textures. We here put\\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\\nhuman observers on images with a texture-shape cue conflict. We show that\\nImageNet-trained CNNs are strongly biased towards recognising textures rather\\nthan shapes, which is in stark contrast to human behavioural evidence and\\nreveals fundamentally different classification strategies. We then demonstrate\\nthat the same standard architecture (ResNet-50) that learns a texture-based\\nrepresentation on ImageNet is able to learn a shape-based representation\\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\\nThis provides a much better fit for human behavioural performance in our\\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\\npsychophysical trials across 97 observers) and comes with a number of\\nunexpected emergent benefits such as improved object detection performance and\\npreviously unseen robustness towards a wide range of image distortions,\\nhighlighting advantages of a shape-based representation.\\n',\n",
       " '  In this paper, we obtain some formulae for harmonic sums, alternating\\nharmonic sums and Stirling number sums by using the method of integral\\nrepresentations of series. As applications of these formulae, we give explicit\\nformula of several quadratic and cubic Euler sums through zeta values and\\nlinear sums. Furthermore, some relationships between harmonic numbers and\\nStirling numbers of the first kind are established.\\n',\n",
       " '  In this paper we develop cyclic proof systems for the problem of inclusion\\nbetween the least sets of models of mutually recursive predicates, when the\\nground constraints in the inductive definitions belong to the quantifier-free\\nfragments of (i) First Order Logic with the canonical Herbrand interpretation\\nand (ii) Separation Logic, respectively. Inspired by classical\\nautomata-theoretic techniques of proving language inclusion between tree\\nautomata, we give a small set of inference rules, that are proved to be sound\\nand complete, under certain semantic restrictions, involving the set of\\nconstraints in the inductive system. Moreover, we investigate the decidability\\nand computational complexity of these restrictions for all the logical\\nfragments considered and provide a proof search semi-algorithm that becomes a\\ndecision procedure for the entailment problem, for those systems that fulfill\\nthe restrictions.\\n',\n",
       " '  In this paper, we introduce a new model for leveraging unlabeled data to\\nimprove generalization performances of image classifiers: a two-branch\\nencoder-decoder architecture called HybridNet. The first branch receives\\nsupervision signal and is dedicated to the extraction of invariant\\nclass-related representations. The second branch is fully unsupervised and\\ndedicated to model information discarded by the first branch to reconstruct\\ninput data. To further support the expected behavior of our model, we propose\\nan original training objective. It favors stability in the discriminative\\nbranch and complementarity between the learned representations in the two\\nbranches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,\\nSVHN and STL-10 in various semi-supervised settings. In addition,\\nvisualizations and ablation studies validate our contributions and the behavior\\nof the model on both CIFAR-10 and STL-10 datasets.\\n',\n",
       " '  We describe a neural network model that jointly learns distributed\\nrepresentations of texts and knowledge base (KB) entities. Given a text in the\\nKB, we train our proposed model to predict entities that are relevant to the\\ntext. Our model is designed to be generic with the ability to address various\\nNLP tasks with ease. We train the model using a large corpus of texts and their\\nentity annotations extracted from Wikipedia. We evaluated the model on three\\nimportant NLP tasks (i.e., sentence textual similarity, entity linking, and\\nfactoid question answering) involving both unsupervised and supervised\\nsettings. As a result, we achieved state-of-the-art results on all three of\\nthese tasks. Our code and trained models are publicly available for further\\nacademic research.\\n',\n",
       " '  For $n\\\\ge5$, it is well known that the moduli space $\\\\mathfrak{M_{0,\\\\:n}}$ of\\nunordered $n$ points on the Riemann sphere is a quotient space of the Zariski\\nopen set $K_n$ of $\\\\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this\\n$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the\\ngroups fixing the sets of $n$ points on the Riemann sphere. Let $\\\\alpha$ be a\\nsubset of $n$ distinct points on the Riemann sphere. We call the group of all\\nlinear fractional transformations leaving $\\\\alpha$ invariant the stabilizer of\\n$\\\\alpha$, which is finite by observation. For each non-trivial finite subgroup\\n$G$ of the group ${\\\\rm PSL}(2,{\\\\Bbb C})$ of linear fractional transformations,\\nwe give the necessary and sufficient condition for finite subsets of the\\nRiemann sphere under which the stabilizers of them are conjugate to $G$. We\\nalso prove that there does exist some finite subset of the Riemann sphere whose\\nstabilizer coincides with $G$. Next we obtain the irreducible decompositions of\\nthe representations of the stabilizers on the tangent spaces at the\\nsingularities of $\\\\mathfrak{M_{0,\\\\:n}}$. At last, on $\\\\mathfrak{M_{0,\\\\:5}}$ and\\n$\\\\mathfrak{M_{0,\\\\:6}}$, we work out explicitly the singularities and the\\nrepresentations of their stabilizers on the tangent spaces at them.\\n',\n",
       " \"  The radio interferometric positioning system (RIPS) is an accurate node\\nlocalization method featuring a novel phase-based ranging process. Multipath is\\nthe limiting error source for RIPS in ground-deployed scenarios or indoor\\napplications. There are four distinct channels involved in the ranging process\\nfor RIPS. Multipath reflections affect both the phase and amplitude of the\\nranging signal for each channel. By exploiting untapped amplitude information,\\nwe put forward a scheme to estimate each channel's multipath profile, which is\\nthen subsequently used to correct corresponding errors in phase measurements.\\nSimulations show that such a scheme is very effective in reducing multipath\\nphase errors, which are essentially brought down to the level of receiver noise\\nunder moderate multipath conditions. It is further demonstrated that ranging\\nerrors in RIPS are also greatly reduced via the proposed scheme.\\n\",\n",
       " '  We present an affine analog of the evaluation map for quantum groups. Namely\\nwe introduce a surjective homomorphism from the quantum toroidal gl(n) algebra\\nto the quantum affine gl(n) algebra completed with respect to the homogeneous\\ngrading. We give a brief discussion of evaluation modules.\\n',\n",
       " '  In this paper we present a framework for risk-sensitive model predictive\\ncontrol (MPC) of linear systems affected by stochastic multiplicative\\nuncertainty. Our key innovation is to consider a time-consistent, dynamic risk\\nevaluation of the cumulative cost as the objective function to be minimized.\\nThis framework is axiomatically justified in terms of time-consistency of risk\\nassessments, is amenable to dynamic optimization, and is unifying in the sense\\nthat it captures a full range of risk preferences from risk-neutral (i.e.,\\nexpectation) to worst case. Within this framework, we propose and analyze an\\nonline risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,\\nby exploiting the dual representation of time-consistent, dynamic risk\\nmeasures, we cast the computation of the MPC control law as a convex\\noptimization problem amenable to real-time implementation. Simulation results\\nare presented and discussed.\\n',\n",
       " \"  Ground-based astronomical observations may be limited by telluric water vapor\\nabsorption, which is highly variable in time and significantly complicates both\\nspectroscopy and photometry in the near-infrared (NIR). To achieve the\\nsensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous\\nmonitoring of precipitable water vapor (PWV) becomes necessary to mitigate the\\nimpact of variable telluric lines on radial velocity measurements and transit\\nlight curves. To address this issue, we present the Camera for the Automatic\\nMonitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch\\naperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple\\nObservatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to\\ntrace the amount of atmospheric water vapor affecting simultaneous observations\\nwith the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red\\ntelescopes. Here we present the current design of CAMAL, discuss our data\\nanalysis methods, and show results from 11 nights of PWV measurements taken\\nwith CAMAL. For seven nights of data, we have independent PWV measurements\\nextracted from high-resolution stellar spectra taken with the Tillinghast\\nReflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the\\nTRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between\\nCAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm\\nover a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates\\nto PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)\\nconditions. We also find that CAMAL-derived PWVs are highly correlated with\\nthose from a GPS-based water vapor monitor located approximately 90 km away at\\nKitt Peak National Observatory, with a root mean square PWV difference of 0.8\\nmm.\\n\",\n",
       " '  Gossip protocols aim at arriving, by means of point-to-point or group\\ncommunications, at a situation in which all the agents know each other secrets.\\nRecently a number of authors studied distributed epistemic gossip protocols.\\nThese protocols use as guards formulas from a simple epistemic logic, which\\nmakes their analysis and verification substantially easier.\\nWe study here common knowledge in the context of such a logic. First, we\\nanalyze when it can be reduced to iterated knowledge. Then we show that the\\nsemantics and truth for formulas without nested common knowledge operator are\\ndecidable. This implies that implementability, partial correctness and\\ntermination of distributed epistemic gossip protocols that use non-nested\\ncommon knowledge operator is decidable, as well. Given that common knowledge is\\nequivalent to an infinite conjunction of nested knowledge, these results are\\nnon-trivial generalizations of the corresponding decidability results for the\\noriginal epistemic logic, established in (Apt & Wojtczak, 2016).\\nK. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In\\nProc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.\\n',\n",
       " '  We examine discrete vortex dynamics in two-dimensional flow through a\\nnetwork-theoretic approach. The interaction of the vortices is represented with\\na graph, which allows the use of network-theoretic approaches to identify key\\nvortex-to-vortex interactions. We employ sparsification techniques on these\\ngraph representations based on spectral theory for constructing sparsified\\nmodels and evaluating the dynamics of vortices in the sparsified setup.\\nIdentification of vortex structures based on graph sparsification and sparse\\nvortex dynamics are illustrated through an example of point-vortex clusters\\ninteracting amongst themselves. We also evaluate the performance of\\nsparsification with increasing number of point vortices. The\\nsparsified-dynamics model developed with spectral graph theory requires reduced\\nnumber of vortex-to-vortex interactions but agrees well with the full nonlinear\\ndynamics. Furthermore, the sparsified model derived from the sparse graphs\\nconserves the invariants of discrete vortex dynamics. We highlight the\\nsimilarities and differences between the present sparsified-dynamics model and\\nthe reduced-order models.\\n',\n",
       " '  In this paper we study a non-linear partial differential equation (PDE),\\nproposed by N. Kudryashov [arXiv:1611.06813v1[nlin.SI]], using continuum limit\\napproximation of mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models. This\\ngeneralized semi-discrete equation can be considered as a model for the\\ndescription of non-linear dislocation waves in crystal lattice and the\\ncorresponding continuous system can be called mixed generalized potential KdV\\nand sine-Gordon equation. We obtain the Bäcklund transformation of this\\nequation in Riccati form in inverse method. We further study the\\nquasi-integrable deformation of this model.\\n',\n",
       " \"  An important, yet largely unstudied, problem in student data analysis is to\\ndetect misconceptions from students' responses to open-response questions.\\nMisconception detection enables instructors to deliver more targeted feedback\\non the misconceptions exhibited by many students in their class, thus improving\\nthe quality of instruction. In this paper, we propose a new natural language\\nprocessing-based framework to detect the common misconceptions among students'\\ntextual responses to short-answer questions. We propose a probabilistic model\\nfor students' textual responses involving misconceptions and experimentally\\nvalidate it on a real-world student-response dataset. Experimental results show\\nthat our proposed framework excels at classifying whether a response exhibits\\none or more misconceptions. More importantly, it can also automatically detect\\nthe common misconceptions exhibited across responses from multiple students to\\nmultiple questions; this property is especially important at large scale, since\\ninstructors will no longer need to manually specify all possible misconceptions\\nthat students might exhibit.\\n\",\n",
       " '  A Schottky structure on a handlebody $M$ of genus $g$ is provided by a\\nSchottky group of rank $g$. A symmetry (an orientation-reversing involution) of\\n$M$ is known to have at most $(g+1)$ connected components of fixed points. Each\\nof these components is either a point or a compact bordered surface (either\\norientable or not) whose boundary is contained in the border of $M$. In this\\npaper, we derive sharp upper bounds for the total number of connected\\ncomponents of the sets of fixed points of given two or three symmetries of $M$.\\nIn order to obtain such an upper bound, we obtain a geometrical structure\\ndescription of those extended Kleinian groups $K$ containing a Schottky group\\n$\\\\Gamma$ as finite index normal subgroup so that $K/\\\\Gamma$ is a dihedral group\\n(called dihedral Schottky groups). Our upper bounds turn out to be different to\\nthe corresponding ones at the level of closed Riemann surfaces. In contrast to\\nthe case of Riemann surfaces, we observe that $M$ cannot have two different\\nmaximal symmetries.\\n',\n",
       " '  In this paper we propose a new method of speaker diarization that employs a\\ndeep learning architecture to learn speaker embeddings. In contrast to the\\ntraditional approaches that build their speaker embeddings using manually\\nhand-crafted spectral features, we propose to train for this purpose a\\nrecurrent convolutional neural network applied directly on magnitude\\nspectrograms. To compare our approach with the state of the art, we collect and\\nrelease for the public an additional dataset of over 6 hours of fully annotated\\nbroadcast material. The results of our evaluation on the new dataset and three\\nother benchmark datasets show that our proposed method significantly\\noutperforms the competitors and reduces diarization error rate by a large\\nmargin of over 30% with respect to the baseline.\\n',\n",
       " '  This paper describes an implementation of the L-BFGS method designed to deal\\nwith two adversarial situations. The first occurs in distributed computing\\nenvironments where some of the computational nodes devoted to the evaluation of\\nthe function and gradient are unable to return results on time. A similar\\nchallenge occurs in a multi-batch approach in which the data points used to\\ncompute function and gradients are purposely changed at each iteration to\\naccelerate the learning process. Difficulties arise because L-BFGS employs\\ngradient differences to update the Hessian approximations, and when these\\ngradients are computed using different data points the updating process can be\\nunstable. This paper shows how to perform stable quasi-Newton updating in the\\nmulti-batch setting, studies the convergence properties for both convex and\\nnonconvex functions, and illustrates the behavior of the algorithm in a\\ndistributed computing platform on binary classification logistic regression and\\nneural network training problems that arise in machine learning.\\n',\n",
       " '  Networks of vertically c-oriented prism shaped InN nanowalls, are grown on\\nc-GaN/sapphire templates using a CVD technique, where pure indium and ammonia\\nare used as metal and nitrogen precursors. A systematic study of the growth,\\nstructural and electronic properties of these samples shows a preferential\\ngrowth of the islands along [11-20] and [0001] directions leading to the\\nformation of such a network structure, where the vertically [0001] oriented\\ntapered walls are laterally align along one of the three [11-20] directions.\\nInclined facets of these walls are identified as r-planes [(1-102)-planes] of\\nwurtzite InN. Onset of absorption for these samples is observed to be higher\\nthan the band gap of InN suggesting a high background carrier concentration in\\nthis material. Study of the valence band edge through XPS indicates the\\nformation of positive depletion regions below the r-plane side facets of the\\nwalls. This is in contrast with the observation for c-plane InN epilayers,\\nwhere electron accumulation is often reported below the top surface.\\n',\n",
       " '  Machine Learning focuses on the construction and study of systems that can\\nlearn from data. This is connected with the classification problem, which\\nusually is what Machine Learning algorithms are designed to solve. When a\\nmachine learning method is used by people with no special expertise in machine\\nlearning, it is important that the method be robust in classification, in the\\nsense that reasonable performance is obtained with minimal tuning of the\\nproblem at hand. Algorithms are evaluated based on how robust they can classify\\nthe given data. In this paper, we propose a quantifiable measure of robustness,\\nand describe a particular learning method that is robust according to this\\nmeasure in the context of classification problem. We proposed Adaptive Boosting\\n(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold\\n(P) and number of iterations (I) for boosting algorithm. To benchmark the\\nperformance, we used the baseline classifier, AdaBoostM1 with Decision Stump as\\nbase learner without tuning parameters. By tuning parameters and using J48 as\\nbase learner, we are able to reduce the overall average error rate ratio\\n(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2\\nfor evaluation sets of data.\\n',\n",
       " '  This paper presents a simple agent-based model of an economic system,\\npopulated by agents playing different games according to their different view\\nabout social cohesion and tax payment. After a first set of simulations,\\ncorrectly replicating results of existing literature, a wider analysis is\\npresented in order to study the effects of a dynamic-adaptation rule, in which\\ncitizens may possibly decide to modify their individual tax compliance\\naccording to individual criteria, such as, the strength of their ethical\\ncommitment, the satisfaction gained by consumption of the public good and the\\nperceived opinion of neighbors. Results show the presence of thresholds levels\\nin the composition of society - between taxpayers and evaders - which explain\\nthe extent of damages deriving from tax evasion.\\n',\n",
       " \"  The variability response function (VRF) is generalized to statically\\ndeterminate Euler Bernoulli beams with arbitrary stress-strain laws following\\nCauchy elastic behavior. The VRF is a Green's function that maps the spectral\\ndensity function (SDF) of a statistically homogeneous random field describing\\nthe correlation structure of input uncertainty to the variance of a response\\nquantity. The appeal of such Green's functions is that the variance can be\\ndetermined for any correlation structure by a trivial computation of a\\nconvolution integral. The method introduced in this work derives VRFs in closed\\nform for arbitrary nonlinear Cauchy-elastic constitutive laws and is\\ndemonstrated through three examples. It is shown why and how higher order\\nspectra of the random field affect the response variance for nonlinear\\nconstitutive laws. In the general sense, the VRF for a statically determinate\\nbeam is found to be a matrix kernel whose inner product by a matrix of higher\\norder SDFs and statistical moments is integrated to give the response variance.\\nThe resulting VRF matrix is unique regardless of the random field's marginal\\nprobability density function (PDF) and SDFs.\\n\",\n",
       " '  We show that training a deep network using batch normalization is equivalent\\nto approximate inference in Bayesian models. We further demonstrate that this\\nfinding allows us to make meaningful estimates of the model uncertainty using\\nconventional architectures, without modifications to the network or the\\ntraining procedure. Our approach is thoroughly validated by measuring the\\nquality of uncertainty in a series of empirical experiments on different tasks.\\nIt outperforms baselines with strong statistical significance, and displays\\ncompetitive performance with recent Bayesian approaches.\\n',\n",
       " '  In this paper we consider a single-cell downlink scenario where a\\nmultiple-antenna base station delivers contents to multiple cache-enabled user\\nterminals. Based on the multicasting opportunities provided by the so-called\\nCoded Caching technique, we investigate three delivery approaches. Our baseline\\nscheme employs the coded caching technique on top of max-min fair multicasting.\\nThe second one consists of a joint design of Zero-Forcing (ZF) and coded\\ncaching, where the coded chunks are formed in the signal domain (complex\\nfield). The third scheme is similar to the second one with the difference that\\nthe coded chunks are formed in the data domain (finite field). We derive\\nclosed-form rate expressions where our results suggest that the latter two\\nschemes surpass the first one in terms of Degrees of Freedom (DoF). However, at\\nthe intermediate SNR regime forming coded chunks in the signal domain results\\nin power loss, and will deteriorate throughput of the second scheme. The main\\nmessage of our paper is that the schemes performing well in terms of DoF may\\nnot be directly appropriate for intermediate SNR regimes, and modified schemes\\nshould be employed.\\n',\n",
       " '  The block bootstrap approximates sampling distributions from dependent data\\nby resampling data blocks. A fundamental problem is establishing its\\nconsistency for the distribution of a sample mean, as a prototypical statistic.\\nWe use a structural relationship with subsampling to characterize the bootstrap\\nin a new and general manner. While subsampling and block bootstrap differ, the\\nblock bootstrap distribution of a sample mean equals that of a $k$-fold\\nself-convolution of a subsampling distribution. Motivated by this, we provide\\nsimple necessary and sufficient conditions for a convolved subsampling\\nestimator to produce a normal limit that matches the target of bootstrap\\nestimation. These conditions may be linked to consistency properties of an\\noriginal subsampling distribution, which are often obtainable under minimal\\nassumptions. Through several examples, the results are shown to validate the\\nblock bootstrap for means under significantly weakened assumptions in many\\nexisting (and some new) dependence settings, which also addresses a standing\\nconjecture of Politis, Romano and Wolf(1999). Beyond sample means, the\\nconvolved subsampling estimator may not match the block bootstrap, but instead\\nprovides a hybrid-resampling estimator of interest in its own right. For\\ngeneral statistics with normal limits, results also establish the consistency\\nof convolved subsampling under minimal dependence conditions, including\\nnon-stationarity.\\n',\n",
       " '  Given a polynomial system f associated with a simple multiple zero x of\\nmultiplicity {\\\\mu}, we give a computable lower bound on the minimal distance\\nbetween the simple multiple zero x and other zeros of f. If x is only given\\nwith limited accuracy, we propose a numerical criterion that f is certified to\\nhave {\\\\mu} zeros (counting multiplicities) in a small ball around x.\\nFurthermore, for simple double zeros and simple triple zeros whose Jacobian is\\nof normalized form, we define modified Newton iterations and prove the\\nquantified quadratic convergence when the starting point is close to the exact\\nsimple multiple zero. For simple multiple zeros of arbitrary multiplicity whose\\nJacobian matrix may not have a normalized form, we perform unitary\\ntransformations and modified Newton iterations, and prove its non-quantified\\nquadratic convergence and its quantified convergence for simple triple zeros.\\n',\n",
       " '  It can be difficult to tell whether a trained generative model has learned to\\ngenerate novel examples or has simply memorized a specific set of outputs. In\\npublished work, it is common to attempt to address this visually, for example\\nby displaying a generated example and its nearest neighbor(s) in the training\\nset (in, for example, the L2 metric). As any generative model induces a\\nprobability density on its output domain, we propose studying this density\\ndirectly. We first study the geometry of the latent representation and\\ngenerator, relate this to the output density, and then develop techniques to\\ncompute and inspect the output density. As an application, we demonstrate that\\n\"memorization\" tends to a density made of delta functions concentrated on the\\nmemorized examples. We note that without first understanding the geometry, the\\nmeasurement would be essentially impossible to make.\\n',\n",
       " '  Refraction represents one of the most fundamental operations that may be\\nperformed by a metasurface. However, simple phasegradient metasurface designs\\nsuffer from restricted angular deflection due to spurious diffraction orders.\\nIt has been recently shown, using a circuit-based approach, that refraction\\nwithout spurious diffraction, or diffraction-free, can fortunately be achieved\\nby a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,\\nwe rederive these conditions using a medium-based - and hence more insightfull\\n- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface\\nsusceptibility tensors, and experimentally demonstrate two diffraction-free\\nrefractive metasurfaces that are essentially lossless, passive, bianisotropic\\nand reciprocal.\\n',\n",
       " '  We reconsider the classic problem of estimating accurately a 2D\\ntransformation from point matches between images containing outliers. RANSAC\\ndiscriminates outliers by randomly generating minimalistic sampled hypotheses\\nand verifying their consensus over the input data. Its response is based on the\\nsingle hypothesis that obtained the largest inlier support. In this article we\\nshow that the resulting accuracy can be improved by aggregating all generated\\nhypotheses. This yields RANSAAC, a framework that improves systematically over\\nRANSAC and its state-of-the-art variants by statistically aggregating\\nhypotheses. To this end, we introduce a simple strategy that allows to rapidly\\naverage 2D transformations, leading to an almost negligible extra computational\\ncost. We give practical applications on projective transforms and\\nhomography+distortion models and demonstrate a significant performance gain in\\nboth cases.\\n',\n",
       " '  The Landau collision integral is an accurate model for the small-angle\\ndominated Coulomb collisions in fusion plasmas. We investigate a high order\\naccurate, fully conservative, finite element discretization of the nonlinear\\nmulti-species Landau integral with adaptive mesh refinement using the PETSc\\nlibrary (www.mcs.anl.gov/petsc). We develop algorithms and techniques to\\nefficiently utilize emerging architectures with an approach that minimizes\\nmemory usage and movement and is suitable for vector processing. The Landau\\ncollision integral is vectorized with Intel AVX-512 intrinsics and the solver\\nsustains as much as 22% of the theoretical peak flop rate of the Second\\nGeneration Intel Xeon Phi, Knights Landing, processor.\\n',\n",
       " '  In this paper we combine a survey of the most important topological\\nproperties of kinematic maps that appear in robotics, with the exposition of\\nsome basic results regarding the topological complexity of a map. In\\nparticular, we discuss mechanical devices that consist of rigid parts connected\\nby joints and show how the geometry of the joints determines the forward\\nkinematic map that relates the configuration of joints with the pose of the\\nend-effector of the device. We explain how to compute the dimension of the\\njoint space and describe topological obstructions for a kinematic map to be a\\nfibration or to admit a continuous section. In the second part of the paper we\\ndefine the complexity of a continuous map and show how the concept can be\\nviewed as a measure of the difficulty to find a robust manipulation plan for a\\ngiven mechanical device. We also derive some basic estimates for the complexity\\nand relate it to the degree of instability of a manipulation plan.\\n',\n",
       " '  The Butler-Portugal algorithm for obtaining the canonical form of a tensor\\nexpression with respect to slot symmetries and dummy-index renaming suffers, in\\ncertain cases with a high degree of symmetry, from $O(n!)$ explosion in both\\ncomputation time and memory. We present a modified algorithm which alleviates\\nthis problem in the most common cases---tensor expressions with subsets of\\nindices which are totally symmetric or totally antisymmetric---in polynomial\\ntime. We also present an implementation of the label-renaming mechanism which\\nimproves upon that of the original Butler-Portugal algorithm, thus providing a\\nsignificant speed increase for the average case as well as the highly-symmetric\\nspecial case. The worst-case behavior remains $O(n!)$, although it occurs in\\nmore limited situations unlikely to appear in actual computations. We comment\\non possible strategies to take if the nature of a computation should make these\\nsituations more likely.\\n',\n",
       " '  The mass-preconditioning (MP) technique has become a standard tool to enhance\\nthe efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with\\ndynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as\\nits extension to the case of one-flavor by taking the square-root of the\\nfermion determinant of 2-flavors with degenerate masses. However, for lattice\\nQCD with domain-wall fermion, the fermion determinant of any single fermion\\nflavor can be expressed as a functional integral with an exact pseudofermion\\naction $ \\\\phi^\\\\dagger H^{-1} \\\\phi $, where $ H^{-1} $ is a positive-definite\\nHermitian operator without taking square-root, and with the chiral structure\\n\\\\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact\\none-flavor action (EOFA) does not necessarily follow the conventional (old) MP\\npattern. In this paper, we present a new mass-preconditioning for the EOFA,\\nwhich is more efficient than the old MP which we have used in Refs.\\n\\\\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD\\nwith $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one\\nmass-preconditioner applied to one of the exact one-flavor actions, and we find\\nthat the efficiency of the new MP is more than 20\\\\% higher than that of the old\\nMP.\\n',\n",
       " \"  A model in which a three-dimensional elastic medium is represented by a\\nnetwork of identical masses connected by springs of random strengths and\\nallowed to vibrate only along a selected axis of the reference frame, exhibits\\nan Anderson localization transition. To study this transition, we assume that\\nthe dynamical matrix of the network is given by a product of a sparse random\\nmatrix with real, independent, Gaussian-distributed non-zero entries and its\\ntranspose. A finite-time scaling analysis of system's response to an initial\\nexcitation allows us to estimate the critical parameters of the localization\\ntransition. The critical exponent is found to be $\\\\nu = 1.57 \\\\pm 0.02$ in\\nagreement with previous studies of Anderson transition belonging to the\\nthree-dimensional orthogonal universality class.\\n\",\n",
       " \"  We analyze the response of a type II superconducting wire to an external\\nmagnetic field parallel to it in the framework of Ginzburg-Landau theory. We\\nfocus on the surface superconductivity regime of applied field between the\\nsecond and third critical values, where the superconducting state survives only\\nclose to the sample's boundary. Our first finding is that, in first\\napproximation, the shape of the boundary plays no role in determining the\\ndensity of superconducting electrons. A second order term is however isolated,\\ndirectly proportional to the mean curvature of the boundary. This demonstrates\\nthat points of higher boundary curvature (counted inwards) attract\\nsuperconducting electrons.\\n\",\n",
       " '  Classical principal component analysis (PCA) is not robust to the presence of\\nsparse outliers in the data. The use of the $\\\\ell_1$ norm in the Robust PCA\\n(RPCA) method successfully eliminates the weakness of PCA in separating the\\nsparse outliers. In this paper, by sticking a simple weight to the Frobenius\\nnorm, we propose a weighted low rank (WLR) method to avoid the often\\ncomputationally expensive algorithms relying on the $\\\\ell_1$ norm. As a proof\\nof concept, a background estimation model has been presented and compared with\\ntwo $\\\\ell_1$ norm minimization algorithms. We illustrate that as long as a\\nsimple weight matrix is inferred from the data, one can use the weighted\\nFrobenius norm and achieve the same or better performance.\\n',\n",
       " '  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the\\ndegeneracy of left- and right-circularly polarized spin waves. This\\nrelativistic coupling increases the efficiency of spin-wave-induced domain wall\\nmotion and leads to higher drift velocities. We show that in biaxial\\nantiferromagnets, the spin-wave helicity controls both the direction and\\nmagnitude of the magnonic force on chiral domain walls. By contrast, in\\nuniaxial antiferromagnets, the magnonic force is propulsive with a helicity\\ndependent strength.\\n',\n",
       " '  In disordered elastic systems, driven by displacing a parabolic confining\\npotential adiabatically slowly, all advance of the system is in bursts, termed\\navalanches. Avalanches have a finite extension in time, which is much smaller\\nthan the waiting-time between them. Avalanches also have a finite extension\\n$\\\\ell$ in space, i.e. only a part of the interface of size $\\\\ell$ moves during\\nan avalanche. Here we study their spatial shape $\\\\left< S(x)\\\\right>_{\\\\ell}$\\ngiven $\\\\ell$, as well as its fluctuations encoded in the second cumulant\\n$\\\\left< S^{2}(x)\\\\right>_{\\\\ell}^{\\\\rm c}$. We establish scaling relations\\ngoverning the behavior close to the boundary. We then give analytic results for\\nthe Brownian force model, in which the microscopic disorder for each degree of\\nfreedom is a random walk. Finally, we confirm these results with numerical\\nsimulations. To do this properly we elucidate the influence of discretization\\neffects, which also confirms the assumptions entering into the scaling ansatz.\\nThis allows us to reach the scaling limit already for avalanches of moderate\\nsize. We find excellent agreement for the universal shape, its fluctuations,\\nincluding all amplitudes.\\n',\n",
       " '  The search for a superconductor with non-s-wave pairing is important not only\\nfor understanding unconventional mechanisms of superconductivity but also for\\nfinding new types of quasiparticles such as Majorana bound states. Materials\\nwith both topological band structure and superconductivity are promising\\ncandidates as $p+ip$ superconducting states can be generated through pairing\\nthe spin-polarized topological surface states. In this work, the electronic and\\nphonon properties of the superconductor molybdenum carbide (MoC) are studied\\nwith first-principles methods. Our calculations show that nontrivial band\\ntopology and superconductivity coexist in both structural phases of MoC,\\nnamely, the cubic $\\\\alpha$ and hexagonal $\\\\gamma$ phases. The $\\\\alpha$ phase is\\na strong topological insulator and the $\\\\gamma$ phase is a topological nodal\\nline semimetal with drumhead surface states. In addition, hole doping can\\nstabilize the crystal structure of the $\\\\alpha$ phase and elevate the\\ntransition temperature in the $\\\\gamma$ phase. Therefore, MoC in different\\nstructural forms can be a practical material platform for studying topological\\nsuperconductivity and elusive Majorana fermions.\\n',\n",
       " '  The extension complexity $\\\\mathsf{xc}(P)$ of a polytope $P$ is the minimum\\nnumber of facets of a polytope that affinely projects to $P$. Let $G$ be a\\nbipartite graph with $n$ vertices, $m$ edges, and no isolated vertices. Let\\n$\\\\mathsf{STAB}(G)$ be the convex hull of the stable sets of $G$. It is easy to\\nsee that $n \\\\leqslant \\\\mathsf{xc} (\\\\mathsf{STAB}(G)) \\\\leqslant n+m$. We improve\\nboth of these bounds. For the upper bound, we show that $\\\\mathsf{xc}\\n(\\\\mathsf{STAB}(G))$ is $O(\\\\frac{n^2}{\\\\log n})$, which is an improvement when\\n$G$ has quadratically many edges. For the lower bound, we prove that\\n$\\\\mathsf{xc} (\\\\mathsf{STAB}(G))$ is $\\\\Omega(n \\\\log n)$ when $G$ is the\\nincidence graph of a finite projective plane. We also provide examples of\\n$3$-regular bipartite graphs $G$ such that the edge vs stable set matrix of $G$\\nhas a fooling set of size $|E(G)|$.\\n',\n",
       " '  This work provides a comprehensive scaling law based performance analysis for\\nmulti-cell multi-user massive multiple-input-multiple-output (MIMO) downlink\\nsystems. Imperfect channel state information (CSI), pilot contamination, and\\nchannel spatial correlation are all considered. First, a sum- rate lower bound\\nis derived by exploiting the asymptotically deterministic property of the\\nreceived signal power, while keeping the random nature of other components in\\nthe signal-to-interference-plus-noise-ratio (SINR) intact. Via a general\\nscaling model on important network parameters, including the number of users,\\nthe channel training energy and the data transmission power, with respect to\\nthe number of base station antennas, the asymptotic scaling law of the\\neffective SINR is obtained, which reveals quantitatively the tradeoff of the\\nnetwork parameters. More importantly, pilot contamination and pilot\\ncontamination elimination (PCE) are considered in the analytical framework. In\\naddition, the applicability of the derived asymptotic scaling law in practical\\nsystems with large but finite antenna numbers are discussed. Finally,\\nsufficient conditions on the parameter scalings for the SINR to be\\nasymptotically deterministic in the sense of mean square convergence are\\nprovided, which covers existing results on such analysis as special cases and\\nshows the effect of PCE explicitly.\\n',\n",
       " \"  In view of a resurgence of concern about the measurement problem, it is\\npointed out that the Relativistic Transactional Interpretation (RTI) remedies\\nissues previously considered as drawbacks or refutations of the original TI.\\nSpecifically, once one takes into account relativistic processes that are not\\nrepresentable at the non-relativistic level (such as particle creation and\\nannihilation, and virtual propagation), absorption is quantitatively defined in\\nunambiguous physical terms. In addition, specifics of the relativistic\\ntransactional model demonstrate that the Maudlin `contingent absorber'\\nchallenge to the original TI cannot even be mounted: basic features of\\nestablished relativistic field theories (in particular, the asymmetry between\\nfield sources and the bosonic fields, and the fact that slow-moving bound\\nstates, such as atoms, are not offer waves) dictate that the `slow-moving offer\\nwave' required for the challenge scenario cannot exist. It is concluded that\\nissues previously considered obstacles for TI are no longer legitimately viewed\\nas such, and that reconsideration of the transactional picture is warranted in\\nconnection with solving the measurement problem.\\n\",\n",
       " '  We explore the response of Ir $5d$ orbitals to pressure in\\n$\\\\beta$-$\\\\mathrm{Li_2IrO_3}$, a hyperhoneycomb iridate in proximity to a Kitaev\\nquantum spin liquid (QSL) ground state. X-ray absorption spectroscopy reveals a\\nreconstruction of the electronic ground state below 2 GPa, the same pressure\\nrange where x-ray magnetic circular dichroism shows an apparent collapse of\\nmagnetic order. The electronic reconstruction, which manifests a reduction in\\nthe effective spin-orbit (SO) interaction in $5d$ orbitals, pushes\\n$\\\\beta$-$\\\\mathrm{Li_2IrO_3}$ further away from the pure $J_{\\\\rm eff}=1/2$\\nlimit. Although lattice symmetry is preserved across the electronic transition,\\nx-ray diffraction shows a highly anisotropic compression of the hyperhoneycomb\\nlattice which affects the balance of bond-directional Ir-Ir exchange\\ninteractions driven by spin-orbit coupling at Ir sites. An enhancement of\\nsymmetric anisotropic exchange over Kitaev and Heisenberg exchange interactions\\nseen in theoretical calculations that use precisely this anisotropic Ir-Ir bond\\ncompression provides one possible route to realization of a QSL state in this\\nhyperhoneycomb iridate at high pressures.\\n',\n",
       " '  An infinite convergent sum of independent and identically distributed random\\nvariables discounted by a multiplicative random walk is called perpetuity,\\nbecause of a possible actuarial application. We give three disjoint groups of\\nsufficient conditions which ensure that the distribution right tail of a\\nperpetuity $\\\\mathbb{P}\\\\{X>x\\\\}$ is asymptotic to $ax^ce^{-bx}$ as $x\\\\to\\\\infty$\\nfor some $a,b>0$ and $c\\\\in\\\\mathbb{R}$. Our results complement those of Denisov\\nand Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we\\nprovide criteria for the finiteness of the one-sided exponential moments of\\nperpetuities. Several examples are given in which the distributions of\\nperpetuities are explicitly identified.\\n',\n",
       " '  We consider the problem of estimating from sample paths the absolute spectral\\ngap $\\\\gamma_*$ of a reversible, irreducible and aperiodic Markov chain\\n$(X_t)_{t \\\\in \\\\mathbb{N}}$ over a finite state $\\\\Omega$. We propose the ${\\\\tt\\nUCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a\\nlow-complexity algorithm which estimates the spectral gap in time ${\\\\cal O}(n)$\\nand memory space ${\\\\cal O}((\\\\ln n)^2)$ given $n$ samples. This is in stark\\ncontrast with most known methods which require at least memory space ${\\\\cal\\nO}(|\\\\Omega|)$, so that they cannot be applied to large state spaces.\\nFurthermore, ${\\\\tt UCPI}$ is amenable to parallel implementation.\\n',\n",
       " '  The state-of-the-art (SOTA) for mixed precision training is dominated by\\nvariants of low precision floating point operations, and in particular, FP16\\naccumulating into FP32 Micikevicius et al. (2017). On the other hand, while a\\nlot of research has also happened in the domain of low and mixed-precision\\nInteger training, these works either present results for non-SOTA networks (for\\ninstance only AlexNet for ImageNet-1K), or relatively small datasets (like\\nCIFAR-10). In this work, we train state-of-the-art visual understanding neural\\nnetworks on the ImageNet-1K dataset, with Integer operations on General Purpose\\n(GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate\\n(FMA) operations which take two pairs of INT16 operands and accumulate results\\ninto an INT32 output.We propose a shared exponent representation of tensors and\\ndevelop a Dynamic Fixed Point (DFP) scheme suitable for common neural network\\noperations. The nuances of developing an efficient integer convolution kernel\\nis examined, including methods to handle overflow of the INT32 accumulator. We\\nimplement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and\\nthese networks achieve or exceed SOTA accuracy within the same number of\\niterations as their FP32 counterparts without any change in hyper-parameters\\nand with a 1.8X improvement in end-to-end training throughput. To the best of\\nour knowledge these results represent the first INT16 training results on GP\\nhardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported\\naccuracy using half-precision\\n',\n",
       " '  The foreseen implementations of the Small Size Telescopes (SST) in CTA will\\nprovide unique insights into the highest energy gamma rays offering fundamental\\nmeans to discover and under- stand the sources populating the Galaxy and our\\nlocal neighborhood. Aiming at such a goal, the SST-1M is one of the three\\ndifferent implementations that are being prototyped and tested for CTA. SST-1M\\nis a Davies-Cotton single mirror telescope equipped with a unique camera\\ntechnology based on SiPMs with demonstrated advantages over classical\\nphotomultipliers in terms of duty-cycle. In this contribution, we describe the\\ntelescope components, the camera, and the trigger and readout system. The\\nresults of the commissioning of the camera using a dedicated test setup are\\nthen presented. The performances of the camera first prototype in terms of\\nexpected trigger rates and trigger efficiencies for different night-sky\\nbackground conditions are presented, and the camera response is compared to\\nend-to-end simulations.\\n',\n",
       " '  We obtain bounded for all $t$ solutions of ordinary differential equations as\\nlimits of the solutions of the corresponding Dirichlet problems on $(-L,L)$,\\nwith $L \\\\rightarrow \\\\infty$. We derive a priori estimates for the Dirichlet\\nproblems, allowing passage to the limit, via a diagonal sequence. This approach\\ncarries over to the PDE case.\\n',\n",
       " '  Waveforms of gravitational waves provide information about a variety of\\nparameters for the binary system merging. However, standard calculations have\\nbeen performed assuming a FLRW universe with no perturbations. In reality this\\nassumption should be dropped: we show that the inclusion of cosmological\\nperturbations translates into corrections to the estimate of astrophysical\\nparameters derived for the merging binary systems. We compute corrections to\\nthe estimate of the luminosity distance due to velocity, volume, lensing and\\ngravitational potential effects. Our results show that the amplitude of the\\ncorrections will be negligible for current instruments, mildly important for\\nexperiments like the planned DECIGO, and very important for future ones such as\\nthe Big Bang Observer.\\n',\n",
       " '  A simple DNA-based data storage scheme is demonstrated in which information\\nis written using \"addressing\" oligonucleotides. In contrast to other methods\\nthat allow arbitrary code to be stored, the resulting DNA is suitable for\\ndownstream enzymatic and biological processing. This capability is crucial for\\nDNA computers, and may allow for a diverse array of computational operations to\\nbe carried out using this DNA. Although here we use gel-based methods for\\ninformation readout, we also propose more advanced methods involving\\nprotein/DNA complexes and atomic force microscopy/nano-pore schemes for data\\nreadout.\\n',\n",
       " '  This paper presents VEC-NBT, a variation on the unsupervised graph clustering\\ntechnique VEC, which improves upon the performance of the original algorithm\\nsignificantly for sparse graphs. VEC employs a novel application of the\\nstate-of-the-art word2vec model to embed a graph in Euclidean space via random\\nwalks on the nodes of the graph. In VEC-NBT, we modify the original algorithm\\nto use a non-backtracking random walk instead of the normal backtracking random\\nwalk used in VEC. We introduce a modification to a non-backtracking random\\nwalk, which we call a begrudgingly-backtracking random walk, and show\\nempirically that using this model of random walks for VEC-NBT requires shorter\\nwalks on the graph to obtain results with comparable or greater accuracy than\\nVEC, especially for sparser graphs.\\n',\n",
       " '  We have used soft x-ray photoemission electron microscopy to image the\\nmagnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands\\narranged in geometrically frustrated configurations such as square ice and\\nkagome ice geometries. Upon thermal randomization, ensembles of nano-islands\\nwith strong inter-island magnetic coupling relax towards low-energy\\nconfigurations. Statistical analysis shows that the likelihood of ensembles\\nfalling into low-energy configurations depends strongly on the annealing\\ntemperature. Annealing to just below the Curie temperature of the ferromagnetic\\nfilm (T$_{C}$ = 338 K) allows for a much greater probability of achieving low\\nenergy configurations as compared to annealing above the Curie temperature. At\\nthis thermally active temperature of 325 K, the ensemble of ferromagnetic\\nnano-islands explore their energy landscape over time and eventually transition\\nto lower energy states as compared to the frozen-in configurations obtained\\nupon cooling from above the Curie temperature. Thus, this materials system\\nallows for a facile method to systematically study thermal evolution of\\nartificial spin ice arrays of nano-islands at temperatures modestly above room\\ntemperature.\\n',\n",
       " '  A set function $f$ on a finite set $V$ is submodular if $f(X) + f(Y) \\\\geq f(X\\n\\\\cup Y) + f(X \\\\cap Y)$ for any pair $X, Y \\\\subseteq V$. The symmetric\\ndifference transformation (SD-transformation) of $f$ by a canonical set $S\\n\\\\subseteq V$ is a set function $g$ given by $g(X) = f(X \\\\vartriangle S)$ for $X\\n\\\\subseteq V$,where $X \\\\vartriangle S = (X \\\\setminus S) \\\\cup (S \\\\setminus X)$\\ndenotes the symmetric difference between $X$ and $S$. Submodularity and\\nSD-transformations are regarded as the counterparts of convexity and affine\\ntransformations in a discrete space, respectively. However, submodularity is\\nnot preserved under SD-transformations, in contrast to the fact that convexity\\nis invariant under affine transformations. This paper presents a\\ncharacterization of SD-stransformations preserving submodularity. Then, we are\\nconcerned with the problem of discovering a canonical set $S$, given the\\nSD-transformation $g$ of a submodular function $f$ by $S$, provided that $g(X)$\\nis given by a function value oracle. A submodular function $f$ on $V$ is said\\nto be strict if $f(X) + f(Y) > f(X \\\\cup Y) + f(X \\\\cap Y)$ holds whenever both\\n$X \\\\setminus Y$ and $Y \\\\setminus X$ are nonempty. We show that the problem is\\nsolved by using ${\\\\rm O}(|V|)$ oracle calls when $f$ is strictly submodular,\\nalthough it requires exponentially many oracle calls in general.\\n',\n",
       " '  Quick Shift is a popular mode-seeking and clustering algorithm. We present\\nfinite sample statistical consistency guarantees for Quick Shift on mode and\\ncluster recovery under mild distributional assumptions. We then apply our\\nresults to construct a consistent modal regression algorithm.\\n',\n",
       " '  In recent years Deep Neural Networks (DNNs) have been rapidly developed in\\nvarious applications, together with increasingly complex architectures. The\\nperformance gain of these DNNs generally comes with high computational costs\\nand large memory consumption, which may not be affordable for mobile platforms.\\nDeep model quantization can be used for reducing the computation and memory\\ncosts of DNNs, and deploying complex DNNs on mobile equipment. In this work, we\\npropose an optimization framework for deep model quantization. First, we\\npropose a measurement to estimate the effect of parameter quantization errors\\nin individual layers on the overall model prediction accuracy. Then, we propose\\nan optimization process based on this measurement for finding optimal\\nquantization bit-width for each layer. This is the first work that\\ntheoretically analyse the relationship between parameter quantization errors of\\nindividual layers and model accuracy. Our new quantization algorithm\\noutperforms previous quantization optimization methods, and achieves 20-40%\\nhigher compression rate compared to equal bit-width quantization at the same\\nmodel prediction accuracy.\\n',\n",
       " '  Convolutional Neural Networks (CNNs) have been recently introduced in the\\ndomain of session-based next item recommendation. An ordered collection of past\\nitems the user has interacted with in a session (or sequence) are embedded into\\na 2-dimensional latent matrix, and treated as an image. The convolution and\\npooling operations are then applied to the mapped item embeddings. In this\\npaper, we first examine the typical session-based CNN recommender and show that\\nboth the generative model and network architecture are suboptimal when modeling\\nlong-range dependencies in the item sequence. To address the issues, we\\nintroduce a simple, but very effective generative model that is capable of\\nlearning high-level representation from both short- and long-range item\\ndependencies. The network architecture of the proposed model is formed of a\\nstack of \\\\emph{holed} convolutional layers, which can efficiently increase the\\nreceptive fields without relying on the pooling operation. Another contribution\\nis the effective use of residual block structure in recommender systems, which\\ncan ease the optimization for much deeper networks. The proposed generative\\nmodel attains state-of-the-art accuracy with less training time in the next\\nitem recommendation task. It accordingly can be used as a powerful\\nrecommendation baseline to beat in future, especially when there are long\\nsequences of user feedback.\\n',\n",
       " '  We present a terahertz spectroscopic study of polar ferrimagnet\\nFeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode\\neffect, or nonreciprocal directional dichroism, where the transmitted light\\nintensity in one direction is over 100 times lower than intensity transmitted\\nin the opposite direction. The effect takes place in the paramagnetic phase\\nwith no long-range magnetic order in the crystal, which contrasts sharply with\\nall existing reports of the terahertz optical diode effect in other\\nmagnetoelectric materials, where the long-range magnetic ordering is a\\nnecessary prerequisite. In \\\\fzmo, the effect occurs resonantly with a strong\\nmagnetic dipole active transition centered at 1.27 THz and assigned as electron\\nspin resonance between the eigenstates of the single-ion anisotropy\\nHamiltonian. We propose that the optical diode effect in paramagnetic\\nFeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.\\n',\n",
       " \"  We compare the social character networks of biographical, legendary and\\nfictional texts, in search for marks of genre differentiation. We examine the\\ndegree distribution of character appearance and find a power law that does not\\ndepend on the literary genre or historical content. We also analyze local and\\nglobal complex networks measures, in particular, correlation plots between the\\nrecently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and\\nCloseness centralities. Assortativity plots, which previous literature claims\\nto separate fictional from real social networks, were also studied. We've found\\nno relevant differences in the books for these network measures and we give a\\nplausible explanation why the previous assortativity result is not correct.\\n\",\n",
       " '  Motivated by the recent experimental realization of the Haldane model by\\nultracold fermions in an optical lattice, we investigate phase diagrams of the\\nhard-core Bose-Hubbard model on a honeycomb lattice. This model is closely\\nrelated with a spin-1/2 antiferromagnetic (AF) quantum spin model.\\nNearest-neighbor (NN) hopping amplitude is positive and it prefers an AF\\nconfigurations of phases of Bose-Einstein condensates. On the other hand, an\\namplitude of the next-NN hopping depends on an angle variable as in the Haldane\\nmodel. Phase diagrams are obtained by means of an extended path-integral\\nMonte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there\\nappear other phases including a Bose metal in which no long-range orders exist.\\n',\n",
       " \"  Exoplanet host star activity, in the form of unocculted star spots or\\nfaculae, alters the observed transmission and emission spectra of the\\nexoplanet. This effect can be exacerbated when combining data from different\\nepochs if the stellar photosphere varies between observations due to activity.\\nredHere we present a method to characterize and correct for relative changes\\ndue to stellar activity by exploiting multi-epoch ($\\\\ge$2 visits/transits)\\nobservations to place them in a consistent reference frame. Using measurements\\nfrom portions of the planet's orbit where negligible planet transmission or\\nemission can be assumed, we determine changes to the stellar spectral\\namplitude. With the analytical methods described here, we predict the impact of\\nstellar variability on transit observations. Supplementing these forecasts with\\nKepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and\\npredicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude\\nthat stellar activity does not impact infrared transiting exoplanet\\nobservations of most presently-known or predicted TESS targets by current or\\nnear-future platforms, such as JWST.\\n\",\n",
       " '  Developers of Molecular Dynamics (MD) codes face significant challenges when\\nadapting existing simulation packages to new hardware. In a continuously\\ndiversifying hardware landscape it becomes increasingly difficult for\\nscientists to be experts both in their own domain (physics/chemistry/biology)\\nand specialists in the low level parallelisation and optimisation of their\\ncodes. To address this challenge, we describe a \"Separation of Concerns\"\\napproach for the development of parallel and optimised MD codes: the science\\nspecialist writes code at a high abstraction level in a domain specific\\nlanguage (DSL), which is then translated into efficient computer code by a\\nscientific programmer. In a related context, an abstraction for the solution of\\npartial differential equations with grid based methods has recently been\\nimplemented in the (Py)OP2 library. Inspired by this approach, we develop a\\nPython code generation system for molecular dynamics simulations on different\\nparallel architectures, including massively parallel distributed memory systems\\nand GPUs. We demonstrate the efficiency of the auto-generated code by studying\\nits performance and scalability on different hardware and compare it to other\\nstate-of-the-art simulation packages. With growing data volumes the extraction\\nof physically meaningful information from the simulation becomes increasingly\\nchallenging and requires equally efficient implementations. A particular\\nadvantage of our approach is the easy expression of such analysis algorithms.\\nWe consider two popular methods for deducing the crystalline structure of a\\nmaterial from the local environment of each atom, show how they can be\\nexpressed in our abstraction and implement them in the code generation\\nframework.\\n',\n",
       " '  We obtain the non-linear generalization of the Sachs-Wolfe + integrated\\nSachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our\\nformula is valid at all orders in perturbation theory, is also valid in all\\ngauges and includes scalar, vector and tensor modes. A direct consequence of\\nour results is that the maps of the logarithmic temperature anisotropies are\\nmuch cleaner than the usual CMB maps, because they automatically remove many\\nsecondary anisotropies. This can for instance, facilitate the search for\\nprimordial non-Gaussianity in future works. It also disentangles the non-linear\\nISW from other effects. Finally, we provide a method which can iteratively be\\nused to obtain the lensing solution at the desired order.\\n',\n",
       " '  We prove sharp decoupling inequalities for a class of two dimensional\\nnon-degenerate surfaces in R^5, introduced by Prendiville. As a consequence, we\\nobtain sharp bounds on the number of integer solutions of the Diophantine\\nsystems associated with these surfaces.\\n',\n",
       " '  In this paper, we introduce a simple, yet powerful pipeline for medical image\\nsegmentation that combines Fully Convolutional Networks (FCNs) with Fully\\nConvolutional Residual Networks (FC-ResNets). We propose and examine a design\\nthat takes particular advantage of recent advances in the understanding of both\\nConvolutional Neural Networks as well as ResNets. Our approach focuses upon the\\nimportance of a trainable pre-processing when using FC-ResNets and we show that\\na low-capacity FCN model can serve as a pre-processor to normalize medical\\ninput data. In our image segmentation pipeline, we use FCNs to obtain\\nnormalized images, which are then iteratively refined by means of a FC-ResNet\\nto generate a segmentation prediction. As in other fully convolutional\\napproaches, our pipeline can be used off-the-shelf on different image\\nmodalities. We show that using this pipeline, we exhibit state-of-the-art\\nperformance on the challenging Electron Microscopy benchmark, when compared to\\nother 2D methods. We improve segmentation results on CT images of liver\\nlesions, when contrasting with standard FCN methods. Moreover, when applying\\nour 2D pipeline on a challenging 3D MRI prostate segmentation challenge we\\nreach results that are competitive even when compared to 3D methods. The\\nobtained results illustrate the strong potential and versatility of the\\npipeline by achieving highly accurate results on multi-modality images from\\ndifferent anatomical regions and organs.\\n',\n",
       " \"  Our purpose is to focus attention on a new criterion for quantum schemes by\\nbringing together the notions of quantum game and game isomorphism. A quantum\\ngame scheme is required to generate the classical game as a special case. Now,\\ngiven a quantum game scheme and two isomorphic classical games, we additionally\\nrequire the resulting quantum games to be isomorphic as well. We show how this\\nisomorphism condition influences the players' strategy sets. We are concerned\\nwith the Marinatto-Weber type quantum game scheme and the strong isomorphism\\nbetween games in strategic form.\\n\",\n",
       " '  Stochastic variance reduction algorithms have recently become popular for\\nminimizing the average of a large but finite number of loss functions. In this\\npaper, we propose a novel Riemannian extension of the Euclidean stochastic\\nvariance reduced gradient algorithm (R-SVRG) to a manifold search space. The\\nkey challenges of averaging, adding, and subtracting multiple gradients are\\naddressed with retraction and vector transport. We present a global convergence\\nanalysis of the proposed algorithm with a decay step size and a local\\nconvergence rate analysis under a fixed step size under some natural\\nassumptions. The proposed algorithm is applied to problems on the Grassmann\\nmanifold, such as principal component analysis, low-rank matrix completion, and\\ncomputation of the Karcher mean of subspaces, and outperforms the standard\\nRiemannian stochastic gradient descent algorithm in each case.\\n',\n",
       " '  The algorithmic Markov condition states that the most likely causal direction\\nbetween two random variables X and Y can be identified as that direction with\\nthe lowest Kolmogorov complexity. Due to the halting problem, however, this\\nnotion is not computable.\\nWe hence propose to do causal inference by stochastic complexity. That is, we\\npropose to approximate Kolmogorov complexity via the Minimum Description Length\\n(MDL) principle, using a score that is mini-max optimal with regard to the\\nmodel class under consideration. This means that even in an adversarial\\nsetting, such as when the true distribution is not in this class, we still\\nobtain the optimal encoding for the data relative to the class.\\nWe instantiate this framework, which we call CISC, for pairs of univariate\\ndiscrete variables, using the class of multinomial distributions. Experiments\\nshow that CISC is highly accurate on synthetic, benchmark, as well as\\nreal-world data, outperforming the state of the art by a margin, and scales\\nextremely well with regard to sample and domain sizes.\\n',\n",
       " '  The recently proposed Temporal Ensembling has achieved state-of-the-art\\nresults in several semi-supervised learning benchmarks. It maintains an\\nexponential moving average of label predictions on each training example, and\\npenalizes predictions that are inconsistent with this target. However, because\\nthe targets change only once per epoch, Temporal Ensembling becomes unwieldy\\nwhen learning large datasets. To overcome this problem, we propose Mean\\nTeacher, a method that averages model weights instead of label predictions. As\\nan additional benefit, Mean Teacher improves test accuracy and enables training\\nwith fewer labels than Temporal Ensembling. Without changing the network\\narchitecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250\\nlabels, outperforming Temporal Ensembling trained with 1000 labels. We also\\nshow that a good network architecture is crucial to performance. Combining Mean\\nTeacher and Residual Networks, we improve the state of the art on CIFAR-10 with\\n4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels\\nfrom 35.24% to 9.11%.\\n',\n",
       " '  We present a new code for astrophysical magneto-hydrodynamics specifically\\ndesigned and optimized for high performance and scaling on modern and future\\nsupercomputers. We describe a novel hybrid OpenMP/MPI programming model that\\nemerged from a collaboration between Cray, Inc. and the University of\\nMinnesota. This design utilizes MPI-RMA optimized for thread scaling, which\\nallows the code to run extremely efficiently at very high thread counts ideal\\nfor the latest generation of the multi-core and many-core architectures. Such\\nperformance characteristics are needed in the era of \"exascale\" computing. We\\ndescribe and demonstrate our high-performance design in detail with the intent\\nthat it may be used as a model for other, future astrophysical codes intended\\nfor applications demanding exceptional performance.\\n',\n",
       " '  Virtual Network Functions as a Service (VNFaaS) is currently under attentive\\nstudy by telecommunications and cloud stakeholders as a promising business and\\ntechnical direction consisting of providing network functions as a service on a\\ncloud (NFV Infrastructure), instead of delivering standalone network\\nappliances, in order to provide higher scalability and reduce maintenance\\ncosts. However, the functioning of such NFVI hosting the VNFs is fundamental\\nfor all the services and applications running on top of it, forcing to\\nguarantee a high availability level. Indeed the availability of an VNFaaS\\nrelies on the failure rate of its single components, namely the servers, the\\nvirtualization software, and the communication network. The proper assignment\\nof the virtual machines implementing network functions to NFVI servers and\\ntheir protection is essential to guarantee high availability. We model the High\\nAvailability Virtual Network Function Placement (HA-VNFP) as the problem of\\nfinding the best assignment of virtual machines to servers guaranteeing\\nprotection by replication. We propose a probabilistic approach to measure the\\nreal availability of a system and design both efficient and effective\\nalgorithms that can be used by stakeholders for both online and offline\\nplanning.\\n',\n",
       " \"  The Zika virus has been found in individual cases but has not been confirmed\\nas the cause of in the large number of cases of microcephaly in Brazil in\\n2015-6. Indeed, disparities between the incidence of Zika and microcephaly\\nacross geographic locations has led to questions about the virus's role. Here\\nwe consider whether the insecticide pyriproxyfen used in Brazilian drinking\\nwater might be the primary cause or a cofactor. Pyriproxifen is a juvenile\\nhormone analog which has been shown to correspond in mammals to a number of fat\\nsoluble regulatory molecules including retinoic acid, a metabolite of vitamin\\nA, with which it has cross-reactivity and whose application during development\\nhas been shown to cause microcephaly. Methoprene, another juvenile hormone\\nanalog approved as an insecticide in the 1970s has been shown to cause\\ndevelopmental disorders in mammals. Isotretinoin is another retinoid causing\\nmicrocephaly via activation of the retinoid X receptor in developing fetuses.\\nWe review tests of pyriproxyfen by the manufacturer Sumitomo, which actually\\nfound some evidence for this effect, including low brain mass and\\narhinencephaly in exposed rat pups. Pyriproxyfen use in Brazil is\\nunprecedented, never having been applied to a water supply on a large scale.\\nClaims that its geographical pattern of use rule it out as a cause have not\\nbeen documented or confirmed. On the other hand, the very few microcephaly\\ncases reported in Colombia and the wide discrepancies of incidence in different\\nstates across Brazil despite large numbers of Zika cases undermine the claim\\nthat Zika is the cause. Given this combination of potential molecular\\nmechanism, toxicological and epidemiological evidence we strongly recommend\\nthat the use of pyriproxyfen in Brazil be suspended until the potential causal\\nlink to microcephaly is investigated further.\\n\",\n",
       " '  This paper considers the problem of decentralized optimization with a\\ncomposite objective containing smooth and non-smooth terms. To solve the\\nproblem, a proximal-gradient scheme is studied. Specifically, the smooth and\\nnonsmooth terms are dealt with by gradient update and proximal update,\\nrespectively. The studied algorithm is closely related to a previous\\ndecentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.\\nFirst of all, in our new scheme, agents use uncoordinated step-sizes and the\\nstable upper bounds on step-sizes are independent from network topology. The\\nstep-sizes depend on local objective functions, and they can be as large as\\nthat of the gradient descent. Secondly, for the special case without non-smooth\\nterms, linear convergence can be achieved under the strong convexity\\nassumption. The dependence of the convergence rate on the objective functions\\nand the network are separated, and the convergence rate of our new scheme is as\\ngood as one of the two convergence rates that match the typical rates for the\\ngeneral gradient descent and the consensus averaging. We also provide some\\nnumerical experiments to demonstrate the efficacy of the introduced algorithms\\nand validate our theoretical discoveries.\\n',\n",
       " \"  Recent studies have demonstrated that near-data processing (NDP) is an\\neffective technique for improving performance and energy efficiency of\\ndata-intensive workloads. However, leveraging NDP in realistic systems with\\nmultiple memory modules introduces a new challenge. In today's systems, where\\nno computation occurs in memory modules, the physical address space is\\ninterleaved at a fine granularity among all memory modules to help improve the\\nutilization of processor-memory interfaces by distributing the memory traffic.\\nHowever, this is at odds with efficient use of NDP, which requires careful\\nplacement of data in memory modules such that near-data computations and their\\nexclusively used data can be localized in individual memory modules, while\\ndistributing shared data among memory modules to reduce hotspots. In order to\\naddress this new challenge, we propose a set of techniques that (1) enable\\ncollections of OS pages to either be fine-grain interleaved among memory\\nmodules (as is done today) or to be placed contiguously on individual memory\\nmodules (as is desirable for NDP private data), and (2) decide whether to\\nlocalize or distribute each memory object based on its anticipated access\\npattern and steer computations to the memory where the data they access is\\nlocated. Our evaluations across a wide range of workloads show that the\\nproposed mechanism improves performance by 31% and reduces 38% remote data\\naccesses over a baseline system that cannot exploit computate-data affinity\\ncharacteristics.\\n\",\n",
       " \"  Historically, machine learning in computer security has prioritized defense:\\nthink intrusion detection systems, malware classification, and botnet traffic\\nidentification. Offense can benefit from data just as well. Social networks,\\nwith their access to extensive personal data, bot-friendly APIs, colloquial\\nsyntax, and prevalence of shortened links, are the perfect venues for spreading\\nmachine-generated malicious content. We aim to discover what capabilities an\\nadversary might utilize in such a domain. We present a long short-term memory\\n(LSTM) neural network that learns to socially engineer specific users into\\nclicking on deceptive URLs. The model is trained with word vector\\nrepresentations of social media posts, and in order to make a click-through\\nmore likely, it is dynamically seeded with topics extracted from the target's\\ntimeline. We augment the model with clustering to triage high value targets\\nbased on their level of social engagement, and measure success of the LSTM's\\nphishing expedition using click-rates of IP-tracked links. We achieve state of\\nthe art success rates, tripling those of historic email attack campaigns, and\\noutperform humans manually performing the same task.\\n\",\n",
       " '  Deep learning has demonstrated tremendous potential for Automatic Text\\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\\nenhances vanilla neural network models with auxiliary neural coherence\\nfeatures. Our new method proposes a new \\\\textsc{SkipFlow} mechanism that models\\nrelationships between snapshots of the hidden representations of a long\\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\\nrelationships between multiple snapshots are used as auxiliary features for\\nprediction. This has two main benefits. Firstly, essays are typically long\\nsequences and therefore the memorization capability of the LSTM network may be\\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\\nby acting as a protection against vanishing gradients. The parameters of the\\n\\\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\\nmodeling relationships between multiple positions allows our model to learn\\nfeatures that represent and approximate textual coherence. In our model, we\\ncall this \\\\textit{neural coherence} features. Overall, we present a unified\\ndeep learning architecture that generates neural coherence features as it reads\\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\\nperformance on the benchmark ASAP dataset, outperforming not only feature\\nengineering baselines but also other deep learning models.\\n',\n",
       " '  This paper introduces a method, based on deep reinforcement learning, for\\nautomatically generating a general purpose decision making function. A Deep\\nQ-Network agent was trained in a simulated environment to handle speed and lane\\nchange decisions for a truck-trailer combination. In a highway driving case, it\\nis shown that the method produced an agent that matched or surpassed the\\nperformance of a commonly used reference model. To demonstrate the generality\\nof the method, the exact same algorithm was also tested by training it for an\\novertaking case on a road with oncoming traffic. Furthermore, a novel way of\\napplying a convolutional neural network to high level input that represents\\ninterchangeable objects is also introduced.\\n',\n",
       " '  One of the most challenging problems in correlated topological systems is a\\nrealization of the reduction of topological classification, but very few\\nexperimental platforms have been proposed so far. We here demonstrate that\\nultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded\\nin an optical lattice of two-leg ladder geometry can be the first promising\\ntestbed for the reduction $\\\\mathbb{Z}\\\\to\\\\mathbb{Z}_4$, where solid evidence for\\nthe reduction is available thanks to their high controllability. We further\\ngive a detailed account of how to experimentally access this phenomenon; around\\nthe edges, the destruction of one-particle gapless excitations can be observed\\nby the local radio frequency spectroscopy, while that of gapless spin\\nexcitations can be observed by a time-dependent spin expectation value of a\\nsuperposed state of the ground state and the first excited state. We clarify\\nthat even when the reduction occurs, a gapless edge mode is recovered around a\\ndislocation, which can be another piece of evidence for the reduction.\\n',\n",
       " '  Cosmological parameter constraints from observations of time-delay lenses are\\nbecoming increasingly precise. However, there may be significant bias and\\nscatter in these measurements due to, among other things, the so-called\\nmass-sheet degeneracy. To estimate these uncertainties, we analyze strong\\nlenses from the largest EAGLE hydrodynamical simulation. We apply a mass-sheet\\ntransformation to the radial density profiles of lenses, and by selecting\\nlenses near isothermality, we find that the bias on H0 can be reduced to 5%\\nwith an intrinsic scatter of 10%, confirming previous results performed on a\\ndifferent simulation data set. We further investigate whether combining lensing\\nobservables with kinematic constraints helps to minimize this bias. We do not\\ndetect any significant dependence of the bias on lens model parameters or\\nobservational properties of the galaxy, but depending on the source--lens\\nconfiguration, a bias may still exist. Cross lenses provide an accurate\\nestimate of the Hubble constant, while fold (double) lenses tend to be biased\\nlow (high). With kinematic constraints, double lenses show bias and intrinsic\\nscatter of 6% and 10%, respectively, while quad lenses show bias and intrinsic\\nscatter of 0.5% and 10%, respectively. For lenses with a reduced $\\\\chi^2 > 1$,\\na power-law dependence of the $\\\\chi^2$ on the lens environment (number of\\nnearby galaxies) is seen. Lastly, we model, in greater detail, the cases of two\\ndouble lenses that are significantly biased. We are able to remove the bias,\\nsuggesting that the remaining biases could also be reduced by carefully taking\\ninto account additional sources of systematic uncertainty.\\n',\n",
       " '  We characterize the response of the quiet time (no substorms or storms)\\nlarge-scale ionospheric transient equivalent currents to north-south and\\nsouth-north IMF turnings by using a dynamical network of ground-based\\nmagnetometers. Canonical correlation between all pairs of SuperMAG magnetometer\\nstations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\\\\circ}$)\\nis used to establish the extent of near-simultaneous magnetic response between\\nregions of magnetic local time-MLAT. Parameters and maps that describe\\nspatial-temporal correlation are used to characterize the system and its\\nresponse to the turnings aggregated over several hundred events. We find that\\nregions that experience large increases in correlation post turning coincide\\nwith typical locations of a two-cell convection system and are influenced by\\nthe interplanetary magnetic field $\\\\mathit{B}_{y}$. The time between the\\nturnings reaching the magnetopause and a network response is found to be\\n$\\\\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the\\nnightside.\\n',\n",
       " '  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th\\nsyzygy of a module of finite lengh is greater than or equal to the rank of the\\nd-th syzygy of the residue field, unless the module has finite projective\\ndimension. Assuming that R is Gorenstein, we prove that if the question is\\naffrmative, then R is a hypersurface. If moreover R has dimension two, then we\\nshow that the converse also holds true.\\n',\n",
       " '  Learning to make decisions from observed data in dynamic environments remains\\na problem of fundamental importance in a number of fields, from artificial\\nintelligence and robotics, to medicine and finance. This paper concerns the\\nproblem of learning control policies for unknown linear dynamical systems so as\\nto maximize a quadratic reward function. We present a method to optimize the\\nexpected value of the reward over the posterior distribution of the unknown\\nsystem parameters, given data. The algorithm involves sequential convex\\nprograming, and enjoys reliable local convergence and robust stability\\nguarantees. Numerical simulations and stabilization of a real-world inverted\\npendulum are used to demonstrate the approach, with strong performance and\\nrobustness properties observed in both.\\n',\n",
       " '  Based on the KP hierarchy reduction method, the general bright-dark mixed\\nmulti-soliton solution of the multi-component Maccari system is constructed.\\nThe multi-component Maccari system considered comprised of multiple (say $M$)\\nshort-wave components and one long-wave component with all possible\\ncombinations of nonlinearities including all-focusing, all-defocusing and mixed\\ntypes. We firstly derive the two-bright-one-dark (2-b-1-d) and\\none-bright-two-dark (1-b-2-d) mixed multi-soliton solutions to the\\nthree-component Maccari system in detail. For the interaction between two\\nsolitons, the asymptotic analysis shows that inelastic collision can take place\\nin a $M$-component Maccari system with $M \\\\geq 3$ only if the bright parts of\\nthe mixed solitons appear at least in two short-wave components. The\\nenergy-exchanging inelastic collision characterized by an intensity\\nredistribution among the bright parts of the mixed solitons. While the dark\\nparts of the mixed solitons and the solitons in the long-wave component always\\nundergo elastic collision which just accompanied by a position shift. In the\\nend, we extend the corresponding analysis to the $M$-component Maccari system\\nto obtain its mixed multi-soliton solution. The formula obtained unifies the\\nall-bright, all-dark and mixed multi-soliton solutions.\\n',\n",
       " \"  Deep learning has been successfully applied to various tasks, but its\\nunderlying mechanism remains unclear. Neural networks associate similar inputs\\nin the visible layer to the same state of hidden variables in deep layers. The\\nfraction of inputs that are associated to the same state is a natural measure\\nof similarity and is simply related to the cost in bits required to represent\\nthese inputs. The degeneracy of states with the same information cost provides\\ninstead a natural measure of noise and is simply related the entropy of the\\nfrequency of states, that we call relevance. Representations with minimal\\nnoise, at a given level of similarity (resolution), are those that maximise the\\nrelevance. A signature of such efficient representations is that frequency\\ndistributions follow power laws. We show, in extensive numerical experiments,\\nthat deep neural networks extract a hierarchy of efficient representations from\\ndata, because they i) achieve low levels of noise (i.e. high relevance) and ii)\\nexhibit power law distributions. We also find that the layer that is most\\nefficient to reliably generate patterns of training data is the one for which\\nrelevance and resolution are traded at the same price, which implies that\\nfrequency distribution follows Zipf's law.\\n\",\n",
       " '  The modular Gromov-Hausdorff propinquity is a distance on classes of modules\\nendowed with quantum metric information, in the form of a metric form of a\\nconnection and a left Hilbert module structure. This paper proves that the\\nfamily of Heisenberg modules over quantum two tori, when endowed with their\\ncanonical connections, form a family of metrized quantum vector bundles, as a\\nfirst step in proving that Heisenberg modules form a continuous family for the\\nmodular Gromov-Hausdorff propinquity.\\n',\n",
       " '  We present a prototype of a software tool for exploration of multiple\\ncombinatorial optimisation problems in large real-world and synthetic complex\\nnetworks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial\\nExplorer), provides a unified framework for scalable computation and\\npresentation of high-quality suboptimal solutions and bounds for a number of\\nwidely studied combinatorial optimisation problems. Efficient representation\\nand applicability to large-scale graphs and complex networks are particularly\\nconsidered in its design. The problems currently supported include maximum\\nclique, graph colouring, maximum independent set, minimum vertex clique\\ncovering, minimum dominating set, as well as the longest simple cycle problem.\\nSuboptimal solutions and intervals for optimal objective values are estimated\\nusing scalable heuristics. The tool is designed with extensibility in mind,\\nwith the view of further problems and both new fast and high-performance\\nheuristics to be added in the future. GraphCombEx has already been successfully\\nused as a support tool in a number of recent research studies using\\ncombinatorial optimisation to analyse complex networks, indicating its promise\\nas a research software tool.\\n',\n",
       " '  Word sense disambiguation (WSD) improves many Natural Language Processing\\n(NLP) applications such as Information Retrieval, Machine Translation or\\nLexical Simplification. WSD is the ability of determining a word sense among\\ndifferent ones within a polysemic lexical unit taking into account the context.\\nThe most straightforward approach uses a semantic proximity measure between the\\nword sense candidates of the target word and those of its context. Such a\\nmethod very easily entails a combinatorial explosion. In this paper, we propose\\ntwo methods based on distributional analysis which enable to reduce the\\nexponential complexity without losing the coherence. We present a comparison\\nbetween the selection of distributional neighbors and the linearly nearest\\nneighbors. The figures obtained show that selecting distributional neighbors\\nleads to better results.\\n',\n",
       " '  We develop a theory for non-degenerate parametric resonance in a tunable\\nsuperconducting cavity. We focus on nonlinear effects that are caused by\\nnonlinear Josephson elements connected to the cavity. We analyze parametric\\namplification in a strong nonlinear regime at the parametric instability\\nthreshold, and calculate maximum gain values. Above the threshold, in the\\nparametric oscillator regime the linear cavity response diverges at the\\noscillator frequency at all pump strengths. We show that this divergence is\\nrelated to the continuous degeneracy of the free oscillator state with respect\\nto the phase. Applying on-resonance input lifts the degeneracy and removes the\\ndivergence. We also investigate the quantum noise squeezing. It is shown that\\nin the strong amplification regime the noise undergoes four-mode squeezing, and\\nthat in this regime the output signal to noise ratio can significantly exceed\\nthe input value. We also analyze the intermode frequency conversion and\\nidentify parameters at which full conversion is achieved.\\n',\n",
       " '  Generic generation and manipulation of text is challenging and has limited\\nsuccess compared to recent deep generative modeling in visual domain. This\\npaper aims at generating plausible natural language sentences, whose attributes\\nare dynamically controlled by learning disentangled latent representations with\\ndesignated semantics. We propose a new neural generative model which combines\\nvariational auto-encoders and holistic attribute discriminators for effective\\nimposition of semantic structures. With differentiable approximation to\\ndiscrete text samples, explicit constraints on independent attribute controls,\\nand efficient collaborative learning of generator and discriminators, our model\\nlearns highly interpretable representations from even only word annotations,\\nand produces realistic sentences with desired attributes. Quantitative\\nevaluation validates the accuracy of sentence and attribute generation.\\n',\n",
       " '  Our eyes sample a disproportionately large amount of information at the\\ncentre of gaze with increasingly sparse sampling into the periphery. This\\nsampling scheme is widely believed to be a wiring constraint whereby high\\nresolution at the centre is achieved by sacrificing spatial acuity in the\\nperiphery. Here we propose that this sampling scheme may be optimal for object\\nrecognition because the relevant spatial content is dense near an object and\\nsparse in the surrounding vicinity. We tested this hypothesis by training deep\\nconvolutional neural networks on full-resolution and foveated images. Our main\\nfinding is that networks trained on images with foveated sampling show better\\nobject classification compared to networks trained on full resolution images.\\nImportantly, blurring images according to the human blur function yielded the\\nbest performance compared to images with shallower or steeper blurring. Taken\\ntogether our results suggest that, peripheral blurring in our eyes may have\\nevolved for optimal object recognition, rather than merely to satisfy wiring\\nconstraints.\\n',\n",
       " '  Molecular dynamics simulates the~movements of atoms. Due to its high cost,\\nmany methods have been developed to \"push the~simulation forward\". One of them,\\nmetadynamics, can hasten the~molecular dynamics with the~help of variables\\ndescribing the~simulated process. However, the~evaluation of these variables\\ncan include numerous mean square distance calculations that introduce\\nsubstantial computational demands, thus jeopardize the~benefit of the~approach.\\nRecently, we proposed an~approximative method that significantly reduces\\nthe~number of these distance calculations. Here we evaluate the~performance and\\nthe~scalability on two molecular systems. We assess the~maximal theoretical\\nspeed-up based on the reduction of distance computations and Ahmdal\\'s law and\\ncompare it to the~practical speed-up achieved with our implementation.\\n',\n",
       " '  Developers increasingly rely on text matching tools to analyze the relation\\nbetween natural language words and APIs. However, semantic gaps, namely textual\\nmismatches between words and APIs, negatively affect these tools. Previous\\nstudies have transformed words or APIs into low-dimensional vectors for\\nmatching; however, inaccurate results were obtained due to the failure of\\nmodeling words and APIs simultaneously. To resolve this problem, two main\\nchallenges are to be addressed: the acquisition of massive words and APIs for\\nmining and the alignment of words and APIs for modeling. Therefore, this study\\nproposes Word2API to effectively estimate relatedness of words and APIs.\\nWord2API collects millions of commonly used words and APIs from code\\nrepositories to address the acquisition challenge. Then, a shuffling strategy\\nis used to transform related words and APIs into tuples to address the\\nalignment challenge. Using these tuples, Word2API models words and APIs\\nsimultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness\\nestimation in terms of precision and NDCG. Word2API is also effective on\\nsolving typical software tasks, e.g., query expansion and API documents\\nlinking. A simple system with Word2API-expanded queries recommends up to 21.4%\\nmore related APIs for developers. Meanwhile, Word2API improves comparison\\nalgorithms by 7.9%-17.4% in linking questions in Question&Answer communities to\\nAPI documents.\\n',\n",
       " '  We prove the existence of an optimal feedback controller for a stochastic\\noptimization problem constituted by a variation of the Heston model, where a\\nstochastic input process is added in order to minimize a given performance\\ncriterion. The stochastic feedback controller is searched by solving a\\nnonlinear backward parabolic equation for which one proves the existence of a\\nmartingale solution.\\n',\n",
       " \"  How can we enable novice users to create effective task plans for\\ncollaborative robots? Must there be a tradeoff between generalizability and\\nease of use? To answer these questions, we conducted a user study with the\\nCoSTAR system, which integrates perception and reasoning into a Behavior\\nTree-based task plan editor. In our study, we ask novice users to perform\\nsimple pick-and-place assembly tasks under varying perception and planning\\ncapabilities. Our study shows that users found Behavior Trees to be an\\neffective way of specifying task plans. Furthermore, users were also able to\\nmore quickly, effectively, and generally author task plans with the addition of\\nCoSTAR's planning, perception, and reasoning capabilities. Despite these\\nimprovements, concepts associated with these capabilities were rated by users\\nas less usable, and our results suggest a direction for further refinement.\\n\",\n",
       " \"  The (torsion) complexity of a finite edge-weighted graph is defined to be the\\norder of the torsion subgroup of the abelian group presented by its Laplacian\\nmatrix. When G is d-periodic (i.e., G has a free action of the rank-d free\\nabelian group by graph automorphisms, with finite quotient) the Mahler measure\\nof its Laplacian determinant polynomial is the growth rate of the complexity of\\nfinite quotients of G. Lehmer's question, an open question about the roots of\\nmonic integral polynomials, is equivalent to a question about the complexity\\ngrowth of edge-weighted 1-periodic graphs.\\n\",\n",
       " '  Neutronic performance is investigated for a potential accident tolerant fuel\\n(ATF),which consists of U$_3$Si$_2$ fuel and FeCrAl cladding. In comparison\\nwith current UO$_2$-Zr system, FeCrAl has a better oxidation resistance but a\\nlarger thermal neutron absorption cross section. U$_3$Si$_2$ has a higher\\nthermal conductivity and a higher uranium density, which can compensate the\\nreactivity suppressed by FeCrAl. Based on neutronic investigations, a possible\\nU$_3$Si$_2$-FeCrAl fuel-cladding systemis taken into consideration. Fundamental\\nproperties of the suggested fuel-cladding combination are investigated in a\\nfuel assembly.These properties include moderator and fuel temperature\\ncoefficients, control rods worth, radial power distribution (in a fuel rod),\\nand different void reactivity coefficients. The present work proves that the\\nnew combination has less reactivity variation during its service lifetime.\\nAlthough, compared with the current system, it has a little larger deviation on\\npower distribution and a little less negative temperature coefficient and void\\nreactivity coefficient and its control rods worth is less important, variations\\nof these parameters are less important during the service lifetime of fuel.\\nHence, U$_3$Si$_2$-FeCrAl system is a potential ATF candidate from a neutronic\\nview.\\n',\n",
       " '  We prove that the only entrywise transforms of rectangular matrices which\\npreserve total positivity or total non-negativity are either constant or\\nlinear. This follows from an extended classification of preservers of these two\\nproperties for matrices of fixed dimension. We also prove that the same\\nassertions hold upon working only with symmetric matrices; for total-positivity\\npreservers our proofs proceed through solving two totally positive completion\\nproblems.\\n',\n",
       " '  The most popular and widely used subtract-with-borrow generator, also known\\nas RANLUX, is reimplemented as a linear congruential generator using large\\ninteger arithmetic with the modulus size of 576 bits. Modern computers, as well\\nas the specific structure of the modulus inferred from RANLUX, allow for the\\ndevelopment of a fast modular multiplication -- the core of the procedure. This\\nwas previously believed to be slow and have too high cost in terms of computing\\nresources. Our tests show a significant gain in generation speed which is\\ncomparable with other fast, high quality random number generators. An\\nadditional feature is the fast skipping of generator states leading to a\\nseeding scheme which guarantees the uniqueness of random number sequences.\\n',\n",
       " \"  We consider the refined topological vertex of Iqbal et al, as a function of\\ntwo parameters (x, y), and deform it by introducing Macdonald parameters (q,\\nt), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald\\nrefined topological vertex'. In the limit q -> t, we recover the refined\\ntopological vertex of Iqbal et al. In the limit x -> y, we obtain a\\nqt-deformation of the topological vertex of Aganagic et al. Copies of the\\nvertex can be glued to obtain qt-deformed 5D instanton partition functions that\\nhave well-defined 4D limits and, for generic values of (q, t), contain\\ninfinite-towers of poles for every pole in the limit q -> t.\\n\",\n",
       " \"  We investigate bias voltage effects on the spin-dependent transport\\nproperties of Fe/MgAl${}_2$O${}_4$/Fe(001) magnetic tunneling junctions (MTJs)\\nby comparing them with those of Fe/MgO/Fe(001) MTJs. By means of the\\nnonequilibrium Green's function method and the density functional theory, we\\ncalculate bias voltage dependences of magnetoresistance (MR) ratios in both the\\nMTJs. We find that in both the MTJs, the MR ratio decreases as the bias voltage\\nincreases and finally vanishes at a critical bias voltage $V_{\\\\rm c}$. We also\\nfind that the critical bias voltage $V_{\\\\rm c}$ of the MgAl${}_2$O${}_4$-based\\nMTJ is clearly larger than that of the MgO-based MTJ. Since the in-plane\\nlattice constant of the Fe/MgAl${}_2$O${}_4$/Fe(001) supercell is twice that of\\nthe Fe/MgO/Fe(001) one, the Fe electrodes in the MgAl${}_2$O${}_4$-based MTJs\\nhave an identical band structure to that obtained by folding the Fe band\\nstructure of the MgO-based MTJs in the Brillouin zone of the in-plane wave\\nvector. We show that such a difference in the Fe band structure is the origin\\nof the difference in the critical bias voltage $V_{\\\\rm c}$ between the\\nMgAl${}_2$O${}_4$- and MgO-based MTJs.\\n\",\n",
       " '  Extensive efforts have been devoted to recognizing facial action units (AUs).\\nHowever, it is still challenging to recognize AUs from spontaneous facial\\ndisplays especially when they are accompanied with speech. Different from all\\nprior work that utilized visual observations for facial AU recognition, this\\npaper presents a novel approach that recognizes speech-related AUs exclusively\\nfrom audio signals based on the fact that facial activities are highly\\ncorrelated with voice during speech. Specifically, dynamic and physiological\\nrelationships between AUs and phonemes are modeled through a continuous time\\nBayesian network (CTBN); then AU recognition is performed by probabilistic\\ninference via the CTBN model.\\nA pilot audiovisual AU-coded database has been constructed to evaluate the\\nproposed audio-based AU recognition framework. The database consists of a\\n\"clean\" subset with frontal and neutral faces and a challenging subset\\ncollected with large head movements and occlusions. Experimental results on\\nthis database show that the proposed CTBN model achieves promising recognition\\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\\nvisual-based methods especially for those AUs that are activated at low\\nintensities or \"hardly visible\" in the visual channel. Furthermore, the CTBN\\nmodel yields more impressive recognition performance on the challenging subset,\\nwhere the visual-based approaches suffer significantly.\\n',\n",
       " '  We found an easy and quick post-learning method named \"Icing on the Cake\" to\\nenhance a classification performance in deep learning. The method is that we\\ntrain only the final classifier again after an ordinary training is done.\\n',\n",
       " '  As traditional neural network consumes a significant amount of computing\\nresources during back propagation, \\\\citet{Sun2017mePropSB} propose a simple yet\\neffective technique to alleviate this problem. In this technique, only a small\\nsubset of the full gradients are computed to update the model parameters. In\\nthis paper we extend this technique into the Convolutional Neural Network(CNN)\\nto reduce calculation in back propagation, and the surprising results verify\\nits validity in CNN: only 5\\\\% of the gradients are passed back but the model\\nstill achieves the same effect as the traditional CNN, or even better. We also\\nshow that the top-$k$ selection of gradients leads to a sparse calculation in\\nback propagation, which may bring significant computational benefits for high\\ncomputational complexity of convolution operation in CNN.\\n',\n",
       " '  Mobile robots are increasingly being used to assist with active pursuit and\\nlaw enforcement. One major limitation for such missions is the resource\\n(battery) allocated to the robot. Factors like nature and agility of evader,\\nterrain over which pursuit is being carried out, plausible traversal velocity\\nand the amount of necessary data to be collected all influence how long the\\nrobot can last in the field and how far it can travel. In this paper, we\\ndevelop an analytical model that analyzes the energy utilization for a variety\\nof components mounted on a robot to estimate the maximum operational range\\nachievable by the robot operating on a single battery discharge. We categorize\\nthe major consumers of energy as: 1.) ancillary robotic functions such as\\ncomputation, communication, sensing etc., and 2.) maneuvering which involves\\npropulsion, steering etc. Both these consumers draw power from the common power\\nsource but the achievable range is largely affected by the proportion of power\\navailable for maneuvering. For this case study, we performed experiments with\\nreal robots on planar and graded surfaces and evaluated the estimation error\\nfor each case.\\n',\n",
       " '  Mixed effects models are widely used to describe heterogeneity in a\\npopulation. A crucial issue when adjusting such a model to data consists in\\nidentifying fixed and random effects. From a statistical point of view, it\\nremains to test the nullity of the variances of a given subset of random\\neffects. Some authors have proposed to use the likelihood ratio test and have\\nestablished its asymptotic distribution in some particular cases. Nevertheless,\\nto the best of our knowledge, no general variance components testing procedure\\nhas been fully investigated yet. In this paper, we study the likelihood ratio\\ntest properties to test that the variances of a general subset of the random\\neffects are equal to zero in both linear and nonlinear mixed effects model,\\nextending the existing results. We prove that the asymptotic distribution of\\nthe test is a chi-bar-square distribution, that is to say a mixture of\\nchi-square distributions, and we identify the corresponding weights. We\\nhighlight in particular that the limiting distribution depends on the presence\\nof correlations between the random effects but not on the linear or nonlinear\\nstructure of the mixed effects model. We illustrate the finite sample size\\nproperties of the test procedure through simulation studies and apply the test\\nprocedure to two real datasets of dental growth and of coucal growth.\\n',\n",
       " '  We show that a positive Borel measure of positive finite total mass, on\\ncompact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic\\nsolution to the Monge-Ampere equation if and only if it is dominated locally by\\nMonge-Ampere measures of Holder continuous plurisubharmonic functions.\\n',\n",
       " \"  The Large European Array for Pulsars combines Europe's largest radio\\ntelescopes to form a tied-array telescope that provides high signal-to-noise\\nobservations of millisecond pulsars (MSPs) with the objective to increase the\\nsensitivity of detecting low-frequency gravitational waves. As part of this\\nendeavor we have developed a software correlator and beamformer which enables\\nthe formation of a tied-array beam from the raw voltages from each of\\ntelescopes. We explain the concepts and techniques involved in the process of\\nadding the raw voltages coherently. We further present the software processing\\npipeline that is specifically designed to deal with data from widely spaced,\\ninhomogeneous radio telescopes and describe the steps involved in preparing,\\ncorrelating and creating the tied-array beam. This includes polarization\\ncalibration, bandpass correction, frequency dependent phase correction,\\ninterference mitigation and pulsar gating. A link is provided where the\\nsoftware can be obtained.\\n\",\n",
       " '  In this paper, we discuss how a suitable family of tensor kernels can be used\\nto efficiently solve nonparametric extensions of $\\\\ell^p$ regularized learning\\nmethods. Our main contribution is proposing a fast dual algorithm, and showing\\nthat it allows to solve the problem efficiently. Our results contrast recent\\nfindings suggesting kernel methods cannot be extended beyond Hilbert setting.\\nNumerical experiments confirm the effectiveness of the method.\\n',\n",
       " '  Deep Learning models are vulnerable to adversarial examples, i.e.\\\\ images\\nobtained via deliberate imperceptible perturbations, such that the model\\nmisclassifies them with high confidence. However, class confidence by itself is\\nan incomplete picture of uncertainty. We therefore use principled Bayesian\\nmethods to capture model uncertainty in prediction for observing adversarial\\nmisclassification. We provide an extensive study with different Bayesian neural\\nnetworks attacked in both white-box and black-box setups. The behaviour of the\\nnetworks for noise, attacks and clean test data is compared. We observe that\\nBayesian neural networks are uncertain in their predictions for adversarial\\nperturbations, a behaviour similar to the one observed for random Gaussian\\nperturbations. Thus, we conclude that Bayesian neural networks can be\\nconsidered for detecting adversarial examples.\\n',\n",
       " '  The recent discovery of the planetary system hosted by the ultracool dwarf\\nstar TRAPPIST-1 could open new perspectives into the investigation of planetary\\nclimates of Earth-sized exoplanets, their atmospheres and their possible\\nhabitability. In this paper, we use a simple climate-vegetation energy-balance\\nmodel to study the climate of the seven TRAPPIST-1 planets and the climate\\ndependence on the global albedo, on the fraction of vegetation that could cover\\ntheir surfaces and on the different greenhouse conditions. The model allows us\\nto investigate whether liquid water could be maintained on the planetary\\nsurfaces (i.e., by defining a \"surface water zone\") in different planetary\\nconditions, with or without the presence of greenhouse effect.\\nIt is shown that planet TRAPPIST-1d seems to be the most stable from an\\nEarth-like perspective, since it resides in the surface water zone for a wide\\nrange of reasonable values of the model parameters. Moreover, according to the\\nmodel outer planets (f, g and h) cannot host liquid water on their surfaces,\\neven for Earth-like conditions, entering a snowball state. Although very\\nsimple, the model allows to extract the main features of the TRAPPIST-1\\nplanetary climates.\\n',\n",
       " '  We show that $\\\\mathbb{Q}$-Fano varieties of fixed dimension with\\nanti-canonical degrees and alpha-invariants bounded from below form a bounded\\nfamily. As a corollary, K-semistable $\\\\mathbb{Q}$-Fano varieties of fixed\\ndimension with anti-canonical degrees bounded from below form a bounded family.\\n',\n",
       " '  We consider quantum, nondterministic and probabilistic versions of known\\ncomputational model Ordered Read-$k$-times Branching Programs or Ordered Binary\\nDecision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We\\nshow width hierarchy for complexity classes of Boolean function computed by\\nthese models and discuss relation between different variants of $k$-OBDD.\\n',\n",
       " '  We study the relation between the microscopic properties of a many-body\\nsystem and the electron spectra, experimentally accessible by photoemission. In\\na recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the\\n\"fluctuation diagnostics\" approach, to extract the dominant wave vector\\ndependent bosonic fluctuations from the electronic self-energy. Here, we first\\nreformulate the theory in terms of fermionic modes, to render its connection\\nwith resonance valence bond (RVB) fluctuations more transparent. Secondly, by\\nusing a large-U expansion, where U is the Coulomb interaction, we relate the\\nfluctuations to real space correlations. Therefore, it becomes possible to\\nstudy how electron spectra are related to charge, spin, superconductivity and\\nRVB-like real space correlations, broadening the analysis of an earlier work\\n[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap\\nphysics of the two-dimensional Hubbard model, studied in the dynamical cluster\\napproximation. We perform calculations for embedded clusters with up to 32\\nsites, having three inequivalent K-points at the Fermi surface. We find that as\\nU is increased, correlation functions gradually attain values consistent with\\nan RVB state. This first happens for correlation functions involving the\\nantinodal point and gradually spreads to the nodal point along the Fermi\\nsurface. Simultaneously a pseudogap opens up along the Fermi surface. We relate\\nthis to a crossover from a Kondo-like state to an RVB-like localized cluster\\nstate and to the presence of RVB and spin fluctuations. These changes are\\ncaused by a strong momentum dependence in the cluster bath-couplings along the\\nFermi surface. We also show, from a more algorithmic perspective, how the\\ntime-consuming calculations in fluctuation diagnostics can be drastically\\nsimplified.\\n',\n",
       " '  We study a generic one-dimensional model for an intracellular cargo driven by\\nN motor proteins against an external applied force. The model includes\\nmotor-cargo and motor-motor interactions. The cargo motion is described by an\\nover-damped Langevin equation, while motor dynamics is specified by hopping\\nrates which follow a local detailed balance condition with respect to change in\\nenergy per hopping event. Based on this model, we show that the stall force,\\nthe mean external force corresponding to zero mean cargo velocity, is\\ncompletely independent of the details of the interactions and is, therefore,\\nalways equal to the sum of the stall forces of the individual motors. This\\nexact result is arrived on the basis of a simple assumption: the (macroscopic)\\nstate of stall of the cargo is analogous to a state of thermodynamic\\nequilibrium, and is characterized by vanishing net probability current between\\nany two microstates, with the latter specified by motor positions relative to\\nthe cargo. The corresponding probability distribution of the microstates under\\nstall is also determined. These predictions are in complete agreement with\\nnumerical simulations, carried out using specific forms of interaction\\npotentials.\\n',\n",
       " '  We report experiments on an agarose gel tablet loaded with camphoric acid\\n(c-boat) set into self-motion by interfacial tension gradients at the air-water\\ninterface. We observe three distinct modes of c-boat motion: harmonic mode\\nwhere the c-boat speed oscillates sinusoidally in time, a steady mode where the\\nc-boat maintains constant speed, and a relaxation oscillation mode where the\\nc-boat maintains near-zero speed between sudden jumps in speed and position at\\nregular time intervals. Whereas all three modes have been separately reported\\nbefore in different systems, we show they belong to a common description.\\nThrough control of the air-water surface tension with Sodium Dodecyl Sulfate\\n(SDS), we experimentally deduce the three self-propulsive modes result from\\nsurface tension difference between Camphoric Acid (CA) and the ambient\\nsurroundings.\\n',\n",
       " \"  Artificial intelligence is revolutionizing our lives at an ever increasing\\npace. At the heart of this revolution is the recent advancements in deep neural\\nnetworks (DNN), learning to perform sophisticated, high-level tasks. However,\\ntraining DNNs requires massive amounts of data and is very computationally\\nintensive. Gaining analytical understanding of the solutions found by DNNs can\\nhelp us devise more efficient training algorithms, replacing the commonly used\\nmthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and\\nshow that, indeed, direct computation of the solutions is possible in many\\ncases. We show that a high performing setup used in DNNs introduces a\\nseparation of time-scales in the training dynamics, allowing SGD to train\\nlayers from the lowest (closest to input) to the highest. We then show that for\\neach layer, the distribution of solutions found by SGD can be estimated using a\\nclass-based principal component analysis (PCA) of the layer's input. This\\nfinding allows us to forgo SGD entirely and directly derive the DNN parameters\\nusing this class-based PCA, which can be well estimated using significantly\\nless data than SGD. We implement these results on image datasets MNIST, CIFAR10\\nand CIFAR100 and find that, in fact, layers derived using our class-based PCA\\nperform comparable or superior to neural networks of the same size and\\narchitecture trained using SGD. We also confirm that the class-based PCA often\\nconverges using a fraction of the data required for SGD. Thus, using our method\\ntraining time can be reduced both by requiring less training data than SGD, and\\nby eliminating layers in the costly backpropagation step of the training.\\n\",\n",
       " '  In this letter, we define the homodyne $q$-deformed quadrature operator.\\nAnalytic expression for the wavefunctions of $q$-deformed oscillator in the\\nquadrature basis are found. Furthermore, we compute the explicit analytical\\nexpression for the tomogram of the $q$-deformed coherent states by finding the\\neigenstates of the $q$-deformed quadrature operator.\\n',\n",
       " '  Graph edit distance (GED) is an important similarity measure adopted in a\\nsimilarity-based analysis between two graphs, and computing GED is a primitive\\noperator in graph database analysis. Partially due to the NP-hardness, the\\nexisting techniques for computing GED are only able to process very small\\ngraphs with less than 30 vertices. Motivated by this, in this paper we\\nsystematically study the problems of both GED computation, and GED verification\\n(i.e., verify whether the GED between two graphs is no larger than a user-given\\nthreshold). Firstly, we develop a unified framework that can be instantiated\\ninto either a best-first search approach AStar+ or a depth-first search\\napproach DFS+. Secondly, we design anchor-aware lower bound estimation\\ntechniques to compute tighter lower bounds for intermediate search states,\\nwhich significantly reduce the search spaces of both AStar+ and DFS+. We also\\npropose efficient techniques to compute the lower bounds. Thirdly, based on our\\nunified framework, we contrast AStar+ with DFS+ regarding their time and space\\ncomplexities, and recommend that AStar+ is better than DFS+ by having a much\\nsmaller search space. Extensive empirical studies validate that AStar+ performs\\nbetter than DFS+, and show that our AStar+-BMa approach outperforms the\\nstate-of-the-art technique by more than four orders of magnitude.\\n',\n",
       " '  We introduce canonical measures on a locally finite simplicial complex $K$\\nand study their asymptotic behavior under infinitely many barycentric\\nsubdivisions. We also compute the face polynomial of the asymptotic link and\\ndual block of a simplex in the $d^{th}$ barycentric subdivision $Sd^d(K)$ of\\n$K$, $d\\\\gg0$. It is almost everywhere constant. When $K$ is finite, we study\\nthe limit face polynomial of $Sd^d(K)$ after F.Brenti-V.Welker and\\nE.Delucchi-A.Pixton-L.Sabalka.\\n',\n",
       " '  Reusing passwords across multiple websites is a common practice that\\ncompromises security. Recently, Blum and Vempala have proposed password\\nstrategies to help people calculate, in their heads, passwords for different\\nsites without dependence on third-party tools or external devices. Thus far,\\nthe security and efficiency of these \"mental algorithms\" has been analyzed only\\ntheoretically. But are such methods usable? We present the first usability\\nstudy of humanly computable password strategies, involving a learning phase (to\\nlearn a password strategy), then a rehearsal phase (to login to a few\\nwebsites), and multiple follow-up tests. In our user study, with training,\\nparticipants were able to calculate a deterministic eight-character password\\nfor an arbitrary new website in under 20 seconds.\\n',\n",
       " '  As affordability pressures and tight rental markets in global cities mount,\\nonline shared accommodation sites proliferate. Home sharing arrangements\\npresent dilemmas for planning that aims to improve health and safety standards,\\nwhile supporting positives such as the usage of dormant stock and the relieving\\nof rental pressures on middle/lower income earners. Currently, no formal data\\nexists on this internationally growing trend. Here, we present a first\\nquantitative glance on shared accommodation practices across all major urban\\ncenters of Australia enabled via collection and analysis of thousands of online\\nlistings. We examine, countrywide, the spatial and short time scale temporal\\ncharacteristics of this market, along with preliminary analysis on rents,\\ndwelling types and other characteristics. Findings have implications for\\nhousing policy makers and planning practitioners seeking to monitor and respond\\nto housing policy and affordability pressures in formal and informal housing\\nmarkets.\\n',\n",
       " '  Interbank markets are often characterised in terms of a core-periphery\\nnetwork structure, with a highly interconnected core of banks holding the\\nmarket together, and a periphery of banks connected mostly to the core but not\\ninternally. This paradigm has recently been challenged for short time scales,\\nwhere interbank markets seem better characterised by a bipartite structure with\\nmore core-periphery connections than inside the core. Using a novel\\ncore-periphery detection method on the eMID interbank market, we enrich this\\npicture by showing that the network is actually characterised by multiple\\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\\nstructures occurs by shortening the temporal scale of data aggregation. We\\nfurther show how the global financial crisis transformed the market, in terms\\nof composition, multiplicity and internal organisation of core-periphery pairs.\\nBy unveiling such a fine-grained organisation and transformation of the\\ninterbank market, our method can find important applications in the\\nunderstanding of how distress can propagate over financial networks.\\n',\n",
       " \"  We present a novel algorithm that uses exact learning and abstraction to\\nextract a deterministic finite automaton describing the state dynamics of a\\ngiven trained RNN. We do this using Angluin's L* algorithm as a learner and the\\ntrained RNN as an oracle. Our technique efficiently extracts accurate automata\\nfrom trained RNNs, even when the state vectors are large and require fine\\ndifferentiation.\\n\",\n",
       " '  Everything in the world is being connected, and things are becoming\\ninteractive. The future of the interactive world depends on the future Internet\\nof Things (IoT). Software-defined networking (SDN) technology, a new paradigm\\nin the networking area, can be useful in creating an IoT because it can handle\\ninteractivity by controlling physical devices, transmission of data among them,\\nand data acquisition. However, digital signage can be one of the promising\\ntechnologies in this era of technology that is progressing toward the\\ninteractive world, connecting users to the IoT network through device-to-device\\ncommunication technology. This article illustrates a novel prototype that is\\nmainly focused on a smart digital signage system comprised of software-defined\\nIoT (SD-IoT) and invisible image sensor communication technology. We have\\nproposed an SDN scheme with a view to initiating its flexibility and\\ncompatibility for an IoT network-based smart digital signage system. The idea\\nof invisible communication can make the users of the technology trendier to it,\\nand the usage of unused resources such as images and videos can be ensured. In\\naddition, this communication has paved the way for interactivity between the\\nuser and digital signage, where the digital signage and the camera of a\\nsmartphone can be operated as a transmitter and a receiver, respectively. The\\nproposed scheme might be applicable to real-world applications because SDN has\\nthe flexibility to adapt with the alteration of network status without any\\nhardware modifications while displays and smartphones are available everywhere.\\nA performance analysis of this system showed the advantages of an SD-IoT\\nnetwork over an Internet protocol-based IoT network considering a queuing\\nanalysis for a dynamic link allocation process in the case of user access to\\nthe IoT network.\\n',\n",
       " '  Recently, an Atacama Large Millimeter/submillimeter Array (ALMA) observation\\nof the water snow line in the protoplanetary disk around the FU Orionis star\\nV883 Ori was reported. The radial variation of the spectral index at\\nmm-wavelengths around the snow line was interpreted as being due to a pileup of\\nparticles interior to the snow line. However, radial transport of solids in the\\nouter disk operates on timescales much longer than the typical timescale of an\\nFU Ori outburst ($10^{1}$--$10^{2}$ yr). Consequently, a steady-state pileup is\\nunlikely. We argue that it is only necessary to consider water evaporation and\\nre-coagulation of silicates to explain the recent ALMA observation of V883 Ori\\nbecause these processes are short enough to have had their impact since the\\noutburst. Our model requires the inner disk to have already been optically\\nthick before the outburst, and our results suggest that the carbon content of\\npebbles is low.\\n',\n",
       " \"  The existence of weak solutions to the stationary Navier-Stokes equations in\\nthe whole plane $\\\\mathbb{R}^2$ is proven. This particular geometry was the only\\ncase left open since the work of Leray in 1933. The reason is that due to the\\nabsence of boundaries the local behavior of the solutions cannot be controlled\\nby the enstrophy in two dimensions. We overcome this difficulty by constructing\\napproximate weak solutions having a prescribed mean velocity on some given\\nbounded set. As a corollary, we obtain infinitely many weak solutions in\\n$\\\\mathbb{R}^2$ parameterized by this mean velocity, which is reminiscent of the\\nexpected convergence of the velocity field at large distances to any prescribed\\nconstant vector field. This explicit parameterization of the weak solutions\\nallows us to prove a weak-strong uniqueness theorem for small data. The\\nquestion of the asymptotic behavior of the weak solutions remains however open,\\nwhen the uniqueness theorem doesn't apply.\\n\",\n",
       " '  New results on the Baire product problem are presented. It is shown that an\\narbitrary product of almost locally ccc Baire spaces is Baire; moreover, the\\nproduct of a Baire space and a 1st countable space which is $\\\\beta$-unfavorable\\nin the strong Choquet game is Baire.\\n',\n",
       " '  In the Ultimatum Game (UG) one player, named \"proposer\", has to decide how to\\nallocate a certain amount of money between herself and a \"responder\". If the\\noffer is greater than or equal to the responder\\'s minimum acceptable offer\\n(MAO), then the money is split as proposed, otherwise, neither the proposer nor\\nthe responder get anything. The UG has intrigued generations of behavioral\\nscientists because people in experiments blatantly violate the equilibrium\\npredictions that self-interested proposers offer the minimum available non-zero\\namount, and self-interested responders accept. Why are these predictions\\nviolated? Previous research has mainly focused on the role of social\\npreferences. Little is known about the role of general moral preferences for\\ndoing the right thing, preferences that have been shown to play a major role in\\nother social interactions (e.g., Dictator Game and Prisoner\\'s Dilemma). Here I\\ndevelop a theoretical model and an experiment designed to pit social\\npreferences against moral preferences. I find that, although people recognize\\nthat offering half and rejecting low offers are the morally right things to do,\\nmoral preferences have no causal impact on UG behavior. The experimental data\\nare indeed well fit by a model according to which: (i) high UG offers are\\nmotivated by inequity aversion and, to a lesser extent, self-interest; (ii)\\nhigh MAOs are motivated by inequity aversion.\\n',\n",
       " \"  In this work, we study the tradeoffs between the error probabilities of\\nclassical-quantum channels and the blocklength $n$ when the transmission rates\\napproach the channel capacity at a rate slower than $1/\\\\sqrt{n}$, a research\\ntopic known as moderate deviation analysis. We show that the optimal error\\nprobability vanishes under this rate convergence. Our main technical\\ncontributions are a tight quantum sphere-packing bound, obtained via Chaganty\\nand Sethuraman's concentration inequality in strong large deviation theory, and\\nasymptotic expansions of error-exponent functions. Moderate deviation analysis\\nfor quantum hypothesis testing is also established. The converse directly\\nfollows from our channel coding result, while the achievability relies on a\\nmartingale inequality.\\n\",\n",
       " '  Constructing tests or confidence regions that control over the error rates in\\nthe long-run is probably one of the most important problem in statistics. Yet,\\nthe theoretical justification for most methods in statistics is asymptotic. The\\nbootstrap for example, despite its simplicity and its widespread usage, is an\\nasymptotic method. There are in general no claim about the exactness of\\ninferential procedures in finite sample. In this paper, we propose an\\nalternative to the parametric bootstrap. We setup general conditions to\\ndemonstrate theoretically that accurate inference can be claimed in finite\\nsample.\\n',\n",
       " '  Generalized Bäcklund-Darboux transformations (GBDTs) of discrete\\nskew-selfadjoint Dirac systems have been successfully used for explicit solving\\nof direct and inverse problems of Weyl-Titchmarsh theory. During explicit\\nsolving of the direct and inverse problems, we considered GBDTs of the trivial\\ninitial systems. However, GBDTs of arbitrary discrete skew-selfadjoint Dirac\\nsystems are important as well and we introduce these transformations in the\\npresent paper. The obtained results are applied to the construction of explicit\\nsolutions of the interesting related non-stationary systems.\\n',\n",
       " '  We describe a procedure called panel collapse for replacing a CAT(0) cube\\ncomplex $\\\\Psi$ by a \"lower complexity\" CAT(0) cube complex $\\\\Psi_\\\\bullet$\\nwhenever $\\\\Psi$ contains a codimension-$2$ hyperplane that is extremal in one\\nof the codimension-$1$ hyperplanes containing it. Although $\\\\Psi_\\\\bullet$ is\\nnot in general a subcomplex of $\\\\Psi$, it is a subspace consisting of a\\nsubcomplex together with some cubes that sit inside $\\\\Psi$ \"diagonally\". The\\nhyperplanes of $\\\\Psi_\\\\bullet$ extend to hyperplanes of $\\\\Psi$. Applying this\\nprocedure, we prove: if a group $G$ acts cocompactly on a CAT(0) cube complex\\n$\\\\Psi$, then there is a CAT(0) cube complex $\\\\Omega$ so that $G$ acts\\ncocompactly on $\\\\Omega$ and for each hyperplane $H$ of $\\\\Omega$, the stabiliser\\nin $G$ of $H$ acts on $H$ essentially.\\nUsing panel collapse, we obtain a new proof of Stallings\\'s theorem on groups\\nwith more than one end. As another illustrative example, we show that panel\\ncollapse applies to the exotic cubulations of free groups constructed by Wise.\\nNext, we show that the CAT(0) cube complexes constructed by Cashen-Macura can\\nbe collapsed to trees while preserving all of the necessary group actions. (It\\nalso illustrates that our result applies to actions of some non-discrete\\ngroups.) We also discuss possible applications to quasi-isometric rigidity for\\ncertain classes of graphs of free groups with cyclic edge groups. Panel\\ncollapse is also used in forthcoming work of the first-named author and Wilton\\nto study fixed-point sets of finite subgroups of $\\\\mathrm{Out}(F_n)$ on the\\nfree splitting complex.\\n',\n",
       " '  The recently proposed self-ensembling methods have achieved promising results\\nin deep semi-supervised learning, which penalize inconsistent predictions of\\nunlabeled data under different perturbations. However, they only consider\\nadding perturbations to each single data point, while ignoring the connections\\nbetween data samples. In this paper, we propose a novel method, called Smooth\\nNeighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on\\nthe predictions of the teacher model, i.e., the implicit self-ensemble of\\nmodels. Then the graph serves as a similarity measure with respect to which the\\nrepresentations of \"similar\" neighboring points are learned to be smooth on the\\nlow-dimensional manifold. We achieve state-of-the-art results on\\nsemi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for\\nCIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,\\nthe improvements are significant when the labels are fewer. For the\\nnon-augmented MNIST with only 20 labels, the error rate is reduced from\\nprevious 4.81% to 1.36%. Our method also shows robustness to noisy labels.\\n',\n",
       " '  Efficient algorithms and techniques to detect and identify large flows in a\\nhigh throughput traffic stream in the SDN match-and-action model are presented.\\nThis is in contrast to previous work that either deviated from the match and\\naction model by requiring additional switch level capabilities or did not\\nexploit the SDN data plane. Our construction has two parts; (a) how to sample\\nin an SDN match and action model, (b) how to detect large flows efficiently and\\nin a scalable way, in the SDN model.\\nOur large flow detection methods provide high accuracy and present a good and\\npractical tradeoff between switch - controller traffic, and the number of\\nentries required in the switch flow table. Based on different parameters, we\\ndifferentiate between heavy flows, elephant flows and bulky flows and present\\nefficient algorithms to detect flows of the different types.\\nAdditionally, as part of our heavy flow detection scheme, we present sampling\\nmethods to sample packets with arbitrary probability $p$ per packet or per byte\\nthat traverses an SDN switch.\\nFinally, we show how our algorithms can be adapted to a distributed\\nmonitoring SDN setting with multiple switches, and easily scale with the number\\nof monitoring switches.\\n',\n",
       " '  With the tremendous increase of the Internet traffic, achieving the best\\nperformance with limited resources is becoming an extremely urgent problem. In\\norder to address this concern, in this paper, we build an optimization problem\\nwhich aims to maximize the total utility of traffic flows with the capacity\\nconstraint of nodes and links in the network. Based on Duality Theory, we\\npropose an iterative algorithm which adjusts the rates of traffic flows and\\ncapacity of nodes and links simultaneously to maximize the total utility.\\nSimulation results show that our algorithm performs better than the NUP\\nalgorithm on BA and ER network models, which has shown to get the best\\nperformance so far. Since our research combines the topology information with\\ncapacity constraint, it may give some insights for resource allocation in real\\ncommunication networks.\\n',\n",
       " '  We consider a fundamental integer programming (IP) model for cost-benefit\\nanalysis flood protection through dike building in the Netherlands, due to\\nVerweij and Zwaneveld.\\nExperimental analysis with data for the Ijsselmeer lead to integral optimal\\nsolution of the linear programming relaxation of the IP model.\\nThis naturally led to the question of integrality of the polytope associated\\nwith the IP model.\\nIn this paper we first give a negative answer to this question by\\nestablishing non-integrality of the polytope.\\nSecond, we establish natural conditions that guarantee the linear programming\\nrelaxation of the IP model to be integral.\\nWe then test the most recent data on flood probabilities, damage and\\ninvestment costs of the IJsselmeer for these conditions.\\nThird, we show that the IP model can be solved in polynomial time when the\\nnumber of dike segments, or the number of feasible barrier heights, are\\nconstant.\\n',\n",
       " '  In wireless communication, heterogeneous technologies such as WiFi, ZigBee\\nand BlueTooth operate in the same ISM band.With the exponential growth in the\\nnumber of wireless devices, the ISM band becomes more and more crowded. These\\nheterogeneous devices have to compete with each other to access spectrum\\nresources, generating cross-technology interference (CTI). Since CTI may\\ndestroy wireless communication, this field is facing an urgent and challenging\\nneed to investigate spectrum efficiency under CTI. In this paper, we introduce\\na novel framework to address this problem from two aspects. On the one hand,\\nfrom the perspective of each communication technology itself, we propose novel\\nchannel/link models to capture the channel/link status under CTI. On the other\\nhand, we investigate spectrum efficiency from the perspective by taking all\\nheterogeneous technologies as a whole and building crosstechnology\\ncommunication among them. The capability of direct communication among\\nheterogeneous devices brings great opportunities to harmoniously sharing the\\nspectrum with collaboration rather than competition.\\n',\n",
       " '  We have derived background corrected intensities of 3-50 MeV galactic\\nelectrons observed by Voyager 1 as it passes through the heliosheath from 95 to\\n122 AU. The overall intensity change of the background corrected data from the\\ninner to the outer boundary of the heliosheath is a maximum of a factor ~100 at\\n15 MeV. At lower energies this fractional change becomes less and the corrected\\nelectron spectra in the heliosheath becomes progressively steeper, reaching\\nvalues ~ -2.5 for the spectral index just outside of the termination shock. At\\nhigher energies the spectra of electrons has an exponent changing from the\\nnegative LIS spectral index of -1.3 to values approaching zero in the\\nheliosheath as a result of the solar modulation of the galactic electron\\ncomponent. The large modulation effects observed below ~100 MV are possible\\nevidence for enhanced diffusion as part of the modulation process for electrons\\nin the heliosheath.\\n',\n",
       " \"  The `beta' is one of the key quantities in the capital asset pricing model\\n(CAPM). In statistical language, the beta can be viewed as the slope of the\\nregression line fitted to financial returns on the market against the returns\\non the asset under consideration. The insurance counterpart of CAPM, called the\\nweighted insurance pricing model (WIPM), gives rise to the so-called\\nweighted-Gini beta. The aforementioned two betas may or may not coincide,\\ndepending on the form of the underlying regression function, and this has\\nprofound implications when designing portfolios and allocating risk capital. To\\nfacilitate these tasks, in this paper we develop large-sample statistical\\ninference results that, in a straightforward fashion, imply confidence\\nintervals for, and hypothesis tests about, the equality of the two betas.\\n\",\n",
       " \"  In partially observed environments, it can be useful for a human to provide\\nthe robot with declarative information that represents probabilistic relational\\nconstraints on properties of objects in the world, augmenting the robot's\\nsensory observations. For instance, a robot tasked with a search-and-rescue\\nmission may be informed by the human that two victims are probably in the same\\nroom. An important question arises: how should we represent the robot's\\ninternal knowledge so that this information is correctly processed and combined\\nwith raw sensory information? In this paper, we provide an efficient belief\\nstate representation that dynamically selects an appropriate factoring,\\ncombining aspects of the belief when they are correlated through information\\nand separating them when they are not. This strategy works in open domains, in\\nwhich the set of possible objects is not known in advance, and provides\\nsignificant improvements in inference time over a static factoring, leading to\\nmore efficient planning for complex partially observed tasks. We validate our\\napproach experimentally in two open-domain planning problems: a 2D discrete\\ngridworld task and a 3D continuous cooking task. A supplementary video can be\\nfound at this http URL.\\n\",\n",
       " \"  Open problems abound in the theory of complex networks, which has found\\nsuccessful application to diverse fields of science. With the aim of further\\nadvancing the understanding of the brain's functional connectivity, we propose\\nto evaluate a network metric which we term the geodesic entropy. This entropy,\\nin a way that can be made precise, quantifies the Shannon entropy of the\\ndistance distribution to a specific node from all other nodes. Measurements of\\ngeodesic entropy allow for the characterization of the structural information\\nof a network that takes into account the distinct role of each node into the\\nnetwork topology. The measurement and characterization of this structural\\ninformation has the potential to greatly improve our understanding of sustained\\nactivity and other emergent behaviors in networks, such as self-organized\\ncriticality sometimes seen in such contexts. We apply these concepts and\\nmethods to study the effects of how the psychedelic Ayahuasca affects the\\nfunctional connectivity of the human brain. We show that the geodesic entropy\\nis able to differentiate the functional networks of the human brain in two\\ndifferent states of consciousness in the resting state: (i) the ordinary waking\\nstate and (ii) a state altered by ingestion of the Ayahuasca. The entropy of\\nthe nodes of brain networks from subjects under the influence of Ayahuasca\\ndiverge significantly from those of the ordinary waking state. The functional\\nbrain networks from subjects in the altered state have, on average, a larger\\ngeodesic entropy compared to the ordinary state. We conclude that geodesic\\nentropy is a useful tool for analyzing complex networks and discuss how and why\\nit may bring even further valuable insights into the study of the human brain\\nand other empirical networks.\\n\",\n",
       " '  The high-energy non-thermal universe is dominated by power law-like spectra.\\nTherefore results in high-energy astronomy are often reported as parameters of\\npower law fits, or, in the case of a non-detection, as an upper limit assuming\\nthe underlying unseen spectrum behaves as a power law. In this paper I\\ndemonstrate a simple and powerful one-to-one relation of the integral upper\\nlimit in the two dimensional power law parameter space into the spectrum\\nparameter space and use this method to unravel the so far convoluted question\\nof the sensitivity of astroparticle telescopes.\\n',\n",
       " '  We develop estimates for the solutions and derive existence and uniqueness\\nresults of various local boundary value problems for Dirac equations that\\nimprove all relevant results known in the literature. With these estimates at\\nhand, we derive a general existence, uniqueness and regularity theorem for\\nsolutions of Dirac equations with such boundary conditions. We also apply these\\nestimates to a new nonlinear elliptic-parabolic problem, the Dirac-harmonic\\nheat flow on Riemannian spin manifolds. This problem is motivated by the\\nsupersymmetric nonlinear $\\\\sigma$-model and combines a harmonic heat flow type\\nequation with a Dirac equation that depends nonlinearly on the flow.\\n',\n",
       " \"  In modern election campaigns, political parties utilize social media to\\nadvertise their policies and candidates and to communicate to the electorate.\\nIn Japan's latest general election in 2017, the 48th general election for the\\nLower House, social media, especially Twitter, was actively used. In this\\npaper, we analyze the users who retweeted tweets of political parties on\\nTwitter during the election. Our aim is to clarify what kinds of users are\\ndiffusing (retweeting) tweets of political parties. The results indicate that\\nthe characteristics of retweeters of the largest ruling party (Liberal\\nDemocratic Party of Japan) and the largest opposition party (The Constitutional\\nDemocratic Party of Japan) were similar, even though the retweeters did not\\noverlap each other. We also found that a particular opposition party (Japanese\\nCommunist Party) had quite different characteristics from other political\\nparties.\\n\",\n",
       " \"  In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle\\nwith two equal bisectors is isosceles. But what about changing the bisectors to\\ncevians? Cevian is any line segment in a triangle with one endpoint on a vertex\\nof the triangle and other endpoint on the opposite side. Not for any pairs of\\nequal cevians the triangle is isosceles. Theorem. If for a triangle ABC there\\nare equal cevians issuing from A and B, which intersect on the bisector or on\\nthe median of the angle C, then AC=BC (so the triangle ABC is isosceles).\\nProposition. Let ABC be an isosceles triangle. Define circle C to be the circle\\nsymmetric relative to AB to the circumscribed circle of the triangle ABC. Then\\nthe locus of intersection points of pairs of equal cevians is the union of the\\nbase AB, the triangle's axis of symmetry, and the circle C.\\n\",\n",
       " '  Anomaly detecting as an important technical in cloud computing is applied to\\nsupport smooth running of the cloud platform. Traditional detecting methods\\nbased on statistic, analysis, etc. lead to the high false-alarm rate due to\\nnon-adaptive and sensitive parameters setting. We presented an online model for\\nanomaly detecting using machine learning theory. However, most existing methods\\nbased on machine learning linked all features from difference sub-systems into\\na long feature vector directly, which is difficult to both exploit the\\ncomplement information between sub-systems and ignore multi-view features\\nenhancing the classification performance. Aiming to this problem, the proposed\\nmethod automatic fuses multi-view features and optimize the discriminative\\nmodel to enhance the accuracy. This model takes advantage of extreme learning\\nmachine (ELM) to improve detection efficiency. ELM is the single hidden layer\\nneural network, which is transforming iterative solution the output weights to\\nsolution of linear equations and avoiding the local optimal solution. Moreover,\\nwe rank anomies according to the relationship between samples and the\\nclassification boundary, and then assigning weights for ranked anomalies,\\nretraining the classification model finally. Our method exploits the complement\\ninformation between sub-systems sufficiently, and avoids the influence from\\nimbalance dataset, therefore, deal with various challenges from the cloud\\ncomputing platform. We deploy the privately cloud platform by Openstack,\\nverifying the proposed model and comparing results to the state-of-the-art\\nmethods with better efficiency and simplicity.\\n',\n",
       " '  We survey the dimension theory of self-affine sets for general mathematical\\naudience. The article is in Finnish.\\n',\n",
       " '  Buoyancy-thermocapillary convection in a layer of volatile liquid driven by a\\nhorizontal temperature gradient arises in a variety of situations. Recent\\nstudies have shown that the composition of the gas phase, which is typically a\\nmixture of vapour and air, has a noticeable effect on the critical Marangoni\\nnumber describing the onset of convection as well as on the observed convection\\npattern. Specifically, as the total pressure or, equivalently, the average\\nconcentration of air is decreased, the threshold of the instability leading to\\nthe emergence of convective rolls is found to increase rather significantly. We\\npresent a linear stability analysis of the problem which shows that this trend\\ncan be readily understood by considering the transport of heat and vapour\\nthrough the gas phase. In particular, we show that transport in the gas phase\\nhas a noticeable effect even at atmospheric conditions, when phase change is\\ngreatly suppressed.\\n',\n",
       " '  In the paper we consider a graph model of message passing processes and\\npresent a method verification of message passing processes. The method is\\nillustrated by an example of a verification of sliding window protocol.\\n',\n",
       " '  In this article, we derive a Bayesian model to learning the sparse and low\\nrank PARAFAC decomposition for the observed tensor with missing values via the\\nelastic net, with property to find the true rank and sparse factor matrix which\\nis robust to the noise. We formulate efficient block coordinate descent\\nalgorithm and admax stochastic block coordinate descent algorithm to solve it,\\nwhich can be used to solve the large scale problem. To choose the appropriate\\nrank and sparsity in PARAFAC decomposition, we will give a solution path by\\ngradually increasing the regularization to increase the sparsity and decrease\\nthe rank. When we find the sparse structure of the factor matrix, we can fixed\\nthe sparse structure, using a small to regularization to decreasing the\\nrecovery error, and one can choose the proper decomposition from the solution\\npath with sufficient sparse factor matrix with low recovery error. We test the\\npower of our algorithm on the simulation data and real data, which show it is\\npowerful.\\n',\n",
       " '  We introduce a new family of thermostat flows on the unit tangent bundle of\\nan oriented Riemannian $2$-manifold. Suitably reparametrised, these flows\\ninclude the geodesic flow of metrics of negative Gauss curvature and the\\ngeodesic flow induced by the Hilbert metric on the quotient surface of\\ndivisible convex sets. We show that the family of flows can be parametrised in\\nterms of certain weighted holomorphic differentials and investigate their\\nproperties. In particular, we prove that they admit a dominated splitting and\\nwe identify special cases in which the flows are Anosov. In the latter case, we\\nstudy when they admit an invariant measure in the Lebesgue class and the\\nregularity of the weak foliations.\\n',\n",
       " '  We introduce the shifted quantum affine algebras. They map homomorphically\\ninto the quantized $K$-theoretic Coulomb branches of $3d\\\\ {\\\\mathcal N}=4$ SUSY\\nquiver gauge theories. In type $A$, they are endowed with a coproduct, and they\\nact on the equivariant $K$-theory of parabolic Laumon spaces. In type $A_1$,\\nthey are closely related to the open relativistic quantum Toda lattice of type\\n$A$.\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Emails\n",
    "import re\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad20f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f05ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e8067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         predictive models allow subject specific infe...\n",
       "1         rotation invariance and translation invarianc...\n",
       "2         we introduce and develop the notion of spheri...\n",
       "3         the stochastic landau lifshitz gilbert llg eq...\n",
       "4         fourier transform infra red ftir spectra of s...\n",
       "                               ...                        \n",
       "20967     machine learning is finding increasingly broa...\n",
       "20968     polycrystalline diamond coatings have been gr...\n",
       "20969     we present a new approach for identifying sit...\n",
       "20970     the sum of log normal variates is encountered...\n",
       "20971     recently optional stopping has been a subject...\n",
       "Length: 20972, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do some preproc3essing and move on \n",
    "# here we are going to use regex for data cleaning\n",
    "import re\n",
    "data_processed = data.apply(lambda x: re.sub(r'[^a-zA-Z/s]+',' ',x).lower())\n",
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671a7a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Musham\n",
      "[nltk_data]     Malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Musham\n",
      "[nltk_data]     Malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7112914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets deal with stopwords \n",
    "# using nltk stopwords \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cbf549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'from',\n",
       " 'subject',\n",
       " 're',\n",
       " 'edu',\n",
       " 'use']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76544d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa4bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data_processed.apply(lambda x:' '.join([word for word in word_tokenize(x) if word not in stop_words and len(word)> 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1fd7f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Predictive models allow subject-specific inference when analyzing disease related alterations in neuroimaging data. Given a subject's data, inference can be made at two levels: global, i.e. identifiying condition presence for the subject, and local, i.e. detecting condition effect on each individual measurement extracted from the subject's data. While global inference is widely used, local inference, which can be used to form subject-specific effect maps, is rarely used because existing models often yield noisy detections composed of dispersed isolated islands. In this article, we propose a reconstruction method, named RSM, to improve subject-specific detections of predictive modeling approaches and in particular, binary classifiers. RSM specifically aims to reduce noise due to sampling error associated with using a finite sample of examples to train classifiers. The proposed method is a wrapper-type algorithm that can be used with different binary classifiers in a diagnostic manner, i.e. without information on condition presence. Reconstruction is posed as a Maximum-A-Posteriori problem with a prior model whose parameters are estimated from training data in a classifier-specific fashion. Experimental evaluation is performed on synthetically generated data and data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on synthetic data demonstrate that using RSM yields higher detection accuracy compared to using models directly or with bootstrap averaging. Analyses on the ADNI dataset show that RSM can also improve correlation between subject-specific detections in cortical thickness data and non-imaging markers of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score and Cerebrospinal Fluid amyloid-$\\\\beta$ levels. Further reliability studies on the longitudinal ADNI dataset show improvement on detection reliability when RSM is used. \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before Stop words removed\n",
    "data[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59ba8544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predictive models allow specific inference analyzing disease related alterations neuroimaging data given data inference made two levels global identifiying condition presence local detecting condition effect individual measurement extracted data global inference widely used local inference used form specific effect maps rarely used existing models often yield noisy detections composed dispersed isolated islands article propose reconstruction method named rsm improve specific detections predictive modeling approaches particular binary classifiers rsm specifically aims reduce noise due sampling error associated using finite sample examples train classifiers proposed method wrapper type algorithm used different binary classifiers diagnostic manner without information condition presence reconstruction posed maximum posteriori problem prior model whose parameters estimated training data classifier specific fashion experimental evaluation performed synthetically generated data data alzheimer disease neuroimaging initiative adni database results synthetic data demonstrate using rsm yields higher detection accuracy compared using models directly bootstrap averaging analyses adni dataset show rsm also improve correlation specific detections cortical thickness data non imaging markers alzheimer disease mini mental state examination score cerebrospinal fluid amyloid beta levels reliability studies longitudinal adni dataset show improvement detection reliability rsm used'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after Stop Words removed\n",
    "final_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc5a4860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 11122),\n",
       " ('model', 10850),\n",
       " ('based', 8466),\n",
       " ('show', 7912),\n",
       " ('results', 7503),\n",
       " ('using', 7502),\n",
       " ('paper', 7297),\n",
       " ('time', 7175),\n",
       " ('two', 7084),\n",
       " ('also', 6658),\n",
       " ('method', 6586),\n",
       " ('learning', 6347),\n",
       " ('problem', 6341),\n",
       " ('models', 5590),\n",
       " ('network', 5361),\n",
       " ('one', 5289),\n",
       " ('approach', 5237),\n",
       " ('new', 5187),\n",
       " ('algorithm', 5118),\n",
       " ('system', 5113)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize Words in the whole final_data object\n",
    "# find the number of times each word came\n",
    "# take the top 20 most repeated words (can be varied as you desire)\n",
    "words_list = []\n",
    "for sentence in final_data:\n",
    "    words_list.extend(nltk.word_tokenize(sentence))\n",
    "freq_dist = nltk.FreqDist(words_list)\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1e079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9959a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sent = [x.split() for x in final_data]\n",
    "words_sent\n",
    "bigram = Phrases(words_sent, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[words_sent], threshold=100)  \n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# bow = [bigram_phraser[word] for word in words_sent] # creating bigram\n",
    "bow = [trigram_phraser[bigram_phraser[word]] for word in words_sent] # creating trigram and bigrambb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3158979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['predictive',\n",
       "  'models',\n",
       "  'allow',\n",
       "  'specific',\n",
       "  'inference',\n",
       "  'analyzing',\n",
       "  'disease',\n",
       "  'related',\n",
       "  'alterations',\n",
       "  'neuroimaging',\n",
       "  'data',\n",
       "  'given',\n",
       "  'data',\n",
       "  'inference',\n",
       "  'made',\n",
       "  'two',\n",
       "  'levels',\n",
       "  'global',\n",
       "  'identifiying',\n",
       "  'condition',\n",
       "  'presence',\n",
       "  'local',\n",
       "  'detecting',\n",
       "  'condition',\n",
       "  'effect',\n",
       "  'individual',\n",
       "  'measurement',\n",
       "  'extracted',\n",
       "  'data',\n",
       "  'global',\n",
       "  'inference',\n",
       "  'widely_used',\n",
       "  'local',\n",
       "  'inference',\n",
       "  'used',\n",
       "  'form',\n",
       "  'specific',\n",
       "  'effect',\n",
       "  'maps',\n",
       "  'rarely',\n",
       "  'used',\n",
       "  'existing',\n",
       "  'models',\n",
       "  'often',\n",
       "  'yield',\n",
       "  'noisy',\n",
       "  'detections',\n",
       "  'composed',\n",
       "  'dispersed',\n",
       "  'isolated',\n",
       "  'islands',\n",
       "  'article',\n",
       "  'propose',\n",
       "  'reconstruction',\n",
       "  'method',\n",
       "  'named',\n",
       "  'rsm',\n",
       "  'improve',\n",
       "  'specific',\n",
       "  'detections',\n",
       "  'predictive',\n",
       "  'modeling',\n",
       "  'approaches',\n",
       "  'particular',\n",
       "  'binary',\n",
       "  'classifiers',\n",
       "  'rsm',\n",
       "  'specifically',\n",
       "  'aims',\n",
       "  'reduce',\n",
       "  'noise',\n",
       "  'due',\n",
       "  'sampling',\n",
       "  'error',\n",
       "  'associated',\n",
       "  'using',\n",
       "  'finite',\n",
       "  'sample',\n",
       "  'examples',\n",
       "  'train',\n",
       "  'classifiers',\n",
       "  'proposed',\n",
       "  'method',\n",
       "  'wrapper',\n",
       "  'type',\n",
       "  'algorithm',\n",
       "  'used',\n",
       "  'different',\n",
       "  'binary',\n",
       "  'classifiers',\n",
       "  'diagnostic',\n",
       "  'manner',\n",
       "  'without',\n",
       "  'information',\n",
       "  'condition',\n",
       "  'presence',\n",
       "  'reconstruction',\n",
       "  'posed',\n",
       "  'maximum_posteriori',\n",
       "  'problem',\n",
       "  'prior',\n",
       "  'model',\n",
       "  'whose',\n",
       "  'parameters',\n",
       "  'estimated',\n",
       "  'training',\n",
       "  'data',\n",
       "  'classifier',\n",
       "  'specific',\n",
       "  'fashion',\n",
       "  'experimental',\n",
       "  'evaluation',\n",
       "  'performed',\n",
       "  'synthetically',\n",
       "  'generated',\n",
       "  'data',\n",
       "  'data',\n",
       "  'alzheimer_disease',\n",
       "  'neuroimaging_initiative',\n",
       "  'adni',\n",
       "  'database',\n",
       "  'results',\n",
       "  'synthetic',\n",
       "  'data',\n",
       "  'demonstrate',\n",
       "  'using',\n",
       "  'rsm',\n",
       "  'yields',\n",
       "  'higher',\n",
       "  'detection',\n",
       "  'accuracy',\n",
       "  'compared',\n",
       "  'using',\n",
       "  'models',\n",
       "  'directly',\n",
       "  'bootstrap',\n",
       "  'averaging',\n",
       "  'analyses',\n",
       "  'adni',\n",
       "  'dataset',\n",
       "  'show',\n",
       "  'rsm',\n",
       "  'also',\n",
       "  'improve',\n",
       "  'correlation',\n",
       "  'specific',\n",
       "  'detections',\n",
       "  'cortical',\n",
       "  'thickness',\n",
       "  'data',\n",
       "  'non',\n",
       "  'imaging',\n",
       "  'markers',\n",
       "  'alzheimer_disease',\n",
       "  'mini',\n",
       "  'mental',\n",
       "  'state',\n",
       "  'examination',\n",
       "  'score',\n",
       "  'cerebrospinal',\n",
       "  'fluid',\n",
       "  'amyloid',\n",
       "  'beta',\n",
       "  'levels',\n",
       "  'reliability',\n",
       "  'studies',\n",
       "  'longitudinal',\n",
       "  'adni',\n",
       "  'dataset',\n",
       "  'show',\n",
       "  'improvement',\n",
       "  'detection',\n",
       "  'reliability',\n",
       "  'rsm',\n",
       "  'used']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b169a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 12:26:56.103664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-10-09 12:26:56.103777: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-09 12:27:08.188628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-10-09 12:27:08.188720: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-09 12:27:08.200940: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-PB99H1J\n",
      "2022-10-09 12:27:08.201202: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-PB99H1J\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "     -------------------------------------- 12.8/12.8 MB 334.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (60.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\musham malik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9abbcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lemmatisation by using spacy and pos tagging\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # disabing the parse and ner from pipeline it increase speed of pipeline\n",
    "\n",
    "# here 'en' in spacy pipeline means the small version of language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1207781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictive => ADJ\n",
      "models => NOUN\n",
      "allow => VERB\n",
      "specific => ADJ\n",
      "inference => NOUN\n",
      "analyzing => VERB\n",
      "disease => NOUN\n",
      "related => VERB\n",
      "alterations => NOUN\n",
      "neuroimaging => VERB\n",
      "data => NOUN\n",
      "given => VERB\n",
      "data => NOUN\n",
      "inference => NOUN\n",
      "made => VERB\n",
      "two => NUM\n",
      "levels => NOUN\n",
      "global => ADJ\n",
      "identifiying => VERB\n",
      "condition => NOUN\n",
      "presence => NOUN\n",
      "local => ADJ\n",
      "detecting => VERB\n",
      "condition => NOUN\n",
      "effect => NOUN\n",
      "individual => ADJ\n",
      "measurement => NOUN\n",
      "extracted => VERB\n",
      "data => NOUN\n",
      "global => ADJ\n",
      "inference => NOUN\n",
      "widely_used => VERB\n",
      "local => ADJ\n",
      "inference => NOUN\n",
      "used => VERB\n",
      "form => NOUN\n",
      "specific => ADJ\n",
      "effect => NOUN\n",
      "maps => NOUN\n",
      "rarely => ADV\n",
      "used => VERB\n",
      "existing => VERB\n",
      "models => NOUN\n",
      "often => ADV\n",
      "yield => VERB\n",
      "noisy => ADJ\n",
      "detections => NOUN\n",
      "composed => VERB\n",
      "dispersed => VERB\n",
      "isolated => VERB\n",
      "islands => NOUN\n",
      "article => NOUN\n",
      "propose => VERB\n",
      "reconstruction => NOUN\n",
      "method => NOUN\n",
      "named => VERB\n",
      "rsm => NOUN\n",
      "improve => VERB\n",
      "specific => ADJ\n",
      "detections => NOUN\n",
      "predictive => ADJ\n",
      "modeling => NOUN\n",
      "approaches => NOUN\n",
      "particular => ADJ\n",
      "binary => ADJ\n",
      "classifiers => NOUN\n",
      "rsm => NOUN\n",
      "specifically => ADV\n",
      "aims => VERB\n",
      "reduce => VERB\n",
      "noise => NOUN\n",
      "due => ADJ\n",
      "sampling => VERB\n",
      "error => NOUN\n",
      "associated => VERB\n",
      "using => VERB\n",
      "finite => PROPN\n",
      "sample => NOUN\n",
      "examples => PROPN\n",
      "train => NOUN\n",
      "classifiers => NOUN\n",
      "proposed => VERB\n",
      "method => NOUN\n",
      "wrapper => NOUN\n",
      "type => NOUN\n",
      "algorithm => NOUN\n",
      "used => VERB\n",
      "different => ADJ\n",
      "binary => ADJ\n",
      "classifiers => NOUN\n",
      "diagnostic => ADJ\n",
      "manner => NOUN\n",
      "without => ADP\n",
      "information => NOUN\n",
      "condition => NOUN\n",
      "presence => NOUN\n",
      "reconstruction => NOUN\n",
      "posed => VERB\n",
      "maximum_posteriori => NUM\n",
      "problem => NOUN\n",
      "prior => ADJ\n",
      "model => NOUN\n",
      "whose => DET\n",
      "parameters => NOUN\n",
      "estimated => VERB\n",
      "training => NOUN\n",
      "data => NOUN\n",
      "classifier => NOUN\n",
      "specific => ADJ\n",
      "fashion => NOUN\n",
      "experimental => ADJ\n",
      "evaluation => NOUN\n",
      "performed => VERB\n",
      "synthetically => ADV\n",
      "generated => VERB\n",
      "data => NOUN\n",
      "data => NOUN\n",
      "alzheimer_disease => NOUN\n",
      "neuroimaging_initiative => ADJ\n",
      "adni => NOUN\n",
      "database => NOUN\n",
      "results => VERB\n",
      "synthetic => ADJ\n",
      "data => NOUN\n",
      "demonstrate => VERB\n",
      "using => VERB\n",
      "rsm => PROPN\n",
      "yields => NOUN\n",
      "higher => ADJ\n",
      "detection => NOUN\n",
      "accuracy => NOUN\n",
      "compared => VERB\n",
      "using => VERB\n",
      "models => NOUN\n",
      "directly => ADV\n",
      "bootstrap => VERB\n",
      "averaging => VERB\n",
      "analyses => X\n",
      "adni => PROPN\n",
      "dataset => NOUN\n",
      "show => VERB\n",
      "rsm => NOUN\n",
      "also => ADV\n",
      "improve => VERB\n",
      "correlation => NOUN\n",
      "specific => ADJ\n",
      "detections => NOUN\n",
      "cortical => ADJ\n",
      "thickness => NOUN\n",
      "data => PROPN\n",
      "non => PROPN\n",
      "imaging => VERB\n",
      "markers => NOUN\n",
      "alzheimer_disease => X\n",
      "mini => ADJ\n",
      "mental => ADJ\n",
      "state => NOUN\n",
      "examination => NOUN\n",
      "score => NOUN\n",
      "cerebrospinal => ADJ\n",
      "fluid => ADJ\n",
      "amyloid => NOUN\n",
      "beta => NOUN\n",
      "levels => NOUN\n",
      "reliability => NOUN\n",
      "studies => NOUN\n",
      "longitudinal => ADJ\n",
      "adni => PROPN\n",
      "dataset => NOUN\n",
      "show => VERB\n",
      "improvement => NOUN\n",
      "detection => NOUN\n",
      "reliability => NOUN\n",
      "rsm => NOUN\n",
      "used => VERB\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(' '.join(bow[0]))\n",
    "for token in doc:\n",
    "    print(token ,'=>', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf69ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lemmatising the whole corpus so that we can reach to the root words \n",
    "def lemmatization(texts, tags=['NOUN', 'ADJ', 'VERB', 'ADV','PROPN']): # filter noun and adjective\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "010fcd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = lemmatization(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d53efad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predictive',\n",
       " 'model',\n",
       " 'allow',\n",
       " 'specific',\n",
       " 'inference',\n",
       " 'analyze',\n",
       " 'disease',\n",
       " 'relate',\n",
       " 'alteration',\n",
       " 'neuroimage',\n",
       " 'datum',\n",
       " 'give',\n",
       " 'data',\n",
       " 'inference',\n",
       " 'make',\n",
       " 'level',\n",
       " 'global',\n",
       " 'identifiye',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'local',\n",
       " 'detect',\n",
       " 'condition',\n",
       " 'effect',\n",
       " 'individual',\n",
       " 'measurement',\n",
       " 'extract',\n",
       " 'datum',\n",
       " 'global',\n",
       " 'inference',\n",
       " 'widely_use',\n",
       " 'local',\n",
       " 'inference',\n",
       " 'use',\n",
       " 'form',\n",
       " 'specific',\n",
       " 'effect',\n",
       " 'map',\n",
       " 'rarely',\n",
       " 'use',\n",
       " 'exist',\n",
       " 'model',\n",
       " 'often',\n",
       " 'yield',\n",
       " 'noisy',\n",
       " 'detection',\n",
       " 'compose',\n",
       " 'disperse',\n",
       " 'isolate',\n",
       " 'island',\n",
       " 'article',\n",
       " 'propose',\n",
       " 'reconstruction',\n",
       " 'method',\n",
       " 'name',\n",
       " 'rsm',\n",
       " 'improve',\n",
       " 'specific',\n",
       " 'detection',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'approach',\n",
       " 'particular',\n",
       " 'binary',\n",
       " 'classifier',\n",
       " 'rsm',\n",
       " 'specifically',\n",
       " 'aim',\n",
       " 'reduce',\n",
       " 'noise',\n",
       " 'due',\n",
       " 'sample',\n",
       " 'error',\n",
       " 'associate',\n",
       " 'use',\n",
       " 'finite',\n",
       " 'sample',\n",
       " 'examples',\n",
       " 'train',\n",
       " 'classifier',\n",
       " 'propose',\n",
       " 'method',\n",
       " 'wrapper',\n",
       " 'type',\n",
       " 'algorithm',\n",
       " 'use',\n",
       " 'different',\n",
       " 'binary',\n",
       " 'classifier',\n",
       " 'diagnostic',\n",
       " 'manner',\n",
       " 'information',\n",
       " 'condition',\n",
       " 'presence',\n",
       " 'reconstruction',\n",
       " 'pose',\n",
       " 'problem',\n",
       " 'prior',\n",
       " 'model',\n",
       " 'parameter',\n",
       " 'estimate',\n",
       " 'training',\n",
       " 'data',\n",
       " 'classifier',\n",
       " 'specific',\n",
       " 'fashion',\n",
       " 'experimental',\n",
       " 'evaluation',\n",
       " 'perform',\n",
       " 'synthetically',\n",
       " 'generate',\n",
       " 'data',\n",
       " 'datum',\n",
       " 'alzheimer_disease',\n",
       " 'neuroimaging_initiative',\n",
       " 'adni',\n",
       " 'database',\n",
       " 'result',\n",
       " 'synthetic',\n",
       " 'datum',\n",
       " 'demonstrate',\n",
       " 'use',\n",
       " 'rsm',\n",
       " 'yield',\n",
       " 'high',\n",
       " 'detection',\n",
       " 'accuracy',\n",
       " 'compare',\n",
       " 'use',\n",
       " 'model',\n",
       " 'directly',\n",
       " 'bootstrap',\n",
       " 'average',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'rsm',\n",
       " 'also',\n",
       " 'improve',\n",
       " 'correlation',\n",
       " 'specific',\n",
       " 'detection',\n",
       " 'cortical',\n",
       " 'thickness',\n",
       " 'data',\n",
       " 'non',\n",
       " 'image',\n",
       " 'marker',\n",
       " 'mini',\n",
       " 'mental',\n",
       " 'state',\n",
       " 'examination',\n",
       " 'score',\n",
       " 'cerebrospinal',\n",
       " 'fluid',\n",
       " 'amyloid',\n",
       " 'beta',\n",
       " 'level',\n",
       " 'reliability',\n",
       " 'study',\n",
       " 'longitudinal',\n",
       " 'adni',\n",
       " 'dataset',\n",
       " 'show',\n",
       " 'improvement',\n",
       " 'detection',\n",
       " 'reliability',\n",
       " 'rsm',\n",
       " 'use']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebdd77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<46384 unique tokens: ['accuracy', 'adni', 'aim', 'algorithm', 'allow']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "id2word = corpora.Dictionary(bag_of_words)\n",
    "print(id2word)\n",
    "# here keys are some numbers and values are our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9313df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now lets create a encoded bag of words \n",
    "corpus_matrix = [id2word.doc2bow(sent) for sent in bag_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a9723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 3),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 4),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 3),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 4),\n",
       " (25, 1),\n",
       " (26, 2),\n",
       " (27, 4),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 5),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 2),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 2),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (56, 2),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 4),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 2),\n",
       " (64, 2),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 2),\n",
       " (73, 1),\n",
       " (74, 4),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 2),\n",
       " (88, 2),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 2),\n",
       " (92, 1),\n",
       " (93, 2),\n",
       " (94, 1),\n",
       " (95, 1),\n",
       " (96, 2),\n",
       " (97, 1),\n",
       " (98, 5),\n",
       " (99, 2),\n",
       " (100, 1),\n",
       " (101, 2),\n",
       " (102, 5),\n",
       " (103, 1),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 7),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aee88c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "import gensim\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LDA(corpus=corpus_matrix,id2word=id2word,\n",
    "                                    num_topics=10, \n",
    "                                    random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=5,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84e84e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.132*\"problem\" + 0.069*\"algorithm\" + 0.039*\"optimization\" + 0.035*\"solve\" + 0.023*\"solution\" + 0.022*\"gradient\" + 0.021*\"convex\" + 0.018*\"matrix\" + 0.017*\"sparse\" + 0.016*\"inverse\"'),\n",
       " (1,\n",
       "  '0.024*\"datum\" + 0.024*\"method\" + 0.023*\"model\" + 0.021*\"use\" + 0.020*\"propose\" + 0.019*\"base\" + 0.019*\"approach\" + 0.011*\"paper\" + 0.010*\"performance\" + 0.009*\"result\"'),\n",
       " (2,\n",
       "  '0.027*\"mass\" + 0.015*\"star\" + 0.013*\"cluster\" + 0.013*\"galaxy\" + 0.013*\"electron\" + 0.012*\"band\" + 0.012*\"gas\" + 0.012*\"find\" + 0.009*\"observation\" + 0.009*\"bias\"'),\n",
       " (3,\n",
       "  '0.014*\"information\" + 0.014*\"user\" + 0.013*\"system\" + 0.011*\"robot\" + 0.011*\"object\" + 0.010*\"detection\" + 0.010*\"image\" + 0.009*\"human\" + 0.008*\"target\" + 0.007*\"code\"'),\n",
       " (4,\n",
       "  '0.068*\"learn\" + 0.048*\"learning\" + 0.046*\"deep\" + 0.038*\"task\" + 0.038*\"training\" + 0.033*\"train\" + 0.029*\"image\" + 0.027*\"neural_network\" + 0.026*\"network\" + 0.019*\"dataset\"'),\n",
       " (5,\n",
       "  '0.014*\"space\" + 0.012*\"prove\" + 0.012*\"show\" + 0.012*\"result\" + 0.011*\"give\" + 0.011*\"function\" + 0.010*\"group\" + 0.009*\"theory\" + 0.009*\"also\" + 0.008*\"equation\"'),\n",
       " (6,\n",
       "  '0.024*\"model\" + 0.018*\"time\" + 0.013*\"show\" + 0.012*\"distribution\" + 0.011*\"number\" + 0.011*\"parameter\" + 0.011*\"sample\" + 0.010*\"graph\" + 0.010*\"estimate\" + 0.009*\"result\"'),\n",
       " (7,\n",
       "  '0.049*\"network\" + 0.018*\"control\" + 0.015*\"dynamic\" + 0.015*\"system\" + 0.013*\"study\" + 0.010*\"change\" + 0.008*\"agent\" + 0.008*\"node\" + 0.008*\"individual\" + 0.008*\"behavior\"'),\n",
       " (8,\n",
       "  '0.017*\"system\" + 0.016*\"state\" + 0.015*\"phase\" + 0.012*\"temperature\" + 0.012*\"quantum\" + 0.010*\"particle\" + 0.010*\"energy\" + 0.009*\"transition\" + 0.009*\"wave\" + 0.008*\"structure\"'),\n",
       " (9,\n",
       "  '0.015*\"high\" + 0.013*\"flow\" + 0.012*\"scale\" + 0.011*\"time\" + 0.011*\"frequency\" + 0.010*\"low\" + 0.009*\"use\" + 0.009*\"power\" + 0.009*\"region\" + 0.008*\"energy\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lda_model.print_topics()\n",
    "# doc_lda = lda_model[corpus_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc2a30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.697054539247882\n",
      "\n",
      "Coherence Score:  0.486303061695405\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus_matrix))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=bag_of_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1733f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"The Data Analyst (Predictive Analytics Analyst) will use advanced data modeling, predictive modeling, and analytical techniques to interpret key findings from company data and leverage these insights into initiatives that will support business outcomes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2eb837bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Data Analyst (Predictive Analytics Analyst) will use advanced data modeling, predictive modeling, and analytical techniques to interpret key findings from company data and leverage these insights into initiatives that will support business outcomes.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f82b88f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The Data Analyst (Predictive Analytics Analyst...\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = pd.Series(sample_text)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd334b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the data analyst predictive analytics analyst ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets do some preproc3essing and move on \n",
    "# here we are going to use regex for data cleaning\n",
    "import re\n",
    "clean_data_processed = clean_data.apply(lambda x: re.sub(r'[^a-zA-Z/s]+',' ',x).lower())\n",
    "clean_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f6897ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_final = clean_data_processed.apply(lambda x:' '.join([word for word in word_tokenize(x) if word not in stop_words and len(word)> 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f8a6d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data analyst predictive analytics analyst advanced data modeling predictive modeling analytical techniques interpret key findings company data leverage insights initiatives support business outcomes'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4973173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []\n",
    "for sentence in clean_data_final:\n",
    "    words_list.extend(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ab2068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sent = [x.split() for x in clean_data_final]\n",
    "words_sent\n",
    "bigram = Phrases(words_sent, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[words_sent], threshold=100)  \n",
    "\n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# bow = [bigram_phraser[word] for word in words_sent] # creating bigram\n",
    "bow = [trigram_phraser[bigram_phraser[word]] for word in words_sent] # creating trigram and bigrambb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbd02d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['data',\n",
       "  'analyst',\n",
       "  'predictive',\n",
       "  'analytics',\n",
       "  'analyst',\n",
       "  'advanced',\n",
       "  'data',\n",
       "  'modeling',\n",
       "  'predictive',\n",
       "  'modeling',\n",
       "  'analytical',\n",
       "  'techniques',\n",
       "  'interpret',\n",
       "  'key',\n",
       "  'findings',\n",
       "  'company',\n",
       "  'data',\n",
       "  'leverage',\n",
       "  'insights',\n",
       "  'initiatives',\n",
       "  'support',\n",
       "  'business',\n",
       "  'outcomes']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b14d1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data => PROPN\n",
      "analyst => NOUN\n",
      "predictive => ADJ\n",
      "analytics => NOUN\n",
      "analyst => NOUN\n",
      "advanced => ADJ\n",
      "data => NOUN\n",
      "modeling => VERB\n",
      "predictive => ADJ\n",
      "modeling => NOUN\n",
      "analytical => ADJ\n",
      "techniques => NOUN\n",
      "interpret => VERB\n",
      "key => ADJ\n",
      "findings => NOUN\n",
      "company => NOUN\n",
      "data => NOUN\n",
      "leverage => NOUN\n",
      "insights => NOUN\n",
      "initiatives => NOUN\n",
      "support => NOUN\n",
      "business => NOUN\n",
      "outcomes => NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(' '.join(bow[0]))\n",
    "for token in doc:\n",
    "    print(token ,'=>', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b581080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = lemmatization(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ec06b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'analyst',\n",
       " 'predictive',\n",
       " 'analytic',\n",
       " 'analyst',\n",
       " 'advanced',\n",
       " 'datum',\n",
       " 'model',\n",
       " 'predictive',\n",
       " 'modeling',\n",
       " 'analytical',\n",
       " 'technique',\n",
       " 'interpret',\n",
       " 'key',\n",
       " 'finding',\n",
       " 'company',\n",
       " 'datum',\n",
       " 'leverage',\n",
       " 'insight',\n",
       " 'initiative',\n",
       " 'support',\n",
       " 'business',\n",
       " 'outcome']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d96200d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<20 unique tokens: ['advanced', 'analyst', 'analytic', 'analytical', 'business']...>\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(bag_of_words)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ad57b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now lets create a encoded bag of words \n",
    "corpus_matrix = [id2word.doc2bow(sent) for sent in bag_of_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e1f9e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 2),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 2),\n",
       " (18, 1),\n",
       " (19, 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24b42be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = lda_model[corpus_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "516f879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0, 0.01341788), (1, 0.2629954), (3, 0.09474433), (4, 0.12797752), (5, 0.16232151), (6, 0.24403106), (7, 0.037070084), (8, 0.020611294), (9, 0.027068192)], [(0, [1]), (1, []), (2, [1, 3, 5]), (3, [6, 1, 0]), (4, [6, 1, 3, 5, 9]), (5, [1, 5, 6, 3]), (6, [3]), (7, [4]), (8, [7]), (9, [6, 1, 7, 3]), (10, [1, 6]), (11, [5, 3, 1, 7]), (12, [5, 7]), (13, [6, 1, 9]), (14, [5]), (15, [6, 1]), (16, [6]), (17, []), (18, [4]), (19, [1, 6, 9])], [(0, [(1, 0.9999044)]), (1, []), (2, [(1, 0.6985662), (3, 0.22685792), (5, 0.06835199)]), (3, [(0, 0.047766995), (1, 0.29956356), (6, 0.65103734)]), (4, [(1, 0.36870322), (3, 0.07847203), (5, 0.05728691), (6, 0.47571325), (9, 0.014982937)]), (5, [(1, 0.33856392), (3, 0.035140496), (5, 0.32854527), (6, 0.28103775)]), (6, [(3, 0.9890747)]), (7, [(4, 1.9960675)]), (8, [(7, 0.050808832)]), (9, [(1, 0.3020469), (3, 0.025637608), (6, 0.5857488), (7, 0.076416366)]), (10, [(1, 0.966022), (6, 0.033837266)]), (11, [(1, 0.06200855), (3, 0.34619772), (5, 0.5666771), (7, 0.024787398)]), (12, [(5, 0.92677385), (7, 0.062644206)]), (13, [(1, 0.04514472), (6, 0.9370056), (9, 0.017209651)]), (14, [(5, 0.9955233)]), (15, [(1, 0.12845282), (6, 0.86832106)]), (16, [(6, 0.99844676)]), (17, []), (18, [(4, 0.999938)]), (19, [(1, 0.9099606), (6, 0.06792596), (9, 0.021568501)])])\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for topic in vector:\n",
    "    print(topic)\n",
    "    result.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a581740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(result,columns=['Topic 1','Topic 2',\n",
    "                                       'Topic 3'])\n",
    "\n",
    "results.to_csv('test_results.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a811674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
